[{"categories":["linux"],"content":"IDLE调度器类 1/* 2* Generic entry points for the idle threads and 3* implementation of the idle task scheduling class. 4* 5* (NOTE: these are not related to SCHED_IDLE batch scheduled 6* tasks which are handled in sched/fair.c ) 7*/ 8#include \u0026#34;sched.h\u0026#34;9 10#include \u0026lt;trace/events/power.h\u0026gt;11 12/* Linker adds these: start and end of __cpuidle functions */ 13extern char __cpuidle_text_start[], __cpuidle_text_end[]; 14 15/** 16* sched_idle_set_state - Record idle state for the current CPU. 17* @idle_state: State to record. 18*/ 19void sched_idle_set_state(struct cpuidle_state *idle_state) 20{ 21\tidle_set_state(this_rq(), idle_state); 22} 23 24static int __read_mostly cpu_idle_force_poll; 25 26void cpu_idle_poll_ctrl(bool enable) 27{ 28\tif (enable) { 29\tcpu_idle_force_poll++; 30\t} else { 31\tcpu_idle_force_poll--; 32\tWARN_ON_ONCE(cpu_idle_force_poll \u0026lt; 0); 33\t} 34} 35 36#ifdef CONFIG_GENERIC_IDLE_POLL_SETUP 37static int __init cpu_idle_poll_setup(char *__unused) 38{ 39\tcpu_idle_force_poll = 1; 40 41\treturn 1; 42} 43__setup(\u0026#34;nohlt\u0026#34;, cpu_idle_poll_setup); 44 45static int __init cpu_idle_nopoll_setup(char *__unused) 46{ 47\tcpu_idle_force_poll = 0; 48 49\treturn 1; 50} 51__setup(\u0026#34;hlt\u0026#34;, cpu_idle_nopoll_setup); 52#endif 53 54static noinline int __cpuidle cpu_idle_poll(void) 55{ 56\trcu_idle_enter(); 57\ttrace_cpu_idle_rcuidle(0, smp_processor_id()); 58\tlocal_irq_enable(); 59\tstop_critical_timings(); 60 61\twhile (!tif_need_resched() \u0026amp;\u0026amp; 62\t(cpu_idle_force_poll || tick_check_broadcast_expired())) 63\tcpu_relax(); 64\tstart_critical_timings(); 65\ttrace_cpu_idle_rcuidle(PWR_EVENT_EXIT, smp_processor_id()); 66\trcu_idle_exit(); 67 68\treturn 1; 69} 70 71/* Weak implementations for optional arch specific functions */ 72void __weak arch_cpu_idle_prepare(void) { } 73void __weak arch_cpu_idle_enter(void) { } 74void __weak arch_cpu_idle_exit(void) { } 75void __weak arch_cpu_idle_dead(void) { } 76void __weak arch_cpu_idle(void) 77{ 78\tcpu_idle_force_poll = 1; 79\tlocal_irq_enable(); 80} 81 82/** 83* default_idle_call - Default CPU idle routine. 84* 85* To use when the cpuidle framework cannot be used. 86*/ 87void __cpuidle default_idle_call(void) 88{ 89\tif (current_clr_polling_and_test()) { 90\tlocal_irq_enable(); 91\t} else { 92\tstop_critical_timings(); 93\tarch_cpu_idle(); 94\tstart_critical_timings(); 95\t} 96} 97 98static int call_cpuidle(struct cpuidle_driver *drv, struct cpuidle_device *dev, 99\tint next_state) 100{ 101\t/* 102* The idle task must be scheduled, it is pointless to go to idle, just 103* update no idle residency and return. 104*/ 105\tif (current_clr_polling_and_test()) { 106\tdev-\u0026gt;last_residency = 0; 107\tlocal_irq_enable(); 108\treturn -EBUSY; 109\t} 110 111\t/* 112* Enter the idle state previously returned by the governor decision. 113* This function will block until an interrupt occurs and will take 114* care of re-enabling the local interrupts 115*/ 116\treturn cpuidle_enter(drv, dev, next_state); 117} 118 119/** 120* cpuidle_idle_call - the main idle function 121* 122* NOTE: no locks or semaphores should be used here 123* 124* On archs that support TIF_POLLING_NRFLAG, is called with polling 125* set, and it returns with polling set. If it ever stops polling, it 126* must clear the polling bit. 127*/ 128static void cpuidle_idle_call(void) 129{ 130\tstruct cpuidle_device *dev = cpuidle_get_device(); 131\tstruct cpuidle_driver *drv = cpuidle_get_cpu_driver(dev); 132\tint next_state, entered_state; 133 134\t/* 135* Check if the idle task must be rescheduled. If it is the 136* case, exit the function after re-enabling the local irq. 137*/ 138\tif (need_resched()) { 139\tlocal_irq_enable(); 140\treturn; 141\t} 142 143\t/* 144* The RCU framework needs to be told that we are entering an idle 145* section, so no more rcu read side critical sections and one more 146* step to the grace period 147*/ 148 149\tif (cpuidle_not_available(drv, dev)) { 150\ttick_nohz_idle_stop_tick(); 151\trcu_idle_enter(); 152 153\tdefault_idle_call(); 154\tgoto exit_idle; 155\t} 156 157\t/* 158* Suspend-to-idle (\u0026#34;s2idle\u0026#34;) is a system state in which all user space 159* has been frozen, all I/O devices have been suspended and the only 160* activity happens here and in iterrupts (if any). In that case bypass 161* the cpuidle governor and go stratight for the deepest idle state 162* available. Possibly also suspend the local tick and the entire 163* timekeeping to prevent timer interrupts from kicking us out of idle 164* until a proper wakeup interrupt happens. 165*/ 166 167\tif (idle_should_enter_s2idle() || dev-\u0026gt;use_deepest_state) { 168\tif (idle_should_enter_s2idle()) { 169\trcu_idle_enter(); 170 171\tentered_state = cpuidle_enter_s2idle(drv, dev); 172\tif (entered_state \u0026gt; 0) { 173\tlocal_irq_enable(); 174\tgoto exit_idle; 175\t} 176 177\trcu_idle_exit(); 178\t} 179 180\ttick_nohz_idle_stop_tick(); 181\trcu_idle_enter(); 182 183\tnext_state = cpuidle_find_deepest_state(drv, dev); 184\tcall_cpuidle(drv, dev, next_state); 185\t} else { 186\tbool stop_tick = true; 187 188\t/* 189* Ask the cpuidle framework to choose a convenient idle state. 190*/ 191\tnext_state = cpuidle_select(drv, dev, \u0026amp;stop_tick); 192 193\tif (stop_tick || tick_nohz_tick_stopped()) 194\ttick_nohz_idle_stop_tick(); 195\telse 196\ttick_nohz_idle_retain_tick(); 197 198\trcu_idle_enter(); 199 200\tentered_state = call_cpuidle(drv, dev, next_state); 201\t/* 202* Give the governor an opportunity to reflect on the outcome 203*/ 204\tcpuidle_reflect(dev, entered_state); 205\t} 206 207exit_idle: 208\t__current_set_polling(); 209 210\t/* 211* It is up to the idle functions to reenable local interrupts 212*/ 213\tif (WARN_ON_ONCE(irqs_disabled())) 214\tlocal_irq_enable(); 215 216\trcu_idle_exit(); 217} 218 219/* 220* Generic idle loop implementation 221* 222* Called with polling cleared. 223*/ 224static void do_idle(void) 225{ 226\tint cpu = smp_processor_id(); 227\t/* 228* If the arch has a polling bit, we maintain an invariant: 229* 230* Our polling bit is clear if we\u0026#39;re not scheduled (i.e. if rq-\u0026gt;curr != 231* rq-\u0026gt;idle). This means that, if rq-\u0026gt;idle has the polling bit set, 232* then setting need_resched is guaranteed to cause the CPU to 233* reschedule. 234*/ 235 236\t__current_set_polling(); 237\ttick_nohz_idle_enter(); 238 239\twhile (!need_resched()) { 240\tcheck_pgt_cache(); 241\trmb(); 242 243\tlocal_irq_disable(); 244 245\tif (cpu_is_offline(cpu)) { 246\ttick_nohz_idle_stop_tick(); 247\tcpuhp_report_idle_dead(); 248\tarch_cpu_idle_dead(); 249\t} 250 251\tarch_cpu_idle_enter(); 252 253\t/* 254* In poll mode we reenable interrupts and spin. Also if we 255* detected in the wakeup from idle path that the tick 256* broadcast device expired for us, we don\u0026#39;t want to go deep 257* idle as we know that the IPI is going to arrive right away. 258*/ 259\tif (cpu_idle_force_poll || tick_check_broadcast_expired()) { 260\ttick_nohz_idle_restart_tick(); 261\tcpu_idle_poll(); 262\t} else { 263\tcpuidle_idle_call(); 264\t} 265\tarch_cpu_idle_exit(); 266\t} 267 268\t/* 269* Since we fell out of the loop above, we know TIF_NEED_RESCHED must 270* be set, propagate it into PREEMPT_NEED_RESCHED. 271* 272* This is required because for polling idle loops we will not have had 273* an IPI to fold the state for us. 274*/ 275\tpreempt_set_need_resched(); 276\ttick_nohz_idle_exit(); 277\t__current_clr_polling(); 278 279\t/* 280* We promise to call sched_ttwu_pending() and reschedule if 281* need_resched() is set while polling is set. That means that clearing 282* polling needs to be visible before doing these things. 283*/ 284\tsmp_mb__after_atomic(); 285 286\tsched_ttwu_pending(); 287\tschedule_idle(); 288 289\tif (unlikely(klp_patch_pending(current))) 290\tklp_update_patch_state(current); 291} 292 293bool cpu_in_idle(unsigned long pc) 294{ 295\treturn pc \u0026gt;= (unsigned long)__cpuidle_text_start \u0026amp;\u0026amp; 296\tpc \u0026lt; (unsigned long)__cpuidle_text_end; 297} 298 299struct idle_timer { 300\tstruct hrtimer timer; 301\tint done; 302}; 303 304static enum hrtimer_restart idle_inject_timer_fn(struct hrtimer *timer) 305{ 306\tstruct idle_timer *it = container_of(timer, struct idle_timer, timer); 307 308\tWRITE_ONCE(it-\u0026gt;done, 1); 309\tset_tsk_need_resched(current); 310 311\treturn HRTIMER_NORESTART; 312} 313 314void play_idle(unsigned long duration_ms) 315{ 316\tstruct idle_timer it; 317 318\t/* 319* Only FIFO tasks can disable the tick since they don\u0026#39;t need the forced 320* preemption. 321*/ 322\tWARN_ON_ONCE(current-\u0026gt;policy != SCHED_FIFO); 323\tWARN_ON_ONCE(current-\u0026gt;nr_cpus_allowed != 1); 324\tWARN_ON_ONCE(!(current-\u0026gt;flags \u0026amp; PF_KTHREAD)); 325\tWARN_ON_ONCE(!(current-\u0026gt;flags \u0026amp; PF_NO_SETAFFINITY)); 326\tWARN_ON_ONCE(!duration_ms); 327 328\trcu_sleep_check(); 329\tpreempt_disable(); 330\tcurrent-\u0026gt;flags |= PF_IDLE; 331\tcpuidle_use_deepest_state(true); 332 333\tit.done = 0; 334\thrtimer_init_on_stack(\u0026amp;it.timer, CLOCK_MONOTONIC, HRTIMER_MODE_REL); 335\tit.timer.function = idle_inject_timer_fn; 336\thrtimer_start(\u0026amp;it.timer, ms_to_ktime(duration_ms), HRTIMER_MODE_REL_PINNED); 337 338\twhile (!READ_ONCE(it.done)) 339\tdo_idle(); 340 341\tcpuidle_use_deepest_state(false); 342\tcurrent-\u0026gt;flags \u0026amp;= ~PF_IDLE; 343 344\tpreempt_fold_need_resched(); 345\tpreempt_enable(); 346} 347EXPORT_SYMBOL_GPL(play_idle); 348 349void cpu_startup_entry(enum cpuhp_state state) 350{ 351\t/* 352* This #ifdef needs to die, but it\u0026#39;s too late in the cycle to 353* make this generic (ARM and SH have never invoked the canary 354* init for the non boot CPUs!). Will be fixed in 3.11 355*/ 356#ifdef CONFIG_X86 357\t/* 358* If we\u0026#39;re the non-boot CPU, nothing set the stack canary up 359* for us. The boot CPU already has it initialized but no harm 360* in doing it again. This is a good place for updating it, as 361* we wont ever return from this function (so the invalid 362* canaries already on the stack wont ever trigger). 363*/ 364\tboot_init_stack_canary(); 365#endif 366\tarch_cpu_idle_prepare(); 367\tcpuhp_online_idle(state); 368\twhile (1) 369\tdo_idle(); 370} 371 372/* 373* idle-task scheduling class. 374*/ 375 376#ifdef CONFIG_SMP 377static int 378select_task_rq_idle(struct task_struct *p, int cpu, int sd_flag, int flags) 379{ 380\treturn task_cpu(p); /* IDLE tasks as never migrated */ 381} 382#endif 383 384/* 385* Idle tasks are unconditionally rescheduled: 386*/ 387static void check_preempt_curr_idle(struct rq *rq, struct task_struct *p, int flags) 388{ 389\tresched_curr(rq); 390} 391 392static struct task_struct * 393pick_next_task_idle(struct rq *rq, struct task_struct *prev, struct rq_flags *rf) 394{ 395\tput_prev_task(rq, prev); 396\tupdate_idle_core(rq); 397\tschedstat_inc(rq-\u0026gt;sched_goidle); 398 399\treturn rq-\u0026gt;idle; 400} 401 402/* 403* It is not legal to sleep in the idle task - print a warning 404* message if some code attempts to do it: 405*/ 406static void 407dequeue_task_idle(struct rq *rq, struct task_struct *p, int flags) 408{ 409\traw_spin_unlock_irq(\u0026amp;rq-\u0026gt;lock); 410\tprintk(KERN_ERR \u0026#34;bad: scheduling from the idle thread!\\n\u0026#34;); 411\tdump_stack(); 412\traw_spin_lock_irq(\u0026amp;rq-\u0026gt;lock); 413} 414 415static void put_prev_task_idle(struct rq *rq, struct task_struct *prev) 416{ 417} 418 419/* 420* scheduler tick hitting a task of our scheduling class. 421* 422* NOTE: This function can be called remotely by the tick offload that 423* goes along full dynticks. Therefore no local assumption can be made 424* and everything must be accessed through the @rq and @curr passed in 425* parameters. 426*/ 427static void task_tick_idle(struct rq *rq, struct task_struct *curr, int queued) 428{ 429} 430 431static void set_curr_task_idle(struct rq *rq) 432{ 433} 434 435static void switched_to_idle(struct rq *rq, struct task_struct *p) 436{ 437\tBUG(); 438} 439 440static void 441prio_changed_idle(struct rq *rq, struct task_struct *p, int oldprio) 442{ 443\tBUG(); 444} 445 446static unsigned int get_rr_interval_idle(struct rq *rq, struct task_struct *task) 447{ 448\treturn 0; 449} 450 451static void update_curr_idle(struct rq *rq) 452{ 453} 454 455/* 456* Simple, special scheduling class for the per-CPU idle tasks: 457*/ 458const struct sched_class idle_sched_class = { 459\t/* .next is NULL */ 460\t/* no enqueue/yield_task for idle tasks */ 461 462\t/* dequeue is not valid, we print a debug message there: */ 463\t.dequeue_task\t= dequeue_task_idle, 464 465\t.check_preempt_curr\t= check_preempt_curr_idle, 466 467\t.pick_next_task\t= pick_next_task_idle, 468\t.put_prev_task\t= put_prev_task_idle, 469 470#ifdef CONFIG_SMP 471\t.select_task_rq\t= select_task_rq_idle, 472\t.set_cpus_allowed\t= set_cpus_allowed_common, 473#endif 474 475\t.set_curr_task = set_curr_task_idle, 476\t.task_tick\t= task_tick_idle, 477 478\t.get_rr_interval\t= get_rr_interval_idle, 479 480\t.prio_changed\t= prio_changed_idle, 481\t.switched_to\t= switched_to_idle, 482\t.update_curr\t= update_curr_idle, 483}; ","date":"Oct 28, 2021","img":"","permalink":"https://mengdemao.github.io/posts/idle/","series":null,"tags":["kernel"],"title":"Idle"},{"categories":["linux"],"content":"1// SPDX-License-Identifier: GPL-2.0 2/* 3* Completely Fair Scheduling (CFS) Class (SCHED_NORMAL/SCHED_BATCH) 4* 5* Copyright (C) 2007 Red Hat, Inc., Ingo Molnar \u0026lt;mingo@redhat.com\u0026gt; 6* 7* Interactivity improvements by Mike Galbraith 8* (C) 2007 Mike Galbraith \u0026lt;efault@gmx.de\u0026gt; 9* 10* Various enhancements by Dmitry Adamushko. 11* (C) 2007 Dmitry Adamushko \u0026lt;dmitry.adamushko@gmail.com\u0026gt; 12* 13* Group scheduling enhancements by Srivatsa Vaddagiri 14* Copyright IBM Corporation, 2007 15* Author: Srivatsa Vaddagiri \u0026lt;vatsa@linux.vnet.ibm.com\u0026gt; 16* 17* Scaled math optimizations by Thomas Gleixner 18* Copyright (C) 2007, Thomas Gleixner \u0026lt;tglx@linutronix.de\u0026gt; 19* 20* Adaptive scheduling granularity, math enhancements by Peter Zijlstra 21* Copyright (C) 2007 Red Hat, Inc., Peter Zijlstra 22*/ 23#include \u0026#34;sched.h\u0026#34;24 25#include \u0026lt;trace/events/sched.h\u0026gt;26 27/* 28* Targeted preemption latency for CPU-bound tasks: 29* 30* NOTE: this latency value is not the same as the concept of 31* \u0026#39;timeslice length\u0026#39; - timeslices in CFS are of variable length 32* and have no persistent notion like in traditional, time-slice 33* based scheduling concepts. 34* 35* (to see the precise effective timeslice length of your workload, 36* run vmstat and monitor the context-switches (cs) field) 37* 38* (default: 6ms * (1 + ilog(ncpus)), units: nanoseconds) 39*/ 40unsigned int sysctl_sched_latency\t= 6000000ULL; 41unsigned int normalized_sysctl_sched_latency\t= 6000000ULL; 42 43/* 44* The initial- and re-scaling of tunables is configurable 45* 46* Options are: 47* 48* SCHED_TUNABLESCALING_NONE - unscaled, always *1 49* SCHED_TUNABLESCALING_LOG - scaled logarithmical, *1+ilog(ncpus) 50* SCHED_TUNABLESCALING_LINEAR - scaled linear, *ncpus 51* 52* (default SCHED_TUNABLESCALING_LOG = *(1+ilog(ncpus)) 53*/ 54enum sched_tunable_scaling sysctl_sched_tunable_scaling = SCHED_TUNABLESCALING_LOG; 55 56/* 57* Minimal preemption granularity for CPU-bound tasks: 58* 59* (default: 0.75 msec * (1 + ilog(ncpus)), units: nanoseconds) 60*/ 61unsigned int sysctl_sched_min_granularity\t= 750000ULL; 62unsigned int normalized_sysctl_sched_min_granularity\t= 750000ULL; 63 64/* 65* This value is kept at sysctl_sched_latency/sysctl_sched_min_granularity 66*/ 67static unsigned int sched_nr_latency = 8; 68 69/* 70* After fork, child runs first. If set to 0 (default) then 71* parent will (try to) run first. 72*/ 73unsigned int sysctl_sched_child_runs_first __read_mostly; 74 75/* 76* SCHED_OTHER wake-up granularity. 77* 78* This option delays the preemption effects of decoupled workloads 79* and reduces their over-scheduling. Synchronous workloads will still 80* have immediate wakeup/sleep latencies. 81* 82* (default: 1 msec * (1 + ilog(ncpus)), units: nanoseconds) 83*/ 84unsigned int sysctl_sched_wakeup_granularity\t= 1000000UL; 85unsigned int normalized_sysctl_sched_wakeup_granularity\t= 1000000UL; 86 87const_debug unsigned int sysctl_sched_migration_cost\t= 500000UL; 88 89#ifdef CONFIG_SMP 90/* 91* For asym packing, by default the lower numbered CPU has higher priority. 92*/ 93int __weak arch_asym_cpu_priority(int cpu) 94{ 95\treturn -cpu; 96} 97#endif 98 99#ifdef CONFIG_CFS_BANDWIDTH 100/* 101* Amount of runtime to allocate from global (tg) to local (per-cfs_rq) pool 102* each time a cfs_rq requests quota. 103* 104* Note: in the case that the slice exceeds the runtime remaining (either due 105* to consumption or the quota being specified to be smaller than the slice) 106* we will always only issue the remaining available time. 107* 108* (default: 5 msec, units: microseconds) 109*/ 110unsigned int sysctl_sched_cfs_bandwidth_slice\t= 5000UL; 111#endif 112 113/* 114* The margin used when comparing utilization with CPU capacity: 115* util * margin \u0026lt; capacity * 1024 116* 117* (default: ~20%) 118*/ 119unsigned int capacity_margin\t= 1280; 120 121static inline void update_load_add(struct load_weight *lw, unsigned long inc) 122{ 123\tlw-\u0026gt;weight += inc; 124\tlw-\u0026gt;inv_weight = 0; 125} 126 127static inline void update_load_sub(struct load_weight *lw, unsigned long dec) 128{ 129\tlw-\u0026gt;weight -= dec; 130\tlw-\u0026gt;inv_weight = 0; 131} 132 133static inline void update_load_set(struct load_weight *lw, unsigned long w) 134{ 135\tlw-\u0026gt;weight = w; 136\tlw-\u0026gt;inv_weight = 0; 137} 138 139/* 140* Increase the granularity value when there are more CPUs, 141* because with more CPUs the \u0026#39;effective latency\u0026#39; as visible 142* to users decreases. But the relationship is not linear, 143* so pick a second-best guess by going with the log2 of the 144* number of CPUs. 145* 146* This idea comes from the SD scheduler of Con Kolivas: 147*/ 148static unsigned int get_update_sysctl_factor(void) 149{ 150\tunsigned int cpus = min_t(unsigned int, num_online_cpus(), 8); 151\tunsigned int factor; 152 153\tswitch (sysctl_sched_tunable_scaling) { 154\tcase SCHED_TUNABLESCALING_NONE: 155\tfactor = 1; 156\tbreak; 157\tcase SCHED_TUNABLESCALING_LINEAR: 158\tfactor = cpus; 159\tbreak; 160\tcase SCHED_TUNABLESCALING_LOG: 161\tdefault: 162\tfactor = 1 + ilog2(cpus); 163\tbreak; 164\t} 165 166\treturn factor; 167} 168 169static void update_sysctl(void) 170{ 171\tunsigned int factor = get_update_sysctl_factor(); 172 173#define SET_SYSCTL(name) \\ 174(sysctl_##name = (factor) * normalized_sysctl_##name) 175\tSET_SYSCTL(sched_min_granularity); 176\tSET_SYSCTL(sched_latency); 177\tSET_SYSCTL(sched_wakeup_granularity); 178#undef SET_SYSCTL 179} 180 181void sched_init_granularity(void) 182{ 183\tupdate_sysctl(); 184} 185 186#define WMULT_CONST\t(~0U) 187#define WMULT_SHIFT\t32 188 189static void __update_inv_weight(struct load_weight *lw) 190{ 191\tunsigned long w; 192 193\tif (likely(lw-\u0026gt;inv_weight)) 194\treturn; 195 196\tw = scale_load_down(lw-\u0026gt;weight); 197 198\tif (BITS_PER_LONG \u0026gt; 32 \u0026amp;\u0026amp; unlikely(w \u0026gt;= WMULT_CONST)) 199\tlw-\u0026gt;inv_weight = 1; 200\telse if (unlikely(!w)) 201\tlw-\u0026gt;inv_weight = WMULT_CONST; 202\telse 203\tlw-\u0026gt;inv_weight = WMULT_CONST / w; 204} 205 206/* 207* delta_exec * weight / lw.weight 208* OR 209* (delta_exec * (weight * lw-\u0026gt;inv_weight)) \u0026gt;\u0026gt; WMULT_SHIFT 210* 211* Either weight := NICE_0_LOAD and lw \\e sched_prio_to_wmult[], in which case 212* we\u0026#39;re guaranteed shift stays positive because inv_weight is guaranteed to 213* fit 32 bits, and NICE_0_LOAD gives another 10 bits; therefore shift \u0026gt;= 22. 214* 215* Or, weight =\u0026lt; lw.weight (because lw.weight is the runqueue weight), thus 216* weight/lw.weight \u0026lt;= 1, and therefore our shift will also be positive. 217*/ 218static u64 __calc_delta(u64 delta_exec, unsigned long weight, struct load_weight *lw) 219{ 220\tu64 fact = scale_load_down(weight); 221\tint shift = WMULT_SHIFT; 222 223\t__update_inv_weight(lw); 224 225\tif (unlikely(fact \u0026gt;\u0026gt; 32)) { 226\twhile (fact \u0026gt;\u0026gt; 32) { 227\tfact \u0026gt;\u0026gt;= 1; 228\tshift--; 229\t} 230\t} 231 232\t/* hint to use a 32x32-\u0026gt;64 mul */ 233\tfact = (u64)(u32)fact * lw-\u0026gt;inv_weight; 234 235\twhile (fact \u0026gt;\u0026gt; 32) { 236\tfact \u0026gt;\u0026gt;= 1; 237\tshift--; 238\t} 239 240\treturn mul_u64_u32_shr(delta_exec, fact, shift); 241} 242 243 244const struct sched_class fair_sched_class; 245 246/************************************************************** 247* CFS operations on generic schedulable entities: 248*/ 249 250#ifdef CONFIG_FAIR_GROUP_SCHED 251 252/* cpu runqueue to which this cfs_rq is attached */ 253static inline struct rq *rq_of(struct cfs_rq *cfs_rq) 254{ 255\treturn cfs_rq-\u0026gt;rq; 256} 257 258static inline struct task_struct *task_of(struct sched_entity *se) 259{ 260\tSCHED_WARN_ON(!entity_is_task(se)); 261\treturn container_of(se, struct task_struct, se); 262} 263 264/* Walk up scheduling entities hierarchy */ 265#define for_each_sched_entity(se) \\ 266for (; se; se = se-\u0026gt;parent) 267 268static inline struct cfs_rq *task_cfs_rq(struct task_struct *p) 269{ 270\treturn p-\u0026gt;se.cfs_rq; 271} 272 273/* runqueue on which this entity is (to be) queued */ 274static inline struct cfs_rq *cfs_rq_of(struct sched_entity *se) 275{ 276\treturn se-\u0026gt;cfs_rq; 277} 278 279/* runqueue \u0026#34;owned\u0026#34; by this group */ 280static inline struct cfs_rq *group_cfs_rq(struct sched_entity *grp) 281{ 282\treturn grp-\u0026gt;my_q; 283} 284 285static inline bool list_add_leaf_cfs_rq(struct cfs_rq *cfs_rq) 286{ 287\tstruct rq *rq = rq_of(cfs_rq); 288\tint cpu = cpu_of(rq); 289 290\tif (cfs_rq-\u0026gt;on_list) 291\treturn rq-\u0026gt;tmp_alone_branch == \u0026amp;rq-\u0026gt;leaf_cfs_rq_list; 292 293\tcfs_rq-\u0026gt;on_list = 1; 294 295\t/* 296* Ensure we either appear before our parent (if already 297* enqueued) or force our parent to appear after us when it is 298* enqueued. The fact that we always enqueue bottom-up 299* reduces this to two cases and a special case for the root 300* cfs_rq. Furthermore, it also means that we will always reset 301* tmp_alone_branch either when the branch is connected 302* to a tree or when we reach the top of the tree 303*/ 304\tif (cfs_rq-\u0026gt;tg-\u0026gt;parent \u0026amp;\u0026amp; 305\tcfs_rq-\u0026gt;tg-\u0026gt;parent-\u0026gt;cfs_rq[cpu]-\u0026gt;on_list) { 306\t/* 307* If parent is already on the list, we add the child 308* just before. Thanks to circular linked property of 309* the list, this means to put the child at the tail 310* of the list that starts by parent. 311*/ 312\tlist_add_tail_rcu(\u0026amp;cfs_rq-\u0026gt;leaf_cfs_rq_list, 313\t\u0026amp;(cfs_rq-\u0026gt;tg-\u0026gt;parent-\u0026gt;cfs_rq[cpu]-\u0026gt;leaf_cfs_rq_list)); 314\t/* 315* The branch is now connected to its tree so we can 316* reset tmp_alone_branch to the beginning of the 317* list. 318*/ 319\trq-\u0026gt;tmp_alone_branch = \u0026amp;rq-\u0026gt;leaf_cfs_rq_list; 320\treturn true; 321\t} 322 323\tif (!cfs_rq-\u0026gt;tg-\u0026gt;parent) { 324\t/* 325* cfs rq without parent should be put 326* at the tail of the list. 327*/ 328\tlist_add_tail_rcu(\u0026amp;cfs_rq-\u0026gt;leaf_cfs_rq_list, 329\t\u0026amp;rq-\u0026gt;leaf_cfs_rq_list); 330\t/* 331* We have reach the top of a tree so we can reset 332* tmp_alone_branch to the beginning of the list. 333*/ 334\trq-\u0026gt;tmp_alone_branch = \u0026amp;rq-\u0026gt;leaf_cfs_rq_list; 335\treturn true; 336\t} 337 338\t/* 339* The parent has not already been added so we want to 340* make sure that it will be put after us. 341* tmp_alone_branch points to the begin of the branch 342* where we will add parent. 343*/ 344\tlist_add_rcu(\u0026amp;cfs_rq-\u0026gt;leaf_cfs_rq_list, rq-\u0026gt;tmp_alone_branch); 345\t/* 346* update tmp_alone_branch to points to the new begin 347* of the branch 348*/ 349\trq-\u0026gt;tmp_alone_branch = \u0026amp;cfs_rq-\u0026gt;leaf_cfs_rq_list; 350\treturn false; 351} 352 353static inline void list_del_leaf_cfs_rq(struct cfs_rq *cfs_rq) 354{ 355\tif (cfs_rq-\u0026gt;on_list) { 356\tstruct rq *rq = rq_of(cfs_rq); 357 358\t/* 359* With cfs_rq being unthrottled/throttled during an enqueue, 360* it can happen the tmp_alone_branch points the a leaf that 361* we finally want to del. In this case, tmp_alone_branch moves 362* to the prev element but it will point to rq-\u0026gt;leaf_cfs_rq_list 363* at the end of the enqueue. 364*/ 365\tif (rq-\u0026gt;tmp_alone_branch == \u0026amp;cfs_rq-\u0026gt;leaf_cfs_rq_list) 366\trq-\u0026gt;tmp_alone_branch = cfs_rq-\u0026gt;leaf_cfs_rq_list.prev; 367 368\tlist_del_rcu(\u0026amp;cfs_rq-\u0026gt;leaf_cfs_rq_list); 369\tcfs_rq-\u0026gt;on_list = 0; 370\t} 371} 372 373static inline void assert_list_leaf_cfs_rq(struct rq *rq) 374{ 375\tSCHED_WARN_ON(rq-\u0026gt;tmp_alone_branch != \u0026amp;rq-\u0026gt;leaf_cfs_rq_list); 376} 377 378/* Iterate thr\u0026#39; all leaf cfs_rq\u0026#39;s on a runqueue */ 379#define for_each_leaf_cfs_rq_safe(rq, cfs_rq, pos)\t\\ 380list_for_each_entry_safe(cfs_rq, pos, \u0026amp;rq-\u0026gt;leaf_cfs_rq_list,\t\\ 381leaf_cfs_rq_list) 382 383/* Do the two (enqueued) entities belong to the same group ? */ 384static inline struct cfs_rq * 385is_same_group(struct sched_entity *se, struct sched_entity *pse) 386{ 387\tif (se-\u0026gt;cfs_rq == pse-\u0026gt;cfs_rq) 388\treturn se-\u0026gt;cfs_rq; 389 390\treturn NULL; 391} 392 393static inline struct sched_entity *parent_entity(struct sched_entity *se) 394{ 395\treturn se-\u0026gt;parent; 396} 397 398static void 399find_matching_se(struct sched_entity **se, struct sched_entity **pse) 400{ 401\tint se_depth, pse_depth; 402 403\t/* 404* preemption test can be made between sibling entities who are in the 405* same cfs_rq i.e who have a common parent. Walk up the hierarchy of 406* both tasks until we find their ancestors who are siblings of common 407* parent. 408*/ 409 410\t/* First walk up until both entities are at same depth */ 411\tse_depth = (*se)-\u0026gt;depth; 412\tpse_depth = (*pse)-\u0026gt;depth; 413 414\twhile (se_depth \u0026gt; pse_depth) { 415\tse_depth--; 416\t*se = parent_entity(*se); 417\t} 418 419\twhile (pse_depth \u0026gt; se_depth) { 420\tpse_depth--; 421\t*pse = parent_entity(*pse); 422\t} 423 424\twhile (!is_same_group(*se, *pse)) { 425\t*se = parent_entity(*se); 426\t*pse = parent_entity(*pse); 427\t} 428} 429 430#else\t/* !CONFIG_FAIR_GROUP_SCHED */431 432static inline struct task_struct *task_of(struct sched_entity *se) 433{ 434\treturn container_of(se, struct task_struct, se); 435} 436 437static inline struct rq *rq_of(struct cfs_rq *cfs_rq) 438{ 439\treturn container_of(cfs_rq, struct rq, cfs); 440} 441 442 443#define for_each_sched_entity(se) \\ 444for (; se; se = NULL) 445 446static inline struct cfs_rq *task_cfs_rq(struct task_struct *p) 447{ 448\treturn \u0026amp;task_rq(p)-\u0026gt;cfs; 449} 450 451static inline struct cfs_rq *cfs_rq_of(struct sched_entity *se) 452{ 453\tstruct task_struct *p = task_of(se); 454\tstruct rq *rq = task_rq(p); 455 456\treturn \u0026amp;rq-\u0026gt;cfs; 457} 458 459/* runqueue \u0026#34;owned\u0026#34; by this group */ 460static inline struct cfs_rq *group_cfs_rq(struct sched_entity *grp) 461{ 462\treturn NULL; 463} 464 465static inline bool list_add_leaf_cfs_rq(struct cfs_rq *cfs_rq) 466{ 467\treturn true; 468} 469 470static inline void list_del_leaf_cfs_rq(struct cfs_rq *cfs_rq) 471{ 472} 473 474static inline void assert_list_leaf_cfs_rq(struct rq *rq) 475{ 476} 477 478#define for_each_leaf_cfs_rq_safe(rq, cfs_rq, pos)\t\\ 479for (cfs_rq = \u0026amp;rq-\u0026gt;cfs, pos = NULL; cfs_rq; cfs_rq = pos) 480 481static inline struct sched_entity *parent_entity(struct sched_entity *se) 482{ 483\treturn NULL; 484} 485 486static inline void 487find_matching_se(struct sched_entity **se, struct sched_entity **pse) 488{ 489} 490 491#endif\t/* CONFIG_FAIR_GROUP_SCHED */492 493static __always_inline 494void account_cfs_rq_runtime(struct cfs_rq *cfs_rq, u64 delta_exec); 495 496/************************************************************** 497* Scheduling class tree data structure manipulation methods: 498*/ 499 500static inline u64 max_vruntime(u64 max_vruntime, u64 vruntime) 501{ 502\ts64 delta = (s64)(vruntime - max_vruntime); 503\tif (delta \u0026gt; 0) 504\tmax_vruntime = vruntime; 505 506\treturn max_vruntime; 507} 508 509static inline u64 min_vruntime(u64 min_vruntime, u64 vruntime) 510{ 511\ts64 delta = (s64)(vruntime - min_vruntime); 512\tif (delta \u0026lt; 0) 513\tmin_vruntime = vruntime; 514 515\treturn min_vruntime; 516} 517 518static inline int entity_before(struct sched_entity *a, 519\tstruct sched_entity *b) 520{ 521\treturn (s64)(a-\u0026gt;vruntime - b-\u0026gt;vruntime) \u0026lt; 0; 522} 523 524static void update_min_vruntime(struct cfs_rq *cfs_rq) 525{ 526\tstruct sched_entity *curr = cfs_rq-\u0026gt;curr; 527\tstruct rb_node *leftmost = rb_first_cached(\u0026amp;cfs_rq-\u0026gt;tasks_timeline); 528 529\tu64 vruntime = cfs_rq-\u0026gt;min_vruntime; 530 531\tif (curr) { 532\tif (curr-\u0026gt;on_rq) 533\tvruntime = curr-\u0026gt;vruntime; 534\telse 535\tcurr = NULL; 536\t} 537 538\tif (leftmost) { /* non-empty tree */ 539\tstruct sched_entity *se; 540\tse = rb_entry(leftmost, struct sched_entity, run_node); 541 542\tif (!curr) 543\tvruntime = se-\u0026gt;vruntime; 544\telse 545\tvruntime = min_vruntime(vruntime, se-\u0026gt;vruntime); 546\t} 547 548\t/* ensure we never gain time by being placed backwards. */ 549\tcfs_rq-\u0026gt;min_vruntime = max_vruntime(cfs_rq-\u0026gt;min_vruntime, vruntime); 550#ifndef CONFIG_64BIT 551\tsmp_wmb(); 552\tcfs_rq-\u0026gt;min_vruntime_copy = cfs_rq-\u0026gt;min_vruntime; 553#endif 554} 555 556/* 557* Enqueue an entity into the rb-tree: 558*/ 559static void __enqueue_entity(struct cfs_rq *cfs_rq, struct sched_entity *se) 560{ 561\tstruct rb_node **link = \u0026amp;cfs_rq-\u0026gt;tasks_timeline.rb_root.rb_node; 562\tstruct rb_node *parent = NULL; 563\tstruct sched_entity *entry; 564\tbool leftmost = true; 565 566\t/* 567* Find the right place in the rbtree: 568*/ 569\twhile (*link) { 570\tparent = *link; 571\tentry = rb_entry(parent, struct sched_entity, run_node); 572\t/* 573* We dont care about collisions. Nodes with 574* the same key stay together. 575*/ 576\tif (entity_before(se, entry)) { 577\tlink = \u0026amp;parent-\u0026gt;rb_left; 578\t} else { 579\tlink = \u0026amp;parent-\u0026gt;rb_right; 580\tleftmost = false; 581\t} 582\t} 583 584\trb_link_node(\u0026amp;se-\u0026gt;run_node, parent, link); 585\trb_insert_color_cached(\u0026amp;se-\u0026gt;run_node, 586\t\u0026amp;cfs_rq-\u0026gt;tasks_timeline, leftmost); 587} 588 589static void __dequeue_entity(struct cfs_rq *cfs_rq, struct sched_entity *se) 590{ 591\trb_erase_cached(\u0026amp;se-\u0026gt;run_node, \u0026amp;cfs_rq-\u0026gt;tasks_timeline); 592} 593 594struct sched_entity *__pick_first_entity(struct cfs_rq *cfs_rq) 595{ 596\tstruct rb_node *left = rb_first_cached(\u0026amp;cfs_rq-\u0026gt;tasks_timeline); 597 598\tif (!left) 599\treturn NULL; 600 601\treturn rb_entry(left, struct sched_entity, run_node); 602} 603 604static struct sched_entity *__pick_next_entity(struct sched_entity *se) 605{ 606\tstruct rb_node *next = rb_next(\u0026amp;se-\u0026gt;run_node); 607 608\tif (!next) 609\treturn NULL; 610 611\treturn rb_entry(next, struct sched_entity, run_node); 612} 613 614#ifdef CONFIG_SCHED_DEBUG 615struct sched_entity *__pick_last_entity(struct cfs_rq *cfs_rq) 616{ 617\tstruct rb_node *last = rb_last(\u0026amp;cfs_rq-\u0026gt;tasks_timeline.rb_root); 618 619\tif (!last) 620\treturn NULL; 621 622\treturn rb_entry(last, struct sched_entity, run_node); 623} 624 625/************************************************************** 626* Scheduling class statistics methods: 627*/ 628 629int sched_proc_update_handler(struct ctl_table *table, int write, 630\tvoid __user *buffer, size_t *lenp, 631\tloff_t *ppos) 632{ 633\tint ret = proc_dointvec_minmax(table, write, buffer, lenp, ppos); 634\tunsigned int factor = get_update_sysctl_factor(); 635 636\tif (ret || !write) 637\treturn ret; 638 639\tsched_nr_latency = DIV_ROUND_UP(sysctl_sched_latency, 640\tsysctl_sched_min_granularity); 641 642#define WRT_SYSCTL(name) \\ 643(normalized_sysctl_##name = sysctl_##name / (factor)) 644\tWRT_SYSCTL(sched_min_granularity); 645\tWRT_SYSCTL(sched_latency); 646\tWRT_SYSCTL(sched_wakeup_granularity); 647#undef WRT_SYSCTL 648 649\treturn 0; 650} 651#endif 652 653/* 654* delta /= w 655*/ 656static inline u64 calc_delta_fair(u64 delta, struct sched_entity *se) 657{ 658\tif (unlikely(se-\u0026gt;load.weight != NICE_0_LOAD)) 659\tdelta = __calc_delta(delta, NICE_0_LOAD, \u0026amp;se-\u0026gt;load); 660 661\treturn delta; 662} 663 664/* 665* The idea is to set a period in which each task runs once. 666* 667* When there are too many tasks (sched_nr_latency) we have to stretch 668* this period because otherwise the slices get too small. 669* 670* p = (nr \u0026lt;= nl) ? l : l*nr/nl 671*/ 672static u64 __sched_period(unsigned long nr_running) 673{ 674\tif (unlikely(nr_running \u0026gt; sched_nr_latency)) 675\treturn nr_running * sysctl_sched_min_granularity; 676\telse 677\treturn sysctl_sched_latency; 678} 679 680/* 681* We calculate the wall-time slice from the period by taking a part 682* proportional to the weight. 683* 684* s = p*P[w/rw] 685*/ 686static u64 sched_slice(struct cfs_rq *cfs_rq, struct sched_entity *se) 687{ 688\tu64 slice = __sched_period(cfs_rq-\u0026gt;nr_running + !se-\u0026gt;on_rq); 689 690\tfor_each_sched_entity(se) { 691\tstruct load_weight *load; 692\tstruct load_weight lw; 693 694\tcfs_rq = cfs_rq_of(se); 695\tload = \u0026amp;cfs_rq-\u0026gt;load; 696 697\tif (unlikely(!se-\u0026gt;on_rq)) { 698\tlw = cfs_rq-\u0026gt;load; 699 700\tupdate_load_add(\u0026amp;lw, se-\u0026gt;load.weight); 701\tload = \u0026amp;lw; 702\t} 703\tslice = __calc_delta(slice, se-\u0026gt;load.weight, load); 704\t} 705\treturn slice; 706} 707 708/* 709* We calculate the vruntime slice of a to-be-inserted task. 710* 711* vs = s/w 712*/ 713static u64 sched_vslice(struct cfs_rq *cfs_rq, struct sched_entity *se) 714{ 715\treturn calc_delta_fair(sched_slice(cfs_rq, se), se); 716} 717 718#ifdef CONFIG_SMP 719#include \u0026#34;pelt.h\u0026#34;720#include \u0026#34;sched-pelt.h\u0026#34;721 722static int select_idle_sibling(struct task_struct *p, int prev_cpu, int cpu); 723static unsigned long task_h_load(struct task_struct *p); 724 725/* Give new sched_entity start runnable values to heavy its load in infant time */ 726void init_entity_runnable_average(struct sched_entity *se) 727{ 728\tstruct sched_avg *sa = \u0026amp;se-\u0026gt;avg; 729 730\tmemset(sa, 0, sizeof(*sa)); 731 732\t/* 733* Tasks are intialized with full load to be seen as heavy tasks until 734* they get a chance to stabilize to their real load level. 735* Group entities are intialized with zero load to reflect the fact that 736* nothing has been attached to the task group yet. 737*/ 738\tif (entity_is_task(se)) 739\tsa-\u0026gt;runnable_load_avg = sa-\u0026gt;load_avg = scale_load_down(se-\u0026gt;load.weight); 740 741\tse-\u0026gt;runnable_weight = se-\u0026gt;load.weight; 742 743\t/* when this task enqueue\u0026#39;ed, it will contribute to its cfs_rq\u0026#39;s load_avg */ 744} 745 746static inline u64 cfs_rq_clock_task(struct cfs_rq *cfs_rq); 747static void attach_entity_cfs_rq(struct sched_entity *se); 748 749/* 750* With new tasks being created, their initial util_avgs are extrapolated 751* based on the cfs_rq\u0026#39;s current util_avg: 752* 753* util_avg = cfs_rq-\u0026gt;util_avg / (cfs_rq-\u0026gt;load_avg + 1) * se.load.weight 754* 755* However, in many cases, the above util_avg does not give a desired 756* value. Moreover, the sum of the util_avgs may be divergent, such 757* as when the series is a harmonic series. 758* 759* To solve this problem, we also cap the util_avg of successive tasks to 760* only 1/2 of the left utilization budget: 761* 762* util_avg_cap = (cpu_scale - cfs_rq-\u0026gt;avg.util_avg) / 2^n 763* 764* where n denotes the nth task and cpu_scale the CPU capacity. 765* 766* For example, for a CPU with 1024 of capacity, a simplest series from 767* the beginning would be like: 768* 769* task util_avg: 512, 256, 128, 64, 32, 16, 8, ... 770* cfs_rq util_avg: 512, 768, 896, 960, 992, 1008, 1016, ... 771* 772* Finally, that extrapolated util_avg is clamped to the cap (util_avg_cap) 773* if util_avg \u0026gt; util_avg_cap. 774*/ 775void post_init_entity_util_avg(struct sched_entity *se) 776{ 777\tstruct cfs_rq *cfs_rq = cfs_rq_of(se); 778\tstruct sched_avg *sa = \u0026amp;se-\u0026gt;avg; 779\tlong cpu_scale = arch_scale_cpu_capacity(NULL, cpu_of(rq_of(cfs_rq))); 780\tlong cap = (long)(cpu_scale - cfs_rq-\u0026gt;avg.util_avg) / 2; 781 782\tif (cap \u0026gt; 0) { 783\tif (cfs_rq-\u0026gt;avg.util_avg != 0) { 784\tsa-\u0026gt;util_avg = cfs_rq-\u0026gt;avg.util_avg * se-\u0026gt;load.weight; 785\tsa-\u0026gt;util_avg /= (cfs_rq-\u0026gt;avg.load_avg + 1); 786 787\tif (sa-\u0026gt;util_avg \u0026gt; cap) 788\tsa-\u0026gt;util_avg = cap; 789\t} else { 790\tsa-\u0026gt;util_avg = cap; 791\t} 792\t} 793 794\tif (entity_is_task(se)) { 795\tstruct task_struct *p = task_of(se); 796\tif (p-\u0026gt;sched_class != \u0026amp;fair_sched_class) { 797\t/* 798* For !fair tasks do: 799* 800update_cfs_rq_load_avg(now, cfs_rq); 801attach_entity_load_avg(cfs_rq, se, 0); 802switched_from_fair(rq, p); 803* 804* such that the next switched_to_fair() has the 805* expected state. 806*/ 807\tse-\u0026gt;avg.last_update_time = cfs_rq_clock_task(cfs_rq); 808\treturn; 809\t} 810\t} 811 812\tattach_entity_cfs_rq(se); 813} 814 815#else /* !CONFIG_SMP */816void init_entity_runnable_average(struct sched_entity *se) 817{ 818} 819void post_init_entity_util_avg(struct sched_entity *se) 820{ 821} 822static void update_tg_load_avg(struct cfs_rq *cfs_rq, int force) 823{ 824} 825#endif /* CONFIG_SMP */826 827/* 828* Update the current task\u0026#39;s runtime statistics. 829*/ 830static void update_curr(struct cfs_rq *cfs_rq) 831{ 832\tstruct sched_entity *curr = cfs_rq-\u0026gt;curr; 833\tu64 now = rq_clock_task(rq_of(cfs_rq)); 834\tu64 delta_exec; 835 836\tif (unlikely(!curr)) 837\treturn; 838 839\tdelta_exec = now - curr-\u0026gt;exec_start; 840\tif (unlikely((s64)delta_exec \u0026lt;= 0)) 841\treturn; 842 843\tcurr-\u0026gt;exec_start = now; 844 845\tschedstat_set(curr-\u0026gt;statistics.exec_max, 846\tmax(delta_exec, curr-\u0026gt;statistics.exec_max)); 847 848\tcurr-\u0026gt;sum_exec_runtime += delta_exec; 849\tschedstat_add(cfs_rq-\u0026gt;exec_clock, delta_exec); 850 851\tcurr-\u0026gt;vruntime += calc_delta_fair(delta_exec, curr); 852\tupdate_min_vruntime(cfs_rq); 853 854\tif (entity_is_task(curr)) { 855\tstruct task_struct *curtask = task_of(curr); 856 857\ttrace_sched_stat_runtime(curtask, delta_exec, curr-\u0026gt;vruntime); 858\tcgroup_account_cputime(curtask, delta_exec); 859\taccount_group_exec_runtime(curtask, delta_exec); 860\t} 861 862\taccount_cfs_rq_runtime(cfs_rq, delta_exec); 863} 864 865static void update_curr_fair(struct rq *rq) 866{ 867\tupdate_curr(cfs_rq_of(\u0026amp;rq-\u0026gt;curr-\u0026gt;se)); 868} 869 870static inline void 871update_stats_wait_start(struct cfs_rq *cfs_rq, struct sched_entity *se) 872{ 873\tu64 wait_start, prev_wait_start; 874 875\tif (!schedstat_enabled()) 876\treturn; 877 878\twait_start = rq_clock(rq_of(cfs_rq)); 879\tprev_wait_start = schedstat_val(se-\u0026gt;statistics.wait_start); 880 881\tif (entity_is_task(se) \u0026amp;\u0026amp; task_on_rq_migrating(task_of(se)) \u0026amp;\u0026amp; 882\tlikely(wait_start \u0026gt; prev_wait_start)) 883\twait_start -= prev_wait_start; 884 885\t__schedstat_set(se-\u0026gt;statistics.wait_start, wait_start); 886} 887 888static inline void 889update_stats_wait_end(struct cfs_rq *cfs_rq, struct sched_entity *se) 890{ 891\tstruct task_struct *p; 892\tu64 delta; 893 894\tif (!schedstat_enabled()) 895\treturn; 896 897\tdelta = rq_clock(rq_of(cfs_rq)) - schedstat_val(se-\u0026gt;statistics.wait_start); 898 899\tif (entity_is_task(se)) { 900\tp = task_of(se); 901\tif (task_on_rq_migrating(p)) { 902\t/* 903* Preserve migrating task\u0026#39;s wait time so wait_start 904* time stamp can be adjusted to accumulate wait time 905* prior to migration. 906*/ 907\t__schedstat_set(se-\u0026gt;statistics.wait_start, delta); 908\treturn; 909\t} 910\ttrace_sched_stat_wait(p, delta); 911\t} 912 913\t__schedstat_set(se-\u0026gt;statistics.wait_max, 914\tmax(schedstat_val(se-\u0026gt;statistics.wait_max), delta)); 915\t__schedstat_inc(se-\u0026gt;statistics.wait_count); 916\t__schedstat_add(se-\u0026gt;statistics.wait_sum, delta); 917\t__schedstat_set(se-\u0026gt;statistics.wait_start, 0); 918} 919 920static inline void 921update_stats_enqueue_sleeper(struct cfs_rq *cfs_rq, struct sched_entity *se) 922{ 923\tstruct task_struct *tsk = NULL; 924\tu64 sleep_start, block_start; 925 926\tif (!schedstat_enabled()) 927\treturn; 928 929\tsleep_start = schedstat_val(se-\u0026gt;statistics.sleep_start); 930\tblock_start = schedstat_val(se-\u0026gt;statistics.block_start); 931 932\tif (entity_is_task(se)) 933\ttsk = task_of(se); 934 935\tif (sleep_start) { 936\tu64 delta = rq_clock(rq_of(cfs_rq)) - sleep_start; 937 938\tif ((s64)delta \u0026lt; 0) 939\tdelta = 0; 940 941\tif (unlikely(delta \u0026gt; schedstat_val(se-\u0026gt;statistics.sleep_max))) 942\t__schedstat_set(se-\u0026gt;statistics.sleep_max, delta); 943 944\t__schedstat_set(se-\u0026gt;statistics.sleep_start, 0); 945\t__schedstat_add(se-\u0026gt;statistics.sum_sleep_runtime, delta); 946 947\tif (tsk) { 948\taccount_scheduler_latency(tsk, delta \u0026gt;\u0026gt; 10, 1); 949\ttrace_sched_stat_sleep(tsk, delta); 950\t} 951\t} 952\tif (block_start) { 953\tu64 delta = rq_clock(rq_of(cfs_rq)) - block_start; 954 955\tif ((s64)delta \u0026lt; 0) 956\tdelta = 0; 957 958\tif (unlikely(delta \u0026gt; schedstat_val(se-\u0026gt;statistics.block_max))) 959\t__schedstat_set(se-\u0026gt;statistics.block_max, delta); 960 961\t__schedstat_set(se-\u0026gt;statistics.block_start, 0); 962\t__schedstat_add(se-\u0026gt;statistics.sum_sleep_runtime, delta); 963 964\tif (tsk) { 965\tif (tsk-\u0026gt;in_iowait) { 966\t__schedstat_add(se-\u0026gt;statistics.iowait_sum, delta); 967\t__schedstat_inc(se-\u0026gt;statistics.iowait_count); 968\ttrace_sched_stat_iowait(tsk, delta); 969\t} 970 971\ttrace_sched_stat_blocked(tsk, delta); 972 973\t/* 974* Blocking time is in units of nanosecs, so shift by 975* 20 to get a milliseconds-range estimation of the 976* amount of time that the task spent sleeping: 977*/ 978\tif (unlikely(prof_on == SLEEP_PROFILING)) { 979\tprofile_hits(SLEEP_PROFILING, 980\t(void *)get_wchan(tsk), 981\tdelta \u0026gt;\u0026gt; 20); 982\t} 983\taccount_scheduler_latency(tsk, delta \u0026gt;\u0026gt; 10, 0); 984\t} 985\t} 986} 987 988/* 989* Task is being enqueued - update stats: 990*/ 991static inline void 992update_stats_enqueue(struct cfs_rq *cfs_rq, struct sched_entity *se, int flags) 993{ 994\tif (!schedstat_enabled()) 995\treturn; 996 997\t/* 998* Are we enqueueing a waiting task? (for current tasks 999* a dequeue/enqueue event is a NOP) 1000*/ 1001\tif (se != cfs_rq-\u0026gt;curr) 1002\tupdate_stats_wait_start(cfs_rq, se); 1003 1004\tif (flags \u0026amp; ENQUEUE_WAKEUP) 1005\tupdate_stats_enqueue_sleeper(cfs_rq, se); 1006} 1007 1008static inline void 1009update_stats_dequeue(struct cfs_rq *cfs_rq, struct sched_entity *se, int flags) 1010{ 1011 1012\tif (!schedstat_enabled()) 1013\treturn; 1014 1015\t/* 1016* Mark the end of the wait period if dequeueing a 1017* waiting task: 1018*/ 1019\tif (se != cfs_rq-\u0026gt;curr) 1020\tupdate_stats_wait_end(cfs_rq, se); 1021 1022\tif ((flags \u0026amp; DEQUEUE_SLEEP) \u0026amp;\u0026amp; entity_is_task(se)) { 1023\tstruct task_struct *tsk = task_of(se); 1024 1025\tif (tsk-\u0026gt;state \u0026amp; TASK_INTERRUPTIBLE) 1026\t__schedstat_set(se-\u0026gt;statistics.sleep_start, 1027\trq_clock(rq_of(cfs_rq))); 1028\tif (tsk-\u0026gt;state \u0026amp; TASK_UNINTERRUPTIBLE) 1029\t__schedstat_set(se-\u0026gt;statistics.block_start, 1030\trq_clock(rq_of(cfs_rq))); 1031\t} 1032} 1033 1034/* 1035* We are picking a new current task - update its stats: 1036*/ 1037static inline void 1038update_stats_curr_start(struct cfs_rq *cfs_rq, struct sched_entity *se) 1039{ 1040\t/* 1041* We are starting a new run period: 1042*/ 1043\tse-\u0026gt;exec_start = rq_clock_task(rq_of(cfs_rq)); 1044} 1045 1046/************************************************** 1047* Scheduling class queueing methods: 1048*/ 1049 1050#ifdef CONFIG_NUMA_BALANCING 1051/* 1052* Approximate time to scan a full NUMA task in ms. The task scan period is 1053* calculated based on the tasks virtual memory size and 1054* numa_balancing_scan_size. 1055*/ 1056unsigned int sysctl_numa_balancing_scan_period_min = 1000; 1057unsigned int sysctl_numa_balancing_scan_period_max = 60000; 1058 1059/* Portion of address space to scan in MB */ 1060unsigned int sysctl_numa_balancing_scan_size = 256; 1061 1062/* Scan @scan_size MB every @scan_period after an initial @scan_delay in ms */ 1063unsigned int sysctl_numa_balancing_scan_delay = 1000; 1064 1065struct numa_group { 1066\tatomic_t refcount; 1067 1068\tspinlock_t lock; /* nr_tasks, tasks */ 1069\tint nr_tasks; 1070\tpid_t gid; 1071\tint active_nodes; 1072 1073\tstruct rcu_head rcu; 1074\tunsigned long total_faults; 1075\tunsigned long max_faults_cpu; 1076\t/* 1077* Faults_cpu is used to decide whether memory should move 1078* towards the CPU. As a consequence, these stats are weighted 1079* more by CPU use than by memory faults. 1080*/ 1081\tunsigned long *faults_cpu; 1082\tunsigned long faults[0]; 1083}; 1084 1085/* 1086* For functions that can be called in multiple contexts that permit reading 1087* -\u0026gt;numa_group (see struct task_struct for locking rules). 1088*/ 1089static struct numa_group *deref_task_numa_group(struct task_struct *p) 1090{ 1091\treturn rcu_dereference_check(p-\u0026gt;numa_group, p == current || 1092\t(lockdep_is_held(\u0026amp;task_rq(p)-\u0026gt;lock) \u0026amp;\u0026amp; !READ_ONCE(p-\u0026gt;on_cpu))); 1093} 1094 1095static struct numa_group *deref_curr_numa_group(struct task_struct *p) 1096{ 1097\treturn rcu_dereference_protected(p-\u0026gt;numa_group, p == current); 1098} 1099 1100static inline unsigned long group_faults_priv(struct numa_group *ng); 1101static inline unsigned long group_faults_shared(struct numa_group *ng); 1102 1103static unsigned int task_nr_scan_windows(struct task_struct *p) 1104{ 1105\tunsigned long rss = 0; 1106\tunsigned long nr_scan_pages; 1107 1108\t/* 1109* Calculations based on RSS as non-present and empty pages are skipped 1110* by the PTE scanner and NUMA hinting faults should be trapped based 1111* on resident pages 1112*/ 1113\tnr_scan_pages = sysctl_numa_balancing_scan_size \u0026lt;\u0026lt; (20 - PAGE_SHIFT); 1114\trss = get_mm_rss(p-\u0026gt;mm); 1115\tif (!rss) 1116\trss = nr_scan_pages; 1117 1118\trss = round_up(rss, nr_scan_pages); 1119\treturn rss / nr_scan_pages; 1120} 1121 1122/* For sanitys sake, never scan more PTEs than MAX_SCAN_WINDOW MB/sec. */ 1123#define MAX_SCAN_WINDOW 2560 1124 1125static unsigned int task_scan_min(struct task_struct *p) 1126{ 1127\tunsigned int scan_size = READ_ONCE(sysctl_numa_balancing_scan_size); 1128\tunsigned int scan, floor; 1129\tunsigned int windows = 1; 1130 1131\tif (scan_size \u0026lt; MAX_SCAN_WINDOW) 1132\twindows = MAX_SCAN_WINDOW / scan_size; 1133\tfloor = 1000 / windows; 1134 1135\tscan = sysctl_numa_balancing_scan_period_min / task_nr_scan_windows(p); 1136\treturn max_t(unsigned int, floor, scan); 1137} 1138 1139static unsigned int task_scan_start(struct task_struct *p) 1140{ 1141\tunsigned long smin = task_scan_min(p); 1142\tunsigned long period = smin; 1143\tstruct numa_group *ng; 1144 1145\t/* Scale the maximum scan period with the amount of shared memory. */ 1146\trcu_read_lock(); 1147\tng = rcu_dereference(p-\u0026gt;numa_group); 1148\tif (ng) { 1149\tunsigned long shared = group_faults_shared(ng); 1150\tunsigned long private = group_faults_priv(ng); 1151 1152\tperiod *= atomic_read(\u0026amp;ng-\u0026gt;refcount); 1153\tperiod *= shared + 1; 1154\tperiod /= private + shared + 1; 1155\t} 1156\trcu_read_unlock(); 1157 1158\treturn max(smin, period); 1159} 1160 1161static unsigned int task_scan_max(struct task_struct *p) 1162{ 1163\tunsigned long smin = task_scan_min(p); 1164\tunsigned long smax; 1165\tstruct numa_group *ng; 1166 1167\t/* Watch for min being lower than max due to floor calculations */ 1168\tsmax = sysctl_numa_balancing_scan_period_max / task_nr_scan_windows(p); 1169 1170\t/* Scale the maximum scan period with the amount of shared memory. */ 1171\tng = deref_curr_numa_group(p); 1172\tif (ng) { 1173\tunsigned long shared = group_faults_shared(ng); 1174\tunsigned long private = group_faults_priv(ng); 1175\tunsigned long period = smax; 1176 1177\tperiod *= atomic_read(\u0026amp;ng-\u0026gt;refcount); 1178\tperiod *= shared + 1; 1179\tperiod /= private + shared + 1; 1180 1181\tsmax = max(smax, period); 1182\t} 1183 1184\treturn max(smin, smax); 1185} 1186 1187void init_numa_balancing(unsigned long clone_flags, struct task_struct *p) 1188{ 1189\tint mm_users = 0; 1190\tstruct mm_struct *mm = p-\u0026gt;mm; 1191 1192\tif (mm) { 1193\tmm_users = atomic_read(\u0026amp;mm-\u0026gt;mm_users); 1194\tif (mm_users == 1) { 1195\tmm-\u0026gt;numa_next_scan = jiffies + msecs_to_jiffies(sysctl_numa_balancing_scan_delay); 1196\tmm-\u0026gt;numa_scan_seq = 0; 1197\t} 1198\t} 1199\tp-\u0026gt;node_stamp\t= 0; 1200\tp-\u0026gt;numa_scan_seq\t= mm ? mm-\u0026gt;numa_scan_seq : 0; 1201\tp-\u0026gt;numa_scan_period\t= sysctl_numa_balancing_scan_delay; 1202\tp-\u0026gt;numa_work.next\t= \u0026amp;p-\u0026gt;numa_work; 1203\tp-\u0026gt;numa_faults\t= NULL; 1204\tRCU_INIT_POINTER(p-\u0026gt;numa_group, NULL); 1205\tp-\u0026gt;last_task_numa_placement\t= 0; 1206\tp-\u0026gt;last_sum_exec_runtime\t= 0; 1207 1208\t/* New address space, reset the preferred nid */ 1209\tif (!(clone_flags \u0026amp; CLONE_VM)) { 1210\tp-\u0026gt;numa_preferred_nid = -1; 1211\treturn; 1212\t} 1213 1214\t/* 1215* New thread, keep existing numa_preferred_nid which should be copied 1216* already by arch_dup_task_struct but stagger when scans start. 1217*/ 1218\tif (mm) { 1219\tunsigned int delay; 1220 1221\tdelay = min_t(unsigned int, task_scan_max(current), 1222\tcurrent-\u0026gt;numa_scan_period * mm_users * NSEC_PER_MSEC); 1223\tdelay += 2 * TICK_NSEC; 1224\tp-\u0026gt;node_stamp = delay; 1225\t} 1226} 1227 1228static void account_numa_enqueue(struct rq *rq, struct task_struct *p) 1229{ 1230\trq-\u0026gt;nr_numa_running += (p-\u0026gt;numa_preferred_nid != -1); 1231\trq-\u0026gt;nr_preferred_running += (p-\u0026gt;numa_preferred_nid == task_node(p)); 1232} 1233 1234static void account_numa_dequeue(struct rq *rq, struct task_struct *p) 1235{ 1236\trq-\u0026gt;nr_numa_running -= (p-\u0026gt;numa_preferred_nid != -1); 1237\trq-\u0026gt;nr_preferred_running -= (p-\u0026gt;numa_preferred_nid == task_node(p)); 1238} 1239 1240/* Shared or private faults. */ 1241#define NR_NUMA_HINT_FAULT_TYPES 2 1242 1243/* Memory and CPU locality */ 1244#define NR_NUMA_HINT_FAULT_STATS (NR_NUMA_HINT_FAULT_TYPES * 2) 1245 1246/* Averaged statistics, and temporary buffers. */ 1247#define NR_NUMA_HINT_FAULT_BUCKETS (NR_NUMA_HINT_FAULT_STATS * 2) 1248 1249pid_t task_numa_group_id(struct task_struct *p) 1250{ 1251\tstruct numa_group *ng; 1252\tpid_t gid = 0; 1253 1254\trcu_read_lock(); 1255\tng = rcu_dereference(p-\u0026gt;numa_group); 1256\tif (ng) 1257\tgid = ng-\u0026gt;gid; 1258\trcu_read_unlock(); 1259 1260\treturn gid; 1261} 1262 1263/* 1264* The averaged statistics, shared \u0026amp; private, memory \u0026amp; CPU, 1265* occupy the first half of the array. The second half of the 1266* array is for current counters, which are averaged into the 1267* first set by task_numa_placement. 1268*/ 1269static inline int task_faults_idx(enum numa_faults_stats s, int nid, int priv) 1270{ 1271\treturn NR_NUMA_HINT_FAULT_TYPES * (s * nr_node_ids + nid) + priv; 1272} 1273 1274static inline unsigned long task_faults(struct task_struct *p, int nid) 1275{ 1276\tif (!p-\u0026gt;numa_faults) 1277\treturn 0; 1278 1279\treturn p-\u0026gt;numa_faults[task_faults_idx(NUMA_MEM, nid, 0)] + 1280\tp-\u0026gt;numa_faults[task_faults_idx(NUMA_MEM, nid, 1)]; 1281} 1282 1283static inline unsigned long group_faults(struct task_struct *p, int nid) 1284{ 1285\tstruct numa_group *ng = deref_task_numa_group(p); 1286 1287\tif (!ng) 1288\treturn 0; 1289 1290\treturn ng-\u0026gt;faults[task_faults_idx(NUMA_MEM, nid, 0)] + 1291\tng-\u0026gt;faults[task_faults_idx(NUMA_MEM, nid, 1)]; 1292} 1293 1294static inline unsigned long group_faults_cpu(struct numa_group *group, int nid) 1295{ 1296\treturn group-\u0026gt;faults_cpu[task_faults_idx(NUMA_MEM, nid, 0)] + 1297\tgroup-\u0026gt;faults_cpu[task_faults_idx(NUMA_MEM, nid, 1)]; 1298} 1299 1300static inline unsigned long group_faults_priv(struct numa_group *ng) 1301{ 1302\tunsigned long faults = 0; 1303\tint node; 1304 1305\tfor_each_online_node(node) { 1306\tfaults += ng-\u0026gt;faults[task_faults_idx(NUMA_MEM, node, 1)]; 1307\t} 1308 1309\treturn faults; 1310} 1311 1312static inline unsigned long group_faults_shared(struct numa_group *ng) 1313{ 1314\tunsigned long faults = 0; 1315\tint node; 1316 1317\tfor_each_online_node(node) { 1318\tfaults += ng-\u0026gt;faults[task_faults_idx(NUMA_MEM, node, 0)]; 1319\t} 1320 1321\treturn faults; 1322} 1323 1324/* 1325* A node triggering more than 1/3 as many NUMA faults as the maximum is 1326* considered part of a numa group\u0026#39;s pseudo-interleaving set. Migrations 1327* between these nodes are slowed down, to allow things to settle down. 1328*/ 1329#define ACTIVE_NODE_FRACTION 3 1330 1331static bool numa_is_active_node(int nid, struct numa_group *ng) 1332{ 1333\treturn group_faults_cpu(ng, nid) * ACTIVE_NODE_FRACTION \u0026gt; ng-\u0026gt;max_faults_cpu; 1334} 1335 1336/* Handle placement on systems where not all nodes are directly connected. */ 1337static unsigned long score_nearby_nodes(struct task_struct *p, int nid, 1338\tint maxdist, bool task) 1339{ 1340\tunsigned long score = 0; 1341\tint node; 1342 1343\t/* 1344* All nodes are directly connected, and the same distance 1345* from each other. No need for fancy placement algorithms. 1346*/ 1347\tif (sched_numa_topology_type == NUMA_DIRECT) 1348\treturn 0; 1349 1350\t/* 1351* This code is called for each node, introducing N^2 complexity, 1352* which should be ok given the number of nodes rarely exceeds 8. 1353*/ 1354\tfor_each_online_node(node) { 1355\tunsigned long faults; 1356\tint dist = node_distance(nid, node); 1357 1358\t/* 1359* The furthest away nodes in the system are not interesting 1360* for placement; nid was already counted. 1361*/ 1362\tif (dist == sched_max_numa_distance || node == nid) 1363\tcontinue; 1364 1365\t/* 1366* On systems with a backplane NUMA topology, compare groups 1367* of nodes, and move tasks towards the group with the most 1368* memory accesses. When comparing two nodes at distance 1369* \u0026#34;hoplimit\u0026#34;, only nodes closer by than \u0026#34;hoplimit\u0026#34; are part 1370* of each group. Skip other nodes. 1371*/ 1372\tif (sched_numa_topology_type == NUMA_BACKPLANE \u0026amp;\u0026amp; 1373\tdist \u0026gt;= maxdist) 1374\tcontinue; 1375 1376\t/* Add up the faults from nearby nodes. */ 1377\tif (task) 1378\tfaults = task_faults(p, node); 1379\telse 1380\tfaults = group_faults(p, node); 1381 1382\t/* 1383* On systems with a glueless mesh NUMA topology, there are 1384* no fixed \u0026#34;groups of nodes\u0026#34;. Instead, nodes that are not 1385* directly connected bounce traffic through intermediate 1386* nodes; a numa_group can occupy any set of nodes. 1387* The further away a node is, the less the faults count. 1388* This seems to result in good task placement. 1389*/ 1390\tif (sched_numa_topology_type == NUMA_GLUELESS_MESH) { 1391\tfaults *= (sched_max_numa_distance - dist); 1392\tfaults /= (sched_max_numa_distance - LOCAL_DISTANCE); 1393\t} 1394 1395\tscore += faults; 1396\t} 1397 1398\treturn score; 1399} 1400 1401/* 1402* These return the fraction of accesses done by a particular task, or 1403* task group, on a particular numa node. The group weight is given a 1404* larger multiplier, in order to group tasks together that are almost 1405* evenly spread out between numa nodes. 1406*/ 1407static inline unsigned long task_weight(struct task_struct *p, int nid, 1408\tint dist) 1409{ 1410\tunsigned long faults, total_faults; 1411 1412\tif (!p-\u0026gt;numa_faults) 1413\treturn 0; 1414 1415\ttotal_faults = p-\u0026gt;total_numa_faults; 1416 1417\tif (!total_faults) 1418\treturn 0; 1419 1420\tfaults = task_faults(p, nid); 1421\tfaults += score_nearby_nodes(p, nid, dist, true); 1422 1423\treturn 1000 * faults / total_faults; 1424} 1425 1426static inline unsigned long group_weight(struct task_struct *p, int nid, 1427\tint dist) 1428{ 1429\tstruct numa_group *ng = deref_task_numa_group(p); 1430\tunsigned long faults, total_faults; 1431 1432\tif (!ng) 1433\treturn 0; 1434 1435\ttotal_faults = ng-\u0026gt;total_faults; 1436 1437\tif (!total_faults) 1438\treturn 0; 1439 1440\tfaults = group_faults(p, nid); 1441\tfaults += score_nearby_nodes(p, nid, dist, false); 1442 1443\treturn 1000 * faults / total_faults; 1444} 1445 1446bool should_numa_migrate_memory(struct task_struct *p, struct page * page, 1447\tint src_nid, int dst_cpu) 1448{ 1449\tstruct numa_group *ng = deref_curr_numa_group(p); 1450\tint dst_nid = cpu_to_node(dst_cpu); 1451\tint last_cpupid, this_cpupid; 1452 1453\tthis_cpupid = cpu_pid_to_cpupid(dst_cpu, current-\u0026gt;pid); 1454\tlast_cpupid = page_cpupid_xchg_last(page, this_cpupid); 1455 1456\t/* 1457* Allow first faults or private faults to migrate immediately early in 1458* the lifetime of a task. The magic number 4 is based on waiting for 1459* two full passes of the \u0026#34;multi-stage node selection\u0026#34; test that is 1460* executed below. 1461*/ 1462\tif ((p-\u0026gt;numa_preferred_nid == -1 || p-\u0026gt;numa_scan_seq \u0026lt;= 4) \u0026amp;\u0026amp; 1463\t(cpupid_pid_unset(last_cpupid) || cpupid_match_pid(p, last_cpupid))) 1464\treturn true; 1465 1466\t/* 1467* Multi-stage node selection is used in conjunction with a periodic 1468* migration fault to build a temporal task\u0026lt;-\u0026gt;page relation. By using 1469* a two-stage filter we remove short/unlikely relations. 1470* 1471* Using P(p) ~ n_p / n_t as per frequentist probability, we can equate 1472* a task\u0026#39;s usage of a particular page (n_p) per total usage of this 1473* page (n_t) (in a given time-span) to a probability. 1474* 1475* Our periodic faults will sample this probability and getting the 1476* same result twice in a row, given these samples are fully 1477* independent, is then given by P(n)^2, provided our sample period 1478* is sufficiently short compared to the usage pattern. 1479* 1480* This quadric squishes small probabilities, making it less likely we 1481* act on an unlikely task\u0026lt;-\u0026gt;page relation. 1482*/ 1483\tif (!cpupid_pid_unset(last_cpupid) \u0026amp;\u0026amp; 1484\tcpupid_to_nid(last_cpupid) != dst_nid) 1485\treturn false; 1486 1487\t/* Always allow migrate on private faults */ 1488\tif (cpupid_match_pid(p, last_cpupid)) 1489\treturn true; 1490 1491\t/* A shared fault, but p-\u0026gt;numa_group has not been set up yet. */ 1492\tif (!ng) 1493\treturn true; 1494 1495\t/* 1496* Destination node is much more heavily used than the source 1497* node? Allow migration. 1498*/ 1499\tif (group_faults_cpu(ng, dst_nid) \u0026gt; group_faults_cpu(ng, src_nid) * 1500\tACTIVE_NODE_FRACTION) 1501\treturn true; 1502 1503\t/* 1504* Distribute memory according to CPU \u0026amp; memory use on each node, 1505* with 3/4 hysteresis to avoid unnecessary memory migrations: 1506* 1507* faults_cpu(dst) 3 faults_cpu(src) 1508* --------------- * - \u0026gt; --------------- 1509* faults_mem(dst) 4 faults_mem(src) 1510*/ 1511\treturn group_faults_cpu(ng, dst_nid) * group_faults(p, src_nid) * 3 \u0026gt; 1512\tgroup_faults_cpu(ng, src_nid) * group_faults(p, dst_nid) * 4; 1513} 1514 1515static unsigned long weighted_cpuload(struct rq *rq); 1516static unsigned long source_load(int cpu, int type); 1517static unsigned long target_load(int cpu, int type); 1518static unsigned long capacity_of(int cpu); 1519 1520/* Cached statistics for all CPUs within a node */ 1521struct numa_stats { 1522\tunsigned long load; 1523 1524\t/* Total compute capacity of CPUs on a node */ 1525\tunsigned long compute_capacity; 1526 1527\tunsigned int nr_running; 1528}; 1529 1530/* 1531* XXX borrowed from update_sg_lb_stats 1532*/ 1533static void update_numa_stats(struct numa_stats *ns, int nid) 1534{ 1535\tint smt, cpu, cpus = 0; 1536\tunsigned long capacity; 1537 1538\tmemset(ns, 0, sizeof(*ns)); 1539\tfor_each_cpu(cpu, cpumask_of_node(nid)) { 1540\tstruct rq *rq = cpu_rq(cpu); 1541 1542\tns-\u0026gt;nr_running += rq-\u0026gt;nr_running; 1543\tns-\u0026gt;load += weighted_cpuload(rq); 1544\tns-\u0026gt;compute_capacity += capacity_of(cpu); 1545 1546\tcpus++; 1547\t} 1548 1549\t/* 1550* If we raced with hotplug and there are no CPUs left in our mask 1551* the @ns structure is NULL\u0026#39;ed and task_numa_compare() will 1552* not find this node attractive. 1553* 1554* We\u0026#39;ll detect a huge imbalance and bail there. 1555*/ 1556\tif (!cpus) 1557\treturn; 1558 1559\t/* smt := ceil(cpus / capacity), assumes: 1 \u0026lt; smt_power \u0026lt; 2 */ 1560\tsmt = DIV_ROUND_UP(SCHED_CAPACITY_SCALE * cpus, ns-\u0026gt;compute_capacity); 1561\tcapacity = cpus / smt; /* cores */ 1562 1563\tcapacity = min_t(unsigned, capacity, 1564\tDIV_ROUND_CLOSEST(ns-\u0026gt;compute_capacity, SCHED_CAPACITY_SCALE)); 1565} 1566 1567struct task_numa_env { 1568\tstruct task_struct *p; 1569 1570\tint src_cpu, src_nid; 1571\tint dst_cpu, dst_nid; 1572 1573\tstruct numa_stats src_stats, dst_stats; 1574 1575\tint imbalance_pct; 1576\tint dist; 1577 1578\tstruct task_struct *best_task; 1579\tlong best_imp; 1580\tint best_cpu; 1581}; 1582 1583static void task_numa_assign(struct task_numa_env *env, 1584\tstruct task_struct *p, long imp) 1585{ 1586\tstruct rq *rq = cpu_rq(env-\u0026gt;dst_cpu); 1587 1588\t/* Bail out if run-queue part of active NUMA balance. */ 1589\tif (xchg(\u0026amp;rq-\u0026gt;numa_migrate_on, 1)) 1590\treturn; 1591 1592\t/* 1593* Clear previous best_cpu/rq numa-migrate flag, since task now 1594* found a better CPU to move/swap. 1595*/ 1596\tif (env-\u0026gt;best_cpu != -1) { 1597\trq = cpu_rq(env-\u0026gt;best_cpu); 1598\tWRITE_ONCE(rq-\u0026gt;numa_migrate_on, 0); 1599\t} 1600 1601\tif (env-\u0026gt;best_task) 1602\tput_task_struct(env-\u0026gt;best_task); 1603\tif (p) 1604\tget_task_struct(p); 1605 1606\tenv-\u0026gt;best_task = p; 1607\tenv-\u0026gt;best_imp = imp; 1608\tenv-\u0026gt;best_cpu = env-\u0026gt;dst_cpu; 1609} 1610 1611static bool load_too_imbalanced(long src_load, long dst_load, 1612\tstruct task_numa_env *env) 1613{ 1614\tlong imb, old_imb; 1615\tlong orig_src_load, orig_dst_load; 1616\tlong src_capacity, dst_capacity; 1617 1618\t/* 1619* The load is corrected for the CPU capacity available on each node. 1620* 1621* src_load dst_load 1622* ------------ vs --------- 1623* src_capacity dst_capacity 1624*/ 1625\tsrc_capacity = env-\u0026gt;src_stats.compute_capacity; 1626\tdst_capacity = env-\u0026gt;dst_stats.compute_capacity; 1627 1628\timb = abs(dst_load * src_capacity - src_load * dst_capacity); 1629 1630\torig_src_load = env-\u0026gt;src_stats.load; 1631\torig_dst_load = env-\u0026gt;dst_stats.load; 1632 1633\told_imb = abs(orig_dst_load * src_capacity - orig_src_load * dst_capacity); 1634 1635\t/* Would this change make things worse? */ 1636\treturn (imb \u0026gt; old_imb); 1637} 1638 1639/* 1640* Maximum NUMA importance can be 1998 (2*999); 1641* SMALLIMP @ 30 would be close to 1998/64. 1642* Used to deter task migration. 1643*/ 1644#define SMALLIMP\t30 1645 1646/* 1647* This checks if the overall compute and NUMA accesses of the system would 1648* be improved if the source tasks was migrated to the target dst_cpu taking 1649* into account that it might be best if task running on the dst_cpu should 1650* be exchanged with the source task 1651*/ 1652static void task_numa_compare(struct task_numa_env *env, 1653\tlong taskimp, long groupimp, bool maymove) 1654{ 1655\tstruct numa_group *cur_ng, *p_ng = deref_curr_numa_group(env-\u0026gt;p); 1656\tstruct rq *dst_rq = cpu_rq(env-\u0026gt;dst_cpu); 1657\tlong imp = p_ng ? groupimp : taskimp; 1658\tstruct task_struct *cur; 1659\tlong src_load, dst_load; 1660\tint dist = env-\u0026gt;dist; 1661\tlong moveimp = imp; 1662\tlong load; 1663 1664\tif (READ_ONCE(dst_rq-\u0026gt;numa_migrate_on)) 1665\treturn; 1666 1667\trcu_read_lock(); 1668\tcur = task_rcu_dereference(\u0026amp;dst_rq-\u0026gt;curr); 1669\tif (cur \u0026amp;\u0026amp; ((cur-\u0026gt;flags \u0026amp; PF_EXITING) || is_idle_task(cur))) 1670\tcur = NULL; 1671 1672\t/* 1673* Because we have preemption enabled we can get migrated around and 1674* end try selecting ourselves (current == env-\u0026gt;p) as a swap candidate. 1675*/ 1676\tif (cur == env-\u0026gt;p) 1677\tgoto unlock; 1678 1679\tif (!cur) { 1680\tif (maymove \u0026amp;\u0026amp; moveimp \u0026gt;= env-\u0026gt;best_imp) 1681\tgoto assign; 1682\telse 1683\tgoto unlock; 1684\t} 1685 1686\t/* 1687* \u0026#34;imp\u0026#34; is the fault differential for the source task between the 1688* source and destination node. Calculate the total differential for 1689* the source task and potential destination task. The more negative 1690* the value is, the more remote accesses that would be expected to 1691* be incurred if the tasks were swapped. 1692*/ 1693\t/* Skip this swap candidate if cannot move to the source cpu */ 1694\tif (!cpumask_test_cpu(env-\u0026gt;src_cpu, \u0026amp;cur-\u0026gt;cpus_allowed)) 1695\tgoto unlock; 1696 1697\t/* 1698* If dst and source tasks are in the same NUMA group, or not 1699* in any group then look only at task weights. 1700*/ 1701\tcur_ng = rcu_dereference(cur-\u0026gt;numa_group); 1702\tif (cur_ng == p_ng) { 1703\timp = taskimp + task_weight(cur, env-\u0026gt;src_nid, dist) - 1704\ttask_weight(cur, env-\u0026gt;dst_nid, dist); 1705\t/* 1706* Add some hysteresis to prevent swapping the 1707* tasks within a group over tiny differences. 1708*/ 1709\tif (cur_ng) 1710\timp -= imp / 16; 1711\t} else { 1712\t/* 1713* Compare the group weights. If a task is all by itself 1714* (not part of a group), use the task weight instead. 1715*/ 1716\tif (cur_ng \u0026amp;\u0026amp; p_ng) 1717\timp += group_weight(cur, env-\u0026gt;src_nid, dist) - 1718\tgroup_weight(cur, env-\u0026gt;dst_nid, dist); 1719\telse 1720\timp += task_weight(cur, env-\u0026gt;src_nid, dist) - 1721\ttask_weight(cur, env-\u0026gt;dst_nid, dist); 1722\t} 1723 1724\tif (maymove \u0026amp;\u0026amp; moveimp \u0026gt; imp \u0026amp;\u0026amp; moveimp \u0026gt; env-\u0026gt;best_imp) { 1725\timp = moveimp; 1726\tcur = NULL; 1727\tgoto assign; 1728\t} 1729 1730\t/* 1731* If the NUMA importance is less than SMALLIMP, 1732* task migration might only result in ping pong 1733* of tasks and also hurt performance due to cache 1734* misses. 1735*/ 1736\tif (imp \u0026lt; SMALLIMP || imp \u0026lt;= env-\u0026gt;best_imp + SMALLIMP / 2) 1737\tgoto unlock; 1738 1739\t/* 1740* In the overloaded case, try and keep the load balanced. 1741*/ 1742\tload = task_h_load(env-\u0026gt;p) - task_h_load(cur); 1743\tif (!load) 1744\tgoto assign; 1745 1746\tdst_load = env-\u0026gt;dst_stats.load + load; 1747\tsrc_load = env-\u0026gt;src_stats.load - load; 1748 1749\tif (load_too_imbalanced(src_load, dst_load, env)) 1750\tgoto unlock; 1751 1752assign: 1753\t/* 1754* One idle CPU per node is evaluated for a task numa move. 1755* Call select_idle_sibling to maybe find a better one. 1756*/ 1757\tif (!cur) { 1758\t/* 1759* select_idle_siblings() uses an per-CPU cpumask that 1760* can be used from IRQ context. 1761*/ 1762\tlocal_irq_disable(); 1763\tenv-\u0026gt;dst_cpu = select_idle_sibling(env-\u0026gt;p, env-\u0026gt;src_cpu, 1764\tenv-\u0026gt;dst_cpu); 1765\tlocal_irq_enable(); 1766\t} 1767 1768\ttask_numa_assign(env, cur, imp); 1769unlock: 1770\trcu_read_unlock(); 1771} 1772 1773static void task_numa_find_cpu(struct task_numa_env *env, 1774\tlong taskimp, long groupimp) 1775{ 1776\tlong src_load, dst_load, load; 1777\tbool maymove = false; 1778\tint cpu; 1779 1780\tload = task_h_load(env-\u0026gt;p); 1781\tdst_load = env-\u0026gt;dst_stats.load + load; 1782\tsrc_load = env-\u0026gt;src_stats.load - load; 1783 1784\t/* 1785* If the improvement from just moving env-\u0026gt;p direction is better 1786* than swapping tasks around, check if a move is possible. 1787*/ 1788\tmaymove = !load_too_imbalanced(src_load, dst_load, env); 1789 1790\tfor_each_cpu(cpu, cpumask_of_node(env-\u0026gt;dst_nid)) { 1791\t/* Skip this CPU if the source task cannot migrate */ 1792\tif (!cpumask_test_cpu(cpu, \u0026amp;env-\u0026gt;p-\u0026gt;cpus_allowed)) 1793\tcontinue; 1794 1795\tenv-\u0026gt;dst_cpu = cpu; 1796\ttask_numa_compare(env, taskimp, groupimp, maymove); 1797\t} 1798} 1799 1800static int task_numa_migrate(struct task_struct *p) 1801{ 1802\tstruct task_numa_env env = { 1803\t.p = p, 1804 1805\t.src_cpu = task_cpu(p), 1806\t.src_nid = task_node(p), 1807 1808\t.imbalance_pct = 112, 1809 1810\t.best_task = NULL, 1811\t.best_imp = 0, 1812\t.best_cpu = -1, 1813\t}; 1814\tunsigned long taskweight, groupweight; 1815\tstruct sched_domain *sd; 1816\tlong taskimp, groupimp; 1817\tstruct numa_group *ng; 1818\tstruct rq *best_rq; 1819\tint nid, ret, dist; 1820 1821\t/* 1822* Pick the lowest SD_NUMA domain, as that would have the smallest 1823* imbalance and would be the first to start moving tasks about. 1824* 1825* And we want to avoid any moving of tasks about, as that would create 1826* random movement of tasks -- counter the numa conditions we\u0026#39;re trying 1827* to satisfy here. 1828*/ 1829\trcu_read_lock(); 1830\tsd = rcu_dereference(per_cpu(sd_numa, env.src_cpu)); 1831\tif (sd) 1832\tenv.imbalance_pct = 100 + (sd-\u0026gt;imbalance_pct - 100) / 2; 1833\trcu_read_unlock(); 1834 1835\t/* 1836* Cpusets can break the scheduler domain tree into smaller 1837* balance domains, some of which do not cross NUMA boundaries. 1838* Tasks that are \u0026#34;trapped\u0026#34; in such domains cannot be migrated 1839* elsewhere, so there is no point in (re)trying. 1840*/ 1841\tif (unlikely(!sd)) { 1842\tsched_setnuma(p, task_node(p)); 1843\treturn -EINVAL; 1844\t} 1845 1846\tenv.dst_nid = p-\u0026gt;numa_preferred_nid; 1847\tdist = env.dist = node_distance(env.src_nid, env.dst_nid); 1848\ttaskweight = task_weight(p, env.src_nid, dist); 1849\tgroupweight = group_weight(p, env.src_nid, dist); 1850\tupdate_numa_stats(\u0026amp;env.src_stats, env.src_nid); 1851\ttaskimp = task_weight(p, env.dst_nid, dist) - taskweight; 1852\tgroupimp = group_weight(p, env.dst_nid, dist) - groupweight; 1853\tupdate_numa_stats(\u0026amp;env.dst_stats, env.dst_nid); 1854 1855\t/* Try to find a spot on the preferred nid. */ 1856\ttask_numa_find_cpu(\u0026amp;env, taskimp, groupimp); 1857 1858\t/* 1859* Look at other nodes in these cases: 1860* - there is no space available on the preferred_nid 1861* - the task is part of a numa_group that is interleaved across 1862* multiple NUMA nodes; in order to better consolidate the group, 1863* we need to check other locations. 1864*/ 1865\tng = deref_curr_numa_group(p); 1866\tif (env.best_cpu == -1 || (ng \u0026amp;\u0026amp; ng-\u0026gt;active_nodes \u0026gt; 1)) { 1867\tfor_each_online_node(nid) { 1868\tif (nid == env.src_nid || nid == p-\u0026gt;numa_preferred_nid) 1869\tcontinue; 1870 1871\tdist = node_distance(env.src_nid, env.dst_nid); 1872\tif (sched_numa_topology_type == NUMA_BACKPLANE \u0026amp;\u0026amp; 1873\tdist != env.dist) { 1874\ttaskweight = task_weight(p, env.src_nid, dist); 1875\tgroupweight = group_weight(p, env.src_nid, dist); 1876\t} 1877 1878\t/* Only consider nodes where both task and groups benefit */ 1879\ttaskimp = task_weight(p, nid, dist) - taskweight; 1880\tgroupimp = group_weight(p, nid, dist) - groupweight; 1881\tif (taskimp \u0026lt; 0 \u0026amp;\u0026amp; groupimp \u0026lt; 0) 1882\tcontinue; 1883 1884\tenv.dist = dist; 1885\tenv.dst_nid = nid; 1886\tupdate_numa_stats(\u0026amp;env.dst_stats, env.dst_nid); 1887\ttask_numa_find_cpu(\u0026amp;env, taskimp, groupimp); 1888\t} 1889\t} 1890 1891\t/* 1892* If the task is part of a workload that spans multiple NUMA nodes, 1893* and is migrating into one of the workload\u0026#39;s active nodes, remember 1894* this node as the task\u0026#39;s preferred numa node, so the workload can 1895* settle down. 1896* A task that migrated to a second choice node will be better off 1897* trying for a better one later. Do not set the preferred node here. 1898*/ 1899\tif (ng) { 1900\tif (env.best_cpu == -1) 1901\tnid = env.src_nid; 1902\telse 1903\tnid = cpu_to_node(env.best_cpu); 1904 1905\tif (nid != p-\u0026gt;numa_preferred_nid) 1906\tsched_setnuma(p, nid); 1907\t} 1908 1909\t/* No better CPU than the current one was found. */ 1910\tif (env.best_cpu == -1) 1911\treturn -EAGAIN; 1912 1913\tbest_rq = cpu_rq(env.best_cpu); 1914\tif (env.best_task == NULL) { 1915\tret = migrate_task_to(p, env.best_cpu); 1916\tWRITE_ONCE(best_rq-\u0026gt;numa_migrate_on, 0); 1917\tif (ret != 0) 1918\ttrace_sched_stick_numa(p, env.src_cpu, env.best_cpu); 1919\treturn ret; 1920\t} 1921 1922\tret = migrate_swap(p, env.best_task, env.best_cpu, env.src_cpu); 1923\tWRITE_ONCE(best_rq-\u0026gt;numa_migrate_on, 0); 1924 1925\tif (ret != 0) 1926\ttrace_sched_stick_numa(p, env.src_cpu, task_cpu(env.best_task)); 1927\tput_task_struct(env.best_task); 1928\treturn ret; 1929} 1930 1931/* Attempt to migrate a task to a CPU on the preferred node. */ 1932static void numa_migrate_preferred(struct task_struct *p) 1933{ 1934\tunsigned long interval = HZ; 1935 1936\t/* This task has no NUMA fault statistics yet */ 1937\tif (unlikely(p-\u0026gt;numa_preferred_nid == -1 || !p-\u0026gt;numa_faults)) 1938\treturn; 1939 1940\t/* Periodically retry migrating the task to the preferred node */ 1941\tinterval = min(interval, msecs_to_jiffies(p-\u0026gt;numa_scan_period) / 16); 1942\tp-\u0026gt;numa_migrate_retry = jiffies + interval; 1943 1944\t/* Success if task is already running on preferred CPU */ 1945\tif (task_node(p) == p-\u0026gt;numa_preferred_nid) 1946\treturn; 1947 1948\t/* Otherwise, try migrate to a CPU on the preferred node */ 1949\ttask_numa_migrate(p); 1950} 1951 1952/* 1953* Find out how many nodes on the workload is actively running on. Do this by 1954* tracking the nodes from which NUMA hinting faults are triggered. This can 1955* be different from the set of nodes where the workload\u0026#39;s memory is currently 1956* located. 1957*/ 1958static void numa_group_count_active_nodes(struct numa_group *numa_group) 1959{ 1960\tunsigned long faults, max_faults = 0; 1961\tint nid, active_nodes = 0; 1962 1963\tfor_each_online_node(nid) { 1964\tfaults = group_faults_cpu(numa_group, nid); 1965\tif (faults \u0026gt; max_faults) 1966\tmax_faults = faults; 1967\t} 1968 1969\tfor_each_online_node(nid) { 1970\tfaults = group_faults_cpu(numa_group, nid); 1971\tif (faults * ACTIVE_NODE_FRACTION \u0026gt; max_faults) 1972\tactive_nodes++; 1973\t} 1974 1975\tnuma_group-\u0026gt;max_faults_cpu = max_faults; 1976\tnuma_group-\u0026gt;active_nodes = active_nodes; 1977} 1978 1979/* 1980* When adapting the scan rate, the period is divided into NUMA_PERIOD_SLOTS 1981* increments. The more local the fault statistics are, the higher the scan 1982* period will be for the next scan window. If local/(local+remote) ratio is 1983* below NUMA_PERIOD_THRESHOLD (where range of ratio is 1..NUMA_PERIOD_SLOTS) 1984* the scan period will decrease. Aim for 70% local accesses. 1985*/ 1986#define NUMA_PERIOD_SLOTS 10 1987#define NUMA_PERIOD_THRESHOLD 7 1988 1989/* 1990* Increase the scan period (slow down scanning) if the majority of 1991* our memory is already on our local node, or if the majority of 1992* the page accesses are shared with other processes. 1993* Otherwise, decrease the scan period. 1994*/ 1995static void update_task_scan_period(struct task_struct *p, 1996\tunsigned long shared, unsigned long private) 1997{ 1998\tunsigned int period_slot; 1999\tint lr_ratio, ps_ratio; 2000\tint diff; 2001 2002\tunsigned long remote = p-\u0026gt;numa_faults_locality[0]; 2003\tunsigned long local = p-\u0026gt;numa_faults_locality[1]; 2004 2005\t/* 2006* If there were no record hinting faults then either the task is 2007* completely idle or all activity is areas that are not of interest 2008* to automatic numa balancing. Related to that, if there were failed 2009* migration then it implies we are migrating too quickly or the local 2010* node is overloaded. In either case, scan slower 2011*/ 2012\tif (local + shared == 0 || p-\u0026gt;numa_faults_locality[2]) { 2013\tp-\u0026gt;numa_scan_period = min(p-\u0026gt;numa_scan_period_max, 2014\tp-\u0026gt;numa_scan_period \u0026lt;\u0026lt; 1); 2015 2016\tp-\u0026gt;mm-\u0026gt;numa_next_scan = jiffies + 2017\tmsecs_to_jiffies(p-\u0026gt;numa_scan_period); 2018 2019\treturn; 2020\t} 2021 2022\t/* 2023* Prepare to scale scan period relative to the current period. 2024*\t== NUMA_PERIOD_THRESHOLD scan period stays the same 2025* \u0026lt; NUMA_PERIOD_THRESHOLD scan period decreases (scan faster) 2026*\t\u0026gt;= NUMA_PERIOD_THRESHOLD scan period increases (scan slower) 2027*/ 2028\tperiod_slot = DIV_ROUND_UP(p-\u0026gt;numa_scan_period, NUMA_PERIOD_SLOTS); 2029\tlr_ratio = (local * NUMA_PERIOD_SLOTS) / (local + remote); 2030\tps_ratio = (private * NUMA_PERIOD_SLOTS) / (private + shared); 2031 2032\tif (ps_ratio \u0026gt;= NUMA_PERIOD_THRESHOLD) { 2033\t/* 2034* Most memory accesses are local. There is no need to 2035* do fast NUMA scanning, since memory is already local. 2036*/ 2037\tint slot = ps_ratio - NUMA_PERIOD_THRESHOLD; 2038\tif (!slot) 2039\tslot = 1; 2040\tdiff = slot * period_slot; 2041\t} else if (lr_ratio \u0026gt;= NUMA_PERIOD_THRESHOLD) { 2042\t/* 2043* Most memory accesses are shared with other tasks. 2044* There is no point in continuing fast NUMA scanning, 2045* since other tasks may just move the memory elsewhere. 2046*/ 2047\tint slot = lr_ratio - NUMA_PERIOD_THRESHOLD; 2048\tif (!slot) 2049\tslot = 1; 2050\tdiff = slot * period_slot; 2051\t} else { 2052\t/* 2053* Private memory faults exceed (SLOTS-THRESHOLD)/SLOTS, 2054* yet they are not on the local NUMA node. Speed up 2055* NUMA scanning to get the memory moved over. 2056*/ 2057\tint ratio = max(lr_ratio, ps_ratio); 2058\tdiff = -(NUMA_PERIOD_THRESHOLD - ratio) * period_slot; 2059\t} 2060 2061\tp-\u0026gt;numa_scan_period = clamp(p-\u0026gt;numa_scan_period + diff, 2062\ttask_scan_min(p), task_scan_max(p)); 2063\tmemset(p-\u0026gt;numa_faults_locality, 0, sizeof(p-\u0026gt;numa_faults_locality)); 2064} 2065 2066/* 2067* Get the fraction of time the task has been running since the last 2068* NUMA placement cycle. The scheduler keeps similar statistics, but 2069* decays those on a 32ms period, which is orders of magnitude off 2070* from the dozens-of-seconds NUMA balancing period. Use the scheduler 2071* stats only if the task is so new there are no NUMA statistics yet. 2072*/ 2073static u64 numa_get_avg_runtime(struct task_struct *p, u64 *period) 2074{ 2075\tu64 runtime, delta, now; 2076\t/* Use the start of this time slice to avoid calculations. */ 2077\tnow = p-\u0026gt;se.exec_start; 2078\truntime = p-\u0026gt;se.sum_exec_runtime; 2079 2080\tif (p-\u0026gt;last_task_numa_placement) { 2081\tdelta = runtime - p-\u0026gt;last_sum_exec_runtime; 2082\t*period = now - p-\u0026gt;last_task_numa_placement; 2083 2084\t/* Avoid time going backwards, prevent potential divide error: */ 2085\tif (unlikely((s64)*period \u0026lt; 0)) 2086\t*period = 0; 2087\t} else { 2088\tdelta = p-\u0026gt;se.avg.load_sum; 2089\t*period = LOAD_AVG_MAX; 2090\t} 2091 2092\tp-\u0026gt;last_sum_exec_runtime = runtime; 2093\tp-\u0026gt;last_task_numa_placement = now; 2094 2095\treturn delta; 2096} 2097 2098/* 2099* Determine the preferred nid for a task in a numa_group. This needs to 2100* be done in a way that produces consistent results with group_weight, 2101* otherwise workloads might not converge. 2102*/ 2103static int preferred_group_nid(struct task_struct *p, int nid) 2104{ 2105\tnodemask_t nodes; 2106\tint dist; 2107 2108\t/* Direct connections between all NUMA nodes. */ 2109\tif (sched_numa_topology_type == NUMA_DIRECT) 2110\treturn nid; 2111 2112\t/* 2113* On a system with glueless mesh NUMA topology, group_weight 2114* scores nodes according to the number of NUMA hinting faults on 2115* both the node itself, and on nearby nodes. 2116*/ 2117\tif (sched_numa_topology_type == NUMA_GLUELESS_MESH) { 2118\tunsigned long score, max_score = 0; 2119\tint node, max_node = nid; 2120 2121\tdist = sched_max_numa_distance; 2122 2123\tfor_each_online_node(node) { 2124\tscore = group_weight(p, node, dist); 2125\tif (score \u0026gt; max_score) { 2126\tmax_score = score; 2127\tmax_node = node; 2128\t} 2129\t} 2130\treturn max_node; 2131\t} 2132 2133\t/* 2134* Finding the preferred nid in a system with NUMA backplane 2135* interconnect topology is more involved. The goal is to locate 2136* tasks from numa_groups near each other in the system, and 2137* untangle workloads from different sides of the system. This requires 2138* searching down the hierarchy of node groups, recursively searching 2139* inside the highest scoring group of nodes. The nodemask tricks 2140* keep the complexity of the search down. 2141*/ 2142\tnodes = node_online_map; 2143\tfor (dist = sched_max_numa_distance; dist \u0026gt; LOCAL_DISTANCE; dist--) { 2144\tunsigned long max_faults = 0; 2145\tnodemask_t max_group = NODE_MASK_NONE; 2146\tint a, b; 2147 2148\t/* Are there nodes at this distance from each other? */ 2149\tif (!find_numa_distance(dist)) 2150\tcontinue; 2151 2152\tfor_each_node_mask(a, nodes) { 2153\tunsigned long faults = 0; 2154\tnodemask_t this_group; 2155\tnodes_clear(this_group); 2156 2157\t/* Sum group\u0026#39;s NUMA faults; includes a==b case. */ 2158\tfor_each_node_mask(b, nodes) { 2159\tif (node_distance(a, b) \u0026lt; dist) { 2160\tfaults += group_faults(p, b); 2161\tnode_set(b, this_group); 2162\tnode_clear(b, nodes); 2163\t} 2164\t} 2165 2166\t/* Remember the top group. */ 2167\tif (faults \u0026gt; max_faults) { 2168\tmax_faults = faults; 2169\tmax_group = this_group; 2170\t/* 2171* subtle: at the smallest distance there is 2172* just one node left in each \u0026#34;group\u0026#34;, the 2173* winner is the preferred nid. 2174*/ 2175\tnid = a; 2176\t} 2177\t} 2178\t/* Next round, evaluate the nodes within max_group. */ 2179\tif (!max_faults) 2180\tbreak; 2181\tnodes = max_group; 2182\t} 2183\treturn nid; 2184} 2185 2186static void task_numa_placement(struct task_struct *p) 2187{ 2188\tint seq, nid, max_nid = -1; 2189\tunsigned long max_faults = 0; 2190\tunsigned long fault_types[2] = { 0, 0 }; 2191\tunsigned long total_faults; 2192\tu64 runtime, period; 2193\tspinlock_t *group_lock = NULL; 2194\tstruct numa_group *ng; 2195 2196\t/* 2197* The p-\u0026gt;mm-\u0026gt;numa_scan_seq field gets updated without 2198* exclusive access. Use READ_ONCE() here to ensure 2199* that the field is read in a single access: 2200*/ 2201\tseq = READ_ONCE(p-\u0026gt;mm-\u0026gt;numa_scan_seq); 2202\tif (p-\u0026gt;numa_scan_seq == seq) 2203\treturn; 2204\tp-\u0026gt;numa_scan_seq = seq; 2205\tp-\u0026gt;numa_scan_period_max = task_scan_max(p); 2206 2207\ttotal_faults = p-\u0026gt;numa_faults_locality[0] + 2208\tp-\u0026gt;numa_faults_locality[1]; 2209\truntime = numa_get_avg_runtime(p, \u0026amp;period); 2210 2211\t/* If the task is part of a group prevent parallel updates to group stats */ 2212\tng = deref_curr_numa_group(p); 2213\tif (ng) { 2214\tgroup_lock = \u0026amp;ng-\u0026gt;lock; 2215\tspin_lock_irq(group_lock); 2216\t} 2217 2218\t/* Find the node with the highest number of faults */ 2219\tfor_each_online_node(nid) { 2220\t/* Keep track of the offsets in numa_faults array */ 2221\tint mem_idx, membuf_idx, cpu_idx, cpubuf_idx; 2222\tunsigned long faults = 0, group_faults = 0; 2223\tint priv; 2224 2225\tfor (priv = 0; priv \u0026lt; NR_NUMA_HINT_FAULT_TYPES; priv++) { 2226\tlong diff, f_diff, f_weight; 2227 2228\tmem_idx = task_faults_idx(NUMA_MEM, nid, priv); 2229\tmembuf_idx = task_faults_idx(NUMA_MEMBUF, nid, priv); 2230\tcpu_idx = task_faults_idx(NUMA_CPU, nid, priv); 2231\tcpubuf_idx = task_faults_idx(NUMA_CPUBUF, nid, priv); 2232 2233\t/* Decay existing window, copy faults since last scan */ 2234\tdiff = p-\u0026gt;numa_faults[membuf_idx] - p-\u0026gt;numa_faults[mem_idx] / 2; 2235\tfault_types[priv] += p-\u0026gt;numa_faults[membuf_idx]; 2236\tp-\u0026gt;numa_faults[membuf_idx] = 0; 2237 2238\t/* 2239* Normalize the faults_from, so all tasks in a group 2240* count according to CPU use, instead of by the raw 2241* number of faults. Tasks with little runtime have 2242* little over-all impact on throughput, and thus their 2243* faults are less important. 2244*/ 2245\tf_weight = div64_u64(runtime \u0026lt;\u0026lt; 16, period + 1); 2246\tf_weight = (f_weight * p-\u0026gt;numa_faults[cpubuf_idx]) / 2247\t(total_faults + 1); 2248\tf_diff = f_weight - p-\u0026gt;numa_faults[cpu_idx] / 2; 2249\tp-\u0026gt;numa_faults[cpubuf_idx] = 0; 2250 2251\tp-\u0026gt;numa_faults[mem_idx] += diff; 2252\tp-\u0026gt;numa_faults[cpu_idx] += f_diff; 2253\tfaults += p-\u0026gt;numa_faults[mem_idx]; 2254\tp-\u0026gt;total_numa_faults += diff; 2255\tif (ng) { 2256\t/* 2257* safe because we can only change our own group 2258* 2259* mem_idx represents the offset for a given 2260* nid and priv in a specific region because it 2261* is at the beginning of the numa_faults array. 2262*/ 2263\tng-\u0026gt;faults[mem_idx] += diff; 2264\tng-\u0026gt;faults_cpu[mem_idx] += f_diff; 2265\tng-\u0026gt;total_faults += diff; 2266\tgroup_faults += ng-\u0026gt;faults[mem_idx]; 2267\t} 2268\t} 2269 2270\tif (!ng) { 2271\tif (faults \u0026gt; max_faults) { 2272\tmax_faults = faults; 2273\tmax_nid = nid; 2274\t} 2275\t} else if (group_faults \u0026gt; max_faults) { 2276\tmax_faults = group_faults; 2277\tmax_nid = nid; 2278\t} 2279\t} 2280 2281\tif (ng) { 2282\tnuma_group_count_active_nodes(ng); 2283\tspin_unlock_irq(group_lock); 2284\tmax_nid = preferred_group_nid(p, max_nid); 2285\t} 2286 2287\tif (max_faults) { 2288\t/* Set the new preferred node */ 2289\tif (max_nid != p-\u0026gt;numa_preferred_nid) 2290\tsched_setnuma(p, max_nid); 2291\t} 2292 2293\tupdate_task_scan_period(p, fault_types[0], fault_types[1]); 2294} 2295 2296static inline int get_numa_group(struct numa_group *grp) 2297{ 2298\treturn atomic_inc_not_zero(\u0026amp;grp-\u0026gt;refcount); 2299} 2300 2301static inline void put_numa_group(struct numa_group *grp) 2302{ 2303\tif (atomic_dec_and_test(\u0026amp;grp-\u0026gt;refcount)) 2304\tkfree_rcu(grp, rcu); 2305} 2306 2307static void task_numa_group(struct task_struct *p, int cpupid, int flags, 2308\tint *priv) 2309{ 2310\tstruct numa_group *grp, *my_grp; 2311\tstruct task_struct *tsk; 2312\tbool join = false; 2313\tint cpu = cpupid_to_cpu(cpupid); 2314\tint i; 2315 2316\tif (unlikely(!deref_curr_numa_group(p))) { 2317\tunsigned int size = sizeof(struct numa_group) + 2318\t4*nr_node_ids*sizeof(unsigned long); 2319 2320\tgrp = kzalloc(size, GFP_KERNEL | __GFP_NOWARN); 2321\tif (!grp) 2322\treturn; 2323 2324\tatomic_set(\u0026amp;grp-\u0026gt;refcount, 1); 2325\tgrp-\u0026gt;active_nodes = 1; 2326\tgrp-\u0026gt;max_faults_cpu = 0; 2327\tspin_lock_init(\u0026amp;grp-\u0026gt;lock); 2328\tgrp-\u0026gt;gid = p-\u0026gt;pid; 2329\t/* Second half of the array tracks nids where faults happen */ 2330\tgrp-\u0026gt;faults_cpu = grp-\u0026gt;faults + NR_NUMA_HINT_FAULT_TYPES * 2331\tnr_node_ids; 2332 2333\tfor (i = 0; i \u0026lt; NR_NUMA_HINT_FAULT_STATS * nr_node_ids; i++) 2334\tgrp-\u0026gt;faults[i] = p-\u0026gt;numa_faults[i]; 2335 2336\tgrp-\u0026gt;total_faults = p-\u0026gt;total_numa_faults; 2337 2338\tgrp-\u0026gt;nr_tasks++; 2339\trcu_assign_pointer(p-\u0026gt;numa_group, grp); 2340\t} 2341 2342\trcu_read_lock(); 2343\ttsk = READ_ONCE(cpu_rq(cpu)-\u0026gt;curr); 2344 2345\tif (!cpupid_match_pid(tsk, cpupid)) 2346\tgoto no_join; 2347 2348\tgrp = rcu_dereference(tsk-\u0026gt;numa_group); 2349\tif (!grp) 2350\tgoto no_join; 2351 2352\tmy_grp = deref_curr_numa_group(p); 2353\tif (grp == my_grp) 2354\tgoto no_join; 2355 2356\t/* 2357* Only join the other group if its bigger; if we\u0026#39;re the bigger group, 2358* the other task will join us. 2359*/ 2360\tif (my_grp-\u0026gt;nr_tasks \u0026gt; grp-\u0026gt;nr_tasks) 2361\tgoto no_join; 2362 2363\t/* 2364* Tie-break on the grp address. 2365*/ 2366\tif (my_grp-\u0026gt;nr_tasks == grp-\u0026gt;nr_tasks \u0026amp;\u0026amp; my_grp \u0026gt; grp) 2367\tgoto no_join; 2368 2369\t/* Always join threads in the same process. */ 2370\tif (tsk-\u0026gt;mm == current-\u0026gt;mm) 2371\tjoin = true; 2372 2373\t/* Simple filter to avoid false positives due to PID collisions */ 2374\tif (flags \u0026amp; TNF_SHARED) 2375\tjoin = true; 2376 2377\t/* Update priv based on whether false sharing was detected */ 2378\t*priv = !join; 2379 2380\tif (join \u0026amp;\u0026amp; !get_numa_group(grp)) 2381\tgoto no_join; 2382 2383\trcu_read_unlock(); 2384 2385\tif (!join) 2386\treturn; 2387 2388\tBUG_ON(irqs_disabled()); 2389\tdouble_lock_irq(\u0026amp;my_grp-\u0026gt;lock, \u0026amp;grp-\u0026gt;lock); 2390 2391\tfor (i = 0; i \u0026lt; NR_NUMA_HINT_FAULT_STATS * nr_node_ids; i++) { 2392\tmy_grp-\u0026gt;faults[i] -= p-\u0026gt;numa_faults[i]; 2393\tgrp-\u0026gt;faults[i] += p-\u0026gt;numa_faults[i]; 2394\t} 2395\tmy_grp-\u0026gt;total_faults -= p-\u0026gt;total_numa_faults; 2396\tgrp-\u0026gt;total_faults += p-\u0026gt;total_numa_faults; 2397 2398\tmy_grp-\u0026gt;nr_tasks--; 2399\tgrp-\u0026gt;nr_tasks++; 2400 2401\tspin_unlock(\u0026amp;my_grp-\u0026gt;lock); 2402\tspin_unlock_irq(\u0026amp;grp-\u0026gt;lock); 2403 2404\trcu_assign_pointer(p-\u0026gt;numa_group, grp); 2405 2406\tput_numa_group(my_grp); 2407\treturn; 2408 2409no_join: 2410\trcu_read_unlock(); 2411\treturn; 2412} 2413 2414/* 2415* Get rid of NUMA staticstics associated with a task (either current or dead). 2416* If @final is set, the task is dead and has reached refcount zero, so we can 2417* safely free all relevant data structures. Otherwise, there might be 2418* concurrent reads from places like load balancing and procfs, and we should 2419* reset the data back to default state without freeing -\u0026gt;numa_faults. 2420*/ 2421void task_numa_free(struct task_struct *p, bool final) 2422{ 2423\t/* safe: p either is current or is being freed by current */ 2424\tstruct numa_group *grp = rcu_dereference_raw(p-\u0026gt;numa_group); 2425\tunsigned long *numa_faults = p-\u0026gt;numa_faults; 2426\tunsigned long flags; 2427\tint i; 2428 2429\tif (!numa_faults) 2430\treturn; 2431 2432\tif (grp) { 2433\tspin_lock_irqsave(\u0026amp;grp-\u0026gt;lock, flags); 2434\tfor (i = 0; i \u0026lt; NR_NUMA_HINT_FAULT_STATS * nr_node_ids; i++) 2435\tgrp-\u0026gt;faults[i] -= p-\u0026gt;numa_faults[i]; 2436\tgrp-\u0026gt;total_faults -= p-\u0026gt;total_numa_faults; 2437 2438\tgrp-\u0026gt;nr_tasks--; 2439\tspin_unlock_irqrestore(\u0026amp;grp-\u0026gt;lock, flags); 2440\tRCU_INIT_POINTER(p-\u0026gt;numa_group, NULL); 2441\tput_numa_group(grp); 2442\t} 2443 2444\tif (final) { 2445\tp-\u0026gt;numa_faults = NULL; 2446\tkfree(numa_faults); 2447\t} else { 2448\tp-\u0026gt;total_numa_faults = 0; 2449\tfor (i = 0; i \u0026lt; NR_NUMA_HINT_FAULT_STATS * nr_node_ids; i++) 2450\tnuma_faults[i] = 0; 2451\t} 2452} 2453 2454/* 2455* Got a PROT_NONE fault for a page on @node. 2456*/ 2457void task_numa_fault(int last_cpupid, int mem_node, int pages, int flags) 2458{ 2459\tstruct task_struct *p = current; 2460\tbool migrated = flags \u0026amp; TNF_MIGRATED; 2461\tint cpu_node = task_node(current); 2462\tint local = !!(flags \u0026amp; TNF_FAULT_LOCAL); 2463\tstruct numa_group *ng; 2464\tint priv; 2465 2466\tif (!static_branch_likely(\u0026amp;sched_numa_balancing)) 2467\treturn; 2468 2469\t/* for example, ksmd faulting in a user\u0026#39;s mm */ 2470\tif (!p-\u0026gt;mm) 2471\treturn; 2472 2473\t/* Allocate buffer to track faults on a per-node basis */ 2474\tif (unlikely(!p-\u0026gt;numa_faults)) { 2475\tint size = sizeof(*p-\u0026gt;numa_faults) * 2476\tNR_NUMA_HINT_FAULT_BUCKETS * nr_node_ids; 2477 2478\tp-\u0026gt;numa_faults = kzalloc(size, GFP_KERNEL|__GFP_NOWARN); 2479\tif (!p-\u0026gt;numa_faults) 2480\treturn; 2481 2482\tp-\u0026gt;total_numa_faults = 0; 2483\tmemset(p-\u0026gt;numa_faults_locality, 0, sizeof(p-\u0026gt;numa_faults_locality)); 2484\t} 2485 2486\t/* 2487* First accesses are treated as private, otherwise consider accesses 2488* to be private if the accessing pid has not changed 2489*/ 2490\tif (unlikely(last_cpupid == (-1 \u0026amp; LAST_CPUPID_MASK))) { 2491\tpriv = 1; 2492\t} else { 2493\tpriv = cpupid_match_pid(p, last_cpupid); 2494\tif (!priv \u0026amp;\u0026amp; !(flags \u0026amp; TNF_NO_GROUP)) 2495\ttask_numa_group(p, last_cpupid, flags, \u0026amp;priv); 2496\t} 2497 2498\t/* 2499* If a workload spans multiple NUMA nodes, a shared fault that 2500* occurs wholly within the set of nodes that the workload is 2501* actively using should be counted as local. This allows the 2502* scan rate to slow down when a workload has settled down. 2503*/ 2504\tng = deref_curr_numa_group(p); 2505\tif (!priv \u0026amp;\u0026amp; !local \u0026amp;\u0026amp; ng \u0026amp;\u0026amp; ng-\u0026gt;active_nodes \u0026gt; 1 \u0026amp;\u0026amp; 2506\tnuma_is_active_node(cpu_node, ng) \u0026amp;\u0026amp; 2507\tnuma_is_active_node(mem_node, ng)) 2508\tlocal = 1; 2509 2510\t/* 2511* Retry task to preferred node migration periodically, in case it 2512* case it previously failed, or the scheduler moved us. 2513*/ 2514\tif (time_after(jiffies, p-\u0026gt;numa_migrate_retry)) { 2515\ttask_numa_placement(p); 2516\tnuma_migrate_preferred(p); 2517\t} 2518 2519\tif (migrated) 2520\tp-\u0026gt;numa_pages_migrated += pages; 2521\tif (flags \u0026amp; TNF_MIGRATE_FAIL) 2522\tp-\u0026gt;numa_faults_locality[2] += pages; 2523 2524\tp-\u0026gt;numa_faults[task_faults_idx(NUMA_MEMBUF, mem_node, priv)] += pages; 2525\tp-\u0026gt;numa_faults[task_faults_idx(NUMA_CPUBUF, cpu_node, priv)] += pages; 2526\tp-\u0026gt;numa_faults_locality[local] += pages; 2527} 2528 2529static void reset_ptenuma_scan(struct task_struct *p) 2530{ 2531\t/* 2532* We only did a read acquisition of the mmap sem, so 2533* p-\u0026gt;mm-\u0026gt;numa_scan_seq is written to without exclusive access 2534* and the update is not guaranteed to be atomic. That\u0026#39;s not 2535* much of an issue though, since this is just used for 2536* statistical sampling. Use READ_ONCE/WRITE_ONCE, which are not 2537* expensive, to avoid any form of compiler optimizations: 2538*/ 2539\tWRITE_ONCE(p-\u0026gt;mm-\u0026gt;numa_scan_seq, READ_ONCE(p-\u0026gt;mm-\u0026gt;numa_scan_seq) + 1); 2540\tp-\u0026gt;mm-\u0026gt;numa_scan_offset = 0; 2541} 2542 2543/* 2544* The expensive part of numa migration is done from task_work context. 2545* Triggered from task_tick_numa(). 2546*/ 2547void task_numa_work(struct callback_head *work) 2548{ 2549\tunsigned long migrate, next_scan, now = jiffies; 2550\tstruct task_struct *p = current; 2551\tstruct mm_struct *mm = p-\u0026gt;mm; 2552\tu64 runtime = p-\u0026gt;se.sum_exec_runtime; 2553\tstruct vm_area_struct *vma; 2554\tunsigned long start, end; 2555\tunsigned long nr_pte_updates = 0; 2556\tlong pages, virtpages; 2557 2558\tSCHED_WARN_ON(p != container_of(work, struct task_struct, numa_work)); 2559 2560\twork-\u0026gt;next = work; /* protect against double add */ 2561\t/* 2562* Who cares about NUMA placement when they\u0026#39;re dying. 2563* 2564* NOTE: make sure not to dereference p-\u0026gt;mm before this check, 2565* exit_task_work() happens _after_ exit_mm() so we could be called 2566* without p-\u0026gt;mm even though we still had it when we enqueued this 2567* work. 2568*/ 2569\tif (p-\u0026gt;flags \u0026amp; PF_EXITING) 2570\treturn; 2571 2572\tif (!mm-\u0026gt;numa_next_scan) { 2573\tmm-\u0026gt;numa_next_scan = now + 2574\tmsecs_to_jiffies(sysctl_numa_balancing_scan_delay); 2575\t} 2576 2577\t/* 2578* Enforce maximal scan/migration frequency.. 2579*/ 2580\tmigrate = mm-\u0026gt;numa_next_scan; 2581\tif (time_before(now, migrate)) 2582\treturn; 2583 2584\tif (p-\u0026gt;numa_scan_period == 0) { 2585\tp-\u0026gt;numa_scan_period_max = task_scan_max(p); 2586\tp-\u0026gt;numa_scan_period = task_scan_start(p); 2587\t} 2588 2589\tnext_scan = now + msecs_to_jiffies(p-\u0026gt;numa_scan_period); 2590\tif (cmpxchg(\u0026amp;mm-\u0026gt;numa_next_scan, migrate, next_scan) != migrate) 2591\treturn; 2592 2593\t/* 2594* Delay this task enough that another task of this mm will likely win 2595* the next time around. 2596*/ 2597\tp-\u0026gt;node_stamp += 2 * TICK_NSEC; 2598 2599\tstart = mm-\u0026gt;numa_scan_offset; 2600\tpages = sysctl_numa_balancing_scan_size; 2601\tpages \u0026lt;\u0026lt;= 20 - PAGE_SHIFT; /* MB in pages */ 2602\tvirtpages = pages * 8;\t/* Scan up to this much virtual space */ 2603\tif (!pages) 2604\treturn; 2605 2606 2607\tif (!down_read_trylock(\u0026amp;mm-\u0026gt;mmap_sem)) 2608\treturn; 2609\tvma = find_vma(mm, start); 2610\tif (!vma) { 2611\treset_ptenuma_scan(p); 2612\tstart = 0; 2613\tvma = mm-\u0026gt;mmap; 2614\t} 2615\tfor (; vma; vma = vma-\u0026gt;vm_next) { 2616\tif (!vma_migratable(vma) || !vma_policy_mof(vma) || 2617\tis_vm_hugetlb_page(vma) || (vma-\u0026gt;vm_flags \u0026amp; VM_MIXEDMAP)) { 2618\tcontinue; 2619\t} 2620 2621\t/* 2622* Shared library pages mapped by multiple processes are not 2623* migrated as it is expected they are cache replicated. Avoid 2624* hinting faults in read-only file-backed mappings or the vdso 2625* as migrating the pages will be of marginal benefit. 2626*/ 2627\tif (!vma-\u0026gt;vm_mm || 2628\t(vma-\u0026gt;vm_file \u0026amp;\u0026amp; (vma-\u0026gt;vm_flags \u0026amp; (VM_READ|VM_WRITE)) == (VM_READ))) 2629\tcontinue; 2630 2631\t/* 2632* Skip inaccessible VMAs to avoid any confusion between 2633* PROT_NONE and NUMA hinting ptes 2634*/ 2635\tif (!(vma-\u0026gt;vm_flags \u0026amp; (VM_READ | VM_EXEC | VM_WRITE))) 2636\tcontinue; 2637 2638\tdo { 2639\tstart = max(start, vma-\u0026gt;vm_start); 2640\tend = ALIGN(start + (pages \u0026lt;\u0026lt; PAGE_SHIFT), HPAGE_SIZE); 2641\tend = min(end, vma-\u0026gt;vm_end); 2642\tnr_pte_updates = change_prot_numa(vma, start, end); 2643 2644\t/* 2645* Try to scan sysctl_numa_balancing_size worth of 2646* hpages that have at least one present PTE that 2647* is not already pte-numa. If the VMA contains 2648* areas that are unused or already full of prot_numa 2649* PTEs, scan up to virtpages, to skip through those 2650* areas faster. 2651*/ 2652\tif (nr_pte_updates) 2653\tpages -= (end - start) \u0026gt;\u0026gt; PAGE_SHIFT; 2654\tvirtpages -= (end - start) \u0026gt;\u0026gt; PAGE_SHIFT; 2655 2656\tstart = end; 2657\tif (pages \u0026lt;= 0 || virtpages \u0026lt;= 0) 2658\tgoto out; 2659 2660\tcond_resched(); 2661\t} while (end != vma-\u0026gt;vm_end); 2662\t} 2663 2664out: 2665\t/* 2666* It is possible to reach the end of the VMA list but the last few 2667* VMAs are not guaranteed to the vma_migratable. If they are not, we 2668* would find the !migratable VMA on the next scan but not reset the 2669* scanner to the start so check it now. 2670*/ 2671\tif (vma) 2672\tmm-\u0026gt;numa_scan_offset = start; 2673\telse 2674\treset_ptenuma_scan(p); 2675\tup_read(\u0026amp;mm-\u0026gt;mmap_sem); 2676 2677\t/* 2678* Make sure tasks use at least 32x as much time to run other code 2679* than they used here, to limit NUMA PTE scanning overhead to 3% max. 2680* Usually update_task_scan_period slows down scanning enough; on an 2681* overloaded system we need to limit overhead on a per task basis. 2682*/ 2683\tif (unlikely(p-\u0026gt;se.sum_exec_runtime != runtime)) { 2684\tu64 diff = p-\u0026gt;se.sum_exec_runtime - runtime; 2685\tp-\u0026gt;node_stamp += 32 * diff; 2686\t} 2687} 2688 2689/* 2690* Drive the periodic memory faults.. 2691*/ 2692void task_tick_numa(struct rq *rq, struct task_struct *curr) 2693{ 2694\tstruct callback_head *work = \u0026amp;curr-\u0026gt;numa_work; 2695\tu64 period, now; 2696 2697\t/* 2698* We don\u0026#39;t care about NUMA placement if we don\u0026#39;t have memory. 2699*/ 2700\tif ((curr-\u0026gt;flags \u0026amp; (PF_EXITING | PF_KTHREAD)) || work-\u0026gt;next != work) 2701\treturn; 2702 2703\t/* 2704* Using runtime rather than walltime has the dual advantage that 2705* we (mostly) drive the selection from busy threads and that the 2706* task needs to have done some actual work before we bother with 2707* NUMA placement. 2708*/ 2709\tnow = curr-\u0026gt;se.sum_exec_runtime; 2710\tperiod = (u64)curr-\u0026gt;numa_scan_period * NSEC_PER_MSEC; 2711 2712\tif (now \u0026gt; curr-\u0026gt;node_stamp + period) { 2713\tif (!curr-\u0026gt;node_stamp) 2714\tcurr-\u0026gt;numa_scan_period = task_scan_start(curr); 2715\tcurr-\u0026gt;node_stamp += period; 2716 2717\tif (!time_before(jiffies, curr-\u0026gt;mm-\u0026gt;numa_next_scan)) { 2718\tinit_task_work(work, task_numa_work); /* TODO: move this into sched_fork() */ 2719\ttask_work_add(curr, work, true); 2720\t} 2721\t} 2722} 2723 2724static void update_scan_period(struct task_struct *p, int new_cpu) 2725{ 2726\tint src_nid = cpu_to_node(task_cpu(p)); 2727\tint dst_nid = cpu_to_node(new_cpu); 2728 2729\tif (!static_branch_likely(\u0026amp;sched_numa_balancing)) 2730\treturn; 2731 2732\tif (!p-\u0026gt;mm || !p-\u0026gt;numa_faults || (p-\u0026gt;flags \u0026amp; PF_EXITING)) 2733\treturn; 2734 2735\tif (src_nid == dst_nid) 2736\treturn; 2737 2738\t/* 2739* Allow resets if faults have been trapped before one scan 2740* has completed. This is most likely due to a new task that 2741* is pulled cross-node due to wakeups or load balancing. 2742*/ 2743\tif (p-\u0026gt;numa_scan_seq) { 2744\t/* 2745* Avoid scan adjustments if moving to the preferred 2746* node or if the task was not previously running on 2747* the preferred node. 2748*/ 2749\tif (dst_nid == p-\u0026gt;numa_preferred_nid || 2750\t(p-\u0026gt;numa_preferred_nid != -1 \u0026amp;\u0026amp; src_nid != p-\u0026gt;numa_preferred_nid)) 2751\treturn; 2752\t} 2753 2754\tp-\u0026gt;numa_scan_period = task_scan_start(p); 2755} 2756 2757#else 2758static void task_tick_numa(struct rq *rq, struct task_struct *curr) 2759{ 2760} 2761 2762static inline void account_numa_enqueue(struct rq *rq, struct task_struct *p) 2763{ 2764} 2765 2766static inline void account_numa_dequeue(struct rq *rq, struct task_struct *p) 2767{ 2768} 2769 2770static inline void update_scan_period(struct task_struct *p, int new_cpu) 2771{ 2772} 2773 2774#endif /* CONFIG_NUMA_BALANCING */2775 2776static void 2777account_entity_enqueue(struct cfs_rq *cfs_rq, struct sched_entity *se) 2778{ 2779\tupdate_load_add(\u0026amp;cfs_rq-\u0026gt;load, se-\u0026gt;load.weight); 2780\tif (!parent_entity(se)) 2781\tupdate_load_add(\u0026amp;rq_of(cfs_rq)-\u0026gt;load, se-\u0026gt;load.weight); 2782#ifdef CONFIG_SMP 2783\tif (entity_is_task(se)) { 2784\tstruct rq *rq = rq_of(cfs_rq); 2785 2786\taccount_numa_enqueue(rq, task_of(se)); 2787\tlist_add(\u0026amp;se-\u0026gt;group_node, \u0026amp;rq-\u0026gt;cfs_tasks); 2788\t} 2789#endif 2790\tcfs_rq-\u0026gt;nr_running++; 2791} 2792 2793static void 2794account_entity_dequeue(struct cfs_rq *cfs_rq, struct sched_entity *se) 2795{ 2796\tupdate_load_sub(\u0026amp;cfs_rq-\u0026gt;load, se-\u0026gt;load.weight); 2797\tif (!parent_entity(se)) 2798\tupdate_load_sub(\u0026amp;rq_of(cfs_rq)-\u0026gt;load, se-\u0026gt;load.weight); 2799#ifdef CONFIG_SMP 2800\tif (entity_is_task(se)) { 2801\taccount_numa_dequeue(rq_of(cfs_rq), task_of(se)); 2802\tlist_del_init(\u0026amp;se-\u0026gt;group_node); 2803\t} 2804#endif 2805\tcfs_rq-\u0026gt;nr_running--; 2806} 2807 2808/* 2809* Signed add and clamp on underflow. 2810* 2811* Explicitly do a load-store to ensure the intermediate value never hits 2812* memory. This allows lockless observations without ever seeing the negative 2813* values. 2814*/ 2815#define add_positive(_ptr, _val) do { \\ 2816typeof(_ptr) ptr = (_ptr); \\ 2817typeof(_val) val = (_val); \\ 2818typeof(*ptr) res, var = READ_ONCE(*ptr); \\ 2819\\ 2820res = var + val; \\ 2821\\ 2822if (val \u0026lt; 0 \u0026amp;\u0026amp; res \u0026gt; var) \\ 2823res = 0; \\ 2824\\ 2825WRITE_ONCE(*ptr, res); \\ 2826} while (0) 2827 2828/* 2829* Unsigned subtract and clamp on underflow. 2830* 2831* Explicitly do a load-store to ensure the intermediate value never hits 2832* memory. This allows lockless observations without ever seeing the negative 2833* values. 2834*/ 2835#define sub_positive(_ptr, _val) do {\t\\ 2836typeof(_ptr) ptr = (_ptr);\t\\ 2837typeof(*ptr) val = (_val);\t\\ 2838typeof(*ptr) res, var = READ_ONCE(*ptr);\t\\ 2839res = var - val;\t\\ 2840if (res \u0026gt; var)\t\\ 2841res = 0;\t\\ 2842WRITE_ONCE(*ptr, res);\t\\ 2843} while (0) 2844 2845#ifdef CONFIG_SMP 2846static inline void 2847enqueue_runnable_load_avg(struct cfs_rq *cfs_rq, struct sched_entity *se) 2848{ 2849\tcfs_rq-\u0026gt;runnable_weight += se-\u0026gt;runnable_weight; 2850 2851\tcfs_rq-\u0026gt;avg.runnable_load_avg += se-\u0026gt;avg.runnable_load_avg; 2852\tcfs_rq-\u0026gt;avg.runnable_load_sum += se_runnable(se) * se-\u0026gt;avg.runnable_load_sum; 2853} 2854 2855static inline void 2856dequeue_runnable_load_avg(struct cfs_rq *cfs_rq, struct sched_entity *se) 2857{ 2858\tcfs_rq-\u0026gt;runnable_weight -= se-\u0026gt;runnable_weight; 2859 2860\tsub_positive(\u0026amp;cfs_rq-\u0026gt;avg.runnable_load_avg, se-\u0026gt;avg.runnable_load_avg); 2861\tsub_positive(\u0026amp;cfs_rq-\u0026gt;avg.runnable_load_sum, 2862\tse_runnable(se) * se-\u0026gt;avg.runnable_load_sum); 2863} 2864 2865static inline void 2866enqueue_load_avg(struct cfs_rq *cfs_rq, struct sched_entity *se) 2867{ 2868\tcfs_rq-\u0026gt;avg.load_avg += se-\u0026gt;avg.load_avg; 2869\tcfs_rq-\u0026gt;avg.load_sum += se_weight(se) * se-\u0026gt;avg.load_sum; 2870} 2871 2872static inline void 2873dequeue_load_avg(struct cfs_rq *cfs_rq, struct sched_entity *se) 2874{ 2875\tsub_positive(\u0026amp;cfs_rq-\u0026gt;avg.load_avg, se-\u0026gt;avg.load_avg); 2876\tsub_positive(\u0026amp;cfs_rq-\u0026gt;avg.load_sum, se_weight(se) * se-\u0026gt;avg.load_sum); 2877} 2878#else 2879static inline void 2880enqueue_runnable_load_avg(struct cfs_rq *cfs_rq, struct sched_entity *se) { } 2881static inline void 2882dequeue_runnable_load_avg(struct cfs_rq *cfs_rq, struct sched_entity *se) { } 2883static inline void 2884enqueue_load_avg(struct cfs_rq *cfs_rq, struct sched_entity *se) { } 2885static inline void 2886dequeue_load_avg(struct cfs_rq *cfs_rq, struct sched_entity *se) { } 2887#endif 2888 2889static void reweight_entity(struct cfs_rq *cfs_rq, struct sched_entity *se, 2890\tunsigned long weight, unsigned long runnable) 2891{ 2892\tif (se-\u0026gt;on_rq) { 2893\t/* commit outstanding execution time */ 2894\tif (cfs_rq-\u0026gt;curr == se) 2895\tupdate_curr(cfs_rq); 2896\taccount_entity_dequeue(cfs_rq, se); 2897\tdequeue_runnable_load_avg(cfs_rq, se); 2898\t} 2899\tdequeue_load_avg(cfs_rq, se); 2900 2901\tse-\u0026gt;runnable_weight = runnable; 2902\tupdate_load_set(\u0026amp;se-\u0026gt;load, weight); 2903 2904#ifdef CONFIG_SMP 2905\tdo { 2906\tu32 divider = LOAD_AVG_MAX - 1024 + se-\u0026gt;avg.period_contrib; 2907 2908\tse-\u0026gt;avg.load_avg = div_u64(se_weight(se) * se-\u0026gt;avg.load_sum, divider); 2909\tse-\u0026gt;avg.runnable_load_avg = 2910\tdiv_u64(se_runnable(se) * se-\u0026gt;avg.runnable_load_sum, divider); 2911\t} while (0); 2912#endif 2913 2914\tenqueue_load_avg(cfs_rq, se); 2915\tif (se-\u0026gt;on_rq) { 2916\taccount_entity_enqueue(cfs_rq, se); 2917\tenqueue_runnable_load_avg(cfs_rq, se); 2918\t} 2919} 2920 2921void reweight_task(struct task_struct *p, int prio) 2922{ 2923\tstruct sched_entity *se = \u0026amp;p-\u0026gt;se; 2924\tstruct cfs_rq *cfs_rq = cfs_rq_of(se); 2925\tstruct load_weight *load = \u0026amp;se-\u0026gt;load; 2926\tunsigned long weight = scale_load(sched_prio_to_weight[prio]); 2927 2928\treweight_entity(cfs_rq, se, weight, weight); 2929\tload-\u0026gt;inv_weight = sched_prio_to_wmult[prio]; 2930} 2931 2932#ifdef CONFIG_FAIR_GROUP_SCHED 2933#ifdef CONFIG_SMP 2934/* 2935* All this does is approximate the hierarchical proportion which includes that 2936* global sum we all love to hate. 2937* 2938* That is, the weight of a group entity, is the proportional share of the 2939* group weight based on the group runqueue weights. That is: 2940* 2941* tg-\u0026gt;weight * grq-\u0026gt;load.weight 2942* ge-\u0026gt;load.weight = ----------------------------- (1) 2943*\t\\Sum grq-\u0026gt;load.weight 2944* 2945* Now, because computing that sum is prohibitively expensive to compute (been 2946* there, done that) we approximate it with this average stuff. The average 2947* moves slower and therefore the approximation is cheaper and more stable. 2948* 2949* So instead of the above, we substitute: 2950* 2951* grq-\u0026gt;load.weight -\u0026gt; grq-\u0026gt;avg.load_avg (2) 2952* 2953* which yields the following: 2954* 2955* tg-\u0026gt;weight * grq-\u0026gt;avg.load_avg 2956* ge-\u0026gt;load.weight = ------------------------------ (3) 2957*\ttg-\u0026gt;load_avg 2958* 2959* Where: tg-\u0026gt;load_avg ~= \\Sum grq-\u0026gt;avg.load_avg 2960* 2961* That is shares_avg, and it is right (given the approximation (2)). 2962* 2963* The problem with it is that because the average is slow -- it was designed 2964* to be exactly that of course -- this leads to transients in boundary 2965* conditions. In specific, the case where the group was idle and we start the 2966* one task. It takes time for our CPU\u0026#39;s grq-\u0026gt;avg.load_avg to build up, 2967* yielding bad latency etc.. 2968* 2969* Now, in that special case (1) reduces to: 2970* 2971* tg-\u0026gt;weight * grq-\u0026gt;load.weight 2972* ge-\u0026gt;load.weight = ----------------------------- = tg-\u0026gt;weight (4) 2973*\tgrp-\u0026gt;load.weight 2974* 2975* That is, the sum collapses because all other CPUs are idle; the UP scenario. 2976* 2977* So what we do is modify our approximation (3) to approach (4) in the (near) 2978* UP case, like: 2979* 2980* ge-\u0026gt;load.weight = 2981* 2982* tg-\u0026gt;weight * grq-\u0026gt;load.weight 2983* --------------------------------------------------- (5) 2984* tg-\u0026gt;load_avg - grq-\u0026gt;avg.load_avg + grq-\u0026gt;load.weight 2985* 2986* But because grq-\u0026gt;load.weight can drop to 0, resulting in a divide by zero, 2987* we need to use grq-\u0026gt;avg.load_avg as its lower bound, which then gives: 2988* 2989* 2990* tg-\u0026gt;weight * grq-\u0026gt;load.weight 2991* ge-\u0026gt;load.weight = -----------------------------\t(6) 2992*\ttg_load_avg\u0026#39; 2993* 2994* Where: 2995* 2996* tg_load_avg\u0026#39; = tg-\u0026gt;load_avg - grq-\u0026gt;avg.load_avg + 2997* max(grq-\u0026gt;load.weight, grq-\u0026gt;avg.load_avg) 2998* 2999* And that is shares_weight and is icky. In the (near) UP case it approaches 3000* (4) while in the normal case it approaches (3). It consistently 3001* overestimates the ge-\u0026gt;load.weight and therefore: 3002* 3003* \\Sum ge-\u0026gt;load.weight \u0026gt;= tg-\u0026gt;weight 3004* 3005* hence icky! 3006*/ 3007static long calc_group_shares(struct cfs_rq *cfs_rq) 3008{ 3009\tlong tg_weight, tg_shares, load, shares; 3010\tstruct task_group *tg = cfs_rq-\u0026gt;tg; 3011 3012\ttg_shares = READ_ONCE(tg-\u0026gt;shares); 3013 3014\tload = max(scale_load_down(cfs_rq-\u0026gt;load.weight), cfs_rq-\u0026gt;avg.load_avg); 3015 3016\ttg_weight = atomic_long_read(\u0026amp;tg-\u0026gt;load_avg); 3017 3018\t/* Ensure tg_weight \u0026gt;= load */ 3019\ttg_weight -= cfs_rq-\u0026gt;tg_load_avg_contrib; 3020\ttg_weight += load; 3021 3022\tshares = (tg_shares * load); 3023\tif (tg_weight) 3024\tshares /= tg_weight; 3025 3026\t/* 3027* MIN_SHARES has to be unscaled here to support per-CPU partitioning 3028* of a group with small tg-\u0026gt;shares value. It is a floor value which is 3029* assigned as a minimum load.weight to the sched_entity representing 3030* the group on a CPU. 3031* 3032* E.g. on 64-bit for a group with tg-\u0026gt;shares of scale_load(15)=15*1024 3033* on an 8-core system with 8 tasks each runnable on one CPU shares has 3034* to be 15*1024*1/8=1920 instead of scale_load(MIN_SHARES)=2*1024. In 3035* case no task is runnable on a CPU MIN_SHARES=2 should be returned 3036* instead of 0. 3037*/ 3038\treturn clamp_t(long, shares, MIN_SHARES, tg_shares); 3039} 3040 3041/* 3042* This calculates the effective runnable weight for a group entity based on 3043* the group entity weight calculated above. 3044* 3045* Because of the above approximation (2), our group entity weight is 3046* an load_avg based ratio (3). This means that it includes blocked load and 3047* does not represent the runnable weight. 3048* 3049* Approximate the group entity\u0026#39;s runnable weight per ratio from the group 3050* runqueue: 3051* 3052*\tgrq-\u0026gt;avg.runnable_load_avg 3053* ge-\u0026gt;runnable_weight = ge-\u0026gt;load.weight * -------------------------- (7) 3054*\tgrq-\u0026gt;avg.load_avg 3055* 3056* However, analogous to above, since the avg numbers are slow, this leads to 3057* transients in the from-idle case. Instead we use: 3058* 3059* ge-\u0026gt;runnable_weight = ge-\u0026gt;load.weight * 3060* 3061*\tmax(grq-\u0026gt;avg.runnable_load_avg, grq-\u0026gt;runnable_weight) 3062*\t-----------------------------------------------------\t(8) 3063*\tmax(grq-\u0026gt;avg.load_avg, grq-\u0026gt;load.weight) 3064* 3065* Where these max() serve both to use the \u0026#39;instant\u0026#39; values to fix the slow 3066* from-idle and avoid the /0 on to-idle, similar to (6). 3067*/ 3068static long calc_group_runnable(struct cfs_rq *cfs_rq, long shares) 3069{ 3070\tlong runnable, load_avg; 3071 3072\tload_avg = max(cfs_rq-\u0026gt;avg.load_avg, 3073\tscale_load_down(cfs_rq-\u0026gt;load.weight)); 3074 3075\trunnable = max(cfs_rq-\u0026gt;avg.runnable_load_avg, 3076\tscale_load_down(cfs_rq-\u0026gt;runnable_weight)); 3077 3078\trunnable *= shares; 3079\tif (load_avg) 3080\trunnable /= load_avg; 3081 3082\treturn clamp_t(long, runnable, MIN_SHARES, shares); 3083} 3084#endif /* CONFIG_SMP */3085 3086static inline int throttled_hierarchy(struct cfs_rq *cfs_rq); 3087 3088/* 3089* Recomputes the group entity based on the current state of its group 3090* runqueue. 3091*/ 3092static void update_cfs_group(struct sched_entity *se) 3093{ 3094\tstruct cfs_rq *gcfs_rq = group_cfs_rq(se); 3095\tlong shares, runnable; 3096 3097\tif (!gcfs_rq) 3098\treturn; 3099 3100\tif (throttled_hierarchy(gcfs_rq)) 3101\treturn; 3102 3103#ifndef CONFIG_SMP 3104\trunnable = shares = READ_ONCE(gcfs_rq-\u0026gt;tg-\u0026gt;shares); 3105 3106\tif (likely(se-\u0026gt;load.weight == shares)) 3107\treturn; 3108#else 3109\tshares = calc_group_shares(gcfs_rq); 3110\trunnable = calc_group_runnable(gcfs_rq, shares); 3111#endif 3112 3113\treweight_entity(cfs_rq_of(se), se, shares, runnable); 3114} 3115 3116#else /* CONFIG_FAIR_GROUP_SCHED */3117static inline void update_cfs_group(struct sched_entity *se) 3118{ 3119} 3120#endif /* CONFIG_FAIR_GROUP_SCHED */3121 3122static inline void cfs_rq_util_change(struct cfs_rq *cfs_rq, int flags) 3123{ 3124\tstruct rq *rq = rq_of(cfs_rq); 3125 3126\tif (\u0026amp;rq-\u0026gt;cfs == cfs_rq || (flags \u0026amp; SCHED_CPUFREQ_MIGRATION)) { 3127\t/* 3128* There are a few boundary cases this might miss but it should 3129* get called often enough that that should (hopefully) not be 3130* a real problem. 3131* 3132* It will not get called when we go idle, because the idle 3133* thread is a different class (!fair), nor will the utilization 3134* number include things like RT tasks. 3135* 3136* As is, the util number is not freq-invariant (we\u0026#39;d have to 3137* implement arch_scale_freq_capacity() for that). 3138* 3139* See cpu_util(). 3140*/ 3141\tcpufreq_update_util(rq, flags); 3142\t} 3143} 3144 3145#ifdef CONFIG_SMP 3146#ifdef CONFIG_FAIR_GROUP_SCHED 3147/** 3148* update_tg_load_avg - update the tg\u0026#39;s load avg 3149* @cfs_rq: the cfs_rq whose avg changed 3150* @force: update regardless of how small the difference 3151* 3152* This function \u0026#39;ensures\u0026#39;: tg-\u0026gt;load_avg := \\Sum tg-\u0026gt;cfs_rq[]-\u0026gt;avg.load. 3153* However, because tg-\u0026gt;load_avg is a global value there are performance 3154* considerations. 3155* 3156* In order to avoid having to look at the other cfs_rq\u0026#39;s, we use a 3157* differential update where we store the last value we propagated. This in 3158* turn allows skipping updates if the differential is \u0026#39;small\u0026#39;. 3159* 3160* Updating tg\u0026#39;s load_avg is necessary before update_cfs_share(). 3161*/ 3162static inline void update_tg_load_avg(struct cfs_rq *cfs_rq, int force) 3163{ 3164\tlong delta = cfs_rq-\u0026gt;avg.load_avg - cfs_rq-\u0026gt;tg_load_avg_contrib; 3165 3166\t/* 3167* No need to update load_avg for root_task_group as it is not used. 3168*/ 3169\tif (cfs_rq-\u0026gt;tg == \u0026amp;root_task_group) 3170\treturn; 3171 3172\tif (force || abs(delta) \u0026gt; cfs_rq-\u0026gt;tg_load_avg_contrib / 64) { 3173\tatomic_long_add(delta, \u0026amp;cfs_rq-\u0026gt;tg-\u0026gt;load_avg); 3174\tcfs_rq-\u0026gt;tg_load_avg_contrib = cfs_rq-\u0026gt;avg.load_avg; 3175\t} 3176} 3177 3178/* 3179* Called within set_task_rq() right before setting a task\u0026#39;s CPU. The 3180* caller only guarantees p-\u0026gt;pi_lock is held; no other assumptions, 3181* including the state of rq-\u0026gt;lock, should be made. 3182*/ 3183void set_task_rq_fair(struct sched_entity *se, 3184\tstruct cfs_rq *prev, struct cfs_rq *next) 3185{ 3186\tu64 p_last_update_time; 3187\tu64 n_last_update_time; 3188 3189\tif (!sched_feat(ATTACH_AGE_LOAD)) 3190\treturn; 3191 3192\t/* 3193* We are supposed to update the task to \u0026#34;current\u0026#34; time, then its up to 3194* date and ready to go to new CPU/cfs_rq. But we have difficulty in 3195* getting what current time is, so simply throw away the out-of-date 3196* time. This will result in the wakee task is less decayed, but giving 3197* the wakee more load sounds not bad. 3198*/ 3199\tif (!(se-\u0026gt;avg.last_update_time \u0026amp;\u0026amp; prev)) 3200\treturn; 3201 3202#ifndef CONFIG_64BIT 3203\t{ 3204\tu64 p_last_update_time_copy; 3205\tu64 n_last_update_time_copy; 3206 3207\tdo { 3208\tp_last_update_time_copy = prev-\u0026gt;load_last_update_time_copy; 3209\tn_last_update_time_copy = next-\u0026gt;load_last_update_time_copy; 3210 3211\tsmp_rmb(); 3212 3213\tp_last_update_time = prev-\u0026gt;avg.last_update_time; 3214\tn_last_update_time = next-\u0026gt;avg.last_update_time; 3215 3216\t} while (p_last_update_time != p_last_update_time_copy || 3217\tn_last_update_time != n_last_update_time_copy); 3218\t} 3219#else 3220\tp_last_update_time = prev-\u0026gt;avg.last_update_time; 3221\tn_last_update_time = next-\u0026gt;avg.last_update_time; 3222#endif 3223\t__update_load_avg_blocked_se(p_last_update_time, cpu_of(rq_of(prev)), se); 3224\tse-\u0026gt;avg.last_update_time = n_last_update_time; 3225} 3226 3227 3228/* 3229* When on migration a sched_entity joins/leaves the PELT hierarchy, we need to 3230* propagate its contribution. The key to this propagation is the invariant 3231* that for each group: 3232* 3233* ge-\u0026gt;avg == grq-\u0026gt;avg\t(1) 3234* 3235* _IFF_ we look at the pure running and runnable sums. Because they 3236* represent the very same entity, just at different points in the hierarchy. 3237* 3238* Per the above update_tg_cfs_util() is trivial and simply copies the running 3239* sum over (but still wrong, because the group entity and group rq do not have 3240* their PELT windows aligned). 3241* 3242* However, update_tg_cfs_runnable() is more complex. So we have: 3243* 3244* ge-\u0026gt;avg.load_avg = ge-\u0026gt;load.weight * ge-\u0026gt;avg.runnable_avg\t(2) 3245* 3246* And since, like util, the runnable part should be directly transferable, 3247* the following would _appear_ to be the straight forward approach: 3248* 3249* grq-\u0026gt;avg.load_avg = grq-\u0026gt;load.weight * grq-\u0026gt;avg.runnable_avg\t(3) 3250* 3251* And per (1) we have: 3252* 3253* ge-\u0026gt;avg.runnable_avg == grq-\u0026gt;avg.runnable_avg 3254* 3255* Which gives: 3256* 3257* ge-\u0026gt;load.weight * grq-\u0026gt;avg.load_avg 3258* ge-\u0026gt;avg.load_avg = -----------------------------------\t(4) 3259* grq-\u0026gt;load.weight 3260* 3261* Except that is wrong! 3262* 3263* Because while for entities historical weight is not important and we 3264* really only care about our future and therefore can consider a pure 3265* runnable sum, runqueues can NOT do this. 3266* 3267* We specifically want runqueues to have a load_avg that includes 3268* historical weights. Those represent the blocked load, the load we expect 3269* to (shortly) return to us. This only works by keeping the weights as 3270* integral part of the sum. We therefore cannot decompose as per (3). 3271* 3272* Another reason this doesn\u0026#39;t work is that runnable isn\u0026#39;t a 0-sum entity. 3273* Imagine a rq with 2 tasks that each are runnable 2/3 of the time. Then the 3274* rq itself is runnable anywhere between 2/3 and 1 depending on how the 3275* runnable section of these tasks overlap (or not). If they were to perfectly 3276* align the rq as a whole would be runnable 2/3 of the time. If however we 3277* always have at least 1 runnable task, the rq as a whole is always runnable. 3278* 3279* So we\u0026#39;ll have to approximate.. :/ 3280* 3281* Given the constraint: 3282* 3283* ge-\u0026gt;avg.running_sum \u0026lt;= ge-\u0026gt;avg.runnable_sum \u0026lt;= LOAD_AVG_MAX 3284* 3285* We can construct a rule that adds runnable to a rq by assuming minimal 3286* overlap. 3287* 3288* On removal, we\u0026#39;ll assume each task is equally runnable; which yields: 3289* 3290* grq-\u0026gt;avg.runnable_sum = grq-\u0026gt;avg.load_sum / grq-\u0026gt;load.weight 3291* 3292* XXX: only do this for the part of runnable \u0026gt; running ? 3293* 3294*/ 3295 3296static inline void 3297update_tg_cfs_util(struct cfs_rq *cfs_rq, struct sched_entity *se, struct cfs_rq *gcfs_rq) 3298{ 3299\tlong delta = gcfs_rq-\u0026gt;avg.util_avg - se-\u0026gt;avg.util_avg; 3300 3301\t/* Nothing to update */ 3302\tif (!delta) 3303\treturn; 3304 3305\t/* 3306* The relation between sum and avg is: 3307* 3308* LOAD_AVG_MAX - 1024 + sa-\u0026gt;period_contrib 3309* 3310* however, the PELT windows are not aligned between grq and gse. 3311*/ 3312 3313\t/* Set new sched_entity\u0026#39;s utilization */ 3314\tse-\u0026gt;avg.util_avg = gcfs_rq-\u0026gt;avg.util_avg; 3315\tse-\u0026gt;avg.util_sum = se-\u0026gt;avg.util_avg * LOAD_AVG_MAX; 3316 3317\t/* Update parent cfs_rq utilization */ 3318\tadd_positive(\u0026amp;cfs_rq-\u0026gt;avg.util_avg, delta); 3319\tcfs_rq-\u0026gt;avg.util_sum = cfs_rq-\u0026gt;avg.util_avg * LOAD_AVG_MAX; 3320} 3321 3322static inline void 3323update_tg_cfs_runnable(struct cfs_rq *cfs_rq, struct sched_entity *se, struct cfs_rq *gcfs_rq) 3324{ 3325\tlong delta_avg, running_sum, runnable_sum = gcfs_rq-\u0026gt;prop_runnable_sum; 3326\tunsigned long runnable_load_avg, load_avg; 3327\tu64 runnable_load_sum, load_sum = 0; 3328\ts64 delta_sum; 3329 3330\tif (!runnable_sum) 3331\treturn; 3332 3333\tgcfs_rq-\u0026gt;prop_runnable_sum = 0; 3334 3335\tif (runnable_sum \u0026gt;= 0) { 3336\t/* 3337* Add runnable; clip at LOAD_AVG_MAX. Reflects that until 3338* the CPU is saturated running == runnable. 3339*/ 3340\trunnable_sum += se-\u0026gt;avg.load_sum; 3341\trunnable_sum = min(runnable_sum, (long)LOAD_AVG_MAX); 3342\t} else { 3343\t/* 3344* Estimate the new unweighted runnable_sum of the gcfs_rq by 3345* assuming all tasks are equally runnable. 3346*/ 3347\tif (scale_load_down(gcfs_rq-\u0026gt;load.weight)) { 3348\tload_sum = div_s64(gcfs_rq-\u0026gt;avg.load_sum, 3349\tscale_load_down(gcfs_rq-\u0026gt;load.weight)); 3350\t} 3351 3352\t/* But make sure to not inflate se\u0026#39;s runnable */ 3353\trunnable_sum = min(se-\u0026gt;avg.load_sum, load_sum); 3354\t} 3355 3356\t/* 3357* runnable_sum can\u0026#39;t be lower than running_sum 3358* As running sum is scale with CPU capacity wehreas the runnable sum 3359* is not we rescale running_sum 1st 3360*/ 3361\trunning_sum = se-\u0026gt;avg.util_sum / 3362\tarch_scale_cpu_capacity(NULL, cpu_of(rq_of(cfs_rq))); 3363\trunnable_sum = max(runnable_sum, running_sum); 3364 3365\tload_sum = (s64)se_weight(se) * runnable_sum; 3366\tload_avg = div_s64(load_sum, LOAD_AVG_MAX); 3367 3368\tdelta_sum = load_sum - (s64)se_weight(se) * se-\u0026gt;avg.load_sum; 3369\tdelta_avg = load_avg - se-\u0026gt;avg.load_avg; 3370 3371\tse-\u0026gt;avg.load_sum = runnable_sum; 3372\tse-\u0026gt;avg.load_avg = load_avg; 3373\tadd_positive(\u0026amp;cfs_rq-\u0026gt;avg.load_avg, delta_avg); 3374\tadd_positive(\u0026amp;cfs_rq-\u0026gt;avg.load_sum, delta_sum); 3375 3376\trunnable_load_sum = (s64)se_runnable(se) * runnable_sum; 3377\trunnable_load_avg = div_s64(runnable_load_sum, LOAD_AVG_MAX); 3378\tdelta_sum = runnable_load_sum - se_weight(se) * se-\u0026gt;avg.runnable_load_sum; 3379\tdelta_avg = runnable_load_avg - se-\u0026gt;avg.runnable_load_avg; 3380 3381\tse-\u0026gt;avg.runnable_load_sum = runnable_sum; 3382\tse-\u0026gt;avg.runnable_load_avg = runnable_load_avg; 3383 3384\tif (se-\u0026gt;on_rq) { 3385\tadd_positive(\u0026amp;cfs_rq-\u0026gt;avg.runnable_load_avg, delta_avg); 3386\tadd_positive(\u0026amp;cfs_rq-\u0026gt;avg.runnable_load_sum, delta_sum); 3387\t} 3388} 3389 3390static inline void add_tg_cfs_propagate(struct cfs_rq *cfs_rq, long runnable_sum) 3391{ 3392\tcfs_rq-\u0026gt;propagate = 1; 3393\tcfs_rq-\u0026gt;prop_runnable_sum += runnable_sum; 3394} 3395 3396/* Update task and its cfs_rq load average */ 3397static inline int propagate_entity_load_avg(struct sched_entity *se) 3398{ 3399\tstruct cfs_rq *cfs_rq, *gcfs_rq; 3400 3401\tif (entity_is_task(se)) 3402\treturn 0; 3403 3404\tgcfs_rq = group_cfs_rq(se); 3405\tif (!gcfs_rq-\u0026gt;propagate) 3406\treturn 0; 3407 3408\tgcfs_rq-\u0026gt;propagate = 0; 3409 3410\tcfs_rq = cfs_rq_of(se); 3411 3412\tadd_tg_cfs_propagate(cfs_rq, gcfs_rq-\u0026gt;prop_runnable_sum); 3413 3414\tupdate_tg_cfs_util(cfs_rq, se, gcfs_rq); 3415\tupdate_tg_cfs_runnable(cfs_rq, se, gcfs_rq); 3416 3417\treturn 1; 3418} 3419 3420/* 3421* Check if we need to update the load and the utilization of a blocked 3422* group_entity: 3423*/ 3424static inline bool skip_blocked_update(struct sched_entity *se) 3425{ 3426\tstruct cfs_rq *gcfs_rq = group_cfs_rq(se); 3427 3428\t/* 3429* If sched_entity still have not zero load or utilization, we have to 3430* decay it: 3431*/ 3432\tif (se-\u0026gt;avg.load_avg || se-\u0026gt;avg.util_avg) 3433\treturn false; 3434 3435\t/* 3436* If there is a pending propagation, we have to update the load and 3437* the utilization of the sched_entity: 3438*/ 3439\tif (gcfs_rq-\u0026gt;propagate) 3440\treturn false; 3441 3442\t/* 3443* Otherwise, the load and the utilization of the sched_entity is 3444* already zero and there is no pending propagation, so it will be a 3445* waste of time to try to decay it: 3446*/ 3447\treturn true; 3448} 3449 3450#else /* CONFIG_FAIR_GROUP_SCHED */3451 3452static inline void update_tg_load_avg(struct cfs_rq *cfs_rq, int force) {} 3453 3454static inline int propagate_entity_load_avg(struct sched_entity *se) 3455{ 3456\treturn 0; 3457} 3458 3459static inline void add_tg_cfs_propagate(struct cfs_rq *cfs_rq, long runnable_sum) {} 3460 3461#endif /* CONFIG_FAIR_GROUP_SCHED */3462 3463/** 3464* update_cfs_rq_load_avg - update the cfs_rq\u0026#39;s load/util averages 3465* @now: current time, as per cfs_rq_clock_task() 3466* @cfs_rq: cfs_rq to update 3467* 3468* The cfs_rq avg is the direct sum of all its entities (blocked and runnable) 3469* avg. The immediate corollary is that all (fair) tasks must be attached, see 3470* post_init_entity_util_avg(). 3471* 3472* cfs_rq-\u0026gt;avg is used for task_h_load() and update_cfs_share() for example. 3473* 3474* Returns true if the load decayed or we removed load. 3475* 3476* Since both these conditions indicate a changed cfs_rq-\u0026gt;avg.load we should 3477* call update_tg_load_avg() when this function returns true. 3478*/ 3479static inline int 3480update_cfs_rq_load_avg(u64 now, struct cfs_rq *cfs_rq) 3481{ 3482\tunsigned long removed_load = 0, removed_util = 0, removed_runnable_sum = 0; 3483\tstruct sched_avg *sa = \u0026amp;cfs_rq-\u0026gt;avg; 3484\tint decayed = 0; 3485 3486\tif (cfs_rq-\u0026gt;removed.nr) { 3487\tunsigned long r; 3488\tu32 divider = LOAD_AVG_MAX - 1024 + sa-\u0026gt;period_contrib; 3489 3490\traw_spin_lock(\u0026amp;cfs_rq-\u0026gt;removed.lock); 3491\tswap(cfs_rq-\u0026gt;removed.util_avg, removed_util); 3492\tswap(cfs_rq-\u0026gt;removed.load_avg, removed_load); 3493\tswap(cfs_rq-\u0026gt;removed.runnable_sum, removed_runnable_sum); 3494\tcfs_rq-\u0026gt;removed.nr = 0; 3495\traw_spin_unlock(\u0026amp;cfs_rq-\u0026gt;removed.lock); 3496 3497\tr = removed_load; 3498\tsub_positive(\u0026amp;sa-\u0026gt;load_avg, r); 3499\tsub_positive(\u0026amp;sa-\u0026gt;load_sum, r * divider); 3500 3501\tr = removed_util; 3502\tsub_positive(\u0026amp;sa-\u0026gt;util_avg, r); 3503\tsub_positive(\u0026amp;sa-\u0026gt;util_sum, r * divider); 3504 3505\tadd_tg_cfs_propagate(cfs_rq, -(long)removed_runnable_sum); 3506 3507\tdecayed = 1; 3508\t} 3509 3510\tdecayed |= __update_load_avg_cfs_rq(now, cpu_of(rq_of(cfs_rq)), cfs_rq); 3511 3512#ifndef CONFIG_64BIT 3513\tsmp_wmb(); 3514\tcfs_rq-\u0026gt;load_last_update_time_copy = sa-\u0026gt;last_update_time; 3515#endif 3516 3517\tif (decayed) 3518\tcfs_rq_util_change(cfs_rq, 0); 3519 3520\treturn decayed; 3521} 3522 3523/** 3524* attach_entity_load_avg - attach this entity to its cfs_rq load avg 3525* @cfs_rq: cfs_rq to attach to 3526* @se: sched_entity to attach 3527* @flags: migration hints 3528* 3529* Must call update_cfs_rq_load_avg() before this, since we rely on 3530* cfs_rq-\u0026gt;avg.last_update_time being current. 3531*/ 3532static void attach_entity_load_avg(struct cfs_rq *cfs_rq, struct sched_entity *se, int flags) 3533{ 3534\tu32 divider = LOAD_AVG_MAX - 1024 + cfs_rq-\u0026gt;avg.period_contrib; 3535 3536\t/* 3537* When we attach the @se to the @cfs_rq, we must align the decay 3538* window because without that, really weird and wonderful things can 3539* happen. 3540* 3541* XXX illustrate 3542*/ 3543\tse-\u0026gt;avg.last_update_time = cfs_rq-\u0026gt;avg.last_update_time; 3544\tse-\u0026gt;avg.period_contrib = cfs_rq-\u0026gt;avg.period_contrib; 3545 3546\t/* 3547* Hell(o) Nasty stuff.. we need to recompute _sum based on the new 3548* period_contrib. This isn\u0026#39;t strictly correct, but since we\u0026#39;re 3549* entirely outside of the PELT hierarchy, nobody cares if we truncate 3550* _sum a little. 3551*/ 3552\tse-\u0026gt;avg.util_sum = se-\u0026gt;avg.util_avg * divider; 3553 3554\tse-\u0026gt;avg.load_sum = divider; 3555\tif (se_weight(se)) { 3556\tse-\u0026gt;avg.load_sum = 3557\tdiv_u64(se-\u0026gt;avg.load_avg * se-\u0026gt;avg.load_sum, se_weight(se)); 3558\t} 3559 3560\tse-\u0026gt;avg.runnable_load_sum = se-\u0026gt;avg.load_sum; 3561 3562\tenqueue_load_avg(cfs_rq, se); 3563\tcfs_rq-\u0026gt;avg.util_avg += se-\u0026gt;avg.util_avg; 3564\tcfs_rq-\u0026gt;avg.util_sum += se-\u0026gt;avg.util_sum; 3565 3566\tadd_tg_cfs_propagate(cfs_rq, se-\u0026gt;avg.load_sum); 3567 3568\tcfs_rq_util_change(cfs_rq, flags); 3569} 3570 3571/** 3572* detach_entity_load_avg - detach this entity from its cfs_rq load avg 3573* @cfs_rq: cfs_rq to detach from 3574* @se: sched_entity to detach 3575* 3576* Must call update_cfs_rq_load_avg() before this, since we rely on 3577* cfs_rq-\u0026gt;avg.last_update_time being current. 3578*/ 3579static void detach_entity_load_avg(struct cfs_rq *cfs_rq, struct sched_entity *se) 3580{ 3581\tdequeue_load_avg(cfs_rq, se); 3582\tsub_positive(\u0026amp;cfs_rq-\u0026gt;avg.util_avg, se-\u0026gt;avg.util_avg); 3583\tsub_positive(\u0026amp;cfs_rq-\u0026gt;avg.util_sum, se-\u0026gt;avg.util_sum); 3584 3585\tadd_tg_cfs_propagate(cfs_rq, -se-\u0026gt;avg.load_sum); 3586 3587\tcfs_rq_util_change(cfs_rq, 0); 3588} 3589 3590/* 3591* Optional action to be done while updating the load average 3592*/ 3593#define UPDATE_TG\t0x1 3594#define SKIP_AGE_LOAD\t0x2 3595#define DO_ATTACH\t0x4 3596 3597/* Update task and its cfs_rq load average */ 3598static inline void update_load_avg(struct cfs_rq *cfs_rq, struct sched_entity *se, int flags) 3599{ 3600\tu64 now = cfs_rq_clock_task(cfs_rq); 3601\tstruct rq *rq = rq_of(cfs_rq); 3602\tint cpu = cpu_of(rq); 3603\tint decayed; 3604 3605\t/* 3606* Track task load average for carrying it to new CPU after migrated, and 3607* track group sched_entity load average for task_h_load calc in migration 3608*/ 3609\tif (se-\u0026gt;avg.last_update_time \u0026amp;\u0026amp; !(flags \u0026amp; SKIP_AGE_LOAD)) 3610\t__update_load_avg_se(now, cpu, cfs_rq, se); 3611 3612\tdecayed = update_cfs_rq_load_avg(now, cfs_rq); 3613\tdecayed |= propagate_entity_load_avg(se); 3614 3615\tif (!se-\u0026gt;avg.last_update_time \u0026amp;\u0026amp; (flags \u0026amp; DO_ATTACH)) { 3616 3617\t/* 3618* DO_ATTACH means we\u0026#39;re here from enqueue_entity(). 3619* !last_update_time means we\u0026#39;ve passed through 3620* migrate_task_rq_fair() indicating we migrated. 3621* 3622* IOW we\u0026#39;re enqueueing a task on a new CPU. 3623*/ 3624\tattach_entity_load_avg(cfs_rq, se, SCHED_CPUFREQ_MIGRATION); 3625\tupdate_tg_load_avg(cfs_rq, 0); 3626 3627\t} else if (decayed \u0026amp;\u0026amp; (flags \u0026amp; UPDATE_TG)) 3628\tupdate_tg_load_avg(cfs_rq, 0); 3629} 3630 3631#ifndef CONFIG_64BIT 3632static inline u64 cfs_rq_last_update_time(struct cfs_rq *cfs_rq) 3633{ 3634\tu64 last_update_time_copy; 3635\tu64 last_update_time; 3636 3637\tdo { 3638\tlast_update_time_copy = cfs_rq-\u0026gt;load_last_update_time_copy; 3639\tsmp_rmb(); 3640\tlast_update_time = cfs_rq-\u0026gt;avg.last_update_time; 3641\t} while (last_update_time != last_update_time_copy); 3642 3643\treturn last_update_time; 3644} 3645#else 3646static inline u64 cfs_rq_last_update_time(struct cfs_rq *cfs_rq) 3647{ 3648\treturn cfs_rq-\u0026gt;avg.last_update_time; 3649} 3650#endif 3651 3652/* 3653* Synchronize entity load avg of dequeued entity without locking 3654* the previous rq. 3655*/ 3656void sync_entity_load_avg(struct sched_entity *se) 3657{ 3658\tstruct cfs_rq *cfs_rq = cfs_rq_of(se); 3659\tu64 last_update_time; 3660 3661\tlast_update_time = cfs_rq_last_update_time(cfs_rq); 3662\t__update_load_avg_blocked_se(last_update_time, cpu_of(rq_of(cfs_rq)), se); 3663} 3664 3665/* 3666* Task first catches up with cfs_rq, and then subtract 3667* itself from the cfs_rq (task must be off the queue now). 3668*/ 3669void remove_entity_load_avg(struct sched_entity *se) 3670{ 3671\tstruct cfs_rq *cfs_rq = cfs_rq_of(se); 3672\tunsigned long flags; 3673 3674\t/* 3675* tasks cannot exit without having gone through wake_up_new_task() -\u0026gt; 3676* post_init_entity_util_avg() which will have added things to the 3677* cfs_rq, so we can remove unconditionally. 3678* 3679* Similarly for groups, they will have passed through 3680* post_init_entity_util_avg() before unregister_sched_fair_group() 3681* calls this. 3682*/ 3683 3684\tsync_entity_load_avg(se); 3685 3686\traw_spin_lock_irqsave(\u0026amp;cfs_rq-\u0026gt;removed.lock, flags); 3687\t++cfs_rq-\u0026gt;removed.nr; 3688\tcfs_rq-\u0026gt;removed.util_avg\t+= se-\u0026gt;avg.util_avg; 3689\tcfs_rq-\u0026gt;removed.load_avg\t+= se-\u0026gt;avg.load_avg; 3690\tcfs_rq-\u0026gt;removed.runnable_sum\t+= se-\u0026gt;avg.load_sum; /* == runnable_sum */ 3691\traw_spin_unlock_irqrestore(\u0026amp;cfs_rq-\u0026gt;removed.lock, flags); 3692} 3693 3694static inline unsigned long cfs_rq_runnable_load_avg(struct cfs_rq *cfs_rq) 3695{ 3696\treturn cfs_rq-\u0026gt;avg.runnable_load_avg; 3697} 3698 3699static inline unsigned long cfs_rq_load_avg(struct cfs_rq *cfs_rq) 3700{ 3701\treturn cfs_rq-\u0026gt;avg.load_avg; 3702} 3703 3704static int idle_balance(struct rq *this_rq, struct rq_flags *rf); 3705 3706static inline unsigned long task_util(struct task_struct *p) 3707{ 3708\treturn READ_ONCE(p-\u0026gt;se.avg.util_avg); 3709} 3710 3711static inline unsigned long _task_util_est(struct task_struct *p) 3712{ 3713\tstruct util_est ue = READ_ONCE(p-\u0026gt;se.avg.util_est); 3714 3715\treturn max(ue.ewma, ue.enqueued); 3716} 3717 3718static inline unsigned long task_util_est(struct task_struct *p) 3719{ 3720\treturn max(task_util(p), _task_util_est(p)); 3721} 3722 3723static inline void util_est_enqueue(struct cfs_rq *cfs_rq, 3724\tstruct task_struct *p) 3725{ 3726\tunsigned int enqueued; 3727 3728\tif (!sched_feat(UTIL_EST)) 3729\treturn; 3730 3731\t/* Update root cfs_rq\u0026#39;s estimated utilization */ 3732\tenqueued = cfs_rq-\u0026gt;avg.util_est.enqueued; 3733\tenqueued += (_task_util_est(p) | UTIL_AVG_UNCHANGED); 3734\tWRITE_ONCE(cfs_rq-\u0026gt;avg.util_est.enqueued, enqueued); 3735} 3736 3737/* 3738* Check if a (signed) value is within a specified (unsigned) margin, 3739* based on the observation that: 3740* 3741* abs(x) \u0026lt; y := (unsigned)(x + y - 1) \u0026lt; (2 * y - 1) 3742* 3743* NOTE: this only works when value + maring \u0026lt; INT_MAX. 3744*/ 3745static inline bool within_margin(int value, int margin) 3746{ 3747\treturn ((unsigned int)(value + margin - 1) \u0026lt; (2 * margin - 1)); 3748} 3749 3750static void 3751util_est_dequeue(struct cfs_rq *cfs_rq, struct task_struct *p, bool task_sleep) 3752{ 3753\tlong last_ewma_diff; 3754\tstruct util_est ue; 3755 3756\tif (!sched_feat(UTIL_EST)) 3757\treturn; 3758 3759\t/* Update root cfs_rq\u0026#39;s estimated utilization */ 3760\tue.enqueued = cfs_rq-\u0026gt;avg.util_est.enqueued; 3761\tue.enqueued -= min_t(unsigned int, ue.enqueued, 3762\t(_task_util_est(p) | UTIL_AVG_UNCHANGED)); 3763\tWRITE_ONCE(cfs_rq-\u0026gt;avg.util_est.enqueued, ue.enqueued); 3764 3765\t/* 3766* Skip update of task\u0026#39;s estimated utilization when the task has not 3767* yet completed an activation, e.g. being migrated. 3768*/ 3769\tif (!task_sleep) 3770\treturn; 3771 3772\t/* 3773* If the PELT values haven\u0026#39;t changed since enqueue time, 3774* skip the util_est update. 3775*/ 3776\tue = p-\u0026gt;se.avg.util_est; 3777\tif (ue.enqueued \u0026amp; UTIL_AVG_UNCHANGED) 3778\treturn; 3779 3780\t/* 3781* Skip update of task\u0026#39;s estimated utilization when its EWMA is 3782* already ~1% close to its last activation value. 3783*/ 3784\tue.enqueued = (task_util(p) | UTIL_AVG_UNCHANGED); 3785\tlast_ewma_diff = ue.enqueued - ue.ewma; 3786\tif (within_margin(last_ewma_diff, (SCHED_CAPACITY_SCALE / 100))) 3787\treturn; 3788 3789\t/* 3790* Update Task\u0026#39;s estimated utilization 3791* 3792* When *p completes an activation we can consolidate another sample 3793* of the task size. This is done by storing the current PELT value 3794* as ue.enqueued and by using this value to update the Exponential 3795* Weighted Moving Average (EWMA): 3796* 3797* ewma(t) = w * task_util(p) + (1-w) * ewma(t-1) 3798* = w * task_util(p) + ewma(t-1) - w * ewma(t-1) 3799* = w * (task_util(p) - ewma(t-1)) + ewma(t-1) 3800* = w * ( last_ewma_diff ) + ewma(t-1) 3801* = w * (last_ewma_diff + ewma(t-1) / w) 3802* 3803* Where \u0026#39;w\u0026#39; is the weight of new samples, which is configured to be 3804* 0.25, thus making w=1/4 ( \u0026gt;\u0026gt;= UTIL_EST_WEIGHT_SHIFT) 3805*/ 3806\tue.ewma \u0026lt;\u0026lt;= UTIL_EST_WEIGHT_SHIFT; 3807\tue.ewma += last_ewma_diff; 3808\tue.ewma \u0026gt;\u0026gt;= UTIL_EST_WEIGHT_SHIFT; 3809\tWRITE_ONCE(p-\u0026gt;se.avg.util_est, ue); 3810} 3811 3812#else /* CONFIG_SMP */3813 3814#define UPDATE_TG\t0x0 3815#define SKIP_AGE_LOAD\t0x0 3816#define DO_ATTACH\t0x0 3817 3818static inline void update_load_avg(struct cfs_rq *cfs_rq, struct sched_entity *se, int not_used1) 3819{ 3820\tcfs_rq_util_change(cfs_rq, 0); 3821} 3822 3823static inline void remove_entity_load_avg(struct sched_entity *se) {} 3824 3825static inline void 3826attach_entity_load_avg(struct cfs_rq *cfs_rq, struct sched_entity *se, int flags) {} 3827static inline void 3828detach_entity_load_avg(struct cfs_rq *cfs_rq, struct sched_entity *se) {} 3829 3830static inline int idle_balance(struct rq *rq, struct rq_flags *rf) 3831{ 3832\treturn 0; 3833} 3834 3835static inline void 3836util_est_enqueue(struct cfs_rq *cfs_rq, struct task_struct *p) {} 3837 3838static inline void 3839util_est_dequeue(struct cfs_rq *cfs_rq, struct task_struct *p, 3840\tbool task_sleep) {} 3841 3842#endif /* CONFIG_SMP */3843 3844static void check_spread(struct cfs_rq *cfs_rq, struct sched_entity *se) 3845{ 3846#ifdef CONFIG_SCHED_DEBUG 3847\ts64 d = se-\u0026gt;vruntime - cfs_rq-\u0026gt;min_vruntime; 3848 3849\tif (d \u0026lt; 0) 3850\td = -d; 3851 3852\tif (d \u0026gt; 3*sysctl_sched_latency) 3853\tschedstat_inc(cfs_rq-\u0026gt;nr_spread_over); 3854#endif 3855} 3856 3857static void 3858place_entity(struct cfs_rq *cfs_rq, struct sched_entity *se, int initial) 3859{ 3860\tu64 vruntime = cfs_rq-\u0026gt;min_vruntime; 3861 3862\t/* 3863* The \u0026#39;current\u0026#39; period is already promised to the current tasks, 3864* however the extra weight of the new task will slow them down a 3865* little, place the new task so that it fits in the slot that 3866* stays open at the end. 3867*/ 3868\tif (initial \u0026amp;\u0026amp; sched_feat(START_DEBIT)) 3869\tvruntime += sched_vslice(cfs_rq, se); 3870 3871\t/* sleeps up to a single latency don\u0026#39;t count. */ 3872\tif (!initial) { 3873\tunsigned long thresh = sysctl_sched_latency; 3874 3875\t/* 3876* Halve their sleep time\u0026#39;s effect, to allow 3877* for a gentler effect of sleepers: 3878*/ 3879\tif (sched_feat(GENTLE_FAIR_SLEEPERS)) 3880\tthresh \u0026gt;\u0026gt;= 1; 3881 3882\tvruntime -= thresh; 3883\t} 3884 3885\t/* ensure we never gain time by being placed backwards. */ 3886\tse-\u0026gt;vruntime = max_vruntime(se-\u0026gt;vruntime, vruntime); 3887} 3888 3889static void check_enqueue_throttle(struct cfs_rq *cfs_rq); 3890 3891static inline void check_schedstat_required(void) 3892{ 3893#ifdef CONFIG_SCHEDSTATS 3894\tif (schedstat_enabled()) 3895\treturn; 3896 3897\t/* Force schedstat enabled if a dependent tracepoint is active */ 3898\tif (trace_sched_stat_wait_enabled() || 3899\ttrace_sched_stat_sleep_enabled() || 3900\ttrace_sched_stat_iowait_enabled() || 3901\ttrace_sched_stat_blocked_enabled() || 3902\ttrace_sched_stat_runtime_enabled()) { 3903\tprintk_deferred_once(\u0026#34;Scheduler tracepoints stat_sleep, stat_iowait, \u0026#34; 3904\t\u0026#34;stat_blocked and stat_runtime require the \u0026#34; 3905\t\u0026#34;kernel parameter schedstats=enable or \u0026#34; 3906\t\u0026#34;kernel.sched_schedstats=1\\n\u0026#34;); 3907\t} 3908#endif 3909} 3910 3911 3912/* 3913* MIGRATION 3914* 3915*\tdequeue 3916*\tupdate_curr() 3917*\tupdate_min_vruntime() 3918*\tvruntime -= min_vruntime 3919* 3920*\tenqueue 3921*\tupdate_curr() 3922*\tupdate_min_vruntime() 3923*\tvruntime += min_vruntime 3924* 3925* this way the vruntime transition between RQs is done when both 3926* min_vruntime are up-to-date. 3927* 3928* WAKEUP (remote) 3929* 3930*\t-\u0026gt;migrate_task_rq_fair() (p-\u0026gt;state == TASK_WAKING) 3931*\tvruntime -= min_vruntime 3932* 3933*\tenqueue 3934*\tupdate_curr() 3935*\tupdate_min_vruntime() 3936*\tvruntime += min_vruntime 3937* 3938* this way we don\u0026#39;t have the most up-to-date min_vruntime on the originating 3939* CPU and an up-to-date min_vruntime on the destination CPU. 3940*/ 3941 3942static void 3943enqueue_entity(struct cfs_rq *cfs_rq, struct sched_entity *se, int flags) 3944{ 3945\tbool renorm = !(flags \u0026amp; ENQUEUE_WAKEUP) || (flags \u0026amp; ENQUEUE_MIGRATED); 3946\tbool curr = cfs_rq-\u0026gt;curr == se; 3947 3948\t/* 3949* If we\u0026#39;re the current task, we must renormalise before calling 3950* update_curr(). 3951*/ 3952\tif (renorm \u0026amp;\u0026amp; curr) 3953\tse-\u0026gt;vruntime += cfs_rq-\u0026gt;min_vruntime; 3954 3955\tupdate_curr(cfs_rq); 3956 3957\t/* 3958* Otherwise, renormalise after, such that we\u0026#39;re placed at the current 3959* moment in time, instead of some random moment in the past. Being 3960* placed in the past could significantly boost this task to the 3961* fairness detriment of existing tasks. 3962*/ 3963\tif (renorm \u0026amp;\u0026amp; !curr) 3964\tse-\u0026gt;vruntime += cfs_rq-\u0026gt;min_vruntime; 3965 3966\t/* 3967* When enqueuing a sched_entity, we must: 3968* - Update loads to have both entity and cfs_rq synced with now. 3969* - Add its load to cfs_rq-\u0026gt;runnable_avg 3970* - For group_entity, update its weight to reflect the new share of 3971* its group cfs_rq 3972* - Add its new weight to cfs_rq-\u0026gt;load.weight 3973*/ 3974\tupdate_load_avg(cfs_rq, se, UPDATE_TG | DO_ATTACH); 3975\tupdate_cfs_group(se); 3976\tenqueue_runnable_load_avg(cfs_rq, se); 3977\taccount_entity_enqueue(cfs_rq, se); 3978 3979\tif (flags \u0026amp; ENQUEUE_WAKEUP) 3980\tplace_entity(cfs_rq, se, 0); 3981 3982\tcheck_schedstat_required(); 3983\tupdate_stats_enqueue(cfs_rq, se, flags); 3984\tcheck_spread(cfs_rq, se); 3985\tif (!curr) 3986\t__enqueue_entity(cfs_rq, se); 3987\tse-\u0026gt;on_rq = 1; 3988 3989\tif (cfs_rq-\u0026gt;nr_running == 1) { 3990\tlist_add_leaf_cfs_rq(cfs_rq); 3991\tcheck_enqueue_throttle(cfs_rq); 3992\t} 3993} 3994 3995static void __clear_buddies_last(struct sched_entity *se) 3996{ 3997\tfor_each_sched_entity(se) { 3998\tstruct cfs_rq *cfs_rq = cfs_rq_of(se); 3999\tif (cfs_rq-\u0026gt;last != se) 4000\tbreak; 4001 4002\tcfs_rq-\u0026gt;last = NULL; 4003\t} 4004} 4005 4006static void __clear_buddies_next(struct sched_entity *se) 4007{ 4008\tfor_each_sched_entity(se) { 4009\tstruct cfs_rq *cfs_rq = cfs_rq_of(se); 4010\tif (cfs_rq-\u0026gt;next != se) 4011\tbreak; 4012 4013\tcfs_rq-\u0026gt;next = NULL; 4014\t} 4015} 4016 4017static void __clear_buddies_skip(struct sched_entity *se) 4018{ 4019\tfor_each_sched_entity(se) { 4020\tstruct cfs_rq *cfs_rq = cfs_rq_of(se); 4021\tif (cfs_rq-\u0026gt;skip != se) 4022\tbreak; 4023 4024\tcfs_rq-\u0026gt;skip = NULL; 4025\t} 4026} 4027 4028static void clear_buddies(struct cfs_rq *cfs_rq, struct sched_entity *se) 4029{ 4030\tif (cfs_rq-\u0026gt;last == se) 4031\t__clear_buddies_last(se); 4032 4033\tif (cfs_rq-\u0026gt;next == se) 4034\t__clear_buddies_next(se); 4035 4036\tif (cfs_rq-\u0026gt;skip == se) 4037\t__clear_buddies_skip(se); 4038} 4039 4040static __always_inline void return_cfs_rq_runtime(struct cfs_rq *cfs_rq); 4041 4042static void 4043dequeue_entity(struct cfs_rq *cfs_rq, struct sched_entity *se, int flags) 4044{ 4045\t/* 4046* Update run-time statistics of the \u0026#39;current\u0026#39;. 4047*/ 4048\tupdate_curr(cfs_rq); 4049 4050\t/* 4051* When dequeuing a sched_entity, we must: 4052* - Update loads to have both entity and cfs_rq synced with now. 4053* - Substract its load from the cfs_rq-\u0026gt;runnable_avg. 4054* - Substract its previous weight from cfs_rq-\u0026gt;load.weight. 4055* - For group entity, update its weight to reflect the new share 4056* of its group cfs_rq. 4057*/ 4058\tupdate_load_avg(cfs_rq, se, UPDATE_TG); 4059\tdequeue_runnable_load_avg(cfs_rq, se); 4060 4061\tupdate_stats_dequeue(cfs_rq, se, flags); 4062 4063\tclear_buddies(cfs_rq, se); 4064 4065\tif (se != cfs_rq-\u0026gt;curr) 4066\t__dequeue_entity(cfs_rq, se); 4067\tse-\u0026gt;on_rq = 0; 4068\taccount_entity_dequeue(cfs_rq, se); 4069 4070\t/* 4071* Normalize after update_curr(); which will also have moved 4072* min_vruntime if @se is the one holding it back. But before doing 4073* update_min_vruntime() again, which will discount @se\u0026#39;s position and 4074* can move min_vruntime forward still more. 4075*/ 4076\tif (!(flags \u0026amp; DEQUEUE_SLEEP)) 4077\tse-\u0026gt;vruntime -= cfs_rq-\u0026gt;min_vruntime; 4078 4079\t/* return excess runtime on last dequeue */ 4080\treturn_cfs_rq_runtime(cfs_rq); 4081 4082\tupdate_cfs_group(se); 4083 4084\t/* 4085* Now advance min_vruntime if @se was the entity holding it back, 4086* except when: DEQUEUE_SAVE \u0026amp;\u0026amp; !DEQUEUE_MOVE, in this case we\u0026#39;ll be 4087* put back on, and if we advance min_vruntime, we\u0026#39;ll be placed back 4088* further than we started -- ie. we\u0026#39;ll be penalized. 4089*/ 4090\tif ((flags \u0026amp; (DEQUEUE_SAVE | DEQUEUE_MOVE)) != DEQUEUE_SAVE) 4091\tupdate_min_vruntime(cfs_rq); 4092} 4093 4094/* 4095* Preempt the current task with a newly woken task if needed: 4096*/ 4097static void 4098check_preempt_tick(struct cfs_rq *cfs_rq, struct sched_entity *curr) 4099{ 4100\tunsigned long ideal_runtime, delta_exec; 4101\tstruct sched_entity *se; 4102\ts64 delta; 4103 4104\tideal_runtime = sched_slice(cfs_rq, curr); 4105\tdelta_exec = curr-\u0026gt;sum_exec_runtime - curr-\u0026gt;prev_sum_exec_runtime; 4106\tif (delta_exec \u0026gt; ideal_runtime) { 4107\tresched_curr(rq_of(cfs_rq)); 4108\t/* 4109* The current task ran long enough, ensure it doesn\u0026#39;t get 4110* re-elected due to buddy favours. 4111*/ 4112\tclear_buddies(cfs_rq, curr); 4113\treturn; 4114\t} 4115 4116\t/* 4117* Ensure that a task that missed wakeup preemption by a 4118* narrow margin doesn\u0026#39;t have to wait for a full slice. 4119* This also mitigates buddy induced latencies under load. 4120*/ 4121\tif (delta_exec \u0026lt; sysctl_sched_min_granularity) 4122\treturn; 4123 4124\tse = __pick_first_entity(cfs_rq); 4125\tdelta = curr-\u0026gt;vruntime - se-\u0026gt;vruntime; 4126 4127\tif (delta \u0026lt; 0) 4128\treturn; 4129 4130\tif (delta \u0026gt; ideal_runtime) 4131\tresched_curr(rq_of(cfs_rq)); 4132} 4133 4134static void 4135set_next_entity(struct cfs_rq *cfs_rq, struct sched_entity *se) 4136{ 4137\t/* \u0026#39;current\u0026#39; is not kept within the tree. */ 4138\tif (se-\u0026gt;on_rq) { 4139\t/* 4140* Any task has to be enqueued before it get to execute on 4141* a CPU. So account for the time it spent waiting on the 4142* runqueue. 4143*/ 4144\tupdate_stats_wait_end(cfs_rq, se); 4145\t__dequeue_entity(cfs_rq, se); 4146\tupdate_load_avg(cfs_rq, se, UPDATE_TG); 4147\t} 4148 4149\tupdate_stats_curr_start(cfs_rq, se); 4150\tcfs_rq-\u0026gt;curr = se; 4151 4152\t/* 4153* Track our maximum slice length, if the CPU\u0026#39;s load is at 4154* least twice that of our own weight (i.e. dont track it 4155* when there are only lesser-weight tasks around): 4156*/ 4157\tif (schedstat_enabled() \u0026amp;\u0026amp; rq_of(cfs_rq)-\u0026gt;load.weight \u0026gt;= 2*se-\u0026gt;load.weight) { 4158\tschedstat_set(se-\u0026gt;statistics.slice_max, 4159\tmax((u64)schedstat_val(se-\u0026gt;statistics.slice_max), 4160\tse-\u0026gt;sum_exec_runtime - se-\u0026gt;prev_sum_exec_runtime)); 4161\t} 4162 4163\tse-\u0026gt;prev_sum_exec_runtime = se-\u0026gt;sum_exec_runtime; 4164} 4165 4166static int 4167wakeup_preempt_entity(struct sched_entity *curr, struct sched_entity *se); 4168 4169/* 4170* Pick the next process, keeping these things in mind, in this order: 4171* 1) keep things fair between processes/task groups 4172* 2) pick the \u0026#34;next\u0026#34; process, since someone really wants that to run 4173* 3) pick the \u0026#34;last\u0026#34; process, for cache locality 4174* 4) do not run the \u0026#34;skip\u0026#34; process, if something else is available 4175*/ 4176static struct sched_entity * 4177pick_next_entity(struct cfs_rq *cfs_rq, struct sched_entity *curr) 4178{ 4179\tstruct sched_entity *left = __pick_first_entity(cfs_rq); 4180\tstruct sched_entity *se; 4181 4182\t/* 4183* If curr is set we have to see if its left of the leftmost entity 4184* still in the tree, provided there was anything in the tree at all. 4185*/ 4186\tif (!left || (curr \u0026amp;\u0026amp; entity_before(curr, left))) 4187\tleft = curr; 4188 4189\tse = left; /* ideally we run the leftmost entity */ 4190 4191\t/* 4192* Avoid running the skip buddy, if running something else can 4193* be done without getting too unfair. 4194*/ 4195\tif (cfs_rq-\u0026gt;skip == se) { 4196\tstruct sched_entity *second; 4197 4198\tif (se == curr) { 4199\tsecond = __pick_first_entity(cfs_rq); 4200\t} else { 4201\tsecond = __pick_next_entity(se); 4202\tif (!second || (curr \u0026amp;\u0026amp; entity_before(curr, second))) 4203\tsecond = curr; 4204\t} 4205 4206\tif (second \u0026amp;\u0026amp; wakeup_preempt_entity(second, left) \u0026lt; 1) 4207\tse = second; 4208\t} 4209 4210\t/* 4211* Prefer last buddy, try to return the CPU to a preempted task. 4212*/ 4213\tif (cfs_rq-\u0026gt;last \u0026amp;\u0026amp; wakeup_preempt_entity(cfs_rq-\u0026gt;last, left) \u0026lt; 1) 4214\tse = cfs_rq-\u0026gt;last; 4215 4216\t/* 4217* Someone really wants this to run. If it\u0026#39;s not unfair, run it. 4218*/ 4219\tif (cfs_rq-\u0026gt;next \u0026amp;\u0026amp; wakeup_preempt_entity(cfs_rq-\u0026gt;next, left) \u0026lt; 1) 4220\tse = cfs_rq-\u0026gt;next; 4221 4222\tclear_buddies(cfs_rq, se); 4223 4224\treturn se; 4225} 4226 4227static bool check_cfs_rq_runtime(struct cfs_rq *cfs_rq); 4228 4229static void put_prev_entity(struct cfs_rq *cfs_rq, struct sched_entity *prev) 4230{ 4231\t/* 4232* If still on the runqueue then deactivate_task() 4233* was not called and update_curr() has to be done: 4234*/ 4235\tif (prev-\u0026gt;on_rq) 4236\tupdate_curr(cfs_rq); 4237 4238\t/* throttle cfs_rqs exceeding runtime */ 4239\tcheck_cfs_rq_runtime(cfs_rq); 4240 4241\tcheck_spread(cfs_rq, prev); 4242 4243\tif (prev-\u0026gt;on_rq) { 4244\tupdate_stats_wait_start(cfs_rq, prev); 4245\t/* Put \u0026#39;current\u0026#39; back into the tree. */ 4246\t__enqueue_entity(cfs_rq, prev); 4247\t/* in !on_rq case, update occurred at dequeue */ 4248\tupdate_load_avg(cfs_rq, prev, 0); 4249\t} 4250\tcfs_rq-\u0026gt;curr = NULL; 4251} 4252 4253static void 4254entity_tick(struct cfs_rq *cfs_rq, struct sched_entity *curr, int queued) 4255{ 4256\t/* 4257* Update run-time statistics of the \u0026#39;current\u0026#39;. 4258*/ 4259\tupdate_curr(cfs_rq); 4260 4261\t/* 4262* Ensure that runnable average is periodically updated. 4263*/ 4264\tupdate_load_avg(cfs_rq, curr, UPDATE_TG); 4265\tupdate_cfs_group(curr); 4266 4267#ifdef CONFIG_SCHED_HRTICK 4268\t/* 4269* queued ticks are scheduled to match the slice, so don\u0026#39;t bother 4270* validating it and just reschedule. 4271*/ 4272\tif (queued) { 4273\tresched_curr(rq_of(cfs_rq)); 4274\treturn; 4275\t} 4276\t/* 4277* don\u0026#39;t let the period tick interfere with the hrtick preemption 4278*/ 4279\tif (!sched_feat(DOUBLE_TICK) \u0026amp;\u0026amp; 4280\thrtimer_active(\u0026amp;rq_of(cfs_rq)-\u0026gt;hrtick_timer)) 4281\treturn; 4282#endif 4283 4284\tif (cfs_rq-\u0026gt;nr_running \u0026gt; 1) 4285\tcheck_preempt_tick(cfs_rq, curr); 4286} 4287 4288 4289/************************************************** 4290* CFS bandwidth control machinery 4291*/ 4292 4293#ifdef CONFIG_CFS_BANDWIDTH 4294 4295#ifdef CONFIG_JUMP_LABEL 4296static struct static_key __cfs_bandwidth_used; 4297 4298static inline bool cfs_bandwidth_used(void) 4299{ 4300\treturn static_key_false(\u0026amp;__cfs_bandwidth_used); 4301} 4302 4303void cfs_bandwidth_usage_inc(void) 4304{ 4305\tstatic_key_slow_inc_cpuslocked(\u0026amp;__cfs_bandwidth_used); 4306} 4307 4308void cfs_bandwidth_usage_dec(void) 4309{ 4310\tstatic_key_slow_dec_cpuslocked(\u0026amp;__cfs_bandwidth_used); 4311} 4312#else /* CONFIG_JUMP_LABEL */4313static bool cfs_bandwidth_used(void) 4314{ 4315\treturn true; 4316} 4317 4318void cfs_bandwidth_usage_inc(void) {} 4319void cfs_bandwidth_usage_dec(void) {} 4320#endif /* CONFIG_JUMP_LABEL */4321 4322/* 4323* default period for cfs group bandwidth. 4324* default: 0.1s, units: nanoseconds 4325*/ 4326static inline u64 default_cfs_period(void) 4327{ 4328\treturn 100000000ULL; 4329} 4330 4331static inline u64 sched_cfs_bandwidth_slice(void) 4332{ 4333\treturn (u64)sysctl_sched_cfs_bandwidth_slice * NSEC_PER_USEC; 4334} 4335 4336/* 4337* Replenish runtime according to assigned quota. We use sched_clock_cpu 4338* directly instead of rq-\u0026gt;clock to avoid adding additional synchronization 4339* around rq-\u0026gt;lock. 4340* 4341* requires cfs_b-\u0026gt;lock 4342*/ 4343void __refill_cfs_bandwidth_runtime(struct cfs_bandwidth *cfs_b) 4344{ 4345\tif (cfs_b-\u0026gt;quota != RUNTIME_INF) 4346\tcfs_b-\u0026gt;runtime = cfs_b-\u0026gt;quota; 4347} 4348 4349static inline struct cfs_bandwidth *tg_cfs_bandwidth(struct task_group *tg) 4350{ 4351\treturn \u0026amp;tg-\u0026gt;cfs_bandwidth; 4352} 4353 4354/* rq-\u0026gt;task_clock normalized against any time this cfs_rq has spent throttled */ 4355static inline u64 cfs_rq_clock_task(struct cfs_rq *cfs_rq) 4356{ 4357\tif (unlikely(cfs_rq-\u0026gt;throttle_count)) 4358\treturn cfs_rq-\u0026gt;throttled_clock_task - cfs_rq-\u0026gt;throttled_clock_task_time; 4359 4360\treturn rq_clock_task(rq_of(cfs_rq)) - cfs_rq-\u0026gt;throttled_clock_task_time; 4361} 4362 4363/* returns 0 on failure to allocate runtime */ 4364static int assign_cfs_rq_runtime(struct cfs_rq *cfs_rq) 4365{ 4366\tstruct task_group *tg = cfs_rq-\u0026gt;tg; 4367\tstruct cfs_bandwidth *cfs_b = tg_cfs_bandwidth(tg); 4368\tu64 amount = 0, min_amount; 4369 4370\t/* note: this is a positive sum as runtime_remaining \u0026lt;= 0 */ 4371\tmin_amount = sched_cfs_bandwidth_slice() - cfs_rq-\u0026gt;runtime_remaining; 4372 4373\traw_spin_lock(\u0026amp;cfs_b-\u0026gt;lock); 4374\tif (cfs_b-\u0026gt;quota == RUNTIME_INF) 4375\tamount = min_amount; 4376\telse { 4377\tstart_cfs_bandwidth(cfs_b); 4378 4379\tif (cfs_b-\u0026gt;runtime \u0026gt; 0) { 4380\tamount = min(cfs_b-\u0026gt;runtime, min_amount); 4381\tcfs_b-\u0026gt;runtime -= amount; 4382\tcfs_b-\u0026gt;idle = 0; 4383\t} 4384\t} 4385\traw_spin_unlock(\u0026amp;cfs_b-\u0026gt;lock); 4386 4387\tcfs_rq-\u0026gt;runtime_remaining += amount; 4388 4389\treturn cfs_rq-\u0026gt;runtime_remaining \u0026gt; 0; 4390} 4391 4392static void __account_cfs_rq_runtime(struct cfs_rq *cfs_rq, u64 delta_exec) 4393{ 4394\t/* dock delta_exec before expiring quota (as it could span periods) */ 4395\tcfs_rq-\u0026gt;runtime_remaining -= delta_exec; 4396 4397\tif (likely(cfs_rq-\u0026gt;runtime_remaining \u0026gt; 0)) 4398\treturn; 4399 4400\tif (cfs_rq-\u0026gt;throttled) 4401\treturn; 4402\t/* 4403* if we\u0026#39;re unable to extend our runtime we resched so that the active 4404* hierarchy can be throttled 4405*/ 4406\tif (!assign_cfs_rq_runtime(cfs_rq) \u0026amp;\u0026amp; likely(cfs_rq-\u0026gt;curr)) 4407\tresched_curr(rq_of(cfs_rq)); 4408} 4409 4410static __always_inline 4411void account_cfs_rq_runtime(struct cfs_rq *cfs_rq, u64 delta_exec) 4412{ 4413\tif (!cfs_bandwidth_used() || !cfs_rq-\u0026gt;runtime_enabled) 4414\treturn; 4415 4416\t__account_cfs_rq_runtime(cfs_rq, delta_exec); 4417} 4418 4419static inline int cfs_rq_throttled(struct cfs_rq *cfs_rq) 4420{ 4421\treturn cfs_bandwidth_used() \u0026amp;\u0026amp; cfs_rq-\u0026gt;throttled; 4422} 4423 4424/* check whether cfs_rq, or any parent, is throttled */ 4425static inline int throttled_hierarchy(struct cfs_rq *cfs_rq) 4426{ 4427\treturn cfs_bandwidth_used() \u0026amp;\u0026amp; cfs_rq-\u0026gt;throttle_count; 4428} 4429 4430/* 4431* Ensure that neither of the group entities corresponding to src_cpu or 4432* dest_cpu are members of a throttled hierarchy when performing group 4433* load-balance operations. 4434*/ 4435static inline int throttled_lb_pair(struct task_group *tg, 4436\tint src_cpu, int dest_cpu) 4437{ 4438\tstruct cfs_rq *src_cfs_rq, *dest_cfs_rq; 4439 4440\tsrc_cfs_rq = tg-\u0026gt;cfs_rq[src_cpu]; 4441\tdest_cfs_rq = tg-\u0026gt;cfs_rq[dest_cpu]; 4442 4443\treturn throttled_hierarchy(src_cfs_rq) || 4444\tthrottled_hierarchy(dest_cfs_rq); 4445} 4446 4447static int tg_unthrottle_up(struct task_group *tg, void *data) 4448{ 4449\tstruct rq *rq = data; 4450\tstruct cfs_rq *cfs_rq = tg-\u0026gt;cfs_rq[cpu_of(rq)]; 4451 4452\tcfs_rq-\u0026gt;throttle_count--; 4453\tif (!cfs_rq-\u0026gt;throttle_count) { 4454\t/* adjust cfs_rq_clock_task() */ 4455\tcfs_rq-\u0026gt;throttled_clock_task_time += rq_clock_task(rq) - 4456\tcfs_rq-\u0026gt;throttled_clock_task; 4457 4458\t/* Add cfs_rq with already running entity in the list */ 4459\tif (cfs_rq-\u0026gt;nr_running \u0026gt;= 1) 4460\tlist_add_leaf_cfs_rq(cfs_rq); 4461\t} 4462 4463\treturn 0; 4464} 4465 4466static int tg_throttle_down(struct task_group *tg, void *data) 4467{ 4468\tstruct rq *rq = data; 4469\tstruct cfs_rq *cfs_rq = tg-\u0026gt;cfs_rq[cpu_of(rq)]; 4470 4471\t/* group is entering throttled state, stop time */ 4472\tif (!cfs_rq-\u0026gt;throttle_count) { 4473\tcfs_rq-\u0026gt;throttled_clock_task = rq_clock_task(rq); 4474\tlist_del_leaf_cfs_rq(cfs_rq); 4475\t} 4476\tcfs_rq-\u0026gt;throttle_count++; 4477 4478\treturn 0; 4479} 4480 4481static void throttle_cfs_rq(struct cfs_rq *cfs_rq) 4482{ 4483\tstruct rq *rq = rq_of(cfs_rq); 4484\tstruct cfs_bandwidth *cfs_b = tg_cfs_bandwidth(cfs_rq-\u0026gt;tg); 4485\tstruct sched_entity *se; 4486\tlong task_delta, dequeue = 1; 4487\tbool empty; 4488 4489\tse = cfs_rq-\u0026gt;tg-\u0026gt;se[cpu_of(rq_of(cfs_rq))]; 4490 4491\t/* freeze hierarchy runnable averages while throttled */ 4492\trcu_read_lock(); 4493\twalk_tg_tree_from(cfs_rq-\u0026gt;tg, tg_throttle_down, tg_nop, (void *)rq); 4494\trcu_read_unlock(); 4495 4496\ttask_delta = cfs_rq-\u0026gt;h_nr_running; 4497\tfor_each_sched_entity(se) { 4498\tstruct cfs_rq *qcfs_rq = cfs_rq_of(se); 4499\t/* throttled entity or throttle-on-deactivate */ 4500\tif (!se-\u0026gt;on_rq) 4501\tbreak; 4502 4503\tif (dequeue) 4504\tdequeue_entity(qcfs_rq, se, DEQUEUE_SLEEP); 4505\tqcfs_rq-\u0026gt;h_nr_running -= task_delta; 4506 4507\tif (qcfs_rq-\u0026gt;load.weight) 4508\tdequeue = 0; 4509\t} 4510 4511\tif (!se) 4512\tsub_nr_running(rq, task_delta); 4513 4514\tcfs_rq-\u0026gt;throttled = 1; 4515\tcfs_rq-\u0026gt;throttled_clock = rq_clock(rq); 4516\traw_spin_lock(\u0026amp;cfs_b-\u0026gt;lock); 4517\tempty = list_empty(\u0026amp;cfs_b-\u0026gt;throttled_cfs_rq); 4518 4519\t/* 4520* Add to the _head_ of the list, so that an already-started 4521* distribute_cfs_runtime will not see us. If disribute_cfs_runtime is 4522* not running add to the tail so that later runqueues don\u0026#39;t get starved. 4523*/ 4524\tif (cfs_b-\u0026gt;distribute_running) 4525\tlist_add_rcu(\u0026amp;cfs_rq-\u0026gt;throttled_list, \u0026amp;cfs_b-\u0026gt;throttled_cfs_rq); 4526\telse 4527\tlist_add_tail_rcu(\u0026amp;cfs_rq-\u0026gt;throttled_list, \u0026amp;cfs_b-\u0026gt;throttled_cfs_rq); 4528 4529\t/* 4530* If we\u0026#39;re the first throttled task, make sure the bandwidth 4531* timer is running. 4532*/ 4533\tif (empty) 4534\tstart_cfs_bandwidth(cfs_b); 4535 4536\traw_spin_unlock(\u0026amp;cfs_b-\u0026gt;lock); 4537} 4538 4539void unthrottle_cfs_rq(struct cfs_rq *cfs_rq) 4540{ 4541\tstruct rq *rq = rq_of(cfs_rq); 4542\tstruct cfs_bandwidth *cfs_b = tg_cfs_bandwidth(cfs_rq-\u0026gt;tg); 4543\tstruct sched_entity *se; 4544\tint enqueue = 1; 4545\tlong task_delta; 4546 4547\tse = cfs_rq-\u0026gt;tg-\u0026gt;se[cpu_of(rq)]; 4548 4549\tcfs_rq-\u0026gt;throttled = 0; 4550 4551\tupdate_rq_clock(rq); 4552 4553\traw_spin_lock(\u0026amp;cfs_b-\u0026gt;lock); 4554\tcfs_b-\u0026gt;throttled_time += rq_clock(rq) - cfs_rq-\u0026gt;throttled_clock; 4555\tlist_del_rcu(\u0026amp;cfs_rq-\u0026gt;throttled_list); 4556\traw_spin_unlock(\u0026amp;cfs_b-\u0026gt;lock); 4557 4558\t/* update hierarchical throttle state */ 4559\twalk_tg_tree_from(cfs_rq-\u0026gt;tg, tg_nop, tg_unthrottle_up, (void *)rq); 4560 4561\tif (!cfs_rq-\u0026gt;load.weight) 4562\treturn; 4563 4564\ttask_delta = cfs_rq-\u0026gt;h_nr_running; 4565\tfor_each_sched_entity(se) { 4566\tif (se-\u0026gt;on_rq) 4567\tenqueue = 0; 4568 4569\tcfs_rq = cfs_rq_of(se); 4570\tif (enqueue) 4571\tenqueue_entity(cfs_rq, se, ENQUEUE_WAKEUP); 4572\tcfs_rq-\u0026gt;h_nr_running += task_delta; 4573 4574\tif (cfs_rq_throttled(cfs_rq)) 4575\tbreak; 4576\t} 4577 4578\tassert_list_leaf_cfs_rq(rq); 4579 4580\tif (!se) 4581\tadd_nr_running(rq, task_delta); 4582 4583\t/* Determine whether we need to wake up potentially idle CPU: */ 4584\tif (rq-\u0026gt;curr == rq-\u0026gt;idle \u0026amp;\u0026amp; rq-\u0026gt;cfs.nr_running) 4585\tresched_curr(rq); 4586} 4587 4588static u64 distribute_cfs_runtime(struct cfs_bandwidth *cfs_b, u64 remaining) 4589{ 4590\tstruct cfs_rq *cfs_rq; 4591\tu64 runtime; 4592\tu64 starting_runtime = remaining; 4593 4594\trcu_read_lock(); 4595\tlist_for_each_entry_rcu(cfs_rq, \u0026amp;cfs_b-\u0026gt;throttled_cfs_rq, 4596\tthrottled_list) { 4597\tstruct rq *rq = rq_of(cfs_rq); 4598\tstruct rq_flags rf; 4599 4600\trq_lock(rq, \u0026amp;rf); 4601\tif (!cfs_rq_throttled(cfs_rq)) 4602\tgoto next; 4603 4604\t/* By the above check, this should never be true */ 4605\tSCHED_WARN_ON(cfs_rq-\u0026gt;runtime_remaining \u0026gt; 0); 4606 4607\truntime = -cfs_rq-\u0026gt;runtime_remaining + 1; 4608\tif (runtime \u0026gt; remaining) 4609\truntime = remaining; 4610\tremaining -= runtime; 4611 4612\tcfs_rq-\u0026gt;runtime_remaining += runtime; 4613 4614\t/* we check whether we\u0026#39;re throttled above */ 4615\tif (cfs_rq-\u0026gt;runtime_remaining \u0026gt; 0) 4616\tunthrottle_cfs_rq(cfs_rq); 4617 4618next: 4619\trq_unlock(rq, \u0026amp;rf); 4620 4621\tif (!remaining) 4622\tbreak; 4623\t} 4624\trcu_read_unlock(); 4625 4626\treturn starting_runtime - remaining; 4627} 4628 4629/* 4630* Responsible for refilling a task_group\u0026#39;s bandwidth and unthrottling its 4631* cfs_rqs as appropriate. If there has been no activity within the last 4632* period the timer is deactivated until scheduling resumes; cfs_b-\u0026gt;idle is 4633* used to track this state. 4634*/ 4635static int do_sched_cfs_period_timer(struct cfs_bandwidth *cfs_b, int overrun) 4636{ 4637\tu64 runtime; 4638\tint throttled; 4639 4640\t/* no need to continue the timer with no bandwidth constraint */ 4641\tif (cfs_b-\u0026gt;quota == RUNTIME_INF) 4642\tgoto out_deactivate; 4643 4644\tthrottled = !list_empty(\u0026amp;cfs_b-\u0026gt;throttled_cfs_rq); 4645\tcfs_b-\u0026gt;nr_periods += overrun; 4646 4647\t/* 4648* idle depends on !throttled (for the case of a large deficit), and if 4649* we\u0026#39;re going inactive then everything else can be deferred 4650*/ 4651\tif (cfs_b-\u0026gt;idle \u0026amp;\u0026amp; !throttled) 4652\tgoto out_deactivate; 4653 4654\t__refill_cfs_bandwidth_runtime(cfs_b); 4655 4656\tif (!throttled) { 4657\t/* mark as potentially idle for the upcoming period */ 4658\tcfs_b-\u0026gt;idle = 1; 4659\treturn 0; 4660\t} 4661 4662\t/* account preceding periods in which throttling occurred */ 4663\tcfs_b-\u0026gt;nr_throttled += overrun; 4664 4665\t/* 4666* This check is repeated as we are holding onto the new bandwidth while 4667* we unthrottle. This can potentially race with an unthrottled group 4668* trying to acquire new bandwidth from the global pool. This can result 4669* in us over-using our runtime if it is all used during this loop, but 4670* only by limited amounts in that extreme case. 4671*/ 4672\twhile (throttled \u0026amp;\u0026amp; cfs_b-\u0026gt;runtime \u0026gt; 0 \u0026amp;\u0026amp; !cfs_b-\u0026gt;distribute_running) { 4673\truntime = cfs_b-\u0026gt;runtime; 4674\tcfs_b-\u0026gt;distribute_running = 1; 4675\traw_spin_unlock(\u0026amp;cfs_b-\u0026gt;lock); 4676\t/* we can\u0026#39;t nest cfs_b-\u0026gt;lock while distributing bandwidth */ 4677\truntime = distribute_cfs_runtime(cfs_b, runtime); 4678\traw_spin_lock(\u0026amp;cfs_b-\u0026gt;lock); 4679 4680\tcfs_b-\u0026gt;distribute_running = 0; 4681\tthrottled = !list_empty(\u0026amp;cfs_b-\u0026gt;throttled_cfs_rq); 4682 4683\tcfs_b-\u0026gt;runtime -= min(runtime, cfs_b-\u0026gt;runtime); 4684\t} 4685 4686\t/* 4687* While we are ensured activity in the period following an 4688* unthrottle, this also covers the case in which the new bandwidth is 4689* insufficient to cover the existing bandwidth deficit. (Forcing the 4690* timer to remain active while there are any throttled entities.) 4691*/ 4692\tcfs_b-\u0026gt;idle = 0; 4693 4694\treturn 0; 4695 4696out_deactivate: 4697\treturn 1; 4698} 4699 4700/* a cfs_rq won\u0026#39;t donate quota below this amount */ 4701static const u64 min_cfs_rq_runtime = 1 * NSEC_PER_MSEC; 4702/* minimum remaining period time to redistribute slack quota */ 4703static const u64 min_bandwidth_expiration = 2 * NSEC_PER_MSEC; 4704/* how long we wait to gather additional slack before distributing */ 4705static const u64 cfs_bandwidth_slack_period = 5 * NSEC_PER_MSEC; 4706 4707/* 4708* Are we near the end of the current quota period? 4709* 4710* Requires cfs_b-\u0026gt;lock for hrtimer_expires_remaining to be safe against the 4711* hrtimer base being cleared by hrtimer_start. In the case of 4712* migrate_hrtimers, base is never cleared, so we are fine. 4713*/ 4714static int runtime_refresh_within(struct cfs_bandwidth *cfs_b, u64 min_expire) 4715{ 4716\tstruct hrtimer *refresh_timer = \u0026amp;cfs_b-\u0026gt;period_timer; 4717\tu64 remaining; 4718 4719\t/* if the call-back is running a quota refresh is already occurring */ 4720\tif (hrtimer_callback_running(refresh_timer)) 4721\treturn 1; 4722 4723\t/* is a quota refresh about to occur? */ 4724\tremaining = ktime_to_ns(hrtimer_expires_remaining(refresh_timer)); 4725\tif (remaining \u0026lt; min_expire) 4726\treturn 1; 4727 4728\treturn 0; 4729} 4730 4731static void start_cfs_slack_bandwidth(struct cfs_bandwidth *cfs_b) 4732{ 4733\tu64 min_left = cfs_bandwidth_slack_period + min_bandwidth_expiration; 4734 4735\t/* if there\u0026#39;s a quota refresh soon don\u0026#39;t bother with slack */ 4736\tif (runtime_refresh_within(cfs_b, min_left)) 4737\treturn; 4738 4739\thrtimer_start(\u0026amp;cfs_b-\u0026gt;slack_timer, 4740\tns_to_ktime(cfs_bandwidth_slack_period), 4741\tHRTIMER_MODE_REL); 4742} 4743 4744/* we know any runtime found here is valid as update_curr() precedes return */ 4745static void __return_cfs_rq_runtime(struct cfs_rq *cfs_rq) 4746{ 4747\tstruct cfs_bandwidth *cfs_b = tg_cfs_bandwidth(cfs_rq-\u0026gt;tg); 4748\ts64 slack_runtime = cfs_rq-\u0026gt;runtime_remaining - min_cfs_rq_runtime; 4749 4750\tif (slack_runtime \u0026lt;= 0) 4751\treturn; 4752 4753\traw_spin_lock(\u0026amp;cfs_b-\u0026gt;lock); 4754\tif (cfs_b-\u0026gt;quota != RUNTIME_INF) { 4755\tcfs_b-\u0026gt;runtime += slack_runtime; 4756 4757\t/* we are under rq-\u0026gt;lock, defer unthrottling using a timer */ 4758\tif (cfs_b-\u0026gt;runtime \u0026gt; sched_cfs_bandwidth_slice() \u0026amp;\u0026amp; 4759\t!list_empty(\u0026amp;cfs_b-\u0026gt;throttled_cfs_rq)) 4760\tstart_cfs_slack_bandwidth(cfs_b); 4761\t} 4762\traw_spin_unlock(\u0026amp;cfs_b-\u0026gt;lock); 4763 4764\t/* even if it\u0026#39;s not valid for return we don\u0026#39;t want to try again */ 4765\tcfs_rq-\u0026gt;runtime_remaining -= slack_runtime; 4766} 4767 4768static __always_inline void return_cfs_rq_runtime(struct cfs_rq *cfs_rq) 4769{ 4770\tif (!cfs_bandwidth_used()) 4771\treturn; 4772 4773\tif (!cfs_rq-\u0026gt;runtime_enabled || cfs_rq-\u0026gt;nr_running) 4774\treturn; 4775 4776\t__return_cfs_rq_runtime(cfs_rq); 4777} 4778 4779/* 4780* This is done with a timer (instead of inline with bandwidth return) since 4781* it\u0026#39;s necessary to juggle rq-\u0026gt;locks to unthrottle their respective cfs_rqs. 4782*/ 4783static void do_sched_cfs_slack_timer(struct cfs_bandwidth *cfs_b) 4784{ 4785\tu64 runtime = 0, slice = sched_cfs_bandwidth_slice(); 4786 4787\t/* confirm we\u0026#39;re still not at a refresh boundary */ 4788\traw_spin_lock(\u0026amp;cfs_b-\u0026gt;lock); 4789\tif (cfs_b-\u0026gt;distribute_running) { 4790\traw_spin_unlock(\u0026amp;cfs_b-\u0026gt;lock); 4791\treturn; 4792\t} 4793 4794\tif (runtime_refresh_within(cfs_b, min_bandwidth_expiration)) { 4795\traw_spin_unlock(\u0026amp;cfs_b-\u0026gt;lock); 4796\treturn; 4797\t} 4798 4799\tif (cfs_b-\u0026gt;quota != RUNTIME_INF \u0026amp;\u0026amp; cfs_b-\u0026gt;runtime \u0026gt; slice) 4800\truntime = cfs_b-\u0026gt;runtime; 4801 4802\tif (runtime) 4803\tcfs_b-\u0026gt;distribute_running = 1; 4804 4805\traw_spin_unlock(\u0026amp;cfs_b-\u0026gt;lock); 4806 4807\tif (!runtime) 4808\treturn; 4809 4810\truntime = distribute_cfs_runtime(cfs_b, runtime); 4811 4812\traw_spin_lock(\u0026amp;cfs_b-\u0026gt;lock); 4813\tcfs_b-\u0026gt;runtime -= min(runtime, cfs_b-\u0026gt;runtime); 4814\tcfs_b-\u0026gt;distribute_running = 0; 4815\traw_spin_unlock(\u0026amp;cfs_b-\u0026gt;lock); 4816} 4817 4818/* 4819* When a group wakes up we want to make sure that its quota is not already 4820* expired/exceeded, otherwise it may be allowed to steal additional ticks of 4821* runtime as update_curr() throttling can not not trigger until it\u0026#39;s on-rq. 4822*/ 4823static void check_enqueue_throttle(struct cfs_rq *cfs_rq) 4824{ 4825\tif (!cfs_bandwidth_used()) 4826\treturn; 4827 4828\t/* an active group must be handled by the update_curr()-\u0026gt;put() path */ 4829\tif (!cfs_rq-\u0026gt;runtime_enabled || cfs_rq-\u0026gt;curr) 4830\treturn; 4831 4832\t/* ensure the group is not already throttled */ 4833\tif (cfs_rq_throttled(cfs_rq)) 4834\treturn; 4835 4836\t/* update runtime allocation */ 4837\taccount_cfs_rq_runtime(cfs_rq, 0); 4838\tif (cfs_rq-\u0026gt;runtime_remaining \u0026lt;= 0) 4839\tthrottle_cfs_rq(cfs_rq); 4840} 4841 4842static void sync_throttle(struct task_group *tg, int cpu) 4843{ 4844\tstruct cfs_rq *pcfs_rq, *cfs_rq; 4845 4846\tif (!cfs_bandwidth_used()) 4847\treturn; 4848 4849\tif (!tg-\u0026gt;parent) 4850\treturn; 4851 4852\tcfs_rq = tg-\u0026gt;cfs_rq[cpu]; 4853\tpcfs_rq = tg-\u0026gt;parent-\u0026gt;cfs_rq[cpu]; 4854 4855\tcfs_rq-\u0026gt;throttle_count = pcfs_rq-\u0026gt;throttle_count; 4856\tcfs_rq-\u0026gt;throttled_clock_task = rq_clock_task(cpu_rq(cpu)); 4857} 4858 4859/* conditionally throttle active cfs_rq\u0026#39;s from put_prev_entity() */ 4860static bool check_cfs_rq_runtime(struct cfs_rq *cfs_rq) 4861{ 4862\tif (!cfs_bandwidth_used()) 4863\treturn false; 4864 4865\tif (likely(!cfs_rq-\u0026gt;runtime_enabled || cfs_rq-\u0026gt;runtime_remaining \u0026gt; 0)) 4866\treturn false; 4867 4868\t/* 4869* it\u0026#39;s possible for a throttled entity to be forced into a running 4870* state (e.g. set_curr_task), in this case we\u0026#39;re finished. 4871*/ 4872\tif (cfs_rq_throttled(cfs_rq)) 4873\treturn true; 4874 4875\tthrottle_cfs_rq(cfs_rq); 4876\treturn true; 4877} 4878 4879static enum hrtimer_restart sched_cfs_slack_timer(struct hrtimer *timer) 4880{ 4881\tstruct cfs_bandwidth *cfs_b = 4882\tcontainer_of(timer, struct cfs_bandwidth, slack_timer); 4883 4884\tdo_sched_cfs_slack_timer(cfs_b); 4885 4886\treturn HRTIMER_NORESTART; 4887} 4888 4889extern const u64 max_cfs_quota_period; 4890 4891static enum hrtimer_restart sched_cfs_period_timer(struct hrtimer *timer) 4892{ 4893\tstruct cfs_bandwidth *cfs_b = 4894\tcontainer_of(timer, struct cfs_bandwidth, period_timer); 4895\tint overrun; 4896\tint idle = 0; 4897\tint count = 0; 4898 4899\traw_spin_lock(\u0026amp;cfs_b-\u0026gt;lock); 4900\tfor (;;) { 4901\toverrun = hrtimer_forward_now(timer, cfs_b-\u0026gt;period); 4902\tif (!overrun) 4903\tbreak; 4904 4905\tif (++count \u0026gt; 3) { 4906\tu64 new, old = ktime_to_ns(cfs_b-\u0026gt;period); 4907 4908\t/* 4909* Grow period by a factor of 2 to avoid losing precision. 4910* Precision loss in the quota/period ratio can cause __cfs_schedulable 4911* to fail. 4912*/ 4913\tnew = old * 2; 4914\tif (new \u0026lt; max_cfs_quota_period) { 4915\tcfs_b-\u0026gt;period = ns_to_ktime(new); 4916\tcfs_b-\u0026gt;quota *= 2; 4917 4918\tpr_warn_ratelimited( 4919\t\u0026#34;cfs_period_timer[cpu%d]: period too short, scaling up (new cfs_period_us = %lld, cfs_quota_us = %lld)\\n\u0026#34;, 4920\tsmp_processor_id(), 4921\tdiv_u64(new, NSEC_PER_USEC), 4922\tdiv_u64(cfs_b-\u0026gt;quota, NSEC_PER_USEC)); 4923\t} else { 4924\tpr_warn_ratelimited( 4925\t\u0026#34;cfs_period_timer[cpu%d]: period too short, but cannot scale up without losing precision (cfs_period_us = %lld, cfs_quota_us = %lld)\\n\u0026#34;, 4926\tsmp_processor_id(), 4927\tdiv_u64(old, NSEC_PER_USEC), 4928\tdiv_u64(cfs_b-\u0026gt;quota, NSEC_PER_USEC)); 4929\t} 4930 4931\t/* reset count so we don\u0026#39;t come right back in here */ 4932\tcount = 0; 4933\t} 4934 4935\tidle = do_sched_cfs_period_timer(cfs_b, overrun); 4936\t} 4937\tif (idle) 4938\tcfs_b-\u0026gt;period_active = 0; 4939\traw_spin_unlock(\u0026amp;cfs_b-\u0026gt;lock); 4940 4941\treturn idle ? HRTIMER_NORESTART : HRTIMER_RESTART; 4942} 4943 4944void init_cfs_bandwidth(struct cfs_bandwidth *cfs_b) 4945{ 4946\traw_spin_lock_init(\u0026amp;cfs_b-\u0026gt;lock); 4947\tcfs_b-\u0026gt;runtime = 0; 4948\tcfs_b-\u0026gt;quota = RUNTIME_INF; 4949\tcfs_b-\u0026gt;period = ns_to_ktime(default_cfs_period()); 4950 4951\tINIT_LIST_HEAD(\u0026amp;cfs_b-\u0026gt;throttled_cfs_rq); 4952\thrtimer_init(\u0026amp;cfs_b-\u0026gt;period_timer, CLOCK_MONOTONIC, HRTIMER_MODE_ABS_PINNED); 4953\tcfs_b-\u0026gt;period_timer.function = sched_cfs_period_timer; 4954\thrtimer_init(\u0026amp;cfs_b-\u0026gt;slack_timer, CLOCK_MONOTONIC, HRTIMER_MODE_REL); 4955\tcfs_b-\u0026gt;slack_timer.function = sched_cfs_slack_timer; 4956\tcfs_b-\u0026gt;distribute_running = 0; 4957} 4958 4959static void init_cfs_rq_runtime(struct cfs_rq *cfs_rq) 4960{ 4961\tcfs_rq-\u0026gt;runtime_enabled = 0; 4962\tINIT_LIST_HEAD(\u0026amp;cfs_rq-\u0026gt;throttled_list); 4963} 4964 4965void start_cfs_bandwidth(struct cfs_bandwidth *cfs_b) 4966{ 4967\tlockdep_assert_held(\u0026amp;cfs_b-\u0026gt;lock); 4968 4969\tif (cfs_b-\u0026gt;period_active) 4970\treturn; 4971 4972\tcfs_b-\u0026gt;period_active = 1; 4973\thrtimer_forward_now(\u0026amp;cfs_b-\u0026gt;period_timer, cfs_b-\u0026gt;period); 4974\thrtimer_start_expires(\u0026amp;cfs_b-\u0026gt;period_timer, HRTIMER_MODE_ABS_PINNED); 4975} 4976 4977static void destroy_cfs_bandwidth(struct cfs_bandwidth *cfs_b) 4978{ 4979\t/* init_cfs_bandwidth() was not called */ 4980\tif (!cfs_b-\u0026gt;throttled_cfs_rq.next) 4981\treturn; 4982 4983\thrtimer_cancel(\u0026amp;cfs_b-\u0026gt;period_timer); 4984\thrtimer_cancel(\u0026amp;cfs_b-\u0026gt;slack_timer); 4985} 4986 4987/* 4988* Both these CPU hotplug callbacks race against unregister_fair_sched_group() 4989* 4990* The race is harmless, since modifying bandwidth settings of unhooked group 4991* bits doesn\u0026#39;t do much. 4992*/ 4993 4994/* cpu online calback */ 4995static void __maybe_unused update_runtime_enabled(struct rq *rq) 4996{ 4997\tstruct task_group *tg; 4998 4999\tlockdep_assert_held(\u0026amp;rq-\u0026gt;lock); 5000 5001\trcu_read_lock(); 5002\tlist_for_each_entry_rcu(tg, \u0026amp;task_groups, list) { 5003\tstruct cfs_bandwidth *cfs_b = \u0026amp;tg-\u0026gt;cfs_bandwidth; 5004\tstruct cfs_rq *cfs_rq = tg-\u0026gt;cfs_rq[cpu_of(rq)]; 5005 5006\traw_spin_lock(\u0026amp;cfs_b-\u0026gt;lock); 5007\tcfs_rq-\u0026gt;runtime_enabled = cfs_b-\u0026gt;quota != RUNTIME_INF; 5008\traw_spin_unlock(\u0026amp;cfs_b-\u0026gt;lock); 5009\t} 5010\trcu_read_unlock(); 5011} 5012 5013/* cpu offline callback */ 5014static void __maybe_unused unthrottle_offline_cfs_rqs(struct rq *rq) 5015{ 5016\tstruct task_group *tg; 5017 5018\tlockdep_assert_held(\u0026amp;rq-\u0026gt;lock); 5019 5020\trcu_read_lock(); 5021\tlist_for_each_entry_rcu(tg, \u0026amp;task_groups, list) { 5022\tstruct cfs_rq *cfs_rq = tg-\u0026gt;cfs_rq[cpu_of(rq)]; 5023 5024\tif (!cfs_rq-\u0026gt;runtime_enabled) 5025\tcontinue; 5026 5027\t/* 5028* clock_task is not advancing so we just need to make sure 5029* there\u0026#39;s some valid quota amount 5030*/ 5031\tcfs_rq-\u0026gt;runtime_remaining = 1; 5032\t/* 5033* Offline rq is schedulable till CPU is completely disabled 5034* in take_cpu_down(), so we prevent new cfs throttling here. 5035*/ 5036\tcfs_rq-\u0026gt;runtime_enabled = 0; 5037 5038\tif (cfs_rq_throttled(cfs_rq)) 5039\tunthrottle_cfs_rq(cfs_rq); 5040\t} 5041\trcu_read_unlock(); 5042} 5043 5044#else /* CONFIG_CFS_BANDWIDTH */5045 5046static inline bool cfs_bandwidth_used(void) 5047{ 5048\treturn false; 5049} 5050 5051static inline u64 cfs_rq_clock_task(struct cfs_rq *cfs_rq) 5052{ 5053\treturn rq_clock_task(rq_of(cfs_rq)); 5054} 5055 5056static void account_cfs_rq_runtime(struct cfs_rq *cfs_rq, u64 delta_exec) {} 5057static bool check_cfs_rq_runtime(struct cfs_rq *cfs_rq) { return false; } 5058static void check_enqueue_throttle(struct cfs_rq *cfs_rq) {} 5059static inline void sync_throttle(struct task_group *tg, int cpu) {} 5060static __always_inline void return_cfs_rq_runtime(struct cfs_rq *cfs_rq) {} 5061 5062static inline int cfs_rq_throttled(struct cfs_rq *cfs_rq) 5063{ 5064\treturn 0; 5065} 5066 5067static inline int throttled_hierarchy(struct cfs_rq *cfs_rq) 5068{ 5069\treturn 0; 5070} 5071 5072static inline int throttled_lb_pair(struct task_group *tg, 5073\tint src_cpu, int dest_cpu) 5074{ 5075\treturn 0; 5076} 5077 5078void init_cfs_bandwidth(struct cfs_bandwidth *cfs_b) {} 5079 5080#ifdef CONFIG_FAIR_GROUP_SCHED 5081static void init_cfs_rq_runtime(struct cfs_rq *cfs_rq) {} 5082#endif 5083 5084static inline struct cfs_bandwidth *tg_cfs_bandwidth(struct task_group *tg) 5085{ 5086\treturn NULL; 5087} 5088static inline void destroy_cfs_bandwidth(struct cfs_bandwidth *cfs_b) {} 5089static inline void update_runtime_enabled(struct rq *rq) {} 5090static inline void unthrottle_offline_cfs_rqs(struct rq *rq) {} 5091 5092#endif /* CONFIG_CFS_BANDWIDTH */5093 5094/************************************************** 5095* CFS operations on tasks: 5096*/ 5097 5098#ifdef CONFIG_SCHED_HRTICK 5099static void hrtick_start_fair(struct rq *rq, struct task_struct *p) 5100{ 5101\tstruct sched_entity *se = \u0026amp;p-\u0026gt;se; 5102\tstruct cfs_rq *cfs_rq = cfs_rq_of(se); 5103 5104\tSCHED_WARN_ON(task_rq(p) != rq); 5105 5106\tif (rq-\u0026gt;cfs.h_nr_running \u0026gt; 1) { 5107\tu64 slice = sched_slice(cfs_rq, se); 5108\tu64 ran = se-\u0026gt;sum_exec_runtime - se-\u0026gt;prev_sum_exec_runtime; 5109\ts64 delta = slice - ran; 5110 5111\tif (delta \u0026lt; 0) { 5112\tif (rq-\u0026gt;curr == p) 5113\tresched_curr(rq); 5114\treturn; 5115\t} 5116\thrtick_start(rq, delta); 5117\t} 5118} 5119 5120/* 5121* called from enqueue/dequeue and updates the hrtick when the 5122* current task is from our class and nr_running is low enough 5123* to matter. 5124*/ 5125static void hrtick_update(struct rq *rq) 5126{ 5127\tstruct task_struct *curr = rq-\u0026gt;curr; 5128 5129\tif (!hrtick_enabled(rq) || curr-\u0026gt;sched_class != \u0026amp;fair_sched_class) 5130\treturn; 5131 5132\tif (cfs_rq_of(\u0026amp;curr-\u0026gt;se)-\u0026gt;nr_running \u0026lt; sched_nr_latency) 5133\thrtick_start_fair(rq, curr); 5134} 5135#else /* !CONFIG_SCHED_HRTICK */5136static inline void 5137hrtick_start_fair(struct rq *rq, struct task_struct *p) 5138{ 5139} 5140 5141static inline void hrtick_update(struct rq *rq) 5142{ 5143} 5144#endif 5145 5146/* 5147* The enqueue_task method is called before nr_running is 5148* increased. Here we update the fair scheduling stats and 5149* then put the task into the rbtree: 5150*/ 5151static void 5152enqueue_task_fair(struct rq *rq, struct task_struct *p, int flags) 5153{ 5154\tstruct cfs_rq *cfs_rq; 5155\tstruct sched_entity *se = \u0026amp;p-\u0026gt;se; 5156 5157\t/* 5158* The code below (indirectly) updates schedutil which looks at 5159* the cfs_rq utilization to select a frequency. 5160* Let\u0026#39;s add the task\u0026#39;s estimated utilization to the cfs_rq\u0026#39;s 5161* estimated utilization, before we update schedutil. 5162*/ 5163\tutil_est_enqueue(\u0026amp;rq-\u0026gt;cfs, p); 5164 5165\t/* 5166* If in_iowait is set, the code below may not trigger any cpufreq 5167* utilization updates, so do it here explicitly with the IOWAIT flag 5168* passed. 5169*/ 5170\tif (p-\u0026gt;in_iowait) 5171\tcpufreq_update_util(rq, SCHED_CPUFREQ_IOWAIT); 5172 5173\tfor_each_sched_entity(se) { 5174\tif (se-\u0026gt;on_rq) 5175\tbreak; 5176\tcfs_rq = cfs_rq_of(se); 5177\tenqueue_entity(cfs_rq, se, flags); 5178 5179\t/* 5180* end evaluation on encountering a throttled cfs_rq 5181* 5182* note: in the case of encountering a throttled cfs_rq we will 5183* post the final h_nr_running increment below. 5184*/ 5185\tif (cfs_rq_throttled(cfs_rq)) 5186\tbreak; 5187\tcfs_rq-\u0026gt;h_nr_running++; 5188 5189\tflags = ENQUEUE_WAKEUP; 5190\t} 5191 5192\tfor_each_sched_entity(se) { 5193\tcfs_rq = cfs_rq_of(se); 5194\tcfs_rq-\u0026gt;h_nr_running++; 5195 5196\tif (cfs_rq_throttled(cfs_rq)) 5197\tbreak; 5198 5199\tupdate_load_avg(cfs_rq, se, UPDATE_TG); 5200\tupdate_cfs_group(se); 5201\t} 5202 5203\tif (!se) 5204\tadd_nr_running(rq, 1); 5205 5206\tif (cfs_bandwidth_used()) { 5207\t/* 5208* When bandwidth control is enabled; the cfs_rq_throttled() 5209* breaks in the above iteration can result in incomplete 5210* leaf list maintenance, resulting in triggering the assertion 5211* below. 5212*/ 5213\tfor_each_sched_entity(se) { 5214\tcfs_rq = cfs_rq_of(se); 5215 5216\tif (list_add_leaf_cfs_rq(cfs_rq)) 5217\tbreak; 5218\t} 5219\t} 5220 5221\tassert_list_leaf_cfs_rq(rq); 5222 5223\thrtick_update(rq); 5224} 5225 5226static void set_next_buddy(struct sched_entity *se); 5227 5228/* 5229* The dequeue_task method is called before nr_running is 5230* decreased. We remove the task from the rbtree and 5231* update the fair scheduling stats: 5232*/ 5233static void dequeue_task_fair(struct rq *rq, struct task_struct *p, int flags) 5234{ 5235\tstruct cfs_rq *cfs_rq; 5236\tstruct sched_entity *se = \u0026amp;p-\u0026gt;se; 5237\tint task_sleep = flags \u0026amp; DEQUEUE_SLEEP; 5238 5239\tfor_each_sched_entity(se) { 5240\tcfs_rq = cfs_rq_of(se); 5241\tdequeue_entity(cfs_rq, se, flags); 5242 5243\t/* 5244* end evaluation on encountering a throttled cfs_rq 5245* 5246* note: in the case of encountering a throttled cfs_rq we will 5247* post the final h_nr_running decrement below. 5248*/ 5249\tif (cfs_rq_throttled(cfs_rq)) 5250\tbreak; 5251\tcfs_rq-\u0026gt;h_nr_running--; 5252 5253\t/* Don\u0026#39;t dequeue parent if it has other entities besides us */ 5254\tif (cfs_rq-\u0026gt;load.weight) { 5255\t/* Avoid re-evaluating load for this entity: */ 5256\tse = parent_entity(se); 5257\t/* 5258* Bias pick_next to pick a task from this cfs_rq, as 5259* p is sleeping when it is within its sched_slice. 5260*/ 5261\tif (task_sleep \u0026amp;\u0026amp; se \u0026amp;\u0026amp; !throttled_hierarchy(cfs_rq)) 5262\tset_next_buddy(se); 5263\tbreak; 5264\t} 5265\tflags |= DEQUEUE_SLEEP; 5266\t} 5267 5268\tfor_each_sched_entity(se) { 5269\tcfs_rq = cfs_rq_of(se); 5270\tcfs_rq-\u0026gt;h_nr_running--; 5271 5272\tif (cfs_rq_throttled(cfs_rq)) 5273\tbreak; 5274 5275\tupdate_load_avg(cfs_rq, se, UPDATE_TG); 5276\tupdate_cfs_group(se); 5277\t} 5278 5279\tif (!se) 5280\tsub_nr_running(rq, 1); 5281 5282\tutil_est_dequeue(\u0026amp;rq-\u0026gt;cfs, p, task_sleep); 5283\thrtick_update(rq); 5284} 5285 5286#ifdef CONFIG_SMP 5287 5288/* Working cpumask for: load_balance, load_balance_newidle. */ 5289DEFINE_PER_CPU(cpumask_var_t, load_balance_mask); 5290DEFINE_PER_CPU(cpumask_var_t, select_idle_mask); 5291 5292#ifdef CONFIG_NO_HZ_COMMON 5293/* 5294* per rq \u0026#39;load\u0026#39; arrray crap; XXX kill this. 5295*/ 5296 5297/* 5298* The exact cpuload calculated at every tick would be: 5299* 5300* load\u0026#39; = (1 - 1/2^i) * load + (1/2^i) * cur_load 5301* 5302* If a CPU misses updates for n ticks (as it was idle) and update gets 5303* called on the n+1-th tick when CPU may be busy, then we have: 5304* 5305* load_n = (1 - 1/2^i)^n * load_0 5306* load_n+1 = (1 - 1/2^i) * load_n + (1/2^i) * cur_load 5307* 5308* decay_load_missed() below does efficient calculation of 5309* 5310* load\u0026#39; = (1 - 1/2^i)^n * load 5311* 5312* Because x^(n+m) := x^n * x^m we can decompose any x^n in power-of-2 factors. 5313* This allows us to precompute the above in said factors, thereby allowing the 5314* reduction of an arbitrary n in O(log_2 n) steps. (See also 5315* fixed_power_int()) 5316* 5317* The calculation is approximated on a 128 point scale. 5318*/ 5319#define DEGRADE_SHIFT\t7 5320 5321static const u8 degrade_zero_ticks[CPU_LOAD_IDX_MAX] = {0, 8, 32, 64, 128}; 5322static const u8 degrade_factor[CPU_LOAD_IDX_MAX][DEGRADE_SHIFT + 1] = { 5323\t{ 0, 0, 0, 0, 0, 0, 0, 0 }, 5324\t{ 64, 32, 8, 0, 0, 0, 0, 0 }, 5325\t{ 96, 72, 40, 12, 1, 0, 0, 0 }, 5326\t{ 112, 98, 75, 43, 15, 1, 0, 0 }, 5327\t{ 120, 112, 98, 76, 45, 16, 2, 0 } 5328}; 5329 5330/* 5331* Update cpu_load for any missed ticks, due to tickless idle. The backlog 5332* would be when CPU is idle and so we just decay the old load without 5333* adding any new load. 5334*/ 5335static unsigned long 5336decay_load_missed(unsigned long load, unsigned long missed_updates, int idx) 5337{ 5338\tint j = 0; 5339 5340\tif (!missed_updates) 5341\treturn load; 5342 5343\tif (missed_updates \u0026gt;= degrade_zero_ticks[idx]) 5344\treturn 0; 5345 5346\tif (idx == 1) 5347\treturn load \u0026gt;\u0026gt; missed_updates; 5348 5349\twhile (missed_updates) { 5350\tif (missed_updates % 2) 5351\tload = (load * degrade_factor[idx][j]) \u0026gt;\u0026gt; DEGRADE_SHIFT; 5352 5353\tmissed_updates \u0026gt;\u0026gt;= 1; 5354\tj++; 5355\t} 5356\treturn load; 5357} 5358 5359static struct { 5360\tcpumask_var_t idle_cpus_mask; 5361\tatomic_t nr_cpus; 5362\tint has_blocked;\t/* Idle CPUS has blocked load */ 5363\tunsigned long next_balance; /* in jiffy units */ 5364\tunsigned long next_blocked;\t/* Next update of blocked load in jiffies */ 5365} nohz ____cacheline_aligned; 5366 5367#endif /* CONFIG_NO_HZ_COMMON */5368 5369/** 5370* __cpu_load_update - update the rq-\u0026gt;cpu_load[] statistics 5371* @this_rq: The rq to update statistics for 5372* @this_load: The current load 5373* @pending_updates: The number of missed updates 5374* 5375* Update rq-\u0026gt;cpu_load[] statistics. This function is usually called every 5376* scheduler tick (TICK_NSEC). 5377* 5378* This function computes a decaying average: 5379* 5380* load[i]\u0026#39; = (1 - 1/2^i) * load[i] + (1/2^i) * load 5381* 5382* Because of NOHZ it might not get called on every tick which gives need for 5383* the @pending_updates argument. 5384* 5385* load[i]_n = (1 - 1/2^i) * load[i]_n-1 + (1/2^i) * load_n-1 5386* = A * load[i]_n-1 + B ; A := (1 - 1/2^i), B := (1/2^i) * load 5387* = A * (A * load[i]_n-2 + B) + B 5388* = A * (A * (A * load[i]_n-3 + B) + B) + B 5389* = A^3 * load[i]_n-3 + (A^2 + A + 1) * B 5390* = A^n * load[i]_0 + (A^(n-1) + A^(n-2) + ... + 1) * B 5391* = A^n * load[i]_0 + ((1 - A^n) / (1 - A)) * B 5392* = (1 - 1/2^i)^n * (load[i]_0 - load) + load 5393* 5394* In the above we\u0026#39;ve assumed load_n := load, which is true for NOHZ_FULL as 5395* any change in load would have resulted in the tick being turned back on. 5396* 5397* For regular NOHZ, this reduces to: 5398* 5399* load[i]_n = (1 - 1/2^i)^n * load[i]_0 5400* 5401* see decay_load_misses(). For NOHZ_FULL we get to subtract and add the extra 5402* term. 5403*/ 5404static void cpu_load_update(struct rq *this_rq, unsigned long this_load, 5405\tunsigned long pending_updates) 5406{ 5407\tunsigned long __maybe_unused tickless_load = this_rq-\u0026gt;cpu_load[0]; 5408\tint i, scale; 5409 5410\tthis_rq-\u0026gt;nr_load_updates++; 5411 5412\t/* Update our load: */ 5413\tthis_rq-\u0026gt;cpu_load[0] = this_load; /* Fasttrack for idx 0 */ 5414\tfor (i = 1, scale = 2; i \u0026lt; CPU_LOAD_IDX_MAX; i++, scale += scale) { 5415\tunsigned long old_load, new_load; 5416 5417\t/* scale is effectively 1 \u0026lt;\u0026lt; i now, and \u0026gt;\u0026gt; i divides by scale */ 5418 5419\told_load = this_rq-\u0026gt;cpu_load[i]; 5420#ifdef CONFIG_NO_HZ_COMMON 5421\told_load = decay_load_missed(old_load, pending_updates - 1, i); 5422\tif (tickless_load) { 5423\told_load -= decay_load_missed(tickless_load, pending_updates - 1, i); 5424\t/* 5425* old_load can never be a negative value because a 5426* decayed tickless_load cannot be greater than the 5427* original tickless_load. 5428*/ 5429\told_load += tickless_load; 5430\t} 5431#endif 5432\tnew_load = this_load; 5433\t/* 5434* Round up the averaging division if load is increasing. This 5435* prevents us from getting stuck on 9 if the load is 10, for 5436* example. 5437*/ 5438\tif (new_load \u0026gt; old_load) 5439\tnew_load += scale - 1; 5440 5441\tthis_rq-\u0026gt;cpu_load[i] = (old_load * (scale - 1) + new_load) \u0026gt;\u0026gt; i; 5442\t} 5443} 5444 5445/* Used instead of source_load when we know the type == 0 */ 5446static unsigned long weighted_cpuload(struct rq *rq) 5447{ 5448\treturn cfs_rq_runnable_load_avg(\u0026amp;rq-\u0026gt;cfs); 5449} 5450 5451#ifdef CONFIG_NO_HZ_COMMON 5452/* 5453* There is no sane way to deal with nohz on smp when using jiffies because the 5454* CPU doing the jiffies update might drift wrt the CPU doing the jiffy reading 5455* causing off-by-one errors in observed deltas; {0,2} instead of {1,1}. 5456* 5457* Therefore we need to avoid the delta approach from the regular tick when 5458* possible since that would seriously skew the load calculation. This is why we 5459* use cpu_load_update_periodic() for CPUs out of nohz. However we\u0026#39;ll rely on 5460* jiffies deltas for updates happening while in nohz mode (idle ticks, idle 5461* loop exit, nohz_idle_balance, nohz full exit...) 5462* 5463* This means we might still be one tick off for nohz periods. 5464*/ 5465 5466static void cpu_load_update_nohz(struct rq *this_rq, 5467\tunsigned long curr_jiffies, 5468\tunsigned long load) 5469{ 5470\tunsigned long pending_updates; 5471 5472\tpending_updates = curr_jiffies - this_rq-\u0026gt;last_load_update_tick; 5473\tif (pending_updates) { 5474\tthis_rq-\u0026gt;last_load_update_tick = curr_jiffies; 5475\t/* 5476* In the regular NOHZ case, we were idle, this means load 0. 5477* In the NOHZ_FULL case, we were non-idle, we should consider 5478* its weighted load. 5479*/ 5480\tcpu_load_update(this_rq, load, pending_updates); 5481\t} 5482} 5483 5484/* 5485* Called from nohz_idle_balance() to update the load ratings before doing the 5486* idle balance. 5487*/ 5488static void cpu_load_update_idle(struct rq *this_rq) 5489{ 5490\t/* 5491* bail if there\u0026#39;s load or we\u0026#39;re actually up-to-date. 5492*/ 5493\tif (weighted_cpuload(this_rq)) 5494\treturn; 5495 5496\tcpu_load_update_nohz(this_rq, READ_ONCE(jiffies), 0); 5497} 5498 5499/* 5500* Record CPU load on nohz entry so we know the tickless load to account 5501* on nohz exit. cpu_load[0] happens then to be updated more frequently 5502* than other cpu_load[idx] but it should be fine as cpu_load readers 5503* shouldn\u0026#39;t rely into synchronized cpu_load[*] updates. 5504*/ 5505void cpu_load_update_nohz_start(void) 5506{ 5507\tstruct rq *this_rq = this_rq(); 5508 5509\t/* 5510* This is all lockless but should be fine. If weighted_cpuload changes 5511* concurrently we\u0026#39;ll exit nohz. And cpu_load write can race with 5512* cpu_load_update_idle() but both updater would be writing the same. 5513*/ 5514\tthis_rq-\u0026gt;cpu_load[0] = weighted_cpuload(this_rq); 5515} 5516 5517/* 5518* Account the tickless load in the end of a nohz frame. 5519*/ 5520void cpu_load_update_nohz_stop(void) 5521{ 5522\tunsigned long curr_jiffies = READ_ONCE(jiffies); 5523\tstruct rq *this_rq = this_rq(); 5524\tunsigned long load; 5525\tstruct rq_flags rf; 5526 5527\tif (curr_jiffies == this_rq-\u0026gt;last_load_update_tick) 5528\treturn; 5529 5530\tload = weighted_cpuload(this_rq); 5531\trq_lock(this_rq, \u0026amp;rf); 5532\tupdate_rq_clock(this_rq); 5533\tcpu_load_update_nohz(this_rq, curr_jiffies, load); 5534\trq_unlock(this_rq, \u0026amp;rf); 5535} 5536#else /* !CONFIG_NO_HZ_COMMON */5537static inline void cpu_load_update_nohz(struct rq *this_rq, 5538\tunsigned long curr_jiffies, 5539\tunsigned long load) { } 5540#endif /* CONFIG_NO_HZ_COMMON */5541 5542static void cpu_load_update_periodic(struct rq *this_rq, unsigned long load) 5543{ 5544#ifdef CONFIG_NO_HZ_COMMON 5545\t/* See the mess around cpu_load_update_nohz(). */ 5546\tthis_rq-\u0026gt;last_load_update_tick = READ_ONCE(jiffies); 5547#endif 5548\tcpu_load_update(this_rq, load, 1); 5549} 5550 5551/* 5552* Called from scheduler_tick() 5553*/ 5554void cpu_load_update_active(struct rq *this_rq) 5555{ 5556\tunsigned long load = weighted_cpuload(this_rq); 5557 5558\tif (tick_nohz_tick_stopped()) 5559\tcpu_load_update_nohz(this_rq, READ_ONCE(jiffies), load); 5560\telse 5561\tcpu_load_update_periodic(this_rq, load); 5562} 5563 5564/* 5565* Return a low guess at the load of a migration-source CPU weighted 5566* according to the scheduling class and \u0026#34;nice\u0026#34; value. 5567* 5568* We want to under-estimate the load of migration sources, to 5569* balance conservatively. 5570*/ 5571static unsigned long source_load(int cpu, int type) 5572{ 5573\tstruct rq *rq = cpu_rq(cpu); 5574\tunsigned long total = weighted_cpuload(rq); 5575 5576\tif (type == 0 || !sched_feat(LB_BIAS)) 5577\treturn total; 5578 5579\treturn min(rq-\u0026gt;cpu_load[type-1], total); 5580} 5581 5582/* 5583* Return a high guess at the load of a migration-target CPU weighted 5584* according to the scheduling class and \u0026#34;nice\u0026#34; value. 5585*/ 5586static unsigned long target_load(int cpu, int type) 5587{ 5588\tstruct rq *rq = cpu_rq(cpu); 5589\tunsigned long total = weighted_cpuload(rq); 5590 5591\tif (type == 0 || !sched_feat(LB_BIAS)) 5592\treturn total; 5593 5594\treturn max(rq-\u0026gt;cpu_load[type-1], total); 5595} 5596 5597static unsigned long capacity_of(int cpu) 5598{ 5599\treturn cpu_rq(cpu)-\u0026gt;cpu_capacity; 5600} 5601 5602static unsigned long capacity_orig_of(int cpu) 5603{ 5604\treturn cpu_rq(cpu)-\u0026gt;cpu_capacity_orig; 5605} 5606 5607static unsigned long cpu_avg_load_per_task(int cpu) 5608{ 5609\tstruct rq *rq = cpu_rq(cpu); 5610\tunsigned long nr_running = READ_ONCE(rq-\u0026gt;cfs.h_nr_running); 5611\tunsigned long load_avg = weighted_cpuload(rq); 5612 5613\tif (nr_running) 5614\treturn load_avg / nr_running; 5615 5616\treturn 0; 5617} 5618 5619static void record_wakee(struct task_struct *p) 5620{ 5621\t/* 5622* Only decay a single time; tasks that have less then 1 wakeup per 5623* jiffy will not have built up many flips. 5624*/ 5625\tif (time_after(jiffies, current-\u0026gt;wakee_flip_decay_ts + HZ)) { 5626\tcurrent-\u0026gt;wakee_flips \u0026gt;\u0026gt;= 1; 5627\tcurrent-\u0026gt;wakee_flip_decay_ts = jiffies; 5628\t} 5629 5630\tif (current-\u0026gt;last_wakee != p) { 5631\tcurrent-\u0026gt;last_wakee = p; 5632\tcurrent-\u0026gt;wakee_flips++; 5633\t} 5634} 5635 5636/* 5637* Detect M:N waker/wakee relationships via a switching-frequency heuristic. 5638* 5639* A waker of many should wake a different task than the one last awakened 5640* at a frequency roughly N times higher than one of its wakees. 5641* 5642* In order to determine whether we should let the load spread vs consolidating 5643* to shared cache, we look for a minimum \u0026#39;flip\u0026#39; frequency of llc_size in one 5644* partner, and a factor of lls_size higher frequency in the other. 5645* 5646* With both conditions met, we can be relatively sure that the relationship is 5647* non-monogamous, with partner count exceeding socket size. 5648* 5649* Waker/wakee being client/server, worker/dispatcher, interrupt source or 5650* whatever is irrelevant, spread criteria is apparent partner count exceeds 5651* socket size. 5652*/ 5653static int wake_wide(struct task_struct *p) 5654{ 5655\tunsigned int master = current-\u0026gt;wakee_flips; 5656\tunsigned int slave = p-\u0026gt;wakee_flips; 5657\tint factor = this_cpu_read(sd_llc_size); 5658 5659\tif (master \u0026lt; slave) 5660\tswap(master, slave); 5661\tif (slave \u0026lt; factor || master \u0026lt; slave * factor) 5662\treturn 0; 5663\treturn 1; 5664} 5665 5666/* 5667* The purpose of wake_affine() is to quickly determine on which CPU we can run 5668* soonest. For the purpose of speed we only consider the waking and previous 5669* CPU. 5670* 5671* wake_affine_idle() - only considers \u0026#39;now\u0026#39;, it check if the waking CPU is 5672*\tcache-affine and is (or\twill be) idle. 5673* 5674* wake_affine_weight() - considers the weight to reflect the average 5675*\tscheduling latency of the CPUs. This seems to work 5676*\tfor the overloaded case. 5677*/ 5678static int 5679wake_affine_idle(int this_cpu, int prev_cpu, int sync) 5680{ 5681\t/* 5682* If this_cpu is idle, it implies the wakeup is from interrupt 5683* context. Only allow the move if cache is shared. Otherwise an 5684* interrupt intensive workload could force all tasks onto one 5685* node depending on the IO topology or IRQ affinity settings. 5686* 5687* If the prev_cpu is idle and cache affine then avoid a migration. 5688* There is no guarantee that the cache hot data from an interrupt 5689* is more important than cache hot data on the prev_cpu and from 5690* a cpufreq perspective, it\u0026#39;s better to have higher utilisation 5691* on one CPU. 5692*/ 5693\tif (available_idle_cpu(this_cpu) \u0026amp;\u0026amp; cpus_share_cache(this_cpu, prev_cpu)) 5694\treturn available_idle_cpu(prev_cpu) ? prev_cpu : this_cpu; 5695 5696\tif (sync \u0026amp;\u0026amp; cpu_rq(this_cpu)-\u0026gt;nr_running == 1) 5697\treturn this_cpu; 5698 5699\treturn nr_cpumask_bits; 5700} 5701 5702static int 5703wake_affine_weight(struct sched_domain *sd, struct task_struct *p, 5704\tint this_cpu, int prev_cpu, int sync) 5705{ 5706\ts64 this_eff_load, prev_eff_load; 5707\tunsigned long task_load; 5708 5709\tthis_eff_load = target_load(this_cpu, sd-\u0026gt;wake_idx); 5710 5711\tif (sync) { 5712\tunsigned long current_load = task_h_load(current); 5713 5714\tif (current_load \u0026gt; this_eff_load) 5715\treturn this_cpu; 5716 5717\tthis_eff_load -= current_load; 5718\t} 5719 5720\ttask_load = task_h_load(p); 5721 5722\tthis_eff_load += task_load; 5723\tif (sched_feat(WA_BIAS)) 5724\tthis_eff_load *= 100; 5725\tthis_eff_load *= capacity_of(prev_cpu); 5726 5727\tprev_eff_load = source_load(prev_cpu, sd-\u0026gt;wake_idx); 5728\tprev_eff_load -= task_load; 5729\tif (sched_feat(WA_BIAS)) 5730\tprev_eff_load *= 100 + (sd-\u0026gt;imbalance_pct - 100) / 2; 5731\tprev_eff_load *= capacity_of(this_cpu); 5732 5733\t/* 5734* If sync, adjust the weight of prev_eff_load such that if 5735* prev_eff == this_eff that select_idle_sibling() will consider 5736* stacking the wakee on top of the waker if no other CPU is 5737* idle. 5738*/ 5739\tif (sync) 5740\tprev_eff_load += 1; 5741 5742\treturn this_eff_load \u0026lt; prev_eff_load ? this_cpu : nr_cpumask_bits; 5743} 5744 5745static int wake_affine(struct sched_domain *sd, struct task_struct *p, 5746\tint this_cpu, int prev_cpu, int sync) 5747{ 5748\tint target = nr_cpumask_bits; 5749 5750\tif (sched_feat(WA_IDLE)) 5751\ttarget = wake_affine_idle(this_cpu, prev_cpu, sync); 5752 5753\tif (sched_feat(WA_WEIGHT) \u0026amp;\u0026amp; target == nr_cpumask_bits) 5754\ttarget = wake_affine_weight(sd, p, this_cpu, prev_cpu, sync); 5755 5756\tschedstat_inc(p-\u0026gt;se.statistics.nr_wakeups_affine_attempts); 5757\tif (target == nr_cpumask_bits) 5758\treturn prev_cpu; 5759 5760\tschedstat_inc(sd-\u0026gt;ttwu_move_affine); 5761\tschedstat_inc(p-\u0026gt;se.statistics.nr_wakeups_affine); 5762\treturn target; 5763} 5764 5765static unsigned long cpu_util_without(int cpu, struct task_struct *p); 5766 5767static unsigned long capacity_spare_without(int cpu, struct task_struct *p) 5768{ 5769\treturn max_t(long, capacity_of(cpu) - cpu_util_without(cpu, p), 0); 5770} 5771 5772/* 5773* find_idlest_group finds and returns the least busy CPU group within the 5774* domain. 5775* 5776* Assumes p is allowed on at least one CPU in sd. 5777*/ 5778static struct sched_group * 5779find_idlest_group(struct sched_domain *sd, struct task_struct *p, 5780\tint this_cpu, int sd_flag) 5781{ 5782\tstruct sched_group *idlest = NULL, *group = sd-\u0026gt;groups; 5783\tstruct sched_group *most_spare_sg = NULL; 5784\tunsigned long min_runnable_load = ULONG_MAX; 5785\tunsigned long this_runnable_load = ULONG_MAX; 5786\tunsigned long min_avg_load = ULONG_MAX, this_avg_load = ULONG_MAX; 5787\tunsigned long most_spare = 0, this_spare = 0; 5788\tint load_idx = sd-\u0026gt;forkexec_idx; 5789\tint imbalance_scale = 100 + (sd-\u0026gt;imbalance_pct-100)/2; 5790\tunsigned long imbalance = scale_load_down(NICE_0_LOAD) * 5791\t(sd-\u0026gt;imbalance_pct-100) / 100; 5792 5793\tif (sd_flag \u0026amp; SD_BALANCE_WAKE) 5794\tload_idx = sd-\u0026gt;wake_idx; 5795 5796\tdo { 5797\tunsigned long load, avg_load, runnable_load; 5798\tunsigned long spare_cap, max_spare_cap; 5799\tint local_group; 5800\tint i; 5801 5802\t/* Skip over this group if it has no CPUs allowed */ 5803\tif (!cpumask_intersects(sched_group_span(group), 5804\t\u0026amp;p-\u0026gt;cpus_allowed)) 5805\tcontinue; 5806 5807\tlocal_group = cpumask_test_cpu(this_cpu, 5808\tsched_group_span(group)); 5809 5810\t/* 5811* Tally up the load of all CPUs in the group and find 5812* the group containing the CPU with most spare capacity. 5813*/ 5814\tavg_load = 0; 5815\trunnable_load = 0; 5816\tmax_spare_cap = 0; 5817 5818\tfor_each_cpu(i, sched_group_span(group)) { 5819\t/* Bias balancing toward CPUs of our domain */ 5820\tif (local_group) 5821\tload = source_load(i, load_idx); 5822\telse 5823\tload = target_load(i, load_idx); 5824 5825\trunnable_load += load; 5826 5827\tavg_load += cfs_rq_load_avg(\u0026amp;cpu_rq(i)-\u0026gt;cfs); 5828 5829\tspare_cap = capacity_spare_without(i, p); 5830 5831\tif (spare_cap \u0026gt; max_spare_cap) 5832\tmax_spare_cap = spare_cap; 5833\t} 5834 5835\t/* Adjust by relative CPU capacity of the group */ 5836\tavg_load = (avg_load * SCHED_CAPACITY_SCALE) / 5837\tgroup-\u0026gt;sgc-\u0026gt;capacity; 5838\trunnable_load = (runnable_load * SCHED_CAPACITY_SCALE) / 5839\tgroup-\u0026gt;sgc-\u0026gt;capacity; 5840 5841\tif (local_group) { 5842\tthis_runnable_load = runnable_load; 5843\tthis_avg_load = avg_load; 5844\tthis_spare = max_spare_cap; 5845\t} else { 5846\tif (min_runnable_load \u0026gt; (runnable_load + imbalance)) { 5847\t/* 5848* The runnable load is significantly smaller 5849* so we can pick this new CPU: 5850*/ 5851\tmin_runnable_load = runnable_load; 5852\tmin_avg_load = avg_load; 5853\tidlest = group; 5854\t} else if ((runnable_load \u0026lt; (min_runnable_load + imbalance)) \u0026amp;\u0026amp; 5855\t(100*min_avg_load \u0026gt; imbalance_scale*avg_load)) { 5856\t/* 5857* The runnable loads are close so take the 5858* blocked load into account through avg_load: 5859*/ 5860\tmin_avg_load = avg_load; 5861\tidlest = group; 5862\t} 5863 5864\tif (most_spare \u0026lt; max_spare_cap) { 5865\tmost_spare = max_spare_cap; 5866\tmost_spare_sg = group; 5867\t} 5868\t} 5869\t} while (group = group-\u0026gt;next, group != sd-\u0026gt;groups); 5870 5871\t/* 5872* The cross-over point between using spare capacity or least load 5873* is too conservative for high utilization tasks on partially 5874* utilized systems if we require spare_capacity \u0026gt; task_util(p), 5875* so we allow for some task stuffing by using 5876* spare_capacity \u0026gt; task_util(p)/2. 5877* 5878* Spare capacity can\u0026#39;t be used for fork because the utilization has 5879* not been set yet, we must first select a rq to compute the initial 5880* utilization. 5881*/ 5882\tif (sd_flag \u0026amp; SD_BALANCE_FORK) 5883\tgoto skip_spare; 5884 5885\tif (this_spare \u0026gt; task_util(p) / 2 \u0026amp;\u0026amp; 5886\timbalance_scale*this_spare \u0026gt; 100*most_spare) 5887\treturn NULL; 5888 5889\tif (most_spare \u0026gt; task_util(p) / 2) 5890\treturn most_spare_sg; 5891 5892skip_spare: 5893\tif (!idlest) 5894\treturn NULL; 5895 5896\t/* 5897* When comparing groups across NUMA domains, it\u0026#39;s possible for the 5898* local domain to be very lightly loaded relative to the remote 5899* domains but \u0026#34;imbalance\u0026#34; skews the comparison making remote CPUs 5900* look much more favourable. When considering cross-domain, add 5901* imbalance to the runnable load on the remote node and consider 5902* staying local. 5903*/ 5904\tif ((sd-\u0026gt;flags \u0026amp; SD_NUMA) \u0026amp;\u0026amp; 5905\tmin_runnable_load + imbalance \u0026gt;= this_runnable_load) 5906\treturn NULL; 5907 5908\tif (min_runnable_load \u0026gt; (this_runnable_load + imbalance)) 5909\treturn NULL; 5910 5911\tif ((this_runnable_load \u0026lt; (min_runnable_load + imbalance)) \u0026amp;\u0026amp; 5912\t(100*this_avg_load \u0026lt; imbalance_scale*min_avg_load)) 5913\treturn NULL; 5914 5915\treturn idlest; 5916} 5917 5918/* 5919* find_idlest_group_cpu - find the idlest CPU among the CPUs in the group. 5920*/ 5921static int 5922find_idlest_group_cpu(struct sched_group *group, struct task_struct *p, int this_cpu) 5923{ 5924\tunsigned long load, min_load = ULONG_MAX; 5925\tunsigned int min_exit_latency = UINT_MAX; 5926\tu64 latest_idle_timestamp = 0; 5927\tint least_loaded_cpu = this_cpu; 5928\tint shallowest_idle_cpu = -1; 5929\tint i; 5930 5931\t/* Check if we have any choice: */ 5932\tif (group-\u0026gt;group_weight == 1) 5933\treturn cpumask_first(sched_group_span(group)); 5934 5935\t/* Traverse only the allowed CPUs */ 5936\tfor_each_cpu_and(i, sched_group_span(group), \u0026amp;p-\u0026gt;cpus_allowed) { 5937\tif (available_idle_cpu(i)) { 5938\tstruct rq *rq = cpu_rq(i); 5939\tstruct cpuidle_state *idle = idle_get_state(rq); 5940\tif (idle \u0026amp;\u0026amp; idle-\u0026gt;exit_latency \u0026lt; min_exit_latency) { 5941\t/* 5942* We give priority to a CPU whose idle state 5943* has the smallest exit latency irrespective 5944* of any idle timestamp. 5945*/ 5946\tmin_exit_latency = idle-\u0026gt;exit_latency; 5947\tlatest_idle_timestamp = rq-\u0026gt;idle_stamp; 5948\tshallowest_idle_cpu = i; 5949\t} else if ((!idle || idle-\u0026gt;exit_latency == min_exit_latency) \u0026amp;\u0026amp; 5950\trq-\u0026gt;idle_stamp \u0026gt; latest_idle_timestamp) { 5951\t/* 5952* If equal or no active idle state, then 5953* the most recently idled CPU might have 5954* a warmer cache. 5955*/ 5956\tlatest_idle_timestamp = rq-\u0026gt;idle_stamp; 5957\tshallowest_idle_cpu = i; 5958\t} 5959\t} else if (shallowest_idle_cpu == -1) { 5960\tload = weighted_cpuload(cpu_rq(i)); 5961\tif (load \u0026lt; min_load) { 5962\tmin_load = load; 5963\tleast_loaded_cpu = i; 5964\t} 5965\t} 5966\t} 5967 5968\treturn shallowest_idle_cpu != -1 ? shallowest_idle_cpu : least_loaded_cpu; 5969} 5970 5971static inline int find_idlest_cpu(struct sched_domain *sd, struct task_struct *p, 5972\tint cpu, int prev_cpu, int sd_flag) 5973{ 5974\tint new_cpu = cpu; 5975 5976\tif (!cpumask_intersects(sched_domain_span(sd), \u0026amp;p-\u0026gt;cpus_allowed)) 5977\treturn prev_cpu; 5978 5979\t/* 5980* We need task\u0026#39;s util for capacity_spare_without, sync it up to 5981* prev_cpu\u0026#39;s last_update_time. 5982*/ 5983\tif (!(sd_flag \u0026amp; SD_BALANCE_FORK)) 5984\tsync_entity_load_avg(\u0026amp;p-\u0026gt;se); 5985 5986\twhile (sd) { 5987\tstruct sched_group *group; 5988\tstruct sched_domain *tmp; 5989\tint weight; 5990 5991\tif (!(sd-\u0026gt;flags \u0026amp; sd_flag)) { 5992\tsd = sd-\u0026gt;child; 5993\tcontinue; 5994\t} 5995 5996\tgroup = find_idlest_group(sd, p, cpu, sd_flag); 5997\tif (!group) { 5998\tsd = sd-\u0026gt;child; 5999\tcontinue; 6000\t} 6001 6002\tnew_cpu = find_idlest_group_cpu(group, p, cpu); 6003\tif (new_cpu == cpu) { 6004\t/* Now try balancing at a lower domain level of \u0026#39;cpu\u0026#39;: */ 6005\tsd = sd-\u0026gt;child; 6006\tcontinue; 6007\t} 6008 6009\t/* Now try balancing at a lower domain level of \u0026#39;new_cpu\u0026#39;: */ 6010\tcpu = new_cpu; 6011\tweight = sd-\u0026gt;span_weight; 6012\tsd = NULL; 6013\tfor_each_domain(cpu, tmp) { 6014\tif (weight \u0026lt;= tmp-\u0026gt;span_weight) 6015\tbreak; 6016\tif (tmp-\u0026gt;flags \u0026amp; sd_flag) 6017\tsd = tmp; 6018\t} 6019\t} 6020 6021\treturn new_cpu; 6022} 6023 6024#ifdef CONFIG_SCHED_SMT 6025DEFINE_STATIC_KEY_FALSE(sched_smt_present); 6026EXPORT_SYMBOL_GPL(sched_smt_present); 6027 6028static inline void set_idle_cores(int cpu, int val) 6029{ 6030\tstruct sched_domain_shared *sds; 6031 6032\tsds = rcu_dereference(per_cpu(sd_llc_shared, cpu)); 6033\tif (sds) 6034\tWRITE_ONCE(sds-\u0026gt;has_idle_cores, val); 6035} 6036 6037static inline bool test_idle_cores(int cpu, bool def) 6038{ 6039\tstruct sched_domain_shared *sds; 6040 6041\tsds = rcu_dereference(per_cpu(sd_llc_shared, cpu)); 6042\tif (sds) 6043\treturn READ_ONCE(sds-\u0026gt;has_idle_cores); 6044 6045\treturn def; 6046} 6047 6048/* 6049* Scans the local SMT mask to see if the entire core is idle, and records this 6050* information in sd_llc_shared-\u0026gt;has_idle_cores. 6051* 6052* Since SMT siblings share all cache levels, inspecting this limited remote 6053* state should be fairly cheap. 6054*/ 6055void __update_idle_core(struct rq *rq) 6056{ 6057\tint core = cpu_of(rq); 6058\tint cpu; 6059 6060\trcu_read_lock(); 6061\tif (test_idle_cores(core, true)) 6062\tgoto unlock; 6063 6064\tfor_each_cpu(cpu, cpu_smt_mask(core)) { 6065\tif (cpu == core) 6066\tcontinue; 6067 6068\tif (!available_idle_cpu(cpu)) 6069\tgoto unlock; 6070\t} 6071 6072\tset_idle_cores(core, 1); 6073unlock: 6074\trcu_read_unlock(); 6075} 6076 6077/* 6078* Scan the entire LLC domain for idle cores; this dynamically switches off if 6079* there are no idle cores left in the system; tracked through 6080* sd_llc-\u0026gt;shared-\u0026gt;has_idle_cores and enabled through update_idle_core() above. 6081*/ 6082static int select_idle_core(struct task_struct *p, struct sched_domain *sd, int target) 6083{ 6084\tstruct cpumask *cpus = this_cpu_cpumask_var_ptr(select_idle_mask); 6085\tint core, cpu; 6086 6087\tif (!static_branch_likely(\u0026amp;sched_smt_present)) 6088\treturn -1; 6089 6090\tif (!test_idle_cores(target, false)) 6091\treturn -1; 6092 6093\tcpumask_and(cpus, sched_domain_span(sd), \u0026amp;p-\u0026gt;cpus_allowed); 6094 6095\tfor_each_cpu_wrap(core, cpus, target) { 6096\tbool idle = true; 6097 6098\tfor_each_cpu(cpu, cpu_smt_mask(core)) { 6099\tcpumask_clear_cpu(cpu, cpus); 6100\tif (!available_idle_cpu(cpu)) 6101\tidle = false; 6102\t} 6103 6104\tif (idle) 6105\treturn core; 6106\t} 6107 6108\t/* 6109* Failed to find an idle core; stop looking for one. 6110*/ 6111\tset_idle_cores(target, 0); 6112 6113\treturn -1; 6114} 6115 6116/* 6117* Scan the local SMT mask for idle CPUs. 6118*/ 6119static int select_idle_smt(struct task_struct *p, struct sched_domain *sd, int target) 6120{ 6121\tint cpu; 6122 6123\tif (!static_branch_likely(\u0026amp;sched_smt_present)) 6124\treturn -1; 6125 6126\tfor_each_cpu(cpu, cpu_smt_mask(target)) { 6127\tif (!cpumask_test_cpu(cpu, \u0026amp;p-\u0026gt;cpus_allowed)) 6128\tcontinue; 6129\tif (available_idle_cpu(cpu)) 6130\treturn cpu; 6131\t} 6132 6133\treturn -1; 6134} 6135 6136#else /* CONFIG_SCHED_SMT */6137 6138static inline int select_idle_core(struct task_struct *p, struct sched_domain *sd, int target) 6139{ 6140\treturn -1; 6141} 6142 6143static inline int select_idle_smt(struct task_struct *p, struct sched_domain *sd, int target) 6144{ 6145\treturn -1; 6146} 6147 6148#endif /* CONFIG_SCHED_SMT */6149 6150/* 6151* Scan the LLC domain for idle CPUs; this is dynamically regulated by 6152* comparing the average scan cost (tracked in sd-\u0026gt;avg_scan_cost) against the 6153* average idle time for this rq (as found in rq-\u0026gt;avg_idle). 6154*/ 6155static int select_idle_cpu(struct task_struct *p, struct sched_domain *sd, int target) 6156{ 6157\tstruct sched_domain *this_sd; 6158\tu64 avg_cost, avg_idle; 6159\tu64 time, cost; 6160\ts64 delta; 6161\tint cpu, nr = INT_MAX; 6162 6163\tthis_sd = rcu_dereference(*this_cpu_ptr(\u0026amp;sd_llc)); 6164\tif (!this_sd) 6165\treturn -1; 6166 6167\t/* 6168* Due to large variance we need a large fuzz factor; hackbench in 6169* particularly is sensitive here. 6170*/ 6171\tavg_idle = this_rq()-\u0026gt;avg_idle / 512; 6172\tavg_cost = this_sd-\u0026gt;avg_scan_cost + 1; 6173 6174\tif (sched_feat(SIS_AVG_CPU) \u0026amp;\u0026amp; avg_idle \u0026lt; avg_cost) 6175\treturn -1; 6176 6177\tif (sched_feat(SIS_PROP)) { 6178\tu64 span_avg = sd-\u0026gt;span_weight * avg_idle; 6179\tif (span_avg \u0026gt; 4*avg_cost) 6180\tnr = div_u64(span_avg, avg_cost); 6181\telse 6182\tnr = 4; 6183\t} 6184 6185\ttime = local_clock(); 6186 6187\tfor_each_cpu_wrap(cpu, sched_domain_span(sd), target) { 6188\tif (!--nr) 6189\treturn -1; 6190\tif (!cpumask_test_cpu(cpu, \u0026amp;p-\u0026gt;cpus_allowed)) 6191\tcontinue; 6192\tif (available_idle_cpu(cpu)) 6193\tbreak; 6194\t} 6195 6196\ttime = local_clock() - time; 6197\tcost = this_sd-\u0026gt;avg_scan_cost; 6198\tdelta = (s64)(time - cost) / 8; 6199\tthis_sd-\u0026gt;avg_scan_cost += delta; 6200 6201\treturn cpu; 6202} 6203 6204/* 6205* Try and locate an idle core/thread in the LLC cache domain. 6206*/ 6207static int select_idle_sibling(struct task_struct *p, int prev, int target) 6208{ 6209\tstruct sched_domain *sd; 6210\tint i, recent_used_cpu; 6211 6212\tif (available_idle_cpu(target)) 6213\treturn target; 6214 6215\t/* 6216* If the previous CPU is cache affine and idle, don\u0026#39;t be stupid: 6217*/ 6218\tif (prev != target \u0026amp;\u0026amp; cpus_share_cache(prev, target) \u0026amp;\u0026amp; available_idle_cpu(prev)) 6219\treturn prev; 6220 6221\t/* Check a recently used CPU as a potential idle candidate: */ 6222\trecent_used_cpu = p-\u0026gt;recent_used_cpu; 6223\tif (recent_used_cpu != prev \u0026amp;\u0026amp; 6224\trecent_used_cpu != target \u0026amp;\u0026amp; 6225\tcpus_share_cache(recent_used_cpu, target) \u0026amp;\u0026amp; 6226\tavailable_idle_cpu(recent_used_cpu) \u0026amp;\u0026amp; 6227\tcpumask_test_cpu(p-\u0026gt;recent_used_cpu, \u0026amp;p-\u0026gt;cpus_allowed)) { 6228\t/* 6229* Replace recent_used_cpu with prev as it is a potential 6230* candidate for the next wake: 6231*/ 6232\tp-\u0026gt;recent_used_cpu = prev; 6233\treturn recent_used_cpu; 6234\t} 6235 6236\tsd = rcu_dereference(per_cpu(sd_llc, target)); 6237\tif (!sd) 6238\treturn target; 6239 6240\ti = select_idle_core(p, sd, target); 6241\tif ((unsigned)i \u0026lt; nr_cpumask_bits) 6242\treturn i; 6243 6244\ti = select_idle_cpu(p, sd, target); 6245\tif ((unsigned)i \u0026lt; nr_cpumask_bits) 6246\treturn i; 6247 6248\ti = select_idle_smt(p, sd, target); 6249\tif ((unsigned)i \u0026lt; nr_cpumask_bits) 6250\treturn i; 6251 6252\treturn target; 6253} 6254 6255/** 6256* Amount of capacity of a CPU that is (estimated to be) used by CFS tasks 6257* @cpu: the CPU to get the utilization of 6258* 6259* The unit of the return value must be the one of capacity so we can compare 6260* the utilization with the capacity of the CPU that is available for CFS task 6261* (ie cpu_capacity). 6262* 6263* cfs_rq.avg.util_avg is the sum of running time of runnable tasks plus the 6264* recent utilization of currently non-runnable tasks on a CPU. It represents 6265* the amount of utilization of a CPU in the range [0..capacity_orig] where 6266* capacity_orig is the cpu_capacity available at the highest frequency 6267* (arch_scale_freq_capacity()). 6268* The utilization of a CPU converges towards a sum equal to or less than the 6269* current capacity (capacity_curr \u0026lt;= capacity_orig) of the CPU because it is 6270* the running time on this CPU scaled by capacity_curr. 6271* 6272* The estimated utilization of a CPU is defined to be the maximum between its 6273* cfs_rq.avg.util_avg and the sum of the estimated utilization of the tasks 6274* currently RUNNABLE on that CPU. 6275* This allows to properly represent the expected utilization of a CPU which 6276* has just got a big task running since a long sleep period. At the same time 6277* however it preserves the benefits of the \u0026#34;blocked utilization\u0026#34; in 6278* describing the potential for other tasks waking up on the same CPU. 6279* 6280* Nevertheless, cfs_rq.avg.util_avg can be higher than capacity_curr or even 6281* higher than capacity_orig because of unfortunate rounding in 6282* cfs.avg.util_avg or just after migrating tasks and new task wakeups until 6283* the average stabilizes with the new running time. We need to check that the 6284* utilization stays within the range of [0..capacity_orig] and cap it if 6285* necessary. Without utilization capping, a group could be seen as overloaded 6286* (CPU0 utilization at 121% + CPU1 utilization at 80%) whereas CPU1 has 20% of 6287* available capacity. We allow utilization to overshoot capacity_curr (but not 6288* capacity_orig) as it useful for predicting the capacity required after task 6289* migrations (scheduler-driven DVFS). 6290* 6291* Return: the (estimated) utilization for the specified CPU 6292*/ 6293static inline unsigned long cpu_util(int cpu) 6294{ 6295\tstruct cfs_rq *cfs_rq; 6296\tunsigned int util; 6297 6298\tcfs_rq = \u0026amp;cpu_rq(cpu)-\u0026gt;cfs; 6299\tutil = READ_ONCE(cfs_rq-\u0026gt;avg.util_avg); 6300 6301\tif (sched_feat(UTIL_EST)) 6302\tutil = max(util, READ_ONCE(cfs_rq-\u0026gt;avg.util_est.enqueued)); 6303 6304\treturn min_t(unsigned long, util, capacity_orig_of(cpu)); 6305} 6306 6307/* 6308* cpu_util_without: compute cpu utilization without any contributions from *p 6309* @cpu: the CPU which utilization is requested 6310* @p: the task which utilization should be discounted 6311* 6312* The utilization of a CPU is defined by the utilization of tasks currently 6313* enqueued on that CPU as well as tasks which are currently sleeping after an 6314* execution on that CPU. 6315* 6316* This method returns the utilization of the specified CPU by discounting the 6317* utilization of the specified task, whenever the task is currently 6318* contributing to the CPU utilization. 6319*/ 6320static unsigned long cpu_util_without(int cpu, struct task_struct *p) 6321{ 6322\tstruct cfs_rq *cfs_rq; 6323\tunsigned int util; 6324 6325\t/* Task has no contribution or is new */ 6326\tif (cpu != task_cpu(p) || !READ_ONCE(p-\u0026gt;se.avg.last_update_time)) 6327\treturn cpu_util(cpu); 6328 6329\tcfs_rq = \u0026amp;cpu_rq(cpu)-\u0026gt;cfs; 6330\tutil = READ_ONCE(cfs_rq-\u0026gt;avg.util_avg); 6331 6332\t/* Discount task\u0026#39;s util from CPU\u0026#39;s util */ 6333\tutil -= min_t(unsigned int, util, task_util(p)); 6334 6335\t/* 6336* Covered cases: 6337* 6338* a) if *p is the only task sleeping on this CPU, then: 6339* cpu_util (== task_util) \u0026gt; util_est (== 0) 6340* and thus we return: 6341* cpu_util_without = (cpu_util - task_util) = 0 6342* 6343* b) if other tasks are SLEEPING on this CPU, which is now exiting 6344* IDLE, then: 6345* cpu_util \u0026gt;= task_util 6346* cpu_util \u0026gt; util_est (== 0) 6347* and thus we discount *p\u0026#39;s blocked utilization to return: 6348* cpu_util_without = (cpu_util - task_util) \u0026gt;= 0 6349* 6350* c) if other tasks are RUNNABLE on that CPU and 6351* util_est \u0026gt; cpu_util 6352* then we use util_est since it returns a more restrictive 6353* estimation of the spare capacity on that CPU, by just 6354* considering the expected utilization of tasks already 6355* runnable on that CPU. 6356* 6357* Cases a) and b) are covered by the above code, while case c) is 6358* covered by the following code when estimated utilization is 6359* enabled. 6360*/ 6361\tif (sched_feat(UTIL_EST)) { 6362\tunsigned int estimated = 6363\tREAD_ONCE(cfs_rq-\u0026gt;avg.util_est.enqueued); 6364 6365\t/* 6366* Despite the following checks we still have a small window 6367* for a possible race, when an execl\u0026#39;s select_task_rq_fair() 6368* races with LB\u0026#39;s detach_task(): 6369* 6370* detach_task() 6371* p-\u0026gt;on_rq = TASK_ON_RQ_MIGRATING; 6372* ---------------------------------- A 6373* deactivate_task() \\ 6374* dequeue_task() + RaceTime 6375* util_est_dequeue() / 6376* ---------------------------------- B 6377* 6378* The additional check on \u0026#34;current == p\u0026#34; it\u0026#39;s required to 6379* properly fix the execl regression and it helps in further 6380* reducing the chances for the above race. 6381*/ 6382\tif (unlikely(task_on_rq_queued(p) || current == p)) { 6383\testimated -= min_t(unsigned int, estimated, 6384\t(_task_util_est(p) | UTIL_AVG_UNCHANGED)); 6385\t} 6386\tutil = max(util, estimated); 6387\t} 6388 6389\t/* 6390* Utilization (estimated) can exceed the CPU capacity, thus let\u0026#39;s 6391* clamp to the maximum CPU capacity to ensure consistency with 6392* the cpu_util call. 6393*/ 6394\treturn min_t(unsigned long, util, capacity_orig_of(cpu)); 6395} 6396 6397/* 6398* Disable WAKE_AFFINE in the case where task @p doesn\u0026#39;t fit in the 6399* capacity of either the waking CPU @cpu or the previous CPU @prev_cpu. 6400* 6401* In that case WAKE_AFFINE doesn\u0026#39;t make sense and we\u0026#39;ll let 6402* BALANCE_WAKE sort things out. 6403*/ 6404static int wake_cap(struct task_struct *p, int cpu, int prev_cpu) 6405{ 6406\tlong min_cap, max_cap; 6407 6408\tmin_cap = min(capacity_orig_of(prev_cpu), capacity_orig_of(cpu)); 6409\tmax_cap = cpu_rq(cpu)-\u0026gt;rd-\u0026gt;max_cpu_capacity; 6410 6411\t/* Minimum capacity is close to max, no need to abort wake_affine */ 6412\tif (max_cap - min_cap \u0026lt; max_cap \u0026gt;\u0026gt; 3) 6413\treturn 0; 6414 6415\t/* Bring task utilization in sync with prev_cpu */ 6416\tsync_entity_load_avg(\u0026amp;p-\u0026gt;se); 6417 6418\treturn min_cap * 1024 \u0026lt; task_util(p) * capacity_margin; 6419} 6420 6421/* 6422* select_task_rq_fair: Select target runqueue for the waking task in domains 6423* that have the \u0026#39;sd_flag\u0026#39; flag set. In practice, this is SD_BALANCE_WAKE, 6424* SD_BALANCE_FORK, or SD_BALANCE_EXEC. 6425* 6426* Balances load by selecting the idlest CPU in the idlest group, or under 6427* certain conditions an idle sibling CPU if the domain has SD_WAKE_AFFINE set. 6428* 6429* Returns the target CPU number. 6430* 6431* preempt must be disabled. 6432*/ 6433static int 6434select_task_rq_fair(struct task_struct *p, int prev_cpu, int sd_flag, int wake_flags) 6435{ 6436\tstruct sched_domain *tmp, *sd = NULL; 6437\tint cpu = smp_processor_id(); 6438\tint new_cpu = prev_cpu; 6439\tint want_affine = 0; 6440\tint sync = (wake_flags \u0026amp; WF_SYNC) \u0026amp;\u0026amp; !(current-\u0026gt;flags \u0026amp; PF_EXITING); 6441 6442\tif (sd_flag \u0026amp; SD_BALANCE_WAKE) { 6443\trecord_wakee(p); 6444\twant_affine = !wake_wide(p) \u0026amp;\u0026amp; !wake_cap(p, cpu, prev_cpu) 6445\t\u0026amp;\u0026amp; cpumask_test_cpu(cpu, \u0026amp;p-\u0026gt;cpus_allowed); 6446\t} 6447 6448\trcu_read_lock(); 6449\tfor_each_domain(cpu, tmp) { 6450\tif (!(tmp-\u0026gt;flags \u0026amp; SD_LOAD_BALANCE)) 6451\tbreak; 6452 6453\t/* 6454* If both \u0026#39;cpu\u0026#39; and \u0026#39;prev_cpu\u0026#39; are part of this domain, 6455* cpu is a valid SD_WAKE_AFFINE target. 6456*/ 6457\tif (want_affine \u0026amp;\u0026amp; (tmp-\u0026gt;flags \u0026amp; SD_WAKE_AFFINE) \u0026amp;\u0026amp; 6458\tcpumask_test_cpu(prev_cpu, sched_domain_span(tmp))) { 6459\tif (cpu != prev_cpu) 6460\tnew_cpu = wake_affine(tmp, p, cpu, prev_cpu, sync); 6461 6462\tsd = NULL; /* Prefer wake_affine over balance flags */ 6463\tbreak; 6464\t} 6465 6466\tif (tmp-\u0026gt;flags \u0026amp; sd_flag) 6467\tsd = tmp; 6468\telse if (!want_affine) 6469\tbreak; 6470\t} 6471 6472\tif (unlikely(sd)) { 6473\t/* Slow path */ 6474\tnew_cpu = find_idlest_cpu(sd, p, cpu, prev_cpu, sd_flag); 6475\t} else if (sd_flag \u0026amp; SD_BALANCE_WAKE) { /* XXX always ? */ 6476\t/* Fast path */ 6477 6478\tnew_cpu = select_idle_sibling(p, prev_cpu, new_cpu); 6479 6480\tif (want_affine) 6481\tcurrent-\u0026gt;recent_used_cpu = cpu; 6482\t} 6483\trcu_read_unlock(); 6484 6485\treturn new_cpu; 6486} 6487 6488static void detach_entity_cfs_rq(struct sched_entity *se); 6489 6490/* 6491* Called immediately before a task is migrated to a new CPU; task_cpu(p) and 6492* cfs_rq_of(p) references at time of call are still valid and identify the 6493* previous CPU. The caller guarantees p-\u0026gt;pi_lock or task_rq(p)-\u0026gt;lock is held. 6494*/ 6495static void migrate_task_rq_fair(struct task_struct *p, int new_cpu) 6496{ 6497\t/* 6498* As blocked tasks retain absolute vruntime the migration needs to 6499* deal with this by subtracting the old and adding the new 6500* min_vruntime -- the latter is done by enqueue_entity() when placing 6501* the task on the new runqueue. 6502*/ 6503\tif (p-\u0026gt;state == TASK_WAKING) { 6504\tstruct sched_entity *se = \u0026amp;p-\u0026gt;se; 6505\tstruct cfs_rq *cfs_rq = cfs_rq_of(se); 6506\tu64 min_vruntime; 6507 6508#ifndef CONFIG_64BIT 6509\tu64 min_vruntime_copy; 6510 6511\tdo { 6512\tmin_vruntime_copy = cfs_rq-\u0026gt;min_vruntime_copy; 6513\tsmp_rmb(); 6514\tmin_vruntime = cfs_rq-\u0026gt;min_vruntime; 6515\t} while (min_vruntime != min_vruntime_copy); 6516#else 6517\tmin_vruntime = cfs_rq-\u0026gt;min_vruntime; 6518#endif 6519 6520\tse-\u0026gt;vruntime -= min_vruntime; 6521\t} 6522 6523\tif (p-\u0026gt;on_rq == TASK_ON_RQ_MIGRATING) { 6524\t/* 6525* In case of TASK_ON_RQ_MIGRATING we in fact hold the \u0026#39;old\u0026#39; 6526* rq-\u0026gt;lock and can modify state directly. 6527*/ 6528\tlockdep_assert_held(\u0026amp;task_rq(p)-\u0026gt;lock); 6529\tdetach_entity_cfs_rq(\u0026amp;p-\u0026gt;se); 6530 6531\t} else { 6532\t/* 6533* We are supposed to update the task to \u0026#34;current\u0026#34; time, then 6534* its up to date and ready to go to new CPU/cfs_rq. But we 6535* have difficulty in getting what current time is, so simply 6536* throw away the out-of-date time. This will result in the 6537* wakee task is less decayed, but giving the wakee more load 6538* sounds not bad. 6539*/ 6540\tremove_entity_load_avg(\u0026amp;p-\u0026gt;se); 6541\t} 6542 6543\t/* Tell new CPU we are migrated */ 6544\tp-\u0026gt;se.avg.last_update_time = 0; 6545 6546\t/* We have migrated, no longer consider this task hot */ 6547\tp-\u0026gt;se.exec_start = 0; 6548 6549\tupdate_scan_period(p, new_cpu); 6550} 6551 6552static void task_dead_fair(struct task_struct *p) 6553{ 6554\tremove_entity_load_avg(\u0026amp;p-\u0026gt;se); 6555} 6556#endif /* CONFIG_SMP */6557 6558static unsigned long wakeup_gran(struct sched_entity *se) 6559{ 6560\tunsigned long gran = sysctl_sched_wakeup_granularity; 6561 6562\t/* 6563* Since its curr running now, convert the gran from real-time 6564* to virtual-time in his units. 6565* 6566* By using \u0026#39;se\u0026#39; instead of \u0026#39;curr\u0026#39; we penalize light tasks, so 6567* they get preempted easier. That is, if \u0026#39;se\u0026#39; \u0026lt; \u0026#39;curr\u0026#39; then 6568* the resulting gran will be larger, therefore penalizing the 6569* lighter, if otoh \u0026#39;se\u0026#39; \u0026gt; \u0026#39;curr\u0026#39; then the resulting gran will 6570* be smaller, again penalizing the lighter task. 6571* 6572* This is especially important for buddies when the leftmost 6573* task is higher priority than the buddy. 6574*/ 6575\treturn calc_delta_fair(gran, se); 6576} 6577 6578/* 6579* Should \u0026#39;se\u0026#39; preempt \u0026#39;curr\u0026#39;. 6580* 6581* |s1 6582* |s2 6583* |s3 6584* g 6585* |\u0026lt;---\u0026gt;|c 6586* 6587* w(c, s1) = -1 6588* w(c, s2) = 0 6589* w(c, s3) = 1 6590* 6591*/ 6592static int 6593wakeup_preempt_entity(struct sched_entity *curr, struct sched_entity *se) 6594{ 6595\ts64 gran, vdiff = curr-\u0026gt;vruntime - se-\u0026gt;vruntime; 6596 6597\tif (vdiff \u0026lt;= 0) 6598\treturn -1; 6599 6600\tgran = wakeup_gran(se); 6601\tif (vdiff \u0026gt; gran) 6602\treturn 1; 6603 6604\treturn 0; 6605} 6606 6607static void set_last_buddy(struct sched_entity *se) 6608{ 6609\tif (entity_is_task(se) \u0026amp;\u0026amp; unlikely(task_of(se)-\u0026gt;policy == SCHED_IDLE)) 6610\treturn; 6611 6612\tfor_each_sched_entity(se) { 6613\tif (SCHED_WARN_ON(!se-\u0026gt;on_rq)) 6614\treturn; 6615\tcfs_rq_of(se)-\u0026gt;last = se; 6616\t} 6617} 6618 6619static void set_next_buddy(struct sched_entity *se) 6620{ 6621\tif (entity_is_task(se) \u0026amp;\u0026amp; unlikely(task_of(se)-\u0026gt;policy == SCHED_IDLE)) 6622\treturn; 6623 6624\tfor_each_sched_entity(se) { 6625\tif (SCHED_WARN_ON(!se-\u0026gt;on_rq)) 6626\treturn; 6627\tcfs_rq_of(se)-\u0026gt;next = se; 6628\t} 6629} 6630 6631static void set_skip_buddy(struct sched_entity *se) 6632{ 6633\tfor_each_sched_entity(se) 6634\tcfs_rq_of(se)-\u0026gt;skip = se; 6635} 6636 6637/* 6638* Preempt the current task with a newly woken task if needed: 6639*/ 6640static void check_preempt_wakeup(struct rq *rq, struct task_struct *p, int wake_flags) 6641{ 6642\tstruct task_struct *curr = rq-\u0026gt;curr; 6643\tstruct sched_entity *se = \u0026amp;curr-\u0026gt;se, *pse = \u0026amp;p-\u0026gt;se; 6644\tstruct cfs_rq *cfs_rq = task_cfs_rq(curr); 6645\tint scale = cfs_rq-\u0026gt;nr_running \u0026gt;= sched_nr_latency; 6646\tint next_buddy_marked = 0; 6647 6648\tif (unlikely(se == pse)) 6649\treturn; 6650 6651\t/* 6652* This is possible from callers such as attach_tasks(), in which we 6653* unconditionally check_prempt_curr() after an enqueue (which may have 6654* lead to a throttle). This both saves work and prevents false 6655* next-buddy nomination below. 6656*/ 6657\tif (unlikely(throttled_hierarchy(cfs_rq_of(pse)))) 6658\treturn; 6659 6660\tif (sched_feat(NEXT_BUDDY) \u0026amp;\u0026amp; scale \u0026amp;\u0026amp; !(wake_flags \u0026amp; WF_FORK)) { 6661\tset_next_buddy(pse); 6662\tnext_buddy_marked = 1; 6663\t} 6664 6665\t/* 6666* We can come here with TIF_NEED_RESCHED already set from new task 6667* wake up path. 6668* 6669* Note: this also catches the edge-case of curr being in a throttled 6670* group (e.g. via set_curr_task), since update_curr() (in the 6671* enqueue of curr) will have resulted in resched being set. This 6672* prevents us from potentially nominating it as a false LAST_BUDDY 6673* below. 6674*/ 6675\tif (test_tsk_need_resched(curr)) 6676\treturn; 6677 6678\t/* Idle tasks are by definition preempted by non-idle tasks. */ 6679\tif (unlikely(curr-\u0026gt;policy == SCHED_IDLE) \u0026amp;\u0026amp; 6680\tlikely(p-\u0026gt;policy != SCHED_IDLE)) 6681\tgoto preempt; 6682 6683\t/* 6684* Batch and idle tasks do not preempt non-idle tasks (their preemption 6685* is driven by the tick): 6686*/ 6687\tif (unlikely(p-\u0026gt;policy != SCHED_NORMAL) || !sched_feat(WAKEUP_PREEMPTION)) 6688\treturn; 6689 6690\tfind_matching_se(\u0026amp;se, \u0026amp;pse); 6691\tupdate_curr(cfs_rq_of(se)); 6692\tBUG_ON(!pse); 6693\tif (wakeup_preempt_entity(se, pse) == 1) { 6694\t/* 6695* Bias pick_next to pick the sched entity that is 6696* triggering this preemption. 6697*/ 6698\tif (!next_buddy_marked) 6699\tset_next_buddy(pse); 6700\tgoto preempt; 6701\t} 6702 6703\treturn; 6704 6705preempt: 6706\tresched_curr(rq); 6707\t/* 6708* Only set the backward buddy when the current task is still 6709* on the rq. This can happen when a wakeup gets interleaved 6710* with schedule on the -\u0026gt;pre_schedule() or idle_balance() 6711* point, either of which can * drop the rq lock. 6712* 6713* Also, during early boot the idle thread is in the fair class, 6714* for obvious reasons its a bad idea to schedule back to it. 6715*/ 6716\tif (unlikely(!se-\u0026gt;on_rq || curr == rq-\u0026gt;idle)) 6717\treturn; 6718 6719\tif (sched_feat(LAST_BUDDY) \u0026amp;\u0026amp; scale \u0026amp;\u0026amp; entity_is_task(se)) 6720\tset_last_buddy(se); 6721} 6722 6723static struct task_struct * 6724pick_next_task_fair(struct rq *rq, struct task_struct *prev, struct rq_flags *rf) 6725{ 6726\tstruct cfs_rq *cfs_rq = \u0026amp;rq-\u0026gt;cfs; 6727\tstruct sched_entity *se; 6728\tstruct task_struct *p; 6729\tint new_tasks; 6730 6731again: 6732\tif (!cfs_rq-\u0026gt;nr_running) 6733\tgoto idle; 6734 6735#ifdef CONFIG_FAIR_GROUP_SCHED 6736\tif (prev-\u0026gt;sched_class != \u0026amp;fair_sched_class) 6737\tgoto simple; 6738 6739\t/* 6740* Because of the set_next_buddy() in dequeue_task_fair() it is rather 6741* likely that a next task is from the same cgroup as the current. 6742* 6743* Therefore attempt to avoid putting and setting the entire cgroup 6744* hierarchy, only change the part that actually changes. 6745*/ 6746 6747\tdo { 6748\tstruct sched_entity *curr = cfs_rq-\u0026gt;curr; 6749 6750\t/* 6751* Since we got here without doing put_prev_entity() we also 6752* have to consider cfs_rq-\u0026gt;curr. If it is still a runnable 6753* entity, update_curr() will update its vruntime, otherwise 6754* forget we\u0026#39;ve ever seen it. 6755*/ 6756\tif (curr) { 6757\tif (curr-\u0026gt;on_rq) 6758\tupdate_curr(cfs_rq); 6759\telse 6760\tcurr = NULL; 6761 6762\t/* 6763* This call to check_cfs_rq_runtime() will do the 6764* throttle and dequeue its entity in the parent(s). 6765* Therefore the nr_running test will indeed 6766* be correct. 6767*/ 6768\tif (unlikely(check_cfs_rq_runtime(cfs_rq))) { 6769\tcfs_rq = \u0026amp;rq-\u0026gt;cfs; 6770 6771\tif (!cfs_rq-\u0026gt;nr_running) 6772\tgoto idle; 6773 6774\tgoto simple; 6775\t} 6776\t} 6777 6778\tse = pick_next_entity(cfs_rq, curr); 6779\tcfs_rq = group_cfs_rq(se); 6780\t} while (cfs_rq); 6781 6782\tp = task_of(se); 6783 6784\t/* 6785* Since we haven\u0026#39;t yet done put_prev_entity and if the selected task 6786* is a different task than we started out with, try and touch the 6787* least amount of cfs_rqs. 6788*/ 6789\tif (prev != p) { 6790\tstruct sched_entity *pse = \u0026amp;prev-\u0026gt;se; 6791 6792\twhile (!(cfs_rq = is_same_group(se, pse))) { 6793\tint se_depth = se-\u0026gt;depth; 6794\tint pse_depth = pse-\u0026gt;depth; 6795 6796\tif (se_depth \u0026lt;= pse_depth) { 6797\tput_prev_entity(cfs_rq_of(pse), pse); 6798\tpse = parent_entity(pse); 6799\t} 6800\tif (se_depth \u0026gt;= pse_depth) { 6801\tset_next_entity(cfs_rq_of(se), se); 6802\tse = parent_entity(se); 6803\t} 6804\t} 6805 6806\tput_prev_entity(cfs_rq, pse); 6807\tset_next_entity(cfs_rq, se); 6808\t} 6809 6810\tgoto done; 6811simple: 6812#endif 6813 6814\tput_prev_task(rq, prev); 6815 6816\tdo { 6817\tse = pick_next_entity(cfs_rq, NULL); 6818\tset_next_entity(cfs_rq, se); 6819\tcfs_rq = group_cfs_rq(se); 6820\t} while (cfs_rq); 6821 6822\tp = task_of(se); 6823 6824done: __maybe_unused; 6825#ifdef CONFIG_SMP 6826\t/* 6827* Move the next running task to the front of 6828* the list, so our cfs_tasks list becomes MRU 6829* one. 6830*/ 6831\tlist_move(\u0026amp;p-\u0026gt;se.group_node, \u0026amp;rq-\u0026gt;cfs_tasks); 6832#endif 6833 6834\tif (hrtick_enabled(rq)) 6835\thrtick_start_fair(rq, p); 6836 6837\treturn p; 6838 6839idle: 6840\tnew_tasks = idle_balance(rq, rf); 6841 6842\t/* 6843* Because idle_balance() releases (and re-acquires) rq-\u0026gt;lock, it is 6844* possible for any higher priority task to appear. In that case we 6845* must re-start the pick_next_entity() loop. 6846*/ 6847\tif (new_tasks \u0026lt; 0) 6848\treturn RETRY_TASK; 6849 6850\tif (new_tasks \u0026gt; 0) 6851\tgoto again; 6852 6853\treturn NULL; 6854} 6855 6856/* 6857* Account for a descheduled task: 6858*/ 6859static void put_prev_task_fair(struct rq *rq, struct task_struct *prev) 6860{ 6861\tstruct sched_entity *se = \u0026amp;prev-\u0026gt;se; 6862\tstruct cfs_rq *cfs_rq; 6863 6864\tfor_each_sched_entity(se) { 6865\tcfs_rq = cfs_rq_of(se); 6866\tput_prev_entity(cfs_rq, se); 6867\t} 6868} 6869 6870/* 6871* sched_yield() is very simple 6872* 6873* The magic of dealing with the -\u0026gt;skip buddy is in pick_next_entity. 6874*/ 6875static void yield_task_fair(struct rq *rq) 6876{ 6877\tstruct task_struct *curr = rq-\u0026gt;curr; 6878\tstruct cfs_rq *cfs_rq = task_cfs_rq(curr); 6879\tstruct sched_entity *se = \u0026amp;curr-\u0026gt;se; 6880 6881\t/* 6882* Are we the only task in the tree? 6883*/ 6884\tif (unlikely(rq-\u0026gt;nr_running == 1)) 6885\treturn; 6886 6887\tclear_buddies(cfs_rq, se); 6888 6889\tif (curr-\u0026gt;policy != SCHED_BATCH) { 6890\tupdate_rq_clock(rq); 6891\t/* 6892* Update run-time statistics of the \u0026#39;current\u0026#39;. 6893*/ 6894\tupdate_curr(cfs_rq); 6895\t/* 6896* Tell update_rq_clock() that we\u0026#39;ve just updated, 6897* so we don\u0026#39;t do microscopic update in schedule() 6898* and double the fastpath cost. 6899*/ 6900\trq_clock_skip_update(rq); 6901\t} 6902 6903\tset_skip_buddy(se); 6904} 6905 6906static bool yield_to_task_fair(struct rq *rq, struct task_struct *p, bool preempt) 6907{ 6908\tstruct sched_entity *se = \u0026amp;p-\u0026gt;se; 6909 6910\t/* throttled hierarchies are not runnable */ 6911\tif (!se-\u0026gt;on_rq || throttled_hierarchy(cfs_rq_of(se))) 6912\treturn false; 6913 6914\t/* Tell the scheduler that we\u0026#39;d really like pse to run next. */ 6915\tset_next_buddy(se); 6916 6917\tyield_task_fair(rq); 6918 6919\treturn true; 6920} 6921 6922#ifdef CONFIG_SMP 6923/************************************************** 6924* Fair scheduling class load-balancing methods. 6925* 6926* BASICS 6927* 6928* The purpose of load-balancing is to achieve the same basic fairness the 6929* per-CPU scheduler provides, namely provide a proportional amount of compute 6930* time to each task. This is expressed in the following equation: 6931* 6932* W_i,n/P_i == W_j,n/P_j for all i,j (1) 6933* 6934* Where W_i,n is the n-th weight average for CPU i. The instantaneous weight 6935* W_i,0 is defined as: 6936* 6937* W_i,0 = \\Sum_j w_i,j (2) 6938* 6939* Where w_i,j is the weight of the j-th runnable task on CPU i. This weight 6940* is derived from the nice value as per sched_prio_to_weight[]. 6941* 6942* The weight average is an exponential decay average of the instantaneous 6943* weight: 6944* 6945* W\u0026#39;_i,n = (2^n - 1) / 2^n * W_i,n + 1 / 2^n * W_i,0 (3) 6946* 6947* C_i is the compute capacity of CPU i, typically it is the 6948* fraction of \u0026#39;recent\u0026#39; time available for SCHED_OTHER task execution. But it 6949* can also include other factors [XXX]. 6950* 6951* To achieve this balance we define a measure of imbalance which follows 6952* directly from (1): 6953* 6954* imb_i,j = max{ avg(W/C), W_i/C_i } - min{ avg(W/C), W_j/C_j } (4) 6955* 6956* We them move tasks around to minimize the imbalance. In the continuous 6957* function space it is obvious this converges, in the discrete case we get 6958* a few fun cases generally called infeasible weight scenarios. 6959* 6960* [XXX expand on: 6961* - infeasible weights; 6962* - local vs global optima in the discrete case. ] 6963* 6964* 6965* SCHED DOMAINS 6966* 6967* In order to solve the imbalance equation (4), and avoid the obvious O(n^2) 6968* for all i,j solution, we create a tree of CPUs that follows the hardware 6969* topology where each level pairs two lower groups (or better). This results 6970* in O(log n) layers. Furthermore we reduce the number of CPUs going up the 6971* tree to only the first of the previous level and we decrease the frequency 6972* of load-balance at each level inv. proportional to the number of CPUs in 6973* the groups. 6974* 6975* This yields: 6976* 6977* log_2 n 1 n 6978* \\Sum { --- * --- * 2^i } = O(n) (5) 6979* i = 0 2^i 2^i 6980* `- size of each group 6981* | | `- number of CPUs doing load-balance 6982* | `- freq 6983* `- sum over all levels 6984* 6985* Coupled with a limit on how many tasks we can migrate every balance pass, 6986* this makes (5) the runtime complexity of the balancer. 6987* 6988* An important property here is that each CPU is still (indirectly) connected 6989* to every other CPU in at most O(log n) steps: 6990* 6991* The adjacency matrix of the resulting graph is given by: 6992* 6993* log_2 n 6994* A_i,j = \\Union (i % 2^k == 0) \u0026amp;\u0026amp; i / 2^(k+1) == j / 2^(k+1) (6) 6995* k = 0 6996* 6997* And you\u0026#39;ll find that: 6998* 6999* A^(log_2 n)_i,j != 0 for all i,j (7) 7000* 7001* Showing there\u0026#39;s indeed a path between every CPU in at most O(log n) steps. 7002* The task movement gives a factor of O(m), giving a convergence complexity 7003* of: 7004* 7005* O(nm log n), n := nr_cpus, m := nr_tasks (8) 7006* 7007* 7008* WORK CONSERVING 7009* 7010* In order to avoid CPUs going idle while there\u0026#39;s still work to do, new idle 7011* balancing is more aggressive and has the newly idle CPU iterate up the domain 7012* tree itself instead of relying on other CPUs to bring it work. 7013* 7014* This adds some complexity to both (5) and (8) but it reduces the total idle 7015* time. 7016* 7017* [XXX more?] 7018* 7019* 7020* CGROUPS 7021* 7022* Cgroups make a horror show out of (2), instead of a simple sum we get: 7023* 7024* s_k,i 7025* W_i,0 = \\Sum_j \\Prod_k w_k * ----- (9) 7026* S_k 7027* 7028* Where 7029* 7030* s_k,i = \\Sum_j w_i,j,k and S_k = \\Sum_i s_k,i (10) 7031* 7032* w_i,j,k is the weight of the j-th runnable task in the k-th cgroup on CPU i. 7033* 7034* The big problem is S_k, its a global sum needed to compute a local (W_i) 7035* property. 7036* 7037* [XXX write more on how we solve this.. _after_ merging pjt\u0026#39;s patches that 7038* rewrite all of this once again.] 7039*/ 7040 7041static unsigned long __read_mostly max_load_balance_interval = HZ/10; 7042 7043enum fbq_type { regular, remote, all }; 7044 7045#define LBF_ALL_PINNED\t0x01 7046#define LBF_NEED_BREAK\t0x02 7047#define LBF_DST_PINNED 0x04 7048#define LBF_SOME_PINNED\t0x08 7049#define LBF_NOHZ_STATS\t0x10 7050#define LBF_NOHZ_AGAIN\t0x20 7051 7052struct lb_env { 7053\tstruct sched_domain\t*sd; 7054 7055\tstruct rq\t*src_rq; 7056\tint\tsrc_cpu; 7057 7058\tint\tdst_cpu; 7059\tstruct rq\t*dst_rq; 7060 7061\tstruct cpumask\t*dst_grpmask; 7062\tint\tnew_dst_cpu; 7063\tenum cpu_idle_type\tidle; 7064\tlong\timbalance; 7065\t/* The set of CPUs under consideration for load-balancing */ 7066\tstruct cpumask\t*cpus; 7067 7068\tunsigned int\tflags; 7069 7070\tunsigned int\tloop; 7071\tunsigned int\tloop_break; 7072\tunsigned int\tloop_max; 7073 7074\tenum fbq_type\tfbq_type; 7075\tstruct list_head\ttasks; 7076}; 7077 7078/* 7079* Is this task likely cache-hot: 7080*/ 7081static int task_hot(struct task_struct *p, struct lb_env *env) 7082{ 7083\ts64 delta; 7084 7085\tlockdep_assert_held(\u0026amp;env-\u0026gt;src_rq-\u0026gt;lock); 7086 7087\tif (p-\u0026gt;sched_class != \u0026amp;fair_sched_class) 7088\treturn 0; 7089 7090\tif (unlikely(p-\u0026gt;policy == SCHED_IDLE)) 7091\treturn 0; 7092 7093\t/* 7094* Buddy candidates are cache hot: 7095*/ 7096\tif (sched_feat(CACHE_HOT_BUDDY) \u0026amp;\u0026amp; env-\u0026gt;dst_rq-\u0026gt;nr_running \u0026amp;\u0026amp; 7097\t(\u0026amp;p-\u0026gt;se == cfs_rq_of(\u0026amp;p-\u0026gt;se)-\u0026gt;next || 7098\t\u0026amp;p-\u0026gt;se == cfs_rq_of(\u0026amp;p-\u0026gt;se)-\u0026gt;last)) 7099\treturn 1; 7100 7101\tif (sysctl_sched_migration_cost == -1) 7102\treturn 1; 7103\tif (sysctl_sched_migration_cost == 0) 7104\treturn 0; 7105 7106\tdelta = rq_clock_task(env-\u0026gt;src_rq) - p-\u0026gt;se.exec_start; 7107 7108\treturn delta \u0026lt; (s64)sysctl_sched_migration_cost; 7109} 7110 7111#ifdef CONFIG_NUMA_BALANCING 7112/* 7113* Returns 1, if task migration degrades locality 7114* Returns 0, if task migration improves locality i.e migration preferred. 7115* Returns -1, if task migration is not affected by locality. 7116*/ 7117static int migrate_degrades_locality(struct task_struct *p, struct lb_env *env) 7118{ 7119\tstruct numa_group *numa_group = rcu_dereference(p-\u0026gt;numa_group); 7120\tunsigned long src_weight, dst_weight; 7121\tint src_nid, dst_nid, dist; 7122 7123\tif (!static_branch_likely(\u0026amp;sched_numa_balancing)) 7124\treturn -1; 7125 7126\tif (!p-\u0026gt;numa_faults || !(env-\u0026gt;sd-\u0026gt;flags \u0026amp; SD_NUMA)) 7127\treturn -1; 7128 7129\tsrc_nid = cpu_to_node(env-\u0026gt;src_cpu); 7130\tdst_nid = cpu_to_node(env-\u0026gt;dst_cpu); 7131 7132\tif (src_nid == dst_nid) 7133\treturn -1; 7134 7135\t/* Migrating away from the preferred node is always bad. */ 7136\tif (src_nid == p-\u0026gt;numa_preferred_nid) { 7137\tif (env-\u0026gt;src_rq-\u0026gt;nr_running \u0026gt; env-\u0026gt;src_rq-\u0026gt;nr_preferred_running) 7138\treturn 1; 7139\telse 7140\treturn -1; 7141\t} 7142 7143\t/* Encourage migration to the preferred node. */ 7144\tif (dst_nid == p-\u0026gt;numa_preferred_nid) 7145\treturn 0; 7146 7147\t/* Leaving a core idle is often worse than degrading locality. */ 7148\tif (env-\u0026gt;idle == CPU_IDLE) 7149\treturn -1; 7150 7151\tdist = node_distance(src_nid, dst_nid); 7152\tif (numa_group) { 7153\tsrc_weight = group_weight(p, src_nid, dist); 7154\tdst_weight = group_weight(p, dst_nid, dist); 7155\t} else { 7156\tsrc_weight = task_weight(p, src_nid, dist); 7157\tdst_weight = task_weight(p, dst_nid, dist); 7158\t} 7159 7160\treturn dst_weight \u0026lt; src_weight; 7161} 7162 7163#else 7164static inline int migrate_degrades_locality(struct task_struct *p, 7165\tstruct lb_env *env) 7166{ 7167\treturn -1; 7168} 7169#endif 7170 7171/* 7172* can_migrate_task - may task p from runqueue rq be migrated to this_cpu? 7173*/ 7174static 7175int can_migrate_task(struct task_struct *p, struct lb_env *env) 7176{ 7177\tint tsk_cache_hot; 7178 7179\tlockdep_assert_held(\u0026amp;env-\u0026gt;src_rq-\u0026gt;lock); 7180 7181\t/* 7182* We do not migrate tasks that are: 7183* 1) throttled_lb_pair, or 7184* 2) cannot be migrated to this CPU due to cpus_allowed, or 7185* 3) running (obviously), or 7186* 4) are cache-hot on their current CPU. 7187*/ 7188\tif (throttled_lb_pair(task_group(p), env-\u0026gt;src_cpu, env-\u0026gt;dst_cpu)) 7189\treturn 0; 7190 7191\tif (!cpumask_test_cpu(env-\u0026gt;dst_cpu, \u0026amp;p-\u0026gt;cpus_allowed)) { 7192\tint cpu; 7193 7194\tschedstat_inc(p-\u0026gt;se.statistics.nr_failed_migrations_affine); 7195 7196\tenv-\u0026gt;flags |= LBF_SOME_PINNED; 7197 7198\t/* 7199* Remember if this task can be migrated to any other CPU in 7200* our sched_group. We may want to revisit it if we couldn\u0026#39;t 7201* meet load balance goals by pulling other tasks on src_cpu. 7202* 7203* Avoid computing new_dst_cpu for NEWLY_IDLE or if we have 7204* already computed one in current iteration. 7205*/ 7206\tif (env-\u0026gt;idle == CPU_NEWLY_IDLE || (env-\u0026gt;flags \u0026amp; LBF_DST_PINNED)) 7207\treturn 0; 7208 7209\t/* Prevent to re-select dst_cpu via env\u0026#39;s CPUs: */ 7210\tfor_each_cpu_and(cpu, env-\u0026gt;dst_grpmask, env-\u0026gt;cpus) { 7211\tif (cpumask_test_cpu(cpu, \u0026amp;p-\u0026gt;cpus_allowed)) { 7212\tenv-\u0026gt;flags |= LBF_DST_PINNED; 7213\tenv-\u0026gt;new_dst_cpu = cpu; 7214\tbreak; 7215\t} 7216\t} 7217 7218\treturn 0; 7219\t} 7220 7221\t/* Record that we found atleast one task that could run on dst_cpu */ 7222\tenv-\u0026gt;flags \u0026amp;= ~LBF_ALL_PINNED; 7223 7224\tif (task_running(env-\u0026gt;src_rq, p)) { 7225\tschedstat_inc(p-\u0026gt;se.statistics.nr_failed_migrations_running); 7226\treturn 0; 7227\t} 7228 7229\t/* 7230* Aggressive migration if: 7231* 1) destination numa is preferred 7232* 2) task is cache cold, or 7233* 3) too many balance attempts have failed. 7234*/ 7235\ttsk_cache_hot = migrate_degrades_locality(p, env); 7236\tif (tsk_cache_hot == -1) 7237\ttsk_cache_hot = task_hot(p, env); 7238 7239\tif (tsk_cache_hot \u0026lt;= 0 || 7240\tenv-\u0026gt;sd-\u0026gt;nr_balance_failed \u0026gt; env-\u0026gt;sd-\u0026gt;cache_nice_tries) { 7241\tif (tsk_cache_hot == 1) { 7242\tschedstat_inc(env-\u0026gt;sd-\u0026gt;lb_hot_gained[env-\u0026gt;idle]); 7243\tschedstat_inc(p-\u0026gt;se.statistics.nr_forced_migrations); 7244\t} 7245\treturn 1; 7246\t} 7247 7248\tschedstat_inc(p-\u0026gt;se.statistics.nr_failed_migrations_hot); 7249\treturn 0; 7250} 7251 7252/* 7253* detach_task() -- detach the task for the migration specified in env 7254*/ 7255static void detach_task(struct task_struct *p, struct lb_env *env) 7256{ 7257\tlockdep_assert_held(\u0026amp;env-\u0026gt;src_rq-\u0026gt;lock); 7258 7259\tp-\u0026gt;on_rq = TASK_ON_RQ_MIGRATING; 7260\tdeactivate_task(env-\u0026gt;src_rq, p, DEQUEUE_NOCLOCK); 7261\tset_task_cpu(p, env-\u0026gt;dst_cpu); 7262} 7263 7264/* 7265* detach_one_task() -- tries to dequeue exactly one task from env-\u0026gt;src_rq, as 7266* part of active balancing operations within \u0026#34;domain\u0026#34;. 7267* 7268* Returns a task if successful and NULL otherwise. 7269*/ 7270static struct task_struct *detach_one_task(struct lb_env *env) 7271{ 7272\tstruct task_struct *p; 7273 7274\tlockdep_assert_held(\u0026amp;env-\u0026gt;src_rq-\u0026gt;lock); 7275 7276\tlist_for_each_entry_reverse(p, 7277\t\u0026amp;env-\u0026gt;src_rq-\u0026gt;cfs_tasks, se.group_node) { 7278\tif (!can_migrate_task(p, env)) 7279\tcontinue; 7280 7281\tdetach_task(p, env); 7282 7283\t/* 7284* Right now, this is only the second place where 7285* lb_gained[env-\u0026gt;idle] is updated (other is detach_tasks) 7286* so we can safely collect stats here rather than 7287* inside detach_tasks(). 7288*/ 7289\tschedstat_inc(env-\u0026gt;sd-\u0026gt;lb_gained[env-\u0026gt;idle]); 7290\treturn p; 7291\t} 7292\treturn NULL; 7293} 7294 7295static const unsigned int sched_nr_migrate_break = 32; 7296 7297/* 7298* detach_tasks() -- tries to detach up to imbalance weighted load from 7299* busiest_rq, as part of a balancing operation within domain \u0026#34;sd\u0026#34;. 7300* 7301* Returns number of detached tasks if successful and 0 otherwise. 7302*/ 7303static int detach_tasks(struct lb_env *env) 7304{ 7305\tstruct list_head *tasks = \u0026amp;env-\u0026gt;src_rq-\u0026gt;cfs_tasks; 7306\tstruct task_struct *p; 7307\tunsigned long load; 7308\tint detached = 0; 7309 7310\tlockdep_assert_held(\u0026amp;env-\u0026gt;src_rq-\u0026gt;lock); 7311 7312\tif (env-\u0026gt;imbalance \u0026lt;= 0) 7313\treturn 0; 7314 7315\twhile (!list_empty(tasks)) { 7316\t/* 7317* We don\u0026#39;t want to steal all, otherwise we may be treated likewise, 7318* which could at worst lead to a livelock crash. 7319*/ 7320\tif (env-\u0026gt;idle != CPU_NOT_IDLE \u0026amp;\u0026amp; env-\u0026gt;src_rq-\u0026gt;nr_running \u0026lt;= 1) 7321\tbreak; 7322 7323\tp = list_last_entry(tasks, struct task_struct, se.group_node); 7324 7325\tenv-\u0026gt;loop++; 7326\t/* We\u0026#39;ve more or less seen every task there is, call it quits */ 7327\tif (env-\u0026gt;loop \u0026gt; env-\u0026gt;loop_max) 7328\tbreak; 7329 7330\t/* take a breather every nr_migrate tasks */ 7331\tif (env-\u0026gt;loop \u0026gt; env-\u0026gt;loop_break) { 7332\tenv-\u0026gt;loop_break += sched_nr_migrate_break; 7333\tenv-\u0026gt;flags |= LBF_NEED_BREAK; 7334\tbreak; 7335\t} 7336 7337\tif (!can_migrate_task(p, env)) 7338\tgoto next; 7339 7340\t/* 7341* Depending of the number of CPUs and tasks and the 7342* cgroup hierarchy, task_h_load() can return a null 7343* value. Make sure that env-\u0026gt;imbalance decreases 7344* otherwise detach_tasks() will stop only after 7345* detaching up to loop_max tasks. 7346*/ 7347\tload = max_t(unsigned long, task_h_load(p), 1); 7348 7349 7350\tif (sched_feat(LB_MIN) \u0026amp;\u0026amp; load \u0026lt; 16 \u0026amp;\u0026amp; !env-\u0026gt;sd-\u0026gt;nr_balance_failed) 7351\tgoto next; 7352 7353\tif ((load / 2) \u0026gt; env-\u0026gt;imbalance) 7354\tgoto next; 7355 7356\tdetach_task(p, env); 7357\tlist_add(\u0026amp;p-\u0026gt;se.group_node, \u0026amp;env-\u0026gt;tasks); 7358 7359\tdetached++; 7360\tenv-\u0026gt;imbalance -= load; 7361 7362#ifdef CONFIG_PREEMPT 7363\t/* 7364* NEWIDLE balancing is a source of latency, so preemptible 7365* kernels will stop after the first task is detached to minimize 7366* the critical section. 7367*/ 7368\tif (env-\u0026gt;idle == CPU_NEWLY_IDLE) 7369\tbreak; 7370#endif 7371 7372\t/* 7373* We only want to steal up to the prescribed amount of 7374* weighted load. 7375*/ 7376\tif (env-\u0026gt;imbalance \u0026lt;= 0) 7377\tbreak; 7378 7379\tcontinue; 7380next: 7381\tlist_move(\u0026amp;p-\u0026gt;se.group_node, tasks); 7382\t} 7383 7384\t/* 7385* Right now, this is one of only two places we collect this stat 7386* so we can safely collect detach_one_task() stats here rather 7387* than inside detach_one_task(). 7388*/ 7389\tschedstat_add(env-\u0026gt;sd-\u0026gt;lb_gained[env-\u0026gt;idle], detached); 7390 7391\treturn detached; 7392} 7393 7394/* 7395* attach_task() -- attach the task detached by detach_task() to its new rq. 7396*/ 7397static void attach_task(struct rq *rq, struct task_struct *p) 7398{ 7399\tlockdep_assert_held(\u0026amp;rq-\u0026gt;lock); 7400 7401\tBUG_ON(task_rq(p) != rq); 7402\tactivate_task(rq, p, ENQUEUE_NOCLOCK); 7403\tp-\u0026gt;on_rq = TASK_ON_RQ_QUEUED; 7404\tcheck_preempt_curr(rq, p, 0); 7405} 7406 7407/* 7408* attach_one_task() -- attaches the task returned from detach_one_task() to 7409* its new rq. 7410*/ 7411static void attach_one_task(struct rq *rq, struct task_struct *p) 7412{ 7413\tstruct rq_flags rf; 7414 7415\trq_lock(rq, \u0026amp;rf); 7416\tupdate_rq_clock(rq); 7417\tattach_task(rq, p); 7418\trq_unlock(rq, \u0026amp;rf); 7419} 7420 7421/* 7422* attach_tasks() -- attaches all tasks detached by detach_tasks() to their 7423* new rq. 7424*/ 7425static void attach_tasks(struct lb_env *env) 7426{ 7427\tstruct list_head *tasks = \u0026amp;env-\u0026gt;tasks; 7428\tstruct task_struct *p; 7429\tstruct rq_flags rf; 7430 7431\trq_lock(env-\u0026gt;dst_rq, \u0026amp;rf); 7432\tupdate_rq_clock(env-\u0026gt;dst_rq); 7433 7434\twhile (!list_empty(tasks)) { 7435\tp = list_first_entry(tasks, struct task_struct, se.group_node); 7436\tlist_del_init(\u0026amp;p-\u0026gt;se.group_node); 7437 7438\tattach_task(env-\u0026gt;dst_rq, p); 7439\t} 7440 7441\trq_unlock(env-\u0026gt;dst_rq, \u0026amp;rf); 7442} 7443 7444static inline bool cfs_rq_has_blocked(struct cfs_rq *cfs_rq) 7445{ 7446\tif (cfs_rq-\u0026gt;avg.load_avg) 7447\treturn true; 7448 7449\tif (cfs_rq-\u0026gt;avg.util_avg) 7450\treturn true; 7451 7452\treturn false; 7453} 7454 7455static inline bool others_have_blocked(struct rq *rq) 7456{ 7457\tif (READ_ONCE(rq-\u0026gt;avg_rt.util_avg)) 7458\treturn true; 7459 7460\tif (READ_ONCE(rq-\u0026gt;avg_dl.util_avg)) 7461\treturn true; 7462 7463#ifdef CONFIG_HAVE_SCHED_AVG_IRQ 7464\tif (READ_ONCE(rq-\u0026gt;avg_irq.util_avg)) 7465\treturn true; 7466#endif 7467 7468\treturn false; 7469} 7470 7471#ifdef CONFIG_FAIR_GROUP_SCHED 7472 7473static inline bool cfs_rq_is_decayed(struct cfs_rq *cfs_rq) 7474{ 7475\tif (cfs_rq-\u0026gt;load.weight) 7476\treturn false; 7477 7478\tif (cfs_rq-\u0026gt;avg.load_sum) 7479\treturn false; 7480 7481\tif (cfs_rq-\u0026gt;avg.util_sum) 7482\treturn false; 7483 7484\tif (cfs_rq-\u0026gt;avg.runnable_load_sum) 7485\treturn false; 7486 7487\treturn true; 7488} 7489 7490static void update_blocked_averages(int cpu) 7491{ 7492\tstruct rq *rq = cpu_rq(cpu); 7493\tstruct cfs_rq *cfs_rq, *pos; 7494\tconst struct sched_class *curr_class; 7495\tstruct rq_flags rf; 7496\tbool done = true; 7497 7498\trq_lock_irqsave(rq, \u0026amp;rf); 7499\tupdate_rq_clock(rq); 7500 7501\t/* 7502* Iterates the task_group tree in a bottom up fashion, see 7503* list_add_leaf_cfs_rq() for details. 7504*/ 7505\tfor_each_leaf_cfs_rq_safe(rq, cfs_rq, pos) { 7506\tstruct sched_entity *se; 7507 7508\tif (update_cfs_rq_load_avg(cfs_rq_clock_task(cfs_rq), cfs_rq)) 7509\tupdate_tg_load_avg(cfs_rq, 0); 7510 7511\t/* Propagate pending load changes to the parent, if any: */ 7512\tse = cfs_rq-\u0026gt;tg-\u0026gt;se[cpu]; 7513\tif (se \u0026amp;\u0026amp; !skip_blocked_update(se)) 7514\tupdate_load_avg(cfs_rq_of(se), se, 0); 7515 7516\t/* 7517* There can be a lot of idle CPU cgroups. Don\u0026#39;t let fully 7518* decayed cfs_rqs linger on the list. 7519*/ 7520\tif (cfs_rq_is_decayed(cfs_rq)) 7521\tlist_del_leaf_cfs_rq(cfs_rq); 7522 7523\t/* Don\u0026#39;t need periodic decay once load/util_avg are null */ 7524\tif (cfs_rq_has_blocked(cfs_rq)) 7525\tdone = false; 7526\t} 7527 7528\tcurr_class = rq-\u0026gt;curr-\u0026gt;sched_class; 7529\tupdate_rt_rq_load_avg(rq_clock_task(rq), rq, curr_class == \u0026amp;rt_sched_class); 7530\tupdate_dl_rq_load_avg(rq_clock_task(rq), rq, curr_class == \u0026amp;dl_sched_class); 7531\tupdate_irq_load_avg(rq, 0); 7532\t/* Don\u0026#39;t need periodic decay once load/util_avg are null */ 7533\tif (others_have_blocked(rq)) 7534\tdone = false; 7535 7536#ifdef CONFIG_NO_HZ_COMMON 7537\trq-\u0026gt;last_blocked_load_update_tick = jiffies; 7538\tif (done) 7539\trq-\u0026gt;has_blocked_load = 0; 7540#endif 7541\trq_unlock_irqrestore(rq, \u0026amp;rf); 7542} 7543 7544/* 7545* Compute the hierarchical load factor for cfs_rq and all its ascendants. 7546* This needs to be done in a top-down fashion because the load of a child 7547* group is a fraction of its parents load. 7548*/ 7549static void update_cfs_rq_h_load(struct cfs_rq *cfs_rq) 7550{ 7551\tstruct rq *rq = rq_of(cfs_rq); 7552\tstruct sched_entity *se = cfs_rq-\u0026gt;tg-\u0026gt;se[cpu_of(rq)]; 7553\tunsigned long now = jiffies; 7554\tunsigned long load; 7555 7556\tif (cfs_rq-\u0026gt;last_h_load_update == now) 7557\treturn; 7558 7559\tWRITE_ONCE(cfs_rq-\u0026gt;h_load_next, NULL); 7560\tfor_each_sched_entity(se) { 7561\tcfs_rq = cfs_rq_of(se); 7562\tWRITE_ONCE(cfs_rq-\u0026gt;h_load_next, se); 7563\tif (cfs_rq-\u0026gt;last_h_load_update == now) 7564\tbreak; 7565\t} 7566 7567\tif (!se) { 7568\tcfs_rq-\u0026gt;h_load = cfs_rq_load_avg(cfs_rq); 7569\tcfs_rq-\u0026gt;last_h_load_update = now; 7570\t} 7571 7572\twhile ((se = READ_ONCE(cfs_rq-\u0026gt;h_load_next)) != NULL) { 7573\tload = cfs_rq-\u0026gt;h_load; 7574\tload = div64_ul(load * se-\u0026gt;avg.load_avg, 7575\tcfs_rq_load_avg(cfs_rq) + 1); 7576\tcfs_rq = group_cfs_rq(se); 7577\tcfs_rq-\u0026gt;h_load = load; 7578\tcfs_rq-\u0026gt;last_h_load_update = now; 7579\t} 7580} 7581 7582static unsigned long task_h_load(struct task_struct *p) 7583{ 7584\tstruct cfs_rq *cfs_rq = task_cfs_rq(p); 7585 7586\tupdate_cfs_rq_h_load(cfs_rq); 7587\treturn div64_ul(p-\u0026gt;se.avg.load_avg * cfs_rq-\u0026gt;h_load, 7588\tcfs_rq_load_avg(cfs_rq) + 1); 7589} 7590#else 7591static inline void update_blocked_averages(int cpu) 7592{ 7593\tstruct rq *rq = cpu_rq(cpu); 7594\tstruct cfs_rq *cfs_rq = \u0026amp;rq-\u0026gt;cfs; 7595\tconst struct sched_class *curr_class; 7596\tstruct rq_flags rf; 7597 7598\trq_lock_irqsave(rq, \u0026amp;rf); 7599\tupdate_rq_clock(rq); 7600\tupdate_cfs_rq_load_avg(cfs_rq_clock_task(cfs_rq), cfs_rq); 7601 7602\tcurr_class = rq-\u0026gt;curr-\u0026gt;sched_class; 7603\tupdate_rt_rq_load_avg(rq_clock_task(rq), rq, curr_class == \u0026amp;rt_sched_class); 7604\tupdate_dl_rq_load_avg(rq_clock_task(rq), rq, curr_class == \u0026amp;dl_sched_class); 7605\tupdate_irq_load_avg(rq, 0); 7606#ifdef CONFIG_NO_HZ_COMMON 7607\trq-\u0026gt;last_blocked_load_update_tick = jiffies; 7608\tif (!cfs_rq_has_blocked(cfs_rq) \u0026amp;\u0026amp; !others_have_blocked(rq)) 7609\trq-\u0026gt;has_blocked_load = 0; 7610#endif 7611\trq_unlock_irqrestore(rq, \u0026amp;rf); 7612} 7613 7614static unsigned long task_h_load(struct task_struct *p) 7615{ 7616\treturn p-\u0026gt;se.avg.load_avg; 7617} 7618#endif 7619 7620/********** Helpers for find_busiest_group ************************/ 7621 7622enum group_type { 7623\tgroup_other = 0, 7624\tgroup_imbalanced, 7625\tgroup_overloaded, 7626}; 7627 7628/* 7629* sg_lb_stats - stats of a sched_group required for load_balancing 7630*/ 7631struct sg_lb_stats { 7632\tunsigned long avg_load; /*Avg load across the CPUs of the group */ 7633\tunsigned long group_load; /* Total load over the CPUs of the group */ 7634\tunsigned long sum_weighted_load; /* Weighted load of group\u0026#39;s tasks */ 7635\tunsigned long load_per_task; 7636\tunsigned long group_capacity; 7637\tunsigned long group_util; /* Total utilization of the group */ 7638\tunsigned int sum_nr_running; /* Nr tasks running in the group */ 7639\tunsigned int idle_cpus; 7640\tunsigned int group_weight; 7641\tenum group_type group_type; 7642\tint group_no_capacity; 7643#ifdef CONFIG_NUMA_BALANCING 7644\tunsigned int nr_numa_running; 7645\tunsigned int nr_preferred_running; 7646#endif 7647}; 7648 7649/* 7650* sd_lb_stats - Structure to store the statistics of a sched_domain 7651*\tduring load balancing. 7652*/ 7653struct sd_lb_stats { 7654\tstruct sched_group *busiest;\t/* Busiest group in this sd */ 7655\tstruct sched_group *local;\t/* Local group in this sd */ 7656\tunsigned long total_running; 7657\tunsigned long total_load;\t/* Total load of all groups in sd */ 7658\tunsigned long total_capacity;\t/* Total capacity of all groups in sd */ 7659\tunsigned long avg_load;\t/* Average load across all groups in sd */ 7660 7661\tstruct sg_lb_stats busiest_stat;/* Statistics of the busiest group */ 7662\tstruct sg_lb_stats local_stat;\t/* Statistics of the local group */ 7663}; 7664 7665static inline void init_sd_lb_stats(struct sd_lb_stats *sds) 7666{ 7667\t/* 7668* Skimp on the clearing to avoid duplicate work. We can avoid clearing 7669* local_stat because update_sg_lb_stats() does a full clear/assignment. 7670* We must however clear busiest_stat::avg_load because 7671* update_sd_pick_busiest() reads this before assignment. 7672*/ 7673\t*sds = (struct sd_lb_stats){ 7674\t.busiest = NULL, 7675\t.local = NULL, 7676\t.total_running = 0UL, 7677\t.total_load = 0UL, 7678\t.total_capacity = 0UL, 7679\t.busiest_stat = { 7680\t.avg_load = 0UL, 7681\t.sum_nr_running = 0, 7682\t.group_type = group_other, 7683\t}, 7684\t}; 7685} 7686 7687/** 7688* get_sd_load_idx - Obtain the load index for a given sched domain. 7689* @sd: The sched_domain whose load_idx is to be obtained. 7690* @idle: The idle status of the CPU for whose sd load_idx is obtained. 7691* 7692* Return: The load index. 7693*/ 7694static inline int get_sd_load_idx(struct sched_domain *sd, 7695\tenum cpu_idle_type idle) 7696{ 7697\tint load_idx; 7698 7699\tswitch (idle) { 7700\tcase CPU_NOT_IDLE: 7701\tload_idx = sd-\u0026gt;busy_idx; 7702\tbreak; 7703 7704\tcase CPU_NEWLY_IDLE: 7705\tload_idx = sd-\u0026gt;newidle_idx; 7706\tbreak; 7707\tdefault: 7708\tload_idx = sd-\u0026gt;idle_idx; 7709\tbreak; 7710\t} 7711 7712\treturn load_idx; 7713} 7714 7715static unsigned long scale_rt_capacity(struct sched_domain *sd, int cpu) 7716{ 7717\tstruct rq *rq = cpu_rq(cpu); 7718\tunsigned long max = arch_scale_cpu_capacity(sd, cpu); 7719\tunsigned long used, free; 7720\tunsigned long irq; 7721 7722\tirq = cpu_util_irq(rq); 7723 7724\tif (unlikely(irq \u0026gt;= max)) 7725\treturn 1; 7726 7727\tused = READ_ONCE(rq-\u0026gt;avg_rt.util_avg); 7728\tused += READ_ONCE(rq-\u0026gt;avg_dl.util_avg); 7729 7730\tif (unlikely(used \u0026gt;= max)) 7731\treturn 1; 7732 7733\tfree = max - used; 7734 7735\treturn scale_irq_capacity(free, irq, max); 7736} 7737 7738static void update_cpu_capacity(struct sched_domain *sd, int cpu) 7739{ 7740\tunsigned long capacity = scale_rt_capacity(sd, cpu); 7741\tstruct sched_group *sdg = sd-\u0026gt;groups; 7742 7743\tcpu_rq(cpu)-\u0026gt;cpu_capacity_orig = arch_scale_cpu_capacity(sd, cpu); 7744 7745\tif (!capacity) 7746\tcapacity = 1; 7747 7748\tcpu_rq(cpu)-\u0026gt;cpu_capacity = capacity; 7749\tsdg-\u0026gt;sgc-\u0026gt;capacity = capacity; 7750\tsdg-\u0026gt;sgc-\u0026gt;min_capacity = capacity; 7751} 7752 7753void update_group_capacity(struct sched_domain *sd, int cpu) 7754{ 7755\tstruct sched_domain *child = sd-\u0026gt;child; 7756\tstruct sched_group *group, *sdg = sd-\u0026gt;groups; 7757\tunsigned long capacity, min_capacity; 7758\tunsigned long interval; 7759 7760\tinterval = msecs_to_jiffies(sd-\u0026gt;balance_interval); 7761\tinterval = clamp(interval, 1UL, max_load_balance_interval); 7762\tsdg-\u0026gt;sgc-\u0026gt;next_update = jiffies + interval; 7763 7764\tif (!child) { 7765\tupdate_cpu_capacity(sd, cpu); 7766\treturn; 7767\t} 7768 7769\tcapacity = 0; 7770\tmin_capacity = ULONG_MAX; 7771 7772\tif (child-\u0026gt;flags \u0026amp; SD_OVERLAP) { 7773\t/* 7774* SD_OVERLAP domains cannot assume that child groups 7775* span the current group. 7776*/ 7777 7778\tfor_each_cpu(cpu, sched_group_span(sdg)) { 7779\tstruct sched_group_capacity *sgc; 7780\tstruct rq *rq = cpu_rq(cpu); 7781 7782\t/* 7783* build_sched_domains() -\u0026gt; init_sched_groups_capacity() 7784* gets here before we\u0026#39;ve attached the domains to the 7785* runqueues. 7786* 7787* Use capacity_of(), which is set irrespective of domains 7788* in update_cpu_capacity(). 7789* 7790* This avoids capacity from being 0 and 7791* causing divide-by-zero issues on boot. 7792*/ 7793\tif (unlikely(!rq-\u0026gt;sd)) { 7794\tcapacity += capacity_of(cpu); 7795\t} else { 7796\tsgc = rq-\u0026gt;sd-\u0026gt;groups-\u0026gt;sgc; 7797\tcapacity += sgc-\u0026gt;capacity; 7798\t} 7799 7800\tmin_capacity = min(capacity, min_capacity); 7801\t} 7802\t} else { 7803\t/* 7804* !SD_OVERLAP domains can assume that child groups 7805* span the current group. 7806*/ 7807 7808\tgroup = child-\u0026gt;groups; 7809\tdo { 7810\tstruct sched_group_capacity *sgc = group-\u0026gt;sgc; 7811 7812\tcapacity += sgc-\u0026gt;capacity; 7813\tmin_capacity = min(sgc-\u0026gt;min_capacity, min_capacity); 7814\tgroup = group-\u0026gt;next; 7815\t} while (group != child-\u0026gt;groups); 7816\t} 7817 7818\tsdg-\u0026gt;sgc-\u0026gt;capacity = capacity; 7819\tsdg-\u0026gt;sgc-\u0026gt;min_capacity = min_capacity; 7820} 7821 7822/* 7823* Check whether the capacity of the rq has been noticeably reduced by side 7824* activity. The imbalance_pct is used for the threshold. 7825* Return true is the capacity is reduced 7826*/ 7827static inline int 7828check_cpu_capacity(struct rq *rq, struct sched_domain *sd) 7829{ 7830\treturn ((rq-\u0026gt;cpu_capacity * sd-\u0026gt;imbalance_pct) \u0026lt; 7831\t(rq-\u0026gt;cpu_capacity_orig * 100)); 7832} 7833 7834/* 7835* Group imbalance indicates (and tries to solve) the problem where balancing 7836* groups is inadequate due to -\u0026gt;cpus_allowed constraints. 7837* 7838* Imagine a situation of two groups of 4 CPUs each and 4 tasks each with a 7839* cpumask covering 1 CPU of the first group and 3 CPUs of the second group. 7840* Something like: 7841* 7842*\t{ 0 1 2 3 } { 4 5 6 7 } 7843*\t* * * * 7844* 7845* If we were to balance group-wise we\u0026#39;d place two tasks in the first group and 7846* two tasks in the second group. Clearly this is undesired as it will overload 7847* cpu 3 and leave one of the CPUs in the second group unused. 7848* 7849* The current solution to this issue is detecting the skew in the first group 7850* by noticing the lower domain failed to reach balance and had difficulty 7851* moving tasks due to affinity constraints. 7852* 7853* When this is so detected; this group becomes a candidate for busiest; see 7854* update_sd_pick_busiest(). And calculate_imbalance() and 7855* find_busiest_group() avoid some of the usual balance conditions to allow it 7856* to create an effective group imbalance. 7857* 7858* This is a somewhat tricky proposition since the next run might not find the 7859* group imbalance and decide the groups need to be balanced again. A most 7860* subtle and fragile situation. 7861*/ 7862 7863static inline int sg_imbalanced(struct sched_group *group) 7864{ 7865\treturn group-\u0026gt;sgc-\u0026gt;imbalance; 7866} 7867 7868/* 7869* group_has_capacity returns true if the group has spare capacity that could 7870* be used by some tasks. 7871* We consider that a group has spare capacity if the * number of task is 7872* smaller than the number of CPUs or if the utilization is lower than the 7873* available capacity for CFS tasks. 7874* For the latter, we use a threshold to stabilize the state, to take into 7875* account the variance of the tasks\u0026#39; load and to return true if the available 7876* capacity in meaningful for the load balancer. 7877* As an example, an available capacity of 1% can appear but it doesn\u0026#39;t make 7878* any benefit for the load balance. 7879*/ 7880static inline bool 7881group_has_capacity(struct lb_env *env, struct sg_lb_stats *sgs) 7882{ 7883\tif (sgs-\u0026gt;sum_nr_running \u0026lt; sgs-\u0026gt;group_weight) 7884\treturn true; 7885 7886\tif ((sgs-\u0026gt;group_capacity * 100) \u0026gt; 7887\t(sgs-\u0026gt;group_util * env-\u0026gt;sd-\u0026gt;imbalance_pct)) 7888\treturn true; 7889 7890\treturn false; 7891} 7892 7893/* 7894* group_is_overloaded returns true if the group has more tasks than it can 7895* handle. 7896* group_is_overloaded is not equals to !group_has_capacity because a group 7897* with the exact right number of tasks, has no more spare capacity but is not 7898* overloaded so both group_has_capacity and group_is_overloaded return 7899* false. 7900*/ 7901static inline bool 7902group_is_overloaded(struct lb_env *env, struct sg_lb_stats *sgs) 7903{ 7904\tif (sgs-\u0026gt;sum_nr_running \u0026lt;= sgs-\u0026gt;group_weight) 7905\treturn false; 7906 7907\tif ((sgs-\u0026gt;group_capacity * 100) \u0026lt; 7908\t(sgs-\u0026gt;group_util * env-\u0026gt;sd-\u0026gt;imbalance_pct)) 7909\treturn true; 7910 7911\treturn false; 7912} 7913 7914/* 7915* group_smaller_cpu_capacity: Returns true if sched_group sg has smaller 7916* per-CPU capacity than sched_group ref. 7917*/ 7918static inline bool 7919group_smaller_cpu_capacity(struct sched_group *sg, struct sched_group *ref) 7920{ 7921\treturn sg-\u0026gt;sgc-\u0026gt;min_capacity * capacity_margin \u0026lt; 7922\tref-\u0026gt;sgc-\u0026gt;min_capacity * 1024; 7923} 7924 7925static inline enum 7926group_type group_classify(struct sched_group *group, 7927\tstruct sg_lb_stats *sgs) 7928{ 7929\tif (sgs-\u0026gt;group_no_capacity) 7930\treturn group_overloaded; 7931 7932\tif (sg_imbalanced(group)) 7933\treturn group_imbalanced; 7934 7935\treturn group_other; 7936} 7937 7938static bool update_nohz_stats(struct rq *rq, bool force) 7939{ 7940#ifdef CONFIG_NO_HZ_COMMON 7941\tunsigned int cpu = rq-\u0026gt;cpu; 7942 7943\tif (!rq-\u0026gt;has_blocked_load) 7944\treturn false; 7945 7946\tif (!cpumask_test_cpu(cpu, nohz.idle_cpus_mask)) 7947\treturn false; 7948 7949\tif (!force \u0026amp;\u0026amp; !time_after(jiffies, rq-\u0026gt;last_blocked_load_update_tick)) 7950\treturn true; 7951 7952\tupdate_blocked_averages(cpu); 7953 7954\treturn rq-\u0026gt;has_blocked_load; 7955#else 7956\treturn false; 7957#endif 7958} 7959 7960/** 7961* update_sg_lb_stats - Update sched_group\u0026#39;s statistics for load balancing. 7962* @env: The load balancing environment. 7963* @group: sched_group whose statistics are to be updated. 7964* @load_idx: Load index of sched_domain of this_cpu for load calc. 7965* @local_group: Does group contain this_cpu. 7966* @sgs: variable to hold the statistics for this group. 7967* @overload: Indicate more than one runnable task for any CPU. 7968*/ 7969static inline void update_sg_lb_stats(struct lb_env *env, 7970\tstruct sched_group *group, int load_idx, 7971\tint local_group, struct sg_lb_stats *sgs, 7972\tbool *overload) 7973{ 7974\tunsigned long load; 7975\tint i, nr_running; 7976 7977\tmemset(sgs, 0, sizeof(*sgs)); 7978 7979\tfor_each_cpu_and(i, sched_group_span(group), env-\u0026gt;cpus) { 7980\tstruct rq *rq = cpu_rq(i); 7981 7982\tif ((env-\u0026gt;flags \u0026amp; LBF_NOHZ_STATS) \u0026amp;\u0026amp; update_nohz_stats(rq, false)) 7983\tenv-\u0026gt;flags |= LBF_NOHZ_AGAIN; 7984 7985\t/* Bias balancing toward CPUs of our domain: */ 7986\tif (local_group) 7987\tload = target_load(i, load_idx); 7988\telse 7989\tload = source_load(i, load_idx); 7990 7991\tsgs-\u0026gt;group_load += load; 7992\tsgs-\u0026gt;group_util += cpu_util(i); 7993\tsgs-\u0026gt;sum_nr_running += rq-\u0026gt;cfs.h_nr_running; 7994 7995\tnr_running = rq-\u0026gt;nr_running; 7996\tif (nr_running \u0026gt; 1) 7997\t*overload = true; 7998 7999#ifdef CONFIG_NUMA_BALANCING 8000\tsgs-\u0026gt;nr_numa_running += rq-\u0026gt;nr_numa_running; 8001\tsgs-\u0026gt;nr_preferred_running += rq-\u0026gt;nr_preferred_running; 8002#endif 8003\tsgs-\u0026gt;sum_weighted_load += weighted_cpuload(rq); 8004\t/* 8005* No need to call idle_cpu() if nr_running is not 0 8006*/ 8007\tif (!nr_running \u0026amp;\u0026amp; idle_cpu(i)) 8008\tsgs-\u0026gt;idle_cpus++; 8009\t} 8010 8011\t/* Adjust by relative CPU capacity of the group */ 8012\tsgs-\u0026gt;group_capacity = group-\u0026gt;sgc-\u0026gt;capacity; 8013\tsgs-\u0026gt;avg_load = (sgs-\u0026gt;group_load*SCHED_CAPACITY_SCALE) / sgs-\u0026gt;group_capacity; 8014 8015\tif (sgs-\u0026gt;sum_nr_running) 8016\tsgs-\u0026gt;load_per_task = sgs-\u0026gt;sum_weighted_load / sgs-\u0026gt;sum_nr_running; 8017 8018\tsgs-\u0026gt;group_weight = group-\u0026gt;group_weight; 8019 8020\tsgs-\u0026gt;group_no_capacity = group_is_overloaded(env, sgs); 8021\tsgs-\u0026gt;group_type = group_classify(group, sgs); 8022} 8023 8024/** 8025* update_sd_pick_busiest - return 1 on busiest group 8026* @env: The load balancing environment. 8027* @sds: sched_domain statistics 8028* @sg: sched_group candidate to be checked for being the busiest 8029* @sgs: sched_group statistics 8030* 8031* Determine if @sg is a busier group than the previously selected 8032* busiest group. 8033* 8034* Return: %true if @sg is a busier group than the previously selected 8035* busiest group. %false otherwise. 8036*/ 8037static bool update_sd_pick_busiest(struct lb_env *env, 8038\tstruct sd_lb_stats *sds, 8039\tstruct sched_group *sg, 8040\tstruct sg_lb_stats *sgs) 8041{ 8042\tstruct sg_lb_stats *busiest = \u0026amp;sds-\u0026gt;busiest_stat; 8043 8044\tif (sgs-\u0026gt;group_type \u0026gt; busiest-\u0026gt;group_type) 8045\treturn true; 8046 8047\tif (sgs-\u0026gt;group_type \u0026lt; busiest-\u0026gt;group_type) 8048\treturn false; 8049 8050\tif (sgs-\u0026gt;avg_load \u0026lt;= busiest-\u0026gt;avg_load) 8051\treturn false; 8052 8053\tif (!(env-\u0026gt;sd-\u0026gt;flags \u0026amp; SD_ASYM_CPUCAPACITY)) 8054\tgoto asym_packing; 8055 8056\t/* 8057* Candidate sg has no more than one task per CPU and 8058* has higher per-CPU capacity. Migrating tasks to less 8059* capable CPUs may harm throughput. Maximize throughput, 8060* power/energy consequences are not considered. 8061*/ 8062\tif (sgs-\u0026gt;sum_nr_running \u0026lt;= sgs-\u0026gt;group_weight \u0026amp;\u0026amp; 8063\tgroup_smaller_cpu_capacity(sds-\u0026gt;local, sg)) 8064\treturn false; 8065 8066asym_packing: 8067\t/* This is the busiest node in its class. */ 8068\tif (!(env-\u0026gt;sd-\u0026gt;flags \u0026amp; SD_ASYM_PACKING)) 8069\treturn true; 8070 8071\t/* No ASYM_PACKING if target CPU is already busy */ 8072\tif (env-\u0026gt;idle == CPU_NOT_IDLE) 8073\treturn true; 8074\t/* 8075* ASYM_PACKING needs to move all the work to the highest 8076* prority CPUs in the group, therefore mark all groups 8077* of lower priority than ourself as busy. 8078*/ 8079\tif (sgs-\u0026gt;sum_nr_running \u0026amp;\u0026amp; 8080\tsched_asym_prefer(env-\u0026gt;dst_cpu, sg-\u0026gt;asym_prefer_cpu)) { 8081\tif (!sds-\u0026gt;busiest) 8082\treturn true; 8083 8084\t/* Prefer to move from lowest priority CPU\u0026#39;s work */ 8085\tif (sched_asym_prefer(sds-\u0026gt;busiest-\u0026gt;asym_prefer_cpu, 8086\tsg-\u0026gt;asym_prefer_cpu)) 8087\treturn true; 8088\t} 8089 8090\treturn false; 8091} 8092 8093#ifdef CONFIG_NUMA_BALANCING 8094static inline enum fbq_type fbq_classify_group(struct sg_lb_stats *sgs) 8095{ 8096\tif (sgs-\u0026gt;sum_nr_running \u0026gt; sgs-\u0026gt;nr_numa_running) 8097\treturn regular; 8098\tif (sgs-\u0026gt;sum_nr_running \u0026gt; sgs-\u0026gt;nr_preferred_running) 8099\treturn remote; 8100\treturn all; 8101} 8102 8103static inline enum fbq_type fbq_classify_rq(struct rq *rq) 8104{ 8105\tif (rq-\u0026gt;nr_running \u0026gt; rq-\u0026gt;nr_numa_running) 8106\treturn regular; 8107\tif (rq-\u0026gt;nr_running \u0026gt; rq-\u0026gt;nr_preferred_running) 8108\treturn remote; 8109\treturn all; 8110} 8111#else 8112static inline enum fbq_type fbq_classify_group(struct sg_lb_stats *sgs) 8113{ 8114\treturn all; 8115} 8116 8117static inline enum fbq_type fbq_classify_rq(struct rq *rq) 8118{ 8119\treturn regular; 8120} 8121#endif /* CONFIG_NUMA_BALANCING */8122 8123/** 8124* update_sd_lb_stats - Update sched_domain\u0026#39;s statistics for load balancing. 8125* @env: The load balancing environment. 8126* @sds: variable to hold the statistics for this sched_domain. 8127*/ 8128static inline void update_sd_lb_stats(struct lb_env *env, struct sd_lb_stats *sds) 8129{ 8130\tstruct sched_domain *child = env-\u0026gt;sd-\u0026gt;child; 8131\tstruct sched_group *sg = env-\u0026gt;sd-\u0026gt;groups; 8132\tstruct sg_lb_stats *local = \u0026amp;sds-\u0026gt;local_stat; 8133\tstruct sg_lb_stats tmp_sgs; 8134\tint load_idx, prefer_sibling = 0; 8135\tbool overload = false; 8136 8137\tif (child \u0026amp;\u0026amp; child-\u0026gt;flags \u0026amp; SD_PREFER_SIBLING) 8138\tprefer_sibling = 1; 8139 8140#ifdef CONFIG_NO_HZ_COMMON 8141\tif (env-\u0026gt;idle == CPU_NEWLY_IDLE \u0026amp;\u0026amp; READ_ONCE(nohz.has_blocked)) 8142\tenv-\u0026gt;flags |= LBF_NOHZ_STATS; 8143#endif 8144 8145\tload_idx = get_sd_load_idx(env-\u0026gt;sd, env-\u0026gt;idle); 8146 8147\tdo { 8148\tstruct sg_lb_stats *sgs = \u0026amp;tmp_sgs; 8149\tint local_group; 8150 8151\tlocal_group = cpumask_test_cpu(env-\u0026gt;dst_cpu, sched_group_span(sg)); 8152\tif (local_group) { 8153\tsds-\u0026gt;local = sg; 8154\tsgs = local; 8155 8156\tif (env-\u0026gt;idle != CPU_NEWLY_IDLE || 8157\ttime_after_eq(jiffies, sg-\u0026gt;sgc-\u0026gt;next_update)) 8158\tupdate_group_capacity(env-\u0026gt;sd, env-\u0026gt;dst_cpu); 8159\t} 8160 8161\tupdate_sg_lb_stats(env, sg, load_idx, local_group, sgs, 8162\t\u0026amp;overload); 8163 8164\tif (local_group) 8165\tgoto next_group; 8166 8167\t/* 8168* In case the child domain prefers tasks go to siblings 8169* first, lower the sg capacity so that we\u0026#39;ll try 8170* and move all the excess tasks away. We lower the capacity 8171* of a group only if the local group has the capacity to fit 8172* these excess tasks. The extra check prevents the case where 8173* you always pull from the heaviest group when it is already 8174* under-utilized (possible with a large weight task outweighs 8175* the tasks on the system). 8176*/ 8177\tif (prefer_sibling \u0026amp;\u0026amp; sds-\u0026gt;local \u0026amp;\u0026amp; 8178\tgroup_has_capacity(env, local) \u0026amp;\u0026amp; 8179\t(sgs-\u0026gt;sum_nr_running \u0026gt; local-\u0026gt;sum_nr_running + 1)) { 8180\tsgs-\u0026gt;group_no_capacity = 1; 8181\tsgs-\u0026gt;group_type = group_classify(sg, sgs); 8182\t} 8183 8184\tif (update_sd_pick_busiest(env, sds, sg, sgs)) { 8185\tsds-\u0026gt;busiest = sg; 8186\tsds-\u0026gt;busiest_stat = *sgs; 8187\t} 8188 8189next_group: 8190\t/* Now, start updating sd_lb_stats */ 8191\tsds-\u0026gt;total_running += sgs-\u0026gt;sum_nr_running; 8192\tsds-\u0026gt;total_load += sgs-\u0026gt;group_load; 8193\tsds-\u0026gt;total_capacity += sgs-\u0026gt;group_capacity; 8194 8195\tsg = sg-\u0026gt;next; 8196\t} while (sg != env-\u0026gt;sd-\u0026gt;groups); 8197 8198#ifdef CONFIG_NO_HZ_COMMON 8199\tif ((env-\u0026gt;flags \u0026amp; LBF_NOHZ_AGAIN) \u0026amp;\u0026amp; 8200\tcpumask_subset(nohz.idle_cpus_mask, sched_domain_span(env-\u0026gt;sd))) { 8201 8202\tWRITE_ONCE(nohz.next_blocked, 8203\tjiffies + msecs_to_jiffies(LOAD_AVG_PERIOD)); 8204\t} 8205#endif 8206 8207\tif (env-\u0026gt;sd-\u0026gt;flags \u0026amp; SD_NUMA) 8208\tenv-\u0026gt;fbq_type = fbq_classify_group(\u0026amp;sds-\u0026gt;busiest_stat); 8209 8210\tif (!env-\u0026gt;sd-\u0026gt;parent) { 8211\t/* update overload indicator if we are at root domain */ 8212\tif (env-\u0026gt;dst_rq-\u0026gt;rd-\u0026gt;overload != overload) 8213\tenv-\u0026gt;dst_rq-\u0026gt;rd-\u0026gt;overload = overload; 8214\t} 8215} 8216 8217/** 8218* check_asym_packing - Check to see if the group is packed into the 8219*\tsched domain. 8220* 8221* This is primarily intended to used at the sibling level. Some 8222* cores like POWER7 prefer to use lower numbered SMT threads. In the 8223* case of POWER7, it can move to lower SMT modes only when higher 8224* threads are idle. When in lower SMT modes, the threads will 8225* perform better since they share less core resources. Hence when we 8226* have idle threads, we want them to be the higher ones. 8227* 8228* This packing function is run on idle threads. It checks to see if 8229* the busiest CPU in this domain (core in the P7 case) has a higher 8230* CPU number than the packing function is being run on. Here we are 8231* assuming lower CPU number will be equivalent to lower a SMT thread 8232* number. 8233* 8234* Return: 1 when packing is required and a task should be moved to 8235* this CPU. The amount of the imbalance is returned in env-\u0026gt;imbalance. 8236* 8237* @env: The load balancing environment. 8238* @sds: Statistics of the sched_domain which is to be packed 8239*/ 8240static int check_asym_packing(struct lb_env *env, struct sd_lb_stats *sds) 8241{ 8242\tint busiest_cpu; 8243 8244\tif (!(env-\u0026gt;sd-\u0026gt;flags \u0026amp; SD_ASYM_PACKING)) 8245\treturn 0; 8246 8247\tif (env-\u0026gt;idle == CPU_NOT_IDLE) 8248\treturn 0; 8249 8250\tif (!sds-\u0026gt;busiest) 8251\treturn 0; 8252 8253\tbusiest_cpu = sds-\u0026gt;busiest-\u0026gt;asym_prefer_cpu; 8254\tif (sched_asym_prefer(busiest_cpu, env-\u0026gt;dst_cpu)) 8255\treturn 0; 8256 8257\tenv-\u0026gt;imbalance = DIV_ROUND_CLOSEST( 8258\tsds-\u0026gt;busiest_stat.avg_load * sds-\u0026gt;busiest_stat.group_capacity, 8259\tSCHED_CAPACITY_SCALE); 8260 8261\treturn 1; 8262} 8263 8264/** 8265* fix_small_imbalance - Calculate the minor imbalance that exists 8266*\tamongst the groups of a sched_domain, during 8267*\tload balancing. 8268* @env: The load balancing environment. 8269* @sds: Statistics of the sched_domain whose imbalance is to be calculated. 8270*/ 8271static inline 8272void fix_small_imbalance(struct lb_env *env, struct sd_lb_stats *sds) 8273{ 8274\tunsigned long tmp, capa_now = 0, capa_move = 0; 8275\tunsigned int imbn = 2; 8276\tunsigned long scaled_busy_load_per_task; 8277\tstruct sg_lb_stats *local, *busiest; 8278 8279\tlocal = \u0026amp;sds-\u0026gt;local_stat; 8280\tbusiest = \u0026amp;sds-\u0026gt;busiest_stat; 8281 8282\tif (!local-\u0026gt;sum_nr_running) 8283\tlocal-\u0026gt;load_per_task = cpu_avg_load_per_task(env-\u0026gt;dst_cpu); 8284\telse if (busiest-\u0026gt;load_per_task \u0026gt; local-\u0026gt;load_per_task) 8285\timbn = 1; 8286 8287\tscaled_busy_load_per_task = 8288\t(busiest-\u0026gt;load_per_task * SCHED_CAPACITY_SCALE) / 8289\tbusiest-\u0026gt;group_capacity; 8290 8291\tif (busiest-\u0026gt;avg_load + scaled_busy_load_per_task \u0026gt;= 8292\tlocal-\u0026gt;avg_load + (scaled_busy_load_per_task * imbn)) { 8293\tenv-\u0026gt;imbalance = busiest-\u0026gt;load_per_task; 8294\treturn; 8295\t} 8296 8297\t/* 8298* OK, we don\u0026#39;t have enough imbalance to justify moving tasks, 8299* however we may be able to increase total CPU capacity used by 8300* moving them. 8301*/ 8302 8303\tcapa_now += busiest-\u0026gt;group_capacity * 8304\tmin(busiest-\u0026gt;load_per_task, busiest-\u0026gt;avg_load); 8305\tcapa_now += local-\u0026gt;group_capacity * 8306\tmin(local-\u0026gt;load_per_task, local-\u0026gt;avg_load); 8307\tcapa_now /= SCHED_CAPACITY_SCALE; 8308 8309\t/* Amount of load we\u0026#39;d subtract */ 8310\tif (busiest-\u0026gt;avg_load \u0026gt; scaled_busy_load_per_task) { 8311\tcapa_move += busiest-\u0026gt;group_capacity * 8312\tmin(busiest-\u0026gt;load_per_task, 8313\tbusiest-\u0026gt;avg_load - scaled_busy_load_per_task); 8314\t} 8315 8316\t/* Amount of load we\u0026#39;d add */ 8317\tif (busiest-\u0026gt;avg_load * busiest-\u0026gt;group_capacity \u0026lt; 8318\tbusiest-\u0026gt;load_per_task * SCHED_CAPACITY_SCALE) { 8319\ttmp = (busiest-\u0026gt;avg_load * busiest-\u0026gt;group_capacity) / 8320\tlocal-\u0026gt;group_capacity; 8321\t} else { 8322\ttmp = (busiest-\u0026gt;load_per_task * SCHED_CAPACITY_SCALE) / 8323\tlocal-\u0026gt;group_capacity; 8324\t} 8325\tcapa_move += local-\u0026gt;group_capacity * 8326\tmin(local-\u0026gt;load_per_task, local-\u0026gt;avg_load + tmp); 8327\tcapa_move /= SCHED_CAPACITY_SCALE; 8328 8329\t/* Move if we gain throughput */ 8330\tif (capa_move \u0026gt; capa_now) 8331\tenv-\u0026gt;imbalance = busiest-\u0026gt;load_per_task; 8332} 8333 8334/** 8335* calculate_imbalance - Calculate the amount of imbalance present within the 8336*\tgroups of a given sched_domain during load balance. 8337* @env: load balance environment 8338* @sds: statistics of the sched_domain whose imbalance is to be calculated. 8339*/ 8340static inline void calculate_imbalance(struct lb_env *env, struct sd_lb_stats *sds) 8341{ 8342\tunsigned long max_pull, load_above_capacity = ~0UL; 8343\tstruct sg_lb_stats *local, *busiest; 8344 8345\tlocal = \u0026amp;sds-\u0026gt;local_stat; 8346\tbusiest = \u0026amp;sds-\u0026gt;busiest_stat; 8347 8348\tif (busiest-\u0026gt;group_type == group_imbalanced) { 8349\t/* 8350* In the group_imb case we cannot rely on group-wide averages 8351* to ensure CPU-load equilibrium, look at wider averages. XXX 8352*/ 8353\tbusiest-\u0026gt;load_per_task = 8354\tmin(busiest-\u0026gt;load_per_task, sds-\u0026gt;avg_load); 8355\t} 8356 8357\t/* 8358* Avg load of busiest sg can be less and avg load of local sg can 8359* be greater than avg load across all sgs of sd because avg load 8360* factors in sg capacity and sgs with smaller group_type are 8361* skipped when updating the busiest sg: 8362*/ 8363\tif (busiest-\u0026gt;avg_load \u0026lt;= sds-\u0026gt;avg_load || 8364\tlocal-\u0026gt;avg_load \u0026gt;= sds-\u0026gt;avg_load) { 8365\tenv-\u0026gt;imbalance = 0; 8366\treturn fix_small_imbalance(env, sds); 8367\t} 8368 8369\t/* 8370* If there aren\u0026#39;t any idle CPUs, avoid creating some. 8371*/ 8372\tif (busiest-\u0026gt;group_type == group_overloaded \u0026amp;\u0026amp; 8373\tlocal-\u0026gt;group_type == group_overloaded) { 8374\tload_above_capacity = busiest-\u0026gt;sum_nr_running * SCHED_CAPACITY_SCALE; 8375\tif (load_above_capacity \u0026gt; busiest-\u0026gt;group_capacity) { 8376\tload_above_capacity -= busiest-\u0026gt;group_capacity; 8377\tload_above_capacity *= scale_load_down(NICE_0_LOAD); 8378\tload_above_capacity /= busiest-\u0026gt;group_capacity; 8379\t} else 8380\tload_above_capacity = ~0UL; 8381\t} 8382 8383\t/* 8384* We\u0026#39;re trying to get all the CPUs to the average_load, so we don\u0026#39;t 8385* want to push ourselves above the average load, nor do we wish to 8386* reduce the max loaded CPU below the average load. At the same time, 8387* we also don\u0026#39;t want to reduce the group load below the group 8388* capacity. Thus we look for the minimum possible imbalance. 8389*/ 8390\tmax_pull = min(busiest-\u0026gt;avg_load - sds-\u0026gt;avg_load, load_above_capacity); 8391 8392\t/* How much load to actually move to equalise the imbalance */ 8393\tenv-\u0026gt;imbalance = min( 8394\tmax_pull * busiest-\u0026gt;group_capacity, 8395\t(sds-\u0026gt;avg_load - local-\u0026gt;avg_load) * local-\u0026gt;group_capacity 8396\t) / SCHED_CAPACITY_SCALE; 8397 8398\t/* 8399* if *imbalance is less than the average load per runnable task 8400* there is no guarantee that any tasks will be moved so we\u0026#39;ll have 8401* a think about bumping its value to force at least one task to be 8402* moved 8403*/ 8404\tif (env-\u0026gt;imbalance \u0026lt; busiest-\u0026gt;load_per_task) 8405\treturn fix_small_imbalance(env, sds); 8406} 8407 8408/******* find_busiest_group() helpers end here *********************/ 8409 8410/** 8411* find_busiest_group - Returns the busiest group within the sched_domain 8412* if there is an imbalance. 8413* 8414* Also calculates the amount of weighted load which should be moved 8415* to restore balance. 8416* 8417* @env: The load balancing environment. 8418* 8419* Return:\t- The busiest group if imbalance exists. 8420*/ 8421static struct sched_group *find_busiest_group(struct lb_env *env) 8422{ 8423\tstruct sg_lb_stats *local, *busiest; 8424\tstruct sd_lb_stats sds; 8425 8426\tinit_sd_lb_stats(\u0026amp;sds); 8427 8428\t/* 8429* Compute the various statistics relavent for load balancing at 8430* this level. 8431*/ 8432\tupdate_sd_lb_stats(env, \u0026amp;sds); 8433\tlocal = \u0026amp;sds.local_stat; 8434\tbusiest = \u0026amp;sds.busiest_stat; 8435 8436\t/* ASYM feature bypasses nice load balance check */ 8437\tif (check_asym_packing(env, \u0026amp;sds)) 8438\treturn sds.busiest; 8439 8440\t/* There is no busy sibling group to pull tasks from */ 8441\tif (!sds.busiest || busiest-\u0026gt;sum_nr_running == 0) 8442\tgoto out_balanced; 8443 8444\t/* XXX broken for overlapping NUMA groups */ 8445\tsds.avg_load = (SCHED_CAPACITY_SCALE * sds.total_load) 8446\t/ sds.total_capacity; 8447 8448\t/* 8449* If the busiest group is imbalanced the below checks don\u0026#39;t 8450* work because they assume all things are equal, which typically 8451* isn\u0026#39;t true due to cpus_allowed constraints and the like. 8452*/ 8453\tif (busiest-\u0026gt;group_type == group_imbalanced) 8454\tgoto force_balance; 8455 8456\t/* 8457* When dst_cpu is idle, prevent SMP nice and/or asymmetric group 8458* capacities from resulting in underutilization due to avg_load. 8459*/ 8460\tif (env-\u0026gt;idle != CPU_NOT_IDLE \u0026amp;\u0026amp; group_has_capacity(env, local) \u0026amp;\u0026amp; 8461\tbusiest-\u0026gt;group_no_capacity) 8462\tgoto force_balance; 8463 8464\t/* 8465* If the local group is busier than the selected busiest group 8466* don\u0026#39;t try and pull any tasks. 8467*/ 8468\tif (local-\u0026gt;avg_load \u0026gt;= busiest-\u0026gt;avg_load) 8469\tgoto out_balanced; 8470 8471\t/* 8472* Don\u0026#39;t pull any tasks if this group is already above the domain 8473* average load. 8474*/ 8475\tif (local-\u0026gt;avg_load \u0026gt;= sds.avg_load) 8476\tgoto out_balanced; 8477 8478\tif (env-\u0026gt;idle == CPU_IDLE) { 8479\t/* 8480* This CPU is idle. If the busiest group is not overloaded 8481* and there is no imbalance between this and busiest group 8482* wrt idle CPUs, it is balanced. The imbalance becomes 8483* significant if the diff is greater than 1 otherwise we 8484* might end up to just move the imbalance on another group 8485*/ 8486\tif ((busiest-\u0026gt;group_type != group_overloaded) \u0026amp;\u0026amp; 8487\t(local-\u0026gt;idle_cpus \u0026lt;= (busiest-\u0026gt;idle_cpus + 1))) 8488\tgoto out_balanced; 8489\t} else { 8490\t/* 8491* In the CPU_NEWLY_IDLE, CPU_NOT_IDLE cases, use 8492* imbalance_pct to be conservative. 8493*/ 8494\tif (100 * busiest-\u0026gt;avg_load \u0026lt;= 8495\tenv-\u0026gt;sd-\u0026gt;imbalance_pct * local-\u0026gt;avg_load) 8496\tgoto out_balanced; 8497\t} 8498 8499force_balance: 8500\t/* Looks like there is an imbalance. Compute it */ 8501\tcalculate_imbalance(env, \u0026amp;sds); 8502\treturn env-\u0026gt;imbalance ? sds.busiest : NULL; 8503 8504out_balanced: 8505\tenv-\u0026gt;imbalance = 0; 8506\treturn NULL; 8507} 8508 8509/* 8510* find_busiest_queue - find the busiest runqueue among the CPUs in the group. 8511*/ 8512static struct rq *find_busiest_queue(struct lb_env *env, 8513\tstruct sched_group *group) 8514{ 8515\tstruct rq *busiest = NULL, *rq; 8516\tunsigned long busiest_load = 0, busiest_capacity = 1; 8517\tint i; 8518 8519\tfor_each_cpu_and(i, sched_group_span(group), env-\u0026gt;cpus) { 8520\tunsigned long capacity, wl; 8521\tenum fbq_type rt; 8522 8523\trq = cpu_rq(i); 8524\trt = fbq_classify_rq(rq); 8525 8526\t/* 8527* We classify groups/runqueues into three groups: 8528* - regular: there are !numa tasks 8529* - remote: there are numa tasks that run on the \u0026#39;wrong\u0026#39; node 8530* - all: there is no distinction 8531* 8532* In order to avoid migrating ideally placed numa tasks, 8533* ignore those when there\u0026#39;s better options. 8534* 8535* If we ignore the actual busiest queue to migrate another 8536* task, the next balance pass can still reduce the busiest 8537* queue by moving tasks around inside the node. 8538* 8539* If we cannot move enough load due to this classification 8540* the next pass will adjust the group classification and 8541* allow migration of more tasks. 8542* 8543* Both cases only affect the total convergence complexity. 8544*/ 8545\tif (rt \u0026gt; env-\u0026gt;fbq_type) 8546\tcontinue; 8547 8548\tcapacity = capacity_of(i); 8549 8550\twl = weighted_cpuload(rq); 8551 8552\t/* 8553* When comparing with imbalance, use weighted_cpuload() 8554* which is not scaled with the CPU capacity. 8555*/ 8556 8557\tif (rq-\u0026gt;nr_running == 1 \u0026amp;\u0026amp; wl \u0026gt; env-\u0026gt;imbalance \u0026amp;\u0026amp; 8558\t!check_cpu_capacity(rq, env-\u0026gt;sd)) 8559\tcontinue; 8560 8561\t/* 8562* For the load comparisons with the other CPU\u0026#39;s, consider 8563* the weighted_cpuload() scaled with the CPU capacity, so 8564* that the load can be moved away from the CPU that is 8565* potentially running at a lower capacity. 8566* 8567* Thus we\u0026#39;re looking for max(wl_i / capacity_i), crosswise 8568* multiplication to rid ourselves of the division works out 8569* to: wl_i * capacity_j \u0026gt; wl_j * capacity_i; where j is 8570* our previous maximum. 8571*/ 8572\tif (wl * busiest_capacity \u0026gt; busiest_load * capacity) { 8573\tbusiest_load = wl; 8574\tbusiest_capacity = capacity; 8575\tbusiest = rq; 8576\t} 8577\t} 8578 8579\treturn busiest; 8580} 8581 8582/* 8583* Max backoff if we encounter pinned tasks. Pretty arbitrary value, but 8584* so long as it is large enough. 8585*/ 8586#define MAX_PINNED_INTERVAL\t512 8587 8588static int need_active_balance(struct lb_env *env) 8589{ 8590\tstruct sched_domain *sd = env-\u0026gt;sd; 8591 8592\tif (env-\u0026gt;idle == CPU_NEWLY_IDLE) { 8593 8594\t/* 8595* ASYM_PACKING needs to force migrate tasks from busy but 8596* lower priority CPUs in order to pack all tasks in the 8597* highest priority CPUs. 8598*/ 8599\tif ((sd-\u0026gt;flags \u0026amp; SD_ASYM_PACKING) \u0026amp;\u0026amp; 8600\tsched_asym_prefer(env-\u0026gt;dst_cpu, env-\u0026gt;src_cpu)) 8601\treturn 1; 8602\t} 8603 8604\t/* 8605* The dst_cpu is idle and the src_cpu CPU has only 1 CFS task. 8606* It\u0026#39;s worth migrating the task if the src_cpu\u0026#39;s capacity is reduced 8607* because of other sched_class or IRQs if more capacity stays 8608* available on dst_cpu. 8609*/ 8610\tif ((env-\u0026gt;idle != CPU_NOT_IDLE) \u0026amp;\u0026amp; 8611\t(env-\u0026gt;src_rq-\u0026gt;cfs.h_nr_running == 1)) { 8612\tif ((check_cpu_capacity(env-\u0026gt;src_rq, sd)) \u0026amp;\u0026amp; 8613\t(capacity_of(env-\u0026gt;src_cpu)*sd-\u0026gt;imbalance_pct \u0026lt; capacity_of(env-\u0026gt;dst_cpu)*100)) 8614\treturn 1; 8615\t} 8616 8617\treturn unlikely(sd-\u0026gt;nr_balance_failed \u0026gt; sd-\u0026gt;cache_nice_tries+2); 8618} 8619 8620static int active_load_balance_cpu_stop(void *data); 8621 8622static int should_we_balance(struct lb_env *env) 8623{ 8624\tstruct sched_group *sg = env-\u0026gt;sd-\u0026gt;groups; 8625\tint cpu, balance_cpu = -1; 8626 8627\t/* 8628* Ensure the balancing environment is consistent; can happen 8629* when the softirq triggers \u0026#39;during\u0026#39; hotplug. 8630*/ 8631\tif (!cpumask_test_cpu(env-\u0026gt;dst_cpu, env-\u0026gt;cpus)) 8632\treturn 0; 8633 8634\t/* 8635* In the newly idle case, we will allow all the CPUs 8636* to do the newly idle load balance. 8637*/ 8638\tif (env-\u0026gt;idle == CPU_NEWLY_IDLE) 8639\treturn 1; 8640 8641\t/* Try to find first idle CPU */ 8642\tfor_each_cpu_and(cpu, group_balance_mask(sg), env-\u0026gt;cpus) { 8643\tif (!idle_cpu(cpu)) 8644\tcontinue; 8645 8646\tbalance_cpu = cpu; 8647\tbreak; 8648\t} 8649 8650\tif (balance_cpu == -1) 8651\tbalance_cpu = group_balance_cpu(sg); 8652 8653\t/* 8654* First idle CPU or the first CPU(busiest) in this sched group 8655* is eligible for doing load balancing at this and above domains. 8656*/ 8657\treturn balance_cpu == env-\u0026gt;dst_cpu; 8658} 8659 8660/* 8661* Check this_cpu to ensure it is balanced within domain. Attempt to move 8662* tasks if there is an imbalance. 8663*/ 8664static int load_balance(int this_cpu, struct rq *this_rq, 8665\tstruct sched_domain *sd, enum cpu_idle_type idle, 8666\tint *continue_balancing) 8667{ 8668\tint ld_moved, cur_ld_moved, active_balance = 0; 8669\tstruct sched_domain *sd_parent = sd-\u0026gt;parent; 8670\tstruct sched_group *group; 8671\tstruct rq *busiest; 8672\tstruct rq_flags rf; 8673\tstruct cpumask *cpus = this_cpu_cpumask_var_ptr(load_balance_mask); 8674 8675\tstruct lb_env env = { 8676\t.sd\t= sd, 8677\t.dst_cpu\t= this_cpu, 8678\t.dst_rq\t= this_rq, 8679\t.dst_grpmask = sched_group_span(sd-\u0026gt;groups), 8680\t.idle\t= idle, 8681\t.loop_break\t= sched_nr_migrate_break, 8682\t.cpus\t= cpus, 8683\t.fbq_type\t= all, 8684\t.tasks\t= LIST_HEAD_INIT(env.tasks), 8685\t}; 8686 8687\tcpumask_and(cpus, sched_domain_span(sd), cpu_active_mask); 8688 8689\tschedstat_inc(sd-\u0026gt;lb_count[idle]); 8690 8691redo: 8692\tif (!should_we_balance(\u0026amp;env)) { 8693\t*continue_balancing = 0; 8694\tgoto out_balanced; 8695\t} 8696 8697\tgroup = find_busiest_group(\u0026amp;env); 8698\tif (!group) { 8699\tschedstat_inc(sd-\u0026gt;lb_nobusyg[idle]); 8700\tgoto out_balanced; 8701\t} 8702 8703\tbusiest = find_busiest_queue(\u0026amp;env, group); 8704\tif (!busiest) { 8705\tschedstat_inc(sd-\u0026gt;lb_nobusyq[idle]); 8706\tgoto out_balanced; 8707\t} 8708 8709\tBUG_ON(busiest == env.dst_rq); 8710 8711\tschedstat_add(sd-\u0026gt;lb_imbalance[idle], env.imbalance); 8712 8713\tenv.src_cpu = busiest-\u0026gt;cpu; 8714\tenv.src_rq = busiest; 8715 8716\tld_moved = 0; 8717\tif (busiest-\u0026gt;nr_running \u0026gt; 1) { 8718\t/* 8719* Attempt to move tasks. If find_busiest_group has found 8720* an imbalance but busiest-\u0026gt;nr_running \u0026lt;= 1, the group is 8721* still unbalanced. ld_moved simply stays zero, so it is 8722* correctly treated as an imbalance. 8723*/ 8724\tenv.flags |= LBF_ALL_PINNED; 8725\tenv.loop_max = min(sysctl_sched_nr_migrate, busiest-\u0026gt;nr_running); 8726 8727more_balance: 8728\trq_lock_irqsave(busiest, \u0026amp;rf); 8729\tupdate_rq_clock(busiest); 8730 8731\t/* 8732* cur_ld_moved - load moved in current iteration 8733* ld_moved - cumulative load moved across iterations 8734*/ 8735\tcur_ld_moved = detach_tasks(\u0026amp;env); 8736 8737\t/* 8738* We\u0026#39;ve detached some tasks from busiest_rq. Every 8739* task is masked \u0026#34;TASK_ON_RQ_MIGRATING\u0026#34;, so we can safely 8740* unlock busiest-\u0026gt;lock, and we are able to be sure 8741* that nobody can manipulate the tasks in parallel. 8742* See task_rq_lock() family for the details. 8743*/ 8744 8745\trq_unlock(busiest, \u0026amp;rf); 8746 8747\tif (cur_ld_moved) { 8748\tattach_tasks(\u0026amp;env); 8749\tld_moved += cur_ld_moved; 8750\t} 8751 8752\tlocal_irq_restore(rf.flags); 8753 8754\tif (env.flags \u0026amp; LBF_NEED_BREAK) { 8755\tenv.flags \u0026amp;= ~LBF_NEED_BREAK; 8756\tgoto more_balance; 8757\t} 8758 8759\t/* 8760* Revisit (affine) tasks on src_cpu that couldn\u0026#39;t be moved to 8761* us and move them to an alternate dst_cpu in our sched_group 8762* where they can run. The upper limit on how many times we 8763* iterate on same src_cpu is dependent on number of CPUs in our 8764* sched_group. 8765* 8766* This changes load balance semantics a bit on who can move 8767* load to a given_cpu. In addition to the given_cpu itself 8768* (or a ilb_cpu acting on its behalf where given_cpu is 8769* nohz-idle), we now have balance_cpu in a position to move 8770* load to given_cpu. In rare situations, this may cause 8771* conflicts (balance_cpu and given_cpu/ilb_cpu deciding 8772* _independently_ and at _same_ time to move some load to 8773* given_cpu) causing exceess load to be moved to given_cpu. 8774* This however should not happen so much in practice and 8775* moreover subsequent load balance cycles should correct the 8776* excess load moved. 8777*/ 8778\tif ((env.flags \u0026amp; LBF_DST_PINNED) \u0026amp;\u0026amp; env.imbalance \u0026gt; 0) { 8779 8780\t/* Prevent to re-select dst_cpu via env\u0026#39;s CPUs */ 8781\tcpumask_clear_cpu(env.dst_cpu, env.cpus); 8782 8783\tenv.dst_rq\t= cpu_rq(env.new_dst_cpu); 8784\tenv.dst_cpu\t= env.new_dst_cpu; 8785\tenv.flags\t\u0026amp;= ~LBF_DST_PINNED; 8786\tenv.loop\t= 0; 8787\tenv.loop_break\t= sched_nr_migrate_break; 8788 8789\t/* 8790* Go back to \u0026#34;more_balance\u0026#34; rather than \u0026#34;redo\u0026#34; since we 8791* need to continue with same src_cpu. 8792*/ 8793\tgoto more_balance; 8794\t} 8795 8796\t/* 8797* We failed to reach balance because of affinity. 8798*/ 8799\tif (sd_parent) { 8800\tint *group_imbalance = \u0026amp;sd_parent-\u0026gt;groups-\u0026gt;sgc-\u0026gt;imbalance; 8801 8802\tif ((env.flags \u0026amp; LBF_SOME_PINNED) \u0026amp;\u0026amp; env.imbalance \u0026gt; 0) 8803\t*group_imbalance = 1; 8804\t} 8805 8806\t/* All tasks on this runqueue were pinned by CPU affinity */ 8807\tif (unlikely(env.flags \u0026amp; LBF_ALL_PINNED)) { 8808\tcpumask_clear_cpu(cpu_of(busiest), cpus); 8809\t/* 8810* Attempting to continue load balancing at the current 8811* sched_domain level only makes sense if there are 8812* active CPUs remaining as possible busiest CPUs to 8813* pull load from which are not contained within the 8814* destination group that is receiving any migrated 8815* load. 8816*/ 8817\tif (!cpumask_subset(cpus, env.dst_grpmask)) { 8818\tenv.loop = 0; 8819\tenv.loop_break = sched_nr_migrate_break; 8820\tgoto redo; 8821\t} 8822\tgoto out_all_pinned; 8823\t} 8824\t} 8825 8826\tif (!ld_moved) { 8827\tschedstat_inc(sd-\u0026gt;lb_failed[idle]); 8828\t/* 8829* Increment the failure counter only on periodic balance. 8830* We do not want newidle balance, which can be very 8831* frequent, pollute the failure counter causing 8832* excessive cache_hot migrations and active balances. 8833*/ 8834\tif (idle != CPU_NEWLY_IDLE) 8835\tsd-\u0026gt;nr_balance_failed++; 8836 8837\tif (need_active_balance(\u0026amp;env)) { 8838\tunsigned long flags; 8839 8840\traw_spin_lock_irqsave(\u0026amp;busiest-\u0026gt;lock, flags); 8841 8842\t/* 8843* Don\u0026#39;t kick the active_load_balance_cpu_stop, 8844* if the curr task on busiest CPU can\u0026#39;t be 8845* moved to this_cpu: 8846*/ 8847\tif (!cpumask_test_cpu(this_cpu, \u0026amp;busiest-\u0026gt;curr-\u0026gt;cpus_allowed)) { 8848\traw_spin_unlock_irqrestore(\u0026amp;busiest-\u0026gt;lock, 8849\tflags); 8850\tenv.flags |= LBF_ALL_PINNED; 8851\tgoto out_one_pinned; 8852\t} 8853 8854\t/* 8855* -\u0026gt;active_balance synchronizes accesses to 8856* -\u0026gt;active_balance_work. Once set, it\u0026#39;s cleared 8857* only after active load balance is finished. 8858*/ 8859\tif (!busiest-\u0026gt;active_balance) { 8860\tbusiest-\u0026gt;active_balance = 1; 8861\tbusiest-\u0026gt;push_cpu = this_cpu; 8862\tactive_balance = 1; 8863\t} 8864\traw_spin_unlock_irqrestore(\u0026amp;busiest-\u0026gt;lock, flags); 8865 8866\tif (active_balance) { 8867\tstop_one_cpu_nowait(cpu_of(busiest), 8868\tactive_load_balance_cpu_stop, busiest, 8869\t\u0026amp;busiest-\u0026gt;active_balance_work); 8870\t} 8871 8872\t/* We\u0026#39;ve kicked active balancing, force task migration. */ 8873\tsd-\u0026gt;nr_balance_failed = sd-\u0026gt;cache_nice_tries+1; 8874\t} 8875\t} else 8876\tsd-\u0026gt;nr_balance_failed = 0; 8877 8878\tif (likely(!active_balance)) { 8879\t/* We were unbalanced, so reset the balancing interval */ 8880\tsd-\u0026gt;balance_interval = sd-\u0026gt;min_interval; 8881\t} else { 8882\t/* 8883* If we\u0026#39;ve begun active balancing, start to back off. This 8884* case may not be covered by the all_pinned logic if there 8885* is only 1 task on the busy runqueue (because we don\u0026#39;t call 8886* detach_tasks). 8887*/ 8888\tif (sd-\u0026gt;balance_interval \u0026lt; sd-\u0026gt;max_interval) 8889\tsd-\u0026gt;balance_interval *= 2; 8890\t} 8891 8892\tgoto out; 8893 8894out_balanced: 8895\t/* 8896* We reach balance although we may have faced some affinity 8897* constraints. Clear the imbalance flag only if other tasks got 8898* a chance to move and fix the imbalance. 8899*/ 8900\tif (sd_parent \u0026amp;\u0026amp; !(env.flags \u0026amp; LBF_ALL_PINNED)) { 8901\tint *group_imbalance = \u0026amp;sd_parent-\u0026gt;groups-\u0026gt;sgc-\u0026gt;imbalance; 8902 8903\tif (*group_imbalance) 8904\t*group_imbalance = 0; 8905\t} 8906 8907out_all_pinned: 8908\t/* 8909* We reach balance because all tasks are pinned at this level so 8910* we can\u0026#39;t migrate them. Let the imbalance flag set so parent level 8911* can try to migrate them. 8912*/ 8913\tschedstat_inc(sd-\u0026gt;lb_balanced[idle]); 8914 8915\tsd-\u0026gt;nr_balance_failed = 0; 8916 8917out_one_pinned: 8918\tld_moved = 0; 8919 8920\t/* 8921* idle_balance() disregards balance intervals, so we could repeatedly 8922* reach this code, which would lead to balance_interval skyrocketting 8923* in a short amount of time. Skip the balance_interval increase logic 8924* to avoid that. 8925*/ 8926\tif (env.idle == CPU_NEWLY_IDLE) 8927\tgoto out; 8928 8929\t/* tune up the balancing interval */ 8930\tif (((env.flags \u0026amp; LBF_ALL_PINNED) \u0026amp;\u0026amp; 8931\tsd-\u0026gt;balance_interval \u0026lt; MAX_PINNED_INTERVAL) || 8932\t(sd-\u0026gt;balance_interval \u0026lt; sd-\u0026gt;max_interval)) 8933\tsd-\u0026gt;balance_interval *= 2; 8934out: 8935\treturn ld_moved; 8936} 8937 8938static inline unsigned long 8939get_sd_balance_interval(struct sched_domain *sd, int cpu_busy) 8940{ 8941\tunsigned long interval = sd-\u0026gt;balance_interval; 8942 8943\tif (cpu_busy) 8944\tinterval *= sd-\u0026gt;busy_factor; 8945 8946\t/* scale ms to jiffies */ 8947\tinterval = msecs_to_jiffies(interval); 8948\tinterval = clamp(interval, 1UL, max_load_balance_interval); 8949 8950\treturn interval; 8951} 8952 8953static inline void 8954update_next_balance(struct sched_domain *sd, unsigned long *next_balance) 8955{ 8956\tunsigned long interval, next; 8957 8958\t/* used by idle balance, so cpu_busy = 0 */ 8959\tinterval = get_sd_balance_interval(sd, 0); 8960\tnext = sd-\u0026gt;last_balance + interval; 8961 8962\tif (time_after(*next_balance, next)) 8963\t*next_balance = next; 8964} 8965 8966/* 8967* active_load_balance_cpu_stop is run by the CPU stopper. It pushes 8968* running tasks off the busiest CPU onto idle CPUs. It requires at 8969* least 1 task to be running on each physical CPU where possible, and 8970* avoids physical / logical imbalances. 8971*/ 8972static int active_load_balance_cpu_stop(void *data) 8973{ 8974\tstruct rq *busiest_rq = data; 8975\tint busiest_cpu = cpu_of(busiest_rq); 8976\tint target_cpu = busiest_rq-\u0026gt;push_cpu; 8977\tstruct rq *target_rq = cpu_rq(target_cpu); 8978\tstruct sched_domain *sd; 8979\tstruct task_struct *p = NULL; 8980\tstruct rq_flags rf; 8981 8982\trq_lock_irq(busiest_rq, \u0026amp;rf); 8983\t/* 8984* Between queueing the stop-work and running it is a hole in which 8985* CPUs can become inactive. We should not move tasks from or to 8986* inactive CPUs. 8987*/ 8988\tif (!cpu_active(busiest_cpu) || !cpu_active(target_cpu)) 8989\tgoto out_unlock; 8990 8991\t/* Make sure the requested CPU hasn\u0026#39;t gone down in the meantime: */ 8992\tif (unlikely(busiest_cpu != smp_processor_id() || 8993\t!busiest_rq-\u0026gt;active_balance)) 8994\tgoto out_unlock; 8995 8996\t/* Is there any task to move? */ 8997\tif (busiest_rq-\u0026gt;nr_running \u0026lt;= 1) 8998\tgoto out_unlock; 8999 9000\t/* 9001* This condition is \u0026#34;impossible\u0026#34;, if it occurs 9002* we need to fix it. Originally reported by 9003* Bjorn Helgaas on a 128-CPU setup. 9004*/ 9005\tBUG_ON(busiest_rq == target_rq); 9006 9007\t/* Search for an sd spanning us and the target CPU. */ 9008\trcu_read_lock(); 9009\tfor_each_domain(target_cpu, sd) { 9010\tif ((sd-\u0026gt;flags \u0026amp; SD_LOAD_BALANCE) \u0026amp;\u0026amp; 9011\tcpumask_test_cpu(busiest_cpu, sched_domain_span(sd))) 9012\tbreak; 9013\t} 9014 9015\tif (likely(sd)) { 9016\tstruct lb_env env = { 9017\t.sd\t= sd, 9018\t.dst_cpu\t= target_cpu, 9019\t.dst_rq\t= target_rq, 9020\t.src_cpu\t= busiest_rq-\u0026gt;cpu, 9021\t.src_rq\t= busiest_rq, 9022\t.idle\t= CPU_IDLE, 9023\t/* 9024* can_migrate_task() doesn\u0026#39;t need to compute new_dst_cpu 9025* for active balancing. Since we have CPU_IDLE, but no 9026* @dst_grpmask we need to make that test go away with lying 9027* about DST_PINNED. 9028*/ 9029\t.flags\t= LBF_DST_PINNED, 9030\t}; 9031 9032\tschedstat_inc(sd-\u0026gt;alb_count); 9033\tupdate_rq_clock(busiest_rq); 9034 9035\tp = detach_one_task(\u0026amp;env); 9036\tif (p) { 9037\tschedstat_inc(sd-\u0026gt;alb_pushed); 9038\t/* Active balancing done, reset the failure counter. */ 9039\tsd-\u0026gt;nr_balance_failed = 0; 9040\t} else { 9041\tschedstat_inc(sd-\u0026gt;alb_failed); 9042\t} 9043\t} 9044\trcu_read_unlock(); 9045out_unlock: 9046\tbusiest_rq-\u0026gt;active_balance = 0; 9047\trq_unlock(busiest_rq, \u0026amp;rf); 9048 9049\tif (p) 9050\tattach_one_task(target_rq, p); 9051 9052\tlocal_irq_enable(); 9053 9054\treturn 0; 9055} 9056 9057static DEFINE_SPINLOCK(balancing); 9058 9059/* 9060* Scale the max load_balance interval with the number of CPUs in the system. 9061* This trades load-balance latency on larger machines for less cross talk. 9062*/ 9063void update_max_interval(void) 9064{ 9065\tmax_load_balance_interval = HZ*num_online_cpus()/10; 9066} 9067 9068/* 9069* It checks each scheduling domain to see if it is due to be balanced, 9070* and initiates a balancing operation if so. 9071* 9072* Balancing parameters are set up in init_sched_domains. 9073*/ 9074static void rebalance_domains(struct rq *rq, enum cpu_idle_type idle) 9075{ 9076\tint continue_balancing = 1; 9077\tint cpu = rq-\u0026gt;cpu; 9078\tunsigned long interval; 9079\tstruct sched_domain *sd; 9080\t/* Earliest time when we have to do rebalance again */ 9081\tunsigned long next_balance = jiffies + 60*HZ; 9082\tint update_next_balance = 0; 9083\tint need_serialize, need_decay = 0; 9084\tu64 max_cost = 0; 9085 9086\trcu_read_lock(); 9087\tfor_each_domain(cpu, sd) { 9088\t/* 9089* Decay the newidle max times here because this is a regular 9090* visit to all the domains. Decay ~1% per second. 9091*/ 9092\tif (time_after(jiffies, sd-\u0026gt;next_decay_max_lb_cost)) { 9093\tsd-\u0026gt;max_newidle_lb_cost = 9094\t(sd-\u0026gt;max_newidle_lb_cost * 253) / 256; 9095\tsd-\u0026gt;next_decay_max_lb_cost = jiffies + HZ; 9096\tneed_decay = 1; 9097\t} 9098\tmax_cost += sd-\u0026gt;max_newidle_lb_cost; 9099 9100\tif (!(sd-\u0026gt;flags \u0026amp; SD_LOAD_BALANCE)) 9101\tcontinue; 9102 9103\t/* 9104* Stop the load balance at this level. There is another 9105* CPU in our sched group which is doing load balancing more 9106* actively. 9107*/ 9108\tif (!continue_balancing) { 9109\tif (need_decay) 9110\tcontinue; 9111\tbreak; 9112\t} 9113 9114\tinterval = get_sd_balance_interval(sd, idle != CPU_IDLE); 9115 9116\tneed_serialize = sd-\u0026gt;flags \u0026amp; SD_SERIALIZE; 9117\tif (need_serialize) { 9118\tif (!spin_trylock(\u0026amp;balancing)) 9119\tgoto out; 9120\t} 9121 9122\tif (time_after_eq(jiffies, sd-\u0026gt;last_balance + interval)) { 9123\tif (load_balance(cpu, rq, sd, idle, \u0026amp;continue_balancing)) { 9124\t/* 9125* The LBF_DST_PINNED logic could have changed 9126* env-\u0026gt;dst_cpu, so we can\u0026#39;t know our idle 9127* state even if we migrated tasks. Update it. 9128*/ 9129\tidle = idle_cpu(cpu) ? CPU_IDLE : CPU_NOT_IDLE; 9130\t} 9131\tsd-\u0026gt;last_balance = jiffies; 9132\tinterval = get_sd_balance_interval(sd, idle != CPU_IDLE); 9133\t} 9134\tif (need_serialize) 9135\tspin_unlock(\u0026amp;balancing); 9136out: 9137\tif (time_after(next_balance, sd-\u0026gt;last_balance + interval)) { 9138\tnext_balance = sd-\u0026gt;last_balance + interval; 9139\tupdate_next_balance = 1; 9140\t} 9141\t} 9142\tif (need_decay) { 9143\t/* 9144* Ensure the rq-wide value also decays but keep it at a 9145* reasonable floor to avoid funnies with rq-\u0026gt;avg_idle. 9146*/ 9147\trq-\u0026gt;max_idle_balance_cost = 9148\tmax((u64)sysctl_sched_migration_cost, max_cost); 9149\t} 9150\trcu_read_unlock(); 9151 9152\t/* 9153* next_balance will be updated only when there is a need. 9154* When the cpu is attached to null domain for ex, it will not be 9155* updated. 9156*/ 9157\tif (likely(update_next_balance)) { 9158\trq-\u0026gt;next_balance = next_balance; 9159 9160#ifdef CONFIG_NO_HZ_COMMON 9161\t/* 9162* If this CPU has been elected to perform the nohz idle 9163* balance. Other idle CPUs have already rebalanced with 9164* nohz_idle_balance() and nohz.next_balance has been 9165* updated accordingly. This CPU is now running the idle load 9166* balance for itself and we need to update the 9167* nohz.next_balance accordingly. 9168*/ 9169\tif ((idle == CPU_IDLE) \u0026amp;\u0026amp; time_after(nohz.next_balance, rq-\u0026gt;next_balance)) 9170\tnohz.next_balance = rq-\u0026gt;next_balance; 9171#endif 9172\t} 9173} 9174 9175static inline int on_null_domain(struct rq *rq) 9176{ 9177\treturn unlikely(!rcu_dereference_sched(rq-\u0026gt;sd)); 9178} 9179 9180#ifdef CONFIG_NO_HZ_COMMON 9181/* 9182* idle load balancing details 9183* - When one of the busy CPUs notice that there may be an idle rebalancing 9184* needed, they will kick the idle load balancer, which then does idle 9185* load balancing for all the idle CPUs. 9186* - HK_FLAG_MISC CPUs are used for this task, because HK_FLAG_SCHED not set 9187* anywhere yet. 9188*/ 9189 9190static inline int find_new_ilb(void) 9191{ 9192\tint ilb; 9193 9194\tfor_each_cpu_and(ilb, nohz.idle_cpus_mask, 9195\thousekeeping_cpumask(HK_FLAG_MISC)) { 9196\tif (idle_cpu(ilb)) 9197\treturn ilb; 9198\t} 9199 9200\treturn nr_cpu_ids; 9201} 9202 9203/* 9204* Kick a CPU to do the nohz balancing, if it is time for it. We pick any 9205* idle CPU in the HK_FLAG_MISC housekeeping set (if there is one). 9206*/ 9207static void kick_ilb(unsigned int flags) 9208{ 9209\tint ilb_cpu; 9210 9211\t/* 9212* Increase nohz.next_balance only when if full ilb is triggered but 9213* not if we only update stats. 9214*/ 9215\tif (flags \u0026amp; NOHZ_BALANCE_KICK) 9216\tnohz.next_balance = jiffies+1; 9217 9218\tilb_cpu = find_new_ilb(); 9219 9220\tif (ilb_cpu \u0026gt;= nr_cpu_ids) 9221\treturn; 9222 9223\tflags = atomic_fetch_or(flags, nohz_flags(ilb_cpu)); 9224\tif (flags \u0026amp; NOHZ_KICK_MASK) 9225\treturn; 9226 9227\t/* 9228* Use smp_send_reschedule() instead of resched_cpu(). 9229* This way we generate a sched IPI on the target CPU which 9230* is idle. And the softirq performing nohz idle load balance 9231* will be run before returning from the IPI. 9232*/ 9233\tsmp_send_reschedule(ilb_cpu); 9234} 9235 9236/* 9237* Current heuristic for kicking the idle load balancer in the presence 9238* of an idle cpu in the system. 9239* - This rq has more than one task. 9240* - This rq has at least one CFS task and the capacity of the CPU is 9241* significantly reduced because of RT tasks or IRQs. 9242* - At parent of LLC scheduler domain level, this cpu\u0026#39;s scheduler group has 9243* multiple busy cpu. 9244* - For SD_ASYM_PACKING, if the lower numbered cpu\u0026#39;s in the scheduler 9245* domain span are idle. 9246*/ 9247static void nohz_balancer_kick(struct rq *rq) 9248{ 9249\tunsigned long now = jiffies; 9250\tstruct sched_domain_shared *sds; 9251\tstruct sched_domain *sd; 9252\tint nr_busy, i, cpu = rq-\u0026gt;cpu; 9253\tunsigned int flags = 0; 9254 9255\tif (unlikely(rq-\u0026gt;idle_balance)) 9256\treturn; 9257 9258\t/* 9259* We may be recently in ticked or tickless idle mode. At the first 9260* busy tick after returning from idle, we will update the busy stats. 9261*/ 9262\tnohz_balance_exit_idle(rq); 9263 9264\t/* 9265* None are in tickless mode and hence no need for NOHZ idle load 9266* balancing. 9267*/ 9268\tif (likely(!atomic_read(\u0026amp;nohz.nr_cpus))) 9269\treturn; 9270 9271\tif (READ_ONCE(nohz.has_blocked) \u0026amp;\u0026amp; 9272\ttime_after(now, READ_ONCE(nohz.next_blocked))) 9273\tflags = NOHZ_STATS_KICK; 9274 9275\tif (time_before(now, nohz.next_balance)) 9276\tgoto out; 9277 9278\tif (rq-\u0026gt;nr_running \u0026gt;= 2) { 9279\tflags = NOHZ_KICK_MASK; 9280\tgoto out; 9281\t} 9282 9283\trcu_read_lock(); 9284\tsds = rcu_dereference(per_cpu(sd_llc_shared, cpu)); 9285\tif (sds) { 9286\t/* 9287* XXX: write a coherent comment on why we do this. 9288* See also: http://lkml.kernel.org/r/20111202010832.602203411@sbsiddha-desk.sc.intel.com 9289*/ 9290\tnr_busy = atomic_read(\u0026amp;sds-\u0026gt;nr_busy_cpus); 9291\tif (nr_busy \u0026gt; 1) { 9292\tflags = NOHZ_KICK_MASK; 9293\tgoto unlock; 9294\t} 9295 9296\t} 9297 9298\tsd = rcu_dereference(rq-\u0026gt;sd); 9299\tif (sd) { 9300\tif ((rq-\u0026gt;cfs.h_nr_running \u0026gt;= 1) \u0026amp;\u0026amp; 9301\tcheck_cpu_capacity(rq, sd)) { 9302\tflags = NOHZ_KICK_MASK; 9303\tgoto unlock; 9304\t} 9305\t} 9306 9307\tsd = rcu_dereference(per_cpu(sd_asym, cpu)); 9308\tif (sd) { 9309\tfor_each_cpu(i, sched_domain_span(sd)) { 9310\tif (i == cpu || 9311\t!cpumask_test_cpu(i, nohz.idle_cpus_mask)) 9312\tcontinue; 9313 9314\tif (sched_asym_prefer(i, cpu)) { 9315\tflags = NOHZ_KICK_MASK; 9316\tgoto unlock; 9317\t} 9318\t} 9319\t} 9320unlock: 9321\trcu_read_unlock(); 9322out: 9323\tif (flags) 9324\tkick_ilb(flags); 9325} 9326 9327static void set_cpu_sd_state_busy(int cpu) 9328{ 9329\tstruct sched_domain *sd; 9330 9331\trcu_read_lock(); 9332\tsd = rcu_dereference(per_cpu(sd_llc, cpu)); 9333 9334\tif (!sd || !sd-\u0026gt;nohz_idle) 9335\tgoto unlock; 9336\tsd-\u0026gt;nohz_idle = 0; 9337 9338\tatomic_inc(\u0026amp;sd-\u0026gt;shared-\u0026gt;nr_busy_cpus); 9339unlock: 9340\trcu_read_unlock(); 9341} 9342 9343void nohz_balance_exit_idle(struct rq *rq) 9344{ 9345\tSCHED_WARN_ON(rq != this_rq()); 9346 9347\tif (likely(!rq-\u0026gt;nohz_tick_stopped)) 9348\treturn; 9349 9350\trq-\u0026gt;nohz_tick_stopped = 0; 9351\tcpumask_clear_cpu(rq-\u0026gt;cpu, nohz.idle_cpus_mask); 9352\tatomic_dec(\u0026amp;nohz.nr_cpus); 9353 9354\tset_cpu_sd_state_busy(rq-\u0026gt;cpu); 9355} 9356 9357static void set_cpu_sd_state_idle(int cpu) 9358{ 9359\tstruct sched_domain *sd; 9360 9361\trcu_read_lock(); 9362\tsd = rcu_dereference(per_cpu(sd_llc, cpu)); 9363 9364\tif (!sd || sd-\u0026gt;nohz_idle) 9365\tgoto unlock; 9366\tsd-\u0026gt;nohz_idle = 1; 9367 9368\tatomic_dec(\u0026amp;sd-\u0026gt;shared-\u0026gt;nr_busy_cpus); 9369unlock: 9370\trcu_read_unlock(); 9371} 9372 9373/* 9374* This routine will record that the CPU is going idle with tick stopped. 9375* This info will be used in performing idle load balancing in the future. 9376*/ 9377void nohz_balance_enter_idle(int cpu) 9378{ 9379\tstruct rq *rq = cpu_rq(cpu); 9380 9381\tSCHED_WARN_ON(cpu != smp_processor_id()); 9382 9383\t/* If this CPU is going down, then nothing needs to be done: */ 9384\tif (!cpu_active(cpu)) 9385\treturn; 9386 9387\t/* Spare idle load balancing on CPUs that don\u0026#39;t want to be disturbed: */ 9388\tif (!housekeeping_cpu(cpu, HK_FLAG_SCHED)) 9389\treturn; 9390 9391\t/* 9392* Can be set safely without rq-\u0026gt;lock held 9393* If a clear happens, it will have evaluated last additions because 9394* rq-\u0026gt;lock is held during the check and the clear 9395*/ 9396\trq-\u0026gt;has_blocked_load = 1; 9397 9398\t/* 9399* The tick is still stopped but load could have been added in the 9400* meantime. We set the nohz.has_blocked flag to trig a check of the 9401* *_avg. The CPU is already part of nohz.idle_cpus_mask so the clear 9402* of nohz.has_blocked can only happen after checking the new load 9403*/ 9404\tif (rq-\u0026gt;nohz_tick_stopped) 9405\tgoto out; 9406 9407\t/* If we\u0026#39;re a completely isolated CPU, we don\u0026#39;t play: */ 9408\tif (on_null_domain(rq)) 9409\treturn; 9410 9411\trq-\u0026gt;nohz_tick_stopped = 1; 9412 9413\tcpumask_set_cpu(cpu, nohz.idle_cpus_mask); 9414\tatomic_inc(\u0026amp;nohz.nr_cpus); 9415 9416\t/* 9417* Ensures that if nohz_idle_balance() fails to observe our 9418* @idle_cpus_mask store, it must observe the @has_blocked 9419* store. 9420*/ 9421\tsmp_mb__after_atomic(); 9422 9423\tset_cpu_sd_state_idle(cpu); 9424 9425out: 9426\t/* 9427* Each time a cpu enter idle, we assume that it has blocked load and 9428* enable the periodic update of the load of idle cpus 9429*/ 9430\tWRITE_ONCE(nohz.has_blocked, 1); 9431} 9432 9433/* 9434* Internal function that runs load balance for all idle cpus. The load balance 9435* can be a simple update of blocked load or a complete load balance with 9436* tasks movement depending of flags. 9437* The function returns false if the loop has stopped before running 9438* through all idle CPUs. 9439*/ 9440static bool _nohz_idle_balance(struct rq *this_rq, unsigned int flags, 9441\tenum cpu_idle_type idle) 9442{ 9443\t/* Earliest time when we have to do rebalance again */ 9444\tunsigned long now = jiffies; 9445\tunsigned long next_balance = now + 60*HZ; 9446\tbool has_blocked_load = false; 9447\tint update_next_balance = 0; 9448\tint this_cpu = this_rq-\u0026gt;cpu; 9449\tint balance_cpu; 9450\tint ret = false; 9451\tstruct rq *rq; 9452 9453\tSCHED_WARN_ON((flags \u0026amp; NOHZ_KICK_MASK) == NOHZ_BALANCE_KICK); 9454 9455\t/* 9456* We assume there will be no idle load after this update and clear 9457* the has_blocked flag. If a cpu enters idle in the mean time, it will 9458* set the has_blocked flag and trig another update of idle load. 9459* Because a cpu that becomes idle, is added to idle_cpus_mask before 9460* setting the flag, we are sure to not clear the state and not 9461* check the load of an idle cpu. 9462*/ 9463\tWRITE_ONCE(nohz.has_blocked, 0); 9464 9465\t/* 9466* Ensures that if we miss the CPU, we must see the has_blocked 9467* store from nohz_balance_enter_idle(). 9468*/ 9469\tsmp_mb(); 9470 9471\tfor_each_cpu(balance_cpu, nohz.idle_cpus_mask) { 9472\tif (balance_cpu == this_cpu || !idle_cpu(balance_cpu)) 9473\tcontinue; 9474 9475\t/* 9476* If this CPU gets work to do, stop the load balancing 9477* work being done for other CPUs. Next load 9478* balancing owner will pick it up. 9479*/ 9480\tif (need_resched()) { 9481\thas_blocked_load = true; 9482\tgoto abort; 9483\t} 9484 9485\trq = cpu_rq(balance_cpu); 9486 9487\thas_blocked_load |= update_nohz_stats(rq, true); 9488 9489\t/* 9490* If time for next balance is due, 9491* do the balance. 9492*/ 9493\tif (time_after_eq(jiffies, rq-\u0026gt;next_balance)) { 9494\tstruct rq_flags rf; 9495 9496\trq_lock_irqsave(rq, \u0026amp;rf); 9497\tupdate_rq_clock(rq); 9498\tcpu_load_update_idle(rq); 9499\trq_unlock_irqrestore(rq, \u0026amp;rf); 9500 9501\tif (flags \u0026amp; NOHZ_BALANCE_KICK) 9502\trebalance_domains(rq, CPU_IDLE); 9503\t} 9504 9505\tif (time_after(next_balance, rq-\u0026gt;next_balance)) { 9506\tnext_balance = rq-\u0026gt;next_balance; 9507\tupdate_next_balance = 1; 9508\t} 9509\t} 9510 9511\t/* 9512* next_balance will be updated only when there is a need. 9513* When the CPU is attached to null domain for ex, it will not be 9514* updated. 9515*/ 9516\tif (likely(update_next_balance)) 9517\tnohz.next_balance = next_balance; 9518 9519\t/* Newly idle CPU doesn\u0026#39;t need an update */ 9520\tif (idle != CPU_NEWLY_IDLE) { 9521\tupdate_blocked_averages(this_cpu); 9522\thas_blocked_load |= this_rq-\u0026gt;has_blocked_load; 9523\t} 9524 9525\tif (flags \u0026amp; NOHZ_BALANCE_KICK) 9526\trebalance_domains(this_rq, CPU_IDLE); 9527 9528\tWRITE_ONCE(nohz.next_blocked, 9529\tnow + msecs_to_jiffies(LOAD_AVG_PERIOD)); 9530 9531\t/* The full idle balance loop has been done */ 9532\tret = true; 9533 9534abort: 9535\t/* There is still blocked load, enable periodic update */ 9536\tif (has_blocked_load) 9537\tWRITE_ONCE(nohz.has_blocked, 1); 9538 9539\treturn ret; 9540} 9541 9542/* 9543* In CONFIG_NO_HZ_COMMON case, the idle balance kickee will do the 9544* rebalancing for all the cpus for whom scheduler ticks are stopped. 9545*/ 9546static bool nohz_idle_balance(struct rq *this_rq, enum cpu_idle_type idle) 9547{ 9548\tint this_cpu = this_rq-\u0026gt;cpu; 9549\tunsigned int flags; 9550 9551\tif (!(atomic_read(nohz_flags(this_cpu)) \u0026amp; NOHZ_KICK_MASK)) 9552\treturn false; 9553 9554\tif (idle != CPU_IDLE) { 9555\tatomic_andnot(NOHZ_KICK_MASK, nohz_flags(this_cpu)); 9556\treturn false; 9557\t} 9558 9559\t/* 9560* barrier, pairs with nohz_balance_enter_idle(), ensures ... 9561*/ 9562\tflags = atomic_fetch_andnot(NOHZ_KICK_MASK, nohz_flags(this_cpu)); 9563\tif (!(flags \u0026amp; NOHZ_KICK_MASK)) 9564\treturn false; 9565 9566\t_nohz_idle_balance(this_rq, flags, idle); 9567 9568\treturn true; 9569} 9570 9571static void nohz_newidle_balance(struct rq *this_rq) 9572{ 9573\tint this_cpu = this_rq-\u0026gt;cpu; 9574 9575\t/* 9576* This CPU doesn\u0026#39;t want to be disturbed by scheduler 9577* housekeeping 9578*/ 9579\tif (!housekeeping_cpu(this_cpu, HK_FLAG_SCHED)) 9580\treturn; 9581 9582\t/* Will wake up very soon. No time for doing anything else*/ 9583\tif (this_rq-\u0026gt;avg_idle \u0026lt; sysctl_sched_migration_cost) 9584\treturn; 9585 9586\t/* Don\u0026#39;t need to update blocked load of idle CPUs*/ 9587\tif (!READ_ONCE(nohz.has_blocked) || 9588\ttime_before(jiffies, READ_ONCE(nohz.next_blocked))) 9589\treturn; 9590 9591\traw_spin_unlock(\u0026amp;this_rq-\u0026gt;lock); 9592\t/* 9593* This CPU is going to be idle and blocked load of idle CPUs 9594* need to be updated. Run the ilb locally as it is a good 9595* candidate for ilb instead of waking up another idle CPU. 9596* Kick an normal ilb if we failed to do the update. 9597*/ 9598\tif (!_nohz_idle_balance(this_rq, NOHZ_STATS_KICK, CPU_NEWLY_IDLE)) 9599\tkick_ilb(NOHZ_STATS_KICK); 9600\traw_spin_lock(\u0026amp;this_rq-\u0026gt;lock); 9601} 9602 9603#else /* !CONFIG_NO_HZ_COMMON */9604static inline void nohz_balancer_kick(struct rq *rq) { } 9605 9606static inline bool nohz_idle_balance(struct rq *this_rq, enum cpu_idle_type idle) 9607{ 9608\treturn false; 9609} 9610 9611static inline void nohz_newidle_balance(struct rq *this_rq) { } 9612#endif /* CONFIG_NO_HZ_COMMON */9613 9614/* 9615* idle_balance is called by schedule() if this_cpu is about to become 9616* idle. Attempts to pull tasks from other CPUs. 9617*/ 9618static int idle_balance(struct rq *this_rq, struct rq_flags *rf) 9619{ 9620\tunsigned long next_balance = jiffies + HZ; 9621\tint this_cpu = this_rq-\u0026gt;cpu; 9622\tstruct sched_domain *sd; 9623\tint pulled_task = 0; 9624\tu64 curr_cost = 0; 9625 9626\t/* 9627* We must set idle_stamp _before_ calling idle_balance(), such that we 9628* measure the duration of idle_balance() as idle time. 9629*/ 9630\tthis_rq-\u0026gt;idle_stamp = rq_clock(this_rq); 9631 9632\t/* 9633* Do not pull tasks towards !active CPUs... 9634*/ 9635\tif (!cpu_active(this_cpu)) 9636\treturn 0; 9637 9638\t/* 9639* This is OK, because current is on_cpu, which avoids it being picked 9640* for load-balance and preemption/IRQs are still disabled avoiding 9641* further scheduler activity on it and we\u0026#39;re being very careful to 9642* re-start the picking loop. 9643*/ 9644\trq_unpin_lock(this_rq, rf); 9645 9646\tif (this_rq-\u0026gt;avg_idle \u0026lt; sysctl_sched_migration_cost || 9647\t!this_rq-\u0026gt;rd-\u0026gt;overload) { 9648 9649\trcu_read_lock(); 9650\tsd = rcu_dereference_check_sched_domain(this_rq-\u0026gt;sd); 9651\tif (sd) 9652\tupdate_next_balance(sd, \u0026amp;next_balance); 9653\trcu_read_unlock(); 9654 9655\tnohz_newidle_balance(this_rq); 9656 9657\tgoto out; 9658\t} 9659 9660\traw_spin_unlock(\u0026amp;this_rq-\u0026gt;lock); 9661 9662\tupdate_blocked_averages(this_cpu); 9663\trcu_read_lock(); 9664\tfor_each_domain(this_cpu, sd) { 9665\tint continue_balancing = 1; 9666\tu64 t0, domain_cost; 9667 9668\tif (!(sd-\u0026gt;flags \u0026amp; SD_LOAD_BALANCE)) 9669\tcontinue; 9670 9671\tif (this_rq-\u0026gt;avg_idle \u0026lt; curr_cost + sd-\u0026gt;max_newidle_lb_cost) { 9672\tupdate_next_balance(sd, \u0026amp;next_balance); 9673\tbreak; 9674\t} 9675 9676\tif (sd-\u0026gt;flags \u0026amp; SD_BALANCE_NEWIDLE) { 9677\tt0 = sched_clock_cpu(this_cpu); 9678 9679\tpulled_task = load_balance(this_cpu, this_rq, 9680\tsd, CPU_NEWLY_IDLE, 9681\t\u0026amp;continue_balancing); 9682 9683\tdomain_cost = sched_clock_cpu(this_cpu) - t0; 9684\tif (domain_cost \u0026gt; sd-\u0026gt;max_newidle_lb_cost) 9685\tsd-\u0026gt;max_newidle_lb_cost = domain_cost; 9686 9687\tcurr_cost += domain_cost; 9688\t} 9689 9690\tupdate_next_balance(sd, \u0026amp;next_balance); 9691 9692\t/* 9693* Stop searching for tasks to pull if there are 9694* now runnable tasks on this rq. 9695*/ 9696\tif (pulled_task || this_rq-\u0026gt;nr_running \u0026gt; 0) 9697\tbreak; 9698\t} 9699\trcu_read_unlock(); 9700 9701\traw_spin_lock(\u0026amp;this_rq-\u0026gt;lock); 9702 9703\tif (curr_cost \u0026gt; this_rq-\u0026gt;max_idle_balance_cost) 9704\tthis_rq-\u0026gt;max_idle_balance_cost = curr_cost; 9705 9706out: 9707\t/* 9708* While browsing the domains, we released the rq lock, a task could 9709* have been enqueued in the meantime. Since we\u0026#39;re not going idle, 9710* pretend we pulled a task. 9711*/ 9712\tif (this_rq-\u0026gt;cfs.h_nr_running \u0026amp;\u0026amp; !pulled_task) 9713\tpulled_task = 1; 9714 9715\t/* Move the next balance forward */ 9716\tif (time_after(this_rq-\u0026gt;next_balance, next_balance)) 9717\tthis_rq-\u0026gt;next_balance = next_balance; 9718 9719\t/* Is there a task of a high priority class? */ 9720\tif (this_rq-\u0026gt;nr_running != this_rq-\u0026gt;cfs.h_nr_running) 9721\tpulled_task = -1; 9722 9723\tif (pulled_task) 9724\tthis_rq-\u0026gt;idle_stamp = 0; 9725 9726\trq_repin_lock(this_rq, rf); 9727 9728\treturn pulled_task; 9729} 9730 9731/* 9732* run_rebalance_domains is triggered when needed from the scheduler tick. 9733* Also triggered for nohz idle balancing (with nohz_balancing_kick set). 9734*/ 9735static __latent_entropy void run_rebalance_domains(struct softirq_action *h) 9736{ 9737\tstruct rq *this_rq = this_rq(); 9738\tenum cpu_idle_type idle = this_rq-\u0026gt;idle_balance ? 9739\tCPU_IDLE : CPU_NOT_IDLE; 9740 9741\t/* 9742* If this CPU has a pending nohz_balance_kick, then do the 9743* balancing on behalf of the other idle CPUs whose ticks are 9744* stopped. Do nohz_idle_balance *before* rebalance_domains to 9745* give the idle CPUs a chance to load balance. Else we may 9746* load balance only within the local sched_domain hierarchy 9747* and abort nohz_idle_balance altogether if we pull some load. 9748*/ 9749\tif (nohz_idle_balance(this_rq, idle)) 9750\treturn; 9751 9752\t/* normal load balance */ 9753\tupdate_blocked_averages(this_rq-\u0026gt;cpu); 9754\trebalance_domains(this_rq, idle); 9755} 9756 9757/* 9758* Trigger the SCHED_SOFTIRQ if it is time to do periodic load balancing. 9759*/ 9760void trigger_load_balance(struct rq *rq) 9761{ 9762\t/* Don\u0026#39;t need to rebalance while attached to NULL domain */ 9763\tif (unlikely(on_null_domain(rq))) 9764\treturn; 9765 9766\tif (time_after_eq(jiffies, rq-\u0026gt;next_balance)) 9767\traise_softirq(SCHED_SOFTIRQ); 9768 9769\tnohz_balancer_kick(rq); 9770} 9771 9772static void rq_online_fair(struct rq *rq) 9773{ 9774\tupdate_sysctl(); 9775 9776\tupdate_runtime_enabled(rq); 9777} 9778 9779static void rq_offline_fair(struct rq *rq) 9780{ 9781\tupdate_sysctl(); 9782 9783\t/* Ensure any throttled groups are reachable by pick_next_task */ 9784\tunthrottle_offline_cfs_rqs(rq); 9785} 9786 9787#endif /* CONFIG_SMP */9788 9789/* 9790* scheduler tick hitting a task of our scheduling class. 9791* 9792* NOTE: This function can be called remotely by the tick offload that 9793* goes along full dynticks. Therefore no local assumption can be made 9794* and everything must be accessed through the @rq and @curr passed in 9795* parameters. 9796*/ 9797static void task_tick_fair(struct rq *rq, struct task_struct *curr, int queued) 9798{ 9799\tstruct cfs_rq *cfs_rq; 9800\tstruct sched_entity *se = \u0026amp;curr-\u0026gt;se; 9801 9802\tfor_each_sched_entity(se) { 9803\tcfs_rq = cfs_rq_of(se); 9804\tentity_tick(cfs_rq, se, queued); 9805\t} 9806 9807\tif (static_branch_unlikely(\u0026amp;sched_numa_balancing)) 9808\ttask_tick_numa(rq, curr); 9809} 9810 9811/* 9812* called on fork with the child task as argument from the parent\u0026#39;s context 9813* - child not yet on the tasklist 9814* - preemption disabled 9815*/ 9816static void task_fork_fair(struct task_struct *p) 9817{ 9818\tstruct cfs_rq *cfs_rq; 9819\tstruct sched_entity *se = \u0026amp;p-\u0026gt;se, *curr; 9820\tstruct rq *rq = this_rq(); 9821\tstruct rq_flags rf; 9822 9823\trq_lock(rq, \u0026amp;rf); 9824\tupdate_rq_clock(rq); 9825 9826\tcfs_rq = task_cfs_rq(current); 9827\tcurr = cfs_rq-\u0026gt;curr; 9828\tif (curr) { 9829\tupdate_curr(cfs_rq); 9830\tse-\u0026gt;vruntime = curr-\u0026gt;vruntime; 9831\t} 9832\tplace_entity(cfs_rq, se, 1); 9833 9834\tif (sysctl_sched_child_runs_first \u0026amp;\u0026amp; curr \u0026amp;\u0026amp; entity_before(curr, se)) { 9835\t/* 9836* Upon rescheduling, sched_class::put_prev_task() will place 9837* \u0026#39;current\u0026#39; within the tree based on its new key value. 9838*/ 9839\tswap(curr-\u0026gt;vruntime, se-\u0026gt;vruntime); 9840\tresched_curr(rq); 9841\t} 9842 9843\tse-\u0026gt;vruntime -= cfs_rq-\u0026gt;min_vruntime; 9844\trq_unlock(rq, \u0026amp;rf); 9845} 9846 9847/* 9848* Priority of the task has changed. Check to see if we preempt 9849* the current task. 9850*/ 9851static void 9852prio_changed_fair(struct rq *rq, struct task_struct *p, int oldprio) 9853{ 9854\tif (!task_on_rq_queued(p)) 9855\treturn; 9856 9857\t/* 9858* Reschedule if we are currently running on this runqueue and 9859* our priority decreased, or if we are not currently running on 9860* this runqueue and our priority is higher than the current\u0026#39;s 9861*/ 9862\tif (rq-\u0026gt;curr == p) { 9863\tif (p-\u0026gt;prio \u0026gt; oldprio) 9864\tresched_curr(rq); 9865\t} else 9866\tcheck_preempt_curr(rq, p, 0); 9867} 9868 9869static inline bool vruntime_normalized(struct task_struct *p) 9870{ 9871\tstruct sched_entity *se = \u0026amp;p-\u0026gt;se; 9872 9873\t/* 9874* In both the TASK_ON_RQ_QUEUED and TASK_ON_RQ_MIGRATING cases, 9875* the dequeue_entity(.flags=0) will already have normalized the 9876* vruntime. 9877*/ 9878\tif (p-\u0026gt;on_rq) 9879\treturn true; 9880 9881\t/* 9882* When !on_rq, vruntime of the task has usually NOT been normalized. 9883* But there are some cases where it has already been normalized: 9884* 9885* - A forked child which is waiting for being woken up by 9886* wake_up_new_task(). 9887* - A task which has been woken up by try_to_wake_up() and 9888* waiting for actually being woken up by sched_ttwu_pending(). 9889*/ 9890\tif (!se-\u0026gt;sum_exec_runtime || 9891\t(p-\u0026gt;state == TASK_WAKING \u0026amp;\u0026amp; p-\u0026gt;sched_remote_wakeup)) 9892\treturn true; 9893 9894\treturn false; 9895} 9896 9897#ifdef CONFIG_FAIR_GROUP_SCHED 9898/* 9899* Propagate the changes of the sched_entity across the tg tree to make it 9900* visible to the root 9901*/ 9902static void propagate_entity_cfs_rq(struct sched_entity *se) 9903{ 9904\tstruct cfs_rq *cfs_rq; 9905 9906\t/* Start to propagate at parent */ 9907\tse = se-\u0026gt;parent; 9908 9909\tfor_each_sched_entity(se) { 9910\tcfs_rq = cfs_rq_of(se); 9911 9912\tif (cfs_rq_throttled(cfs_rq)) 9913\tbreak; 9914 9915\tupdate_load_avg(cfs_rq, se, UPDATE_TG); 9916\t} 9917} 9918#else 9919static void propagate_entity_cfs_rq(struct sched_entity *se) { } 9920#endif 9921 9922static void detach_entity_cfs_rq(struct sched_entity *se) 9923{ 9924\tstruct cfs_rq *cfs_rq = cfs_rq_of(se); 9925 9926\t/* Catch up with the cfs_rq and remove our load when we leave */ 9927\tupdate_load_avg(cfs_rq, se, 0); 9928\tdetach_entity_load_avg(cfs_rq, se); 9929\tupdate_tg_load_avg(cfs_rq, false); 9930\tpropagate_entity_cfs_rq(se); 9931} 9932 9933static void attach_entity_cfs_rq(struct sched_entity *se) 9934{ 9935\tstruct cfs_rq *cfs_rq = cfs_rq_of(se); 9936 9937#ifdef CONFIG_FAIR_GROUP_SCHED 9938\t/* 9939* Since the real-depth could have been changed (only FAIR 9940* class maintain depth value), reset depth properly. 9941*/ 9942\tse-\u0026gt;depth = se-\u0026gt;parent ? se-\u0026gt;parent-\u0026gt;depth + 1 : 0; 9943#endif 9944 9945\t/* Synchronize entity with its cfs_rq */ 9946\tupdate_load_avg(cfs_rq, se, sched_feat(ATTACH_AGE_LOAD) ? 0 : SKIP_AGE_LOAD); 9947\tattach_entity_load_avg(cfs_rq, se, 0); 9948\tupdate_tg_load_avg(cfs_rq, false); 9949\tpropagate_entity_cfs_rq(se); 9950} 9951 9952static void detach_task_cfs_rq(struct task_struct *p) 9953{ 9954\tstruct sched_entity *se = \u0026amp;p-\u0026gt;se; 9955\tstruct cfs_rq *cfs_rq = cfs_rq_of(se); 9956 9957\tif (!vruntime_normalized(p)) { 9958\t/* 9959* Fix up our vruntime so that the current sleep doesn\u0026#39;t 9960* cause \u0026#39;unlimited\u0026#39; sleep bonus. 9961*/ 9962\tplace_entity(cfs_rq, se, 0); 9963\tse-\u0026gt;vruntime -= cfs_rq-\u0026gt;min_vruntime; 9964\t} 9965 9966\tdetach_entity_cfs_rq(se); 9967} 9968 9969static void attach_task_cfs_rq(struct task_struct *p) 9970{ 9971\tstruct sched_entity *se = \u0026amp;p-\u0026gt;se; 9972\tstruct cfs_rq *cfs_rq = cfs_rq_of(se); 9973 9974\tattach_entity_cfs_rq(se); 9975 9976\tif (!vruntime_normalized(p)) 9977\tse-\u0026gt;vruntime += cfs_rq-\u0026gt;min_vruntime; 9978} 9979 9980static void switched_from_fair(struct rq *rq, struct task_struct *p) 9981{ 9982\tdetach_task_cfs_rq(p); 9983} 9984 9985static void switched_to_fair(struct rq *rq, struct task_struct *p) 9986{ 9987\tattach_task_cfs_rq(p); 9988 9989\tif (task_on_rq_queued(p)) { 9990\t/* 9991* We were most likely switched from sched_rt, so 9992* kick off the schedule if running, otherwise just see 9993* if we can still preempt the current task. 9994*/ 9995\tif (rq-\u0026gt;curr == p) 9996\tresched_curr(rq); 9997\telse 9998\tcheck_preempt_curr(rq, p, 0); 9999\t} 10000} 10001 10002/* Account for a task changing its policy or group. 10003* 10004* This routine is mostly called to set cfs_rq-\u0026gt;curr field when a task 10005* migrates between groups/classes. 10006*/ 10007static void set_curr_task_fair(struct rq *rq) 10008{ 10009\tstruct sched_entity *se = \u0026amp;rq-\u0026gt;curr-\u0026gt;se; 10010 10011\tfor_each_sched_entity(se) { 10012\tstruct cfs_rq *cfs_rq = cfs_rq_of(se); 10013 10014\tset_next_entity(cfs_rq, se); 10015\t/* ensure bandwidth has been allocated on our new cfs_rq */ 10016\taccount_cfs_rq_runtime(cfs_rq, 0); 10017\t} 10018} 10019 10020void init_cfs_rq(struct cfs_rq *cfs_rq) 10021{ 10022\tcfs_rq-\u0026gt;tasks_timeline = RB_ROOT_CACHED; 10023\tcfs_rq-\u0026gt;min_vruntime = (u64)(-(1LL \u0026lt;\u0026lt; 20)); 10024#ifndef CONFIG_64BIT 10025\tcfs_rq-\u0026gt;min_vruntime_copy = cfs_rq-\u0026gt;min_vruntime; 10026#endif 10027#ifdef CONFIG_SMP 10028\traw_spin_lock_init(\u0026amp;cfs_rq-\u0026gt;removed.lock); 10029#endif 10030} 10031 10032#ifdef CONFIG_FAIR_GROUP_SCHED 10033static void task_set_group_fair(struct task_struct *p) 10034{ 10035\tstruct sched_entity *se = \u0026amp;p-\u0026gt;se; 10036 10037\tset_task_rq(p, task_cpu(p)); 10038\tse-\u0026gt;depth = se-\u0026gt;parent ? se-\u0026gt;parent-\u0026gt;depth + 1 : 0; 10039} 10040 10041static void task_move_group_fair(struct task_struct *p) 10042{ 10043\tdetach_task_cfs_rq(p); 10044\tset_task_rq(p, task_cpu(p)); 10045 10046#ifdef CONFIG_SMP 10047\t/* Tell se\u0026#39;s cfs_rq has been changed -- migrated */ 10048\tp-\u0026gt;se.avg.last_update_time = 0; 10049#endif 10050\tattach_task_cfs_rq(p); 10051} 10052 10053static void task_change_group_fair(struct task_struct *p, int type) 10054{ 10055\tswitch (type) { 10056\tcase TASK_SET_GROUP: 10057\ttask_set_group_fair(p); 10058\tbreak; 10059 10060\tcase TASK_MOVE_GROUP: 10061\ttask_move_group_fair(p); 10062\tbreak; 10063\t} 10064} 10065 10066void free_fair_sched_group(struct task_group *tg) 10067{ 10068\tint i; 10069 10070\tdestroy_cfs_bandwidth(tg_cfs_bandwidth(tg)); 10071 10072\tfor_each_possible_cpu(i) { 10073\tif (tg-\u0026gt;cfs_rq) 10074\tkfree(tg-\u0026gt;cfs_rq[i]); 10075\tif (tg-\u0026gt;se) 10076\tkfree(tg-\u0026gt;se[i]); 10077\t} 10078 10079\tkfree(tg-\u0026gt;cfs_rq); 10080\tkfree(tg-\u0026gt;se); 10081} 10082 10083int alloc_fair_sched_group(struct task_group *tg, struct task_group *parent) 10084{ 10085\tstruct sched_entity *se; 10086\tstruct cfs_rq *cfs_rq; 10087\tint i; 10088 10089\ttg-\u0026gt;cfs_rq = kcalloc(nr_cpu_ids, sizeof(cfs_rq), GFP_KERNEL); 10090\tif (!tg-\u0026gt;cfs_rq) 10091\tgoto err; 10092\ttg-\u0026gt;se = kcalloc(nr_cpu_ids, sizeof(se), GFP_KERNEL); 10093\tif (!tg-\u0026gt;se) 10094\tgoto err; 10095 10096\ttg-\u0026gt;shares = NICE_0_LOAD; 10097 10098\tinit_cfs_bandwidth(tg_cfs_bandwidth(tg)); 10099 10100\tfor_each_possible_cpu(i) { 10101\tcfs_rq = kzalloc_node(sizeof(struct cfs_rq), 10102\tGFP_KERNEL, cpu_to_node(i)); 10103\tif (!cfs_rq) 10104\tgoto err; 10105 10106\tse = kzalloc_node(sizeof(struct sched_entity), 10107\tGFP_KERNEL, cpu_to_node(i)); 10108\tif (!se) 10109\tgoto err_free_rq; 10110 10111\tinit_cfs_rq(cfs_rq); 10112\tinit_tg_cfs_entry(tg, cfs_rq, se, i, parent-\u0026gt;se[i]); 10113\tinit_entity_runnable_average(se); 10114\t} 10115 10116\treturn 1; 10117 10118err_free_rq: 10119\tkfree(cfs_rq); 10120err: 10121\treturn 0; 10122} 10123 10124void online_fair_sched_group(struct task_group *tg) 10125{ 10126\tstruct sched_entity *se; 10127\tstruct rq_flags rf; 10128\tstruct rq *rq; 10129\tint i; 10130 10131\tfor_each_possible_cpu(i) { 10132\trq = cpu_rq(i); 10133\tse = tg-\u0026gt;se[i]; 10134\trq_lock_irq(rq, \u0026amp;rf); 10135\tupdate_rq_clock(rq); 10136\tattach_entity_cfs_rq(se); 10137\tsync_throttle(tg, i); 10138\trq_unlock_irq(rq, \u0026amp;rf); 10139\t} 10140} 10141 10142void unregister_fair_sched_group(struct task_group *tg) 10143{ 10144\tunsigned long flags; 10145\tstruct rq *rq; 10146\tint cpu; 10147 10148\tfor_each_possible_cpu(cpu) { 10149\tif (tg-\u0026gt;se[cpu]) 10150\tremove_entity_load_avg(tg-\u0026gt;se[cpu]); 10151 10152\t/* 10153* Only empty task groups can be destroyed; so we can speculatively 10154* check on_list without danger of it being re-added. 10155*/ 10156\tif (!tg-\u0026gt;cfs_rq[cpu]-\u0026gt;on_list) 10157\tcontinue; 10158 10159\trq = cpu_rq(cpu); 10160 10161\traw_spin_lock_irqsave(\u0026amp;rq-\u0026gt;lock, flags); 10162\tlist_del_leaf_cfs_rq(tg-\u0026gt;cfs_rq[cpu]); 10163\traw_spin_unlock_irqrestore(\u0026amp;rq-\u0026gt;lock, flags); 10164\t} 10165} 10166 10167void init_tg_cfs_entry(struct task_group *tg, struct cfs_rq *cfs_rq, 10168\tstruct sched_entity *se, int cpu, 10169\tstruct sched_entity *parent) 10170{ 10171\tstruct rq *rq = cpu_rq(cpu); 10172 10173\tcfs_rq-\u0026gt;tg = tg; 10174\tcfs_rq-\u0026gt;rq = rq; 10175\tinit_cfs_rq_runtime(cfs_rq); 10176 10177\ttg-\u0026gt;cfs_rq[cpu] = cfs_rq; 10178\ttg-\u0026gt;se[cpu] = se; 10179 10180\t/* se could be NULL for root_task_group */ 10181\tif (!se) 10182\treturn; 10183 10184\tif (!parent) { 10185\tse-\u0026gt;cfs_rq = \u0026amp;rq-\u0026gt;cfs; 10186\tse-\u0026gt;depth = 0; 10187\t} else { 10188\tse-\u0026gt;cfs_rq = parent-\u0026gt;my_q; 10189\tse-\u0026gt;depth = parent-\u0026gt;depth + 1; 10190\t} 10191 10192\tse-\u0026gt;my_q = cfs_rq; 10193\t/* guarantee group entities always have weight */ 10194\tupdate_load_set(\u0026amp;se-\u0026gt;load, NICE_0_LOAD); 10195\tse-\u0026gt;parent = parent; 10196} 10197 10198static DEFINE_MUTEX(shares_mutex); 10199 10200int sched_group_set_shares(struct task_group *tg, unsigned long shares) 10201{ 10202\tint i; 10203 10204\t/* 10205* We can\u0026#39;t change the weight of the root cgroup. 10206*/ 10207\tif (!tg-\u0026gt;se[0]) 10208\treturn -EINVAL; 10209 10210\tshares = clamp(shares, scale_load(MIN_SHARES), scale_load(MAX_SHARES)); 10211 10212\tmutex_lock(\u0026amp;shares_mutex); 10213\tif (tg-\u0026gt;shares == shares) 10214\tgoto done; 10215 10216\ttg-\u0026gt;shares = shares; 10217\tfor_each_possible_cpu(i) { 10218\tstruct rq *rq = cpu_rq(i); 10219\tstruct sched_entity *se = tg-\u0026gt;se[i]; 10220\tstruct rq_flags rf; 10221 10222\t/* Propagate contribution to hierarchy */ 10223\trq_lock_irqsave(rq, \u0026amp;rf); 10224\tupdate_rq_clock(rq); 10225\tfor_each_sched_entity(se) { 10226\tupdate_load_avg(cfs_rq_of(se), se, UPDATE_TG); 10227\tupdate_cfs_group(se); 10228\t} 10229\trq_unlock_irqrestore(rq, \u0026amp;rf); 10230\t} 10231 10232done: 10233\tmutex_unlock(\u0026amp;shares_mutex); 10234\treturn 0; 10235} 10236#else /* CONFIG_FAIR_GROUP_SCHED */10237 10238void free_fair_sched_group(struct task_group *tg) { } 10239 10240int alloc_fair_sched_group(struct task_group *tg, struct task_group *parent) 10241{ 10242\treturn 1; 10243} 10244 10245void online_fair_sched_group(struct task_group *tg) { } 10246 10247void unregister_fair_sched_group(struct task_group *tg) { } 10248 10249#endif /* CONFIG_FAIR_GROUP_SCHED */10250 10251 10252static unsigned int get_rr_interval_fair(struct rq *rq, struct task_struct *task) 10253{ 10254\tstruct sched_entity *se = \u0026amp;task-\u0026gt;se; 10255\tunsigned int rr_interval = 0; 10256 10257\t/* 10258* Time slice is 0 for SCHED_OTHER tasks that are on an otherwise 10259* idle runqueue: 10260*/ 10261\tif (rq-\u0026gt;cfs.load.weight) 10262\trr_interval = NS_TO_JIFFIES(sched_slice(cfs_rq_of(se), se)); 10263 10264\treturn rr_interval; 10265} 10266 10267/* 10268* All the scheduling class methods: 10269*/ 10270const struct sched_class fair_sched_class = { 10271\t.next\t= \u0026amp;idle_sched_class, 10272\t.enqueue_task\t= enqueue_task_fair, 10273\t.dequeue_task\t= dequeue_task_fair, 10274\t.yield_task\t= yield_task_fair, 10275\t.yield_to_task\t= yield_to_task_fair, 10276 10277\t.check_preempt_curr\t= check_preempt_wakeup, 10278 10279\t.pick_next_task\t= pick_next_task_fair, 10280\t.put_prev_task\t= put_prev_task_fair, 10281 10282#ifdef CONFIG_SMP 10283\t.select_task_rq\t= select_task_rq_fair, 10284\t.migrate_task_rq\t= migrate_task_rq_fair, 10285 10286\t.rq_online\t= rq_online_fair, 10287\t.rq_offline\t= rq_offline_fair, 10288 10289\t.task_dead\t= task_dead_fair, 10290\t.set_cpus_allowed\t= set_cpus_allowed_common, 10291#endif 10292 10293\t.set_curr_task = set_curr_task_fair, 10294\t.task_tick\t= task_tick_fair, 10295\t.task_fork\t= task_fork_fair, 10296 10297\t.prio_changed\t= prio_changed_fair, 10298\t.switched_from\t= switched_from_fair, 10299\t.switched_to\t= switched_to_fair, 10300 10301\t.get_rr_interval\t= get_rr_interval_fair, 10302 10303\t.update_curr\t= update_curr_fair, 10304 10305#ifdef CONFIG_FAIR_GROUP_SCHED 10306\t.task_change_group\t= task_change_group_fair, 10307#endif 10308}; 10309 10310#ifdef CONFIG_SCHED_DEBUG 10311void print_cfs_stats(struct seq_file *m, int cpu) 10312{ 10313\tstruct cfs_rq *cfs_rq, *pos; 10314 10315\trcu_read_lock(); 10316\tfor_each_leaf_cfs_rq_safe(cpu_rq(cpu), cfs_rq, pos) 10317\tprint_cfs_rq(m, cpu, cfs_rq); 10318\trcu_read_unlock(); 10319} 10320 10321#ifdef CONFIG_NUMA_BALANCING 10322void show_numa_stats(struct task_struct *p, struct seq_file *m) 10323{ 10324\tint node; 10325\tunsigned long tsf = 0, tpf = 0, gsf = 0, gpf = 0; 10326\tstruct numa_group *ng; 10327 10328\trcu_read_lock(); 10329\tng = rcu_dereference(p-\u0026gt;numa_group); 10330\tfor_each_online_node(node) { 10331\tif (p-\u0026gt;numa_faults) { 10332\ttsf = p-\u0026gt;numa_faults[task_faults_idx(NUMA_MEM, node, 0)]; 10333\ttpf = p-\u0026gt;numa_faults[task_faults_idx(NUMA_MEM, node, 1)]; 10334\t} 10335\tif (ng) { 10336\tgsf = ng-\u0026gt;faults[task_faults_idx(NUMA_MEM, node, 0)], 10337\tgpf = ng-\u0026gt;faults[task_faults_idx(NUMA_MEM, node, 1)]; 10338\t} 10339\tprint_numa_stats(m, node, tsf, tpf, gsf, gpf); 10340\t} 10341\trcu_read_unlock(); 10342} 10343#endif /* CONFIG_NUMA_BALANCING */10344#endif /* CONFIG_SCHED_DEBUG */10345 10346__init void init_sched_fair_class(void) 10347{ 10348#ifdef CONFIG_SMP 10349\topen_softirq(SCHED_SOFTIRQ, run_rebalance_domains); 10350 10351#ifdef CONFIG_NO_HZ_COMMON 10352\tnohz.next_balance = jiffies; 10353\tnohz.next_blocked = jiffies; 10354\tzalloc_cpumask_var(\u0026amp;nohz.idle_cpus_mask, GFP_NOWAIT); 10355#endif 10356#endif /* SMP */10357 10358} ","date":"Oct 28, 2021","img":"","permalink":"https://mengdemao.github.io/posts/fair/","series":null,"tags":["kernel"],"title":"Fair"},{"categories":["python"],"content":"python基础绘图\n","date":"Oct 16, 2021","img":"","permalink":"https://mengdemao.github.io/posts/tkinter/","series":null,"tags":[],"title":"Tkinter"},{"categories":[],"content":"","date":"Oct 7, 2021","img":"","permalink":"https://mengdemao.github.io/posts/javascript/","series":null,"tags":[],"title":"Javascript"},{"categories":[],"content":"CSS开始(层叠样式表) HTML + CSS + JavaScript 名词 + 形容词 + 动词 相当于对原始的HTML进行美化\n快速入门  CSS是什么 CSS怎么用 CSS选择器 美化网页 盒子模型 浮动 定位 网页动画  什么是CSS 美化:字体, 颜色,高度,宽度, 背景图片\nCSS的优势:  内容和表现分离 CSS文件可以复用 样式十分丰富 建议使用独立的CSS文件  CSS导入的方法  行内样式  1\u0026lt;h1 style=\u0026#34;color: red\u0026#34;\u0026gt;一级标题\u0026lt;/h1\u0026gt; style标签  1\u0026lt;style\u0026gt;\u0026lt;/style\u0026gt; 外部样式   链接方式  1\u0026lt;link rel=\u0026#34;stylesheet\u0026#34; href=\u0026#34;style.css\u0026#34;\u0026gt;  导入式  1\u0026lt;style\u0026gt; 2\t@import url(\u0026#34;css/style.css\u0026#34;); 3\u0026lt;/style\u0026gt; 基本语法 1/* 注释语法 */ 2selector { 3\t/* 声明 */ 4\tattr:value; 5} 选择器 基本选择器   标签选择器\n  类选择器\n  ID选择器\n  标签选择器 1h1 { 2 color: red; 3} 4h2 { 5 color: black; 6} 7h3 { 8 color: yellow; 9} 10h4 { 11 color: red; 12} 类选择器 1\u0026lt;h1 class=\u0026#34;test\u0026#34;\u0026gt;测试\u0026lt;/h1\u0026gt; 此时,可以讲HTML选中\n1.test { 2 color: black; 3} ID选择器 1\u0026lt;h1 id=\u0026#34;test\u0026#34;\u0026gt;测试\u0026lt;/h1\u0026gt; 1#test { 2\tcolor: black; 3} ID唯一确定,不可以共享;\n层次选择器 ","date":"Oct 7, 2021","img":"","permalink":"https://mengdemao.github.io/posts/css/","series":null,"tags":[],"title":"Css"},{"categories":[],"content":"开始 网页基础结构\n1\u0026lt;!-- 告诉浏览器,需要使用的规范 --\u0026gt; 2\u0026lt;!DOCTYPE html\u0026gt; 3\u0026lt;html lang=\u0026#34;en\u0026#34;\u0026gt; 4 5\u0026lt;!-- 网页标题 --\u0026gt; 6\u0026lt;head\u0026gt; 7 \u0026lt;!-- 描述标签 --\u0026gt; 8 \u0026lt;meta charset=\u0026#34;UTF-8\u0026#34;\u0026gt; 9 10 \u0026lt;!-- 网页标题 --\u0026gt; 11 \u0026lt;title\u0026gt;Document\u0026lt;/title\u0026gt; 12\u0026lt;/head\u0026gt; 13 14\u0026lt;!-- 网页主体 --\u0026gt; 15\u0026lt;body\u0026gt; 16\u0026lt;/body\u0026gt; 17\u0026lt;/html\u0026gt; 网页基本标签 标题标签 1\u0026lt;h1\u0026gt;一级标签\u0026lt;/h1\u0026gt; 2\u0026lt;h2\u0026gt;二级标签\u0026lt;/h2\u0026gt; 3\u0026lt;h3\u0026gt;三级标签\u0026lt;/h3\u0026gt; 4\u0026lt;h4\u0026gt;四级标签\u0026lt;/h4\u0026gt; 5\u0026lt;h5\u0026gt;五级标签\u0026lt;/h5\u0026gt; 6\u0026lt;h6\u0026gt;六级标签\u0026lt;/h6\u0026gt; 段落标签 1\u0026lt;p\u0026gt;段落标签\u0026lt;/p\u0026gt; 换行标签 1\u0026lt;br/\u0026gt; 水平线标签 1\u0026lt;hr/\u0026gt; 字体样式标签 1\u0026lt;!-- 字体样式标签 --\u0026gt; 2\u0026lt;strong\u0026gt;粗体\u0026lt;/strong\u0026gt;\u0026lt;br/\u0026gt; 3\u0026lt;em\u0026gt;斜体\u0026lt;/em\u0026gt;\u0026lt;br/\u0026gt; 图片标签 1\u0026lt;img src=\u0026#34;测试.png\u0026#34; alt=\u0026#34;测试\u0026#34; title=\u0026#34;测试\u0026#34;/\u0026gt; 链接 1\u0026lt;!-- 当前页打开 --\u0026gt; 2\u0026lt;a href=\u0026#34;http://www.baidu.com\u0026#34; target=\u0026#34;_self\u0026#34;\u0026gt;百度一下\u0026lt;/a\u0026gt;\u0026lt;br/\u0026gt; 3\u0026lt;!-- 新建页打开 --\u0026gt; 4\u0026lt;a href=\u0026#34;http://www.baidu.com\u0026#34; target=\u0026#34;_blank\u0026#34;\u0026gt;百度一下\u0026lt;/a\u0026gt;\u0026lt;br/\u0026gt; 行内元素和块元素 列表标签 有序列表 1\u0026lt;ol\u0026gt; 2 \u0026lt;li\u0026gt;HTML\u0026lt;/li\u0026gt; 3 \u0026lt;li\u0026gt;CSS\u0026lt;/li\u0026gt; 4 \u0026lt;li\u0026gt;JavaScript\u0026lt;/li\u0026gt; 5\u0026lt;/ol\u0026gt; 无序列表 1\u0026lt;ul\u0026gt; 2 \u0026lt;li\u0026gt;HTML\u0026lt;/li\u0026gt; 3 \u0026lt;li\u0026gt;CSS\u0026lt;/li\u0026gt; 4 \u0026lt;li\u0026gt;JavaScript\u0026lt;/li\u0026gt; 5\u0026lt;/ul\u0026gt; 定义列表 1\u0026lt;dl\u0026gt; 2 \u0026lt;dt\u0026gt;前端\u0026lt;/dt\u0026gt; 3 \u0026lt;dd\u0026gt;html\u0026lt;/dd\u0026gt; 4 \u0026lt;dd\u0026gt;CSS\u0026lt;/dd\u0026gt; 5 \u0026lt;dd\u0026gt;JavaScript\u0026lt;/dd\u0026gt; 6\u0026lt;/dl\u0026gt; 表格 1\u0026lt;table border=\u0026#34;1px\u0026#34;\u0026gt; 2\t\u0026lt;tr\u0026gt; 3\t\u0026lt;td\u0026gt;1-1\u0026lt;/td\u0026gt; 4\t\u0026lt;td\u0026gt;1-2\u0026lt;/td\u0026gt; 5\t\u0026lt;/tr\u0026gt; 6\t\u0026lt;tr\u0026gt; 7\t\u0026lt;td\u0026gt;2-1\u0026lt;/td\u0026gt; 8\t\u0026lt;td\u0026gt;2-2\u0026lt;/td\u0026gt; 9\t\u0026lt;/tr\u0026gt; 10\u0026lt;/table\u0026gt; 页面结构分析    元素名 描述     header 标题头部区域   footer 标记尾部内容   section web页面中一块独立的区域   article 独立文章内容   aside 相关页面或者内容   nav 导航类辅助内容    iframe内联框架 1\u0026lt;iframe src=\u0026#34;path\u0026#34; name=\u0026#34;mainFrame\u0026#34;\u0026gt;\u0026lt;/frame\u0026gt; bilibili的例子\n1\u0026lt;iframe src=\u0026#34;//player.bilibili.com/player.html?aid=55631961\u0026amp;bvid=BV1x4411V75C\u0026amp;cid=97257967\u0026amp;page=11\u0026#34; scrolling=\u0026#34;no\u0026#34; border=\u0026#34;0\u0026#34; frameborder=\u0026#34;no\u0026#34; framespacing=\u0026#34;0\u0026#34; allowfullscreen=\u0026#34;true\u0026#34;\u0026gt; \u0026lt;/iframe\u0026gt; 表单 表单form\n1\u0026lt;form action=\u0026#34;开始.html\u0026#34; method=\u0026#34;GET/POST\u0026#34;\u0026gt; 2 \u0026lt;p\u0026gt;名字: \u0026lt;input type=\u0026#34;text\u0026#34; name=\u0026#34;name\u0026#34;\u0026gt;\u0026lt;/p\u0026gt; 3 \u0026lt;p\u0026gt;密码: \u0026lt;input type=\u0026#34;password\u0026#34; name=\u0026#34;password\u0026#34;\u0026gt;\u0026lt;/p\u0026gt; 4 \u0026lt;p\u0026gt; 5 \u0026lt;input type=\u0026#34;submit\u0026#34;\u0026gt; 6 \u0026lt;input type=\u0026#34;reset\u0026#34;\u0026gt; 7 \u0026lt;/p\u0026gt; 8\u0026lt;/form\u0026gt; 产生的效果\n1?name=111\u0026amp;password= ","date":"Oct 7, 2021","img":"","permalink":"https://mengdemao.github.io/posts/html5/","series":null,"tags":[],"title":"Html5"},{"categories":[],"content":" A C dynamic strings library C语言版本动态字符串库\n SDS SDS的类型就是\n1typedef char *sds; 可以明显的看到,sds就是普通的char类型\n下面是sds的数据类型 1+--------+-------------------------------+-----------+ 2| Header | Binary safe C alike string... | Null term | 3+--------+-------------------------------+-----------+ 4 | 5 -\u0026gt; Pointer returned to the user. 1#define SDS_HDR_VAR(T,s) \\ 2struct sdshdr##T *sh = (void*)((s)-(sizeof(struct sdshdr##T))); 3#define SDS_HDR(T,s) \\ 4((struct sdshdr##T *)((s)-(sizeof(struct sdshdr##T)))) 5#define SDS_TYPE_5_LEN(f) ((f)\u0026gt;\u0026gt;SDS_TYPE_BITS) SDS 头 根据不同的标志计算不同的头部数据\n   宏定义 标志     SDS_TYPE_5 sdshdr5   SDS_TYPE_8 sdshdr8   SDS_TYPE_16 sdshdr16   SDS_TYPE_32 sdshdr32   SDS_TYPE_64 sdshdr64    flag标志:\n1unsigned char flags = s[-1]; /* 最后一个头部数据 */ 1#define SDS_TYPE_5 0 2#define SDS_TYPE_8 1 3#define SDS_TYPE_16 2 4#define SDS_TYPE_32 3 5#define SDS_TYPE_64 4 1/* Note: sdshdr5 is never used, we just access the flags byte directly. 2* However is here to document the layout of type 5 SDS strings. */ 3struct __attribute__ ((__packed__)) sdshdr5 { 4 unsigned char flags; /* 3 lsb of type, and 5 msb of string length */ 5 char buf[]; 6}; 7struct __attribute__ ((__packed__)) sdshdr8 { 8 uint8_t len; /* used */ 9 uint8_t alloc; /* excluding the header and null terminator */ 10 unsigned char flags; /* 3 lsb of type, 5 unused bits */ 11 char buf[]; 12}; 13struct __attribute__ ((__packed__)) sdshdr16 { 14 uint16_t len; /* used */ 15 uint16_t alloc; /* excluding the header and null terminator */ 16 unsigned char flags; /* 3 lsb of type, 5 unused bits */ 17 char buf[]; 18}; 19struct __attribute__ ((__packed__)) sdshdr32 { 20 uint32_t len; /* used */ 21 uint32_t alloc; /* excluding the header and null terminator */ 22 unsigned char flags; /* 3 lsb of type, 5 unused bits */ 23 char buf[]; 24}; 25struct __attribute__ ((__packed__)) sdshdr64 { 26 uint64_t len; /* used */ 27 uint64_t alloc; /* excluding the header and null terminator */ 28 unsigned char flags; /* 3 lsb of type, 5 unused bits */ 29 char buf[]; 30}; 1#define SDS_TYPE_MASK 7 2#define SDS_TYPE_BITS 3 创建SDS 函数原型\n1sds sdsnewlen(const void *init, size_t initlen); 扩张字符串缓存区 1sds sdsMakeRoomFor(sds s, size_t addlen) 2{ 3 void *sh; 4 void *newsh; 5 size_t avail = sdsavail(s);\t/* 计算剩余的可以使用的大小 */ 6 size_t len; 7 size_t newlen; 8 char type, oldtype = s[-1] \u0026amp; SDS_TYPE_MASK; 9 int hdrlen; 10 11 if (avail \u0026gt;= addlen) { /* 如果剩余的存储空间超过添加大小,那么就可以直接返回 */ 12 return s; 13 } 14 len = sdslen(s);\t/* 计算字符串大小 */ 15 sh = (char*)s - sdsHdrSize(oldtype); /* 缓冲区地址 */ 16 17 /* 计算得到新的长度 */ 18 newlen = (len+addlen); 19 if (newlen \u0026lt; SDS_MAX_PREALLOC) 20 newlen *= 2; 21 else 22 newlen += SDS_MAX_PREALLOC; 23\t/* 重新生成类型 */ 24 type = sdsReqType(newlen); 25 26 /* Don\u0026#39;t use type 5: the user is appending to the string and type 5 is 27* not able to remember empty space, so sdsMakeRoomFor() must be called 28* at every appending operation. */ 29 if (type == SDS_TYPE_5) { 30 type = SDS_TYPE_8; 31\t} 32\t33 /* 计算头部大小 */ 34 hdrlen = sdsHdrSize(type); 35 36 if (oldtype == type) { 37 newsh = s_realloc(sh, hdrlen + newlen + 1); 38 if (newsh == NULL) { 39 return NULL; 40 } 41 s = (char*)newsh + hdrlen; 42 } else { 43 /* Since the header size changes, need to move the string forward, 44* and can\u0026#39;t use realloc */ 45 newsh = s_malloc(hdrlen+newlen+1); 46 if (newsh == NULL) { 47 return NULL; 48 } 49 memcpy((char*)newsh+hdrlen, s, len+1); 50 s_free(sh); 51 52 s = (char*)newsh + hdrlen; 53 s[-1] = type; 54 55 sdssetlen(s, len); 56 } 57 58 sdssetalloc(s, newlen); 59 return s; 60} 追加字符串 1sds sdscatlen(sds s, const void *t, size_t len) 2{ 3 size_t curlen = sdslen(s);\t/* 计算字符串的长度 */ 4 5 s = sdsMakeRoomFor(s,len);\t/* 扩展字符串缓冲区长度 */ 6 if (s == NULL) { 7 return NULL; 8 } 9 memcpy(s+curlen, t, len);\t/* 添加字符串 */ 10 sdssetlen(s, curlen+len);\t/* 设置长度标志 */ 11 s[curlen+len] = \u0026#39;\\0\u0026#39;;\t/* 补全结束符 */ 12 return s; 13} ","date":"Oct 6, 2021","img":"","permalink":"https://mengdemao.github.io/posts/sds/","series":null,"tags":[],"title":"Sds"},{"categories":[],"content":" STL称为标准模板库(Standard Template Library) 广义上可以分为容器,算法,迭代器 容器和算法通过迭代器进行无缝连接 STL几乎所有的代码都采用了函数模版或者类模板\n STL组件    序号 名称 解释     1 容器 各种数据结构   2 算法 各种常用的算法   3 迭代器 容器域算法的胶合   4 仿函数 行为类似函数   5 适配器 修饰容器或者仿函数迭代器   6 空间配置器 负责空间的配置和管理    容器算法和迭代器 vector vector使用 1/* 创建vector容器 */ 2vector\u0026lt;int\u0026gt; v; 3/* 插入数据 */ 4v.push_back(10); 5v.push_back(20); 6v.push_back(30); 7v.push_back(40); 迭代器使用 迭代器方案1 1vector\u0026lt;int\u0026gt;::iterator itBegin = v.begin(); 2vector\u0026lt;int\u0026gt;::iterator itEnd = v.end(); 3while (itBegin != itEnd) { 4\tcout \u0026lt;\u0026lt; *itBegin \u0026lt;\u0026lt; endl; 5\titBegin += 1; 6} 迭代器2 1for (vector\u0026lt;int\u0026gt;::iterator it = v.begin(); it != v.end(); it++) 2{ 3\tcout \u0026lt;\u0026lt; *it \u0026lt;\u0026lt; endl; 4} 遍历算法 1template \u0026lt;class T\u0026gt; 2void myPrint(T val) 3{ 4\tcout \u0026lt;\u0026lt; val \u0026lt;\u0026lt; endl; 5} 6 7/* 可惜回调函数不支持自动推导 */ 8for_each(v.begin(), v.end(), myPrint\u0026lt;int\u0026gt;); 容器自定义数据 容器嵌套容器 1vector\u0026lt;vector\u0026lt;int\u0026gt;\u0026gt;v; // 外部大容器 2vector\u0026lt;int\u0026gt; vx[10]; // 内部小容器 3 4/* 插入容器 */ 5for (int i = 0; i \u0026lt; 10; i++) 6{ 7\tfor (int j = 0; j \u0026lt; 30; j++) 8\t{ 9\tvx[i].push_back(i + j + 10); 10\t} 11\tv.push_back(vx[i]); 12} 13 14/* 遍历容器 */ 15for (vector\u0026lt;vector\u0026lt;int\u0026gt;\u0026gt;::iterator it = v.begin(); it != v.end(); it++) 16{ 17\tfor (vector\u0026lt;int\u0026gt;::iterator vit = it-\u0026gt;begin(); vit != it-\u0026gt;end(); vit++) 18\t{ 19\tcout \u0026lt;\u0026lt; *vit \u0026lt;\u0026lt; \u0026#34; \u0026#34;; 20\t} 21\tcout \u0026lt;\u0026lt; endl; 22} string string本质上是一个类,封装了char*,提供了许多的成员方法;\n构造函数 1string s1(str); 2string s2 = \u0026#34;Hello World\u0026#34;; 3string s3(s2); 赋值操作  重载操作符**=**  1string s1; 2s1 = \u0026#34;Hello World\u0026#34;; 成员函数assign  1string str; 2str.assign(\u0026#34;Hello World\u0026#34;); 追加操作  重载操作符**+=** 成员函数append  查找和替换 find replace 比较 compare 字符存取  [] at  插入和删除 insert earse 子串 substr array deque hashtable map list queue stack set rbtree ","date":"Oct 6, 2021","img":"","permalink":"https://mengdemao.github.io/posts/stl/","series":null,"tags":[],"title":"STL学习笔记"},{"categories":[],"content":"HelloWorld 1#!/bin/python3 2 3if __name__ == \u0026#39;__main__\u0026#39;: 4 print(\u0026#39;Hello World\u0026#39;) 数据类型 Numbers(数字) 1intA = 10 2print(intA) 布尔类型 1true 2false String(字符串) 1strB = \u0026#34;Hello\u0026#34; 2print(strB) List(列表) 1listC = [\u0026#34;12\u0026#34;, 3, 4] 2print(listC) Tuple(元组) 1tupleD = (\u0026#39;physics\u0026#39;, \u0026#39;chemistry\u0026#39;, 1997, 2000) 2print(tupleD) Dictionary(字典) 1DictE = {\u0026#39;a\u0026#39;: 1, \u0026#39;b\u0026#39;: 2, \u0026#39;b\u0026#39;: \u0026#39;3\u0026#39;} 2print(DictE) 运算符 控制结构 条件语句 单执行语句 1if 判断条件： 2\t执行语句 3else： 4\t执行语句 多条件语句 1if 判断条件1: 2 执行语句1…… 3elif 判断条件2: 4 执行语句2…… 5elif 判断条件3: 6\t执行语句3…… 7else: 8 执行语句4…… while循环 1c = 0 2while (c \u0026lt; 10): 3 print(c) 4 c += 1 5print(\u0026#34;while Loop finish\u0026#34;) ###　for循环\n函数 面向对象 ","date":"Oct 5, 2021","img":"","permalink":"https://mengdemao.github.io/posts/python/","series":null,"tags":[],"title":"Python学习笔记"},{"categories":[],"content":"输入子设备分为三层\n handle core device  input的相关结构体 1 struct input_dev {\t/* 输入设备的描述 */ 2 const char *name;\t/* 设备名称 */ 3\tconst char *phys; 4\tconst char *uniq; 5\tstruct input_id id; 6 7\tunsigned long propbit[BITS_TO_LONGS(INPUT_PROP_CNT)]; 8 9\tunsigned long evbit[BITS_TO_LONGS(EV_CNT)]; 10\tunsigned long keybit[BITS_TO_LONGS(KEY_CNT)]; 11\tunsigned long relbit[BITS_TO_LONGS(REL_CNT)]; 12\tunsigned long absbit[BITS_TO_LONGS(ABS_CNT)]; 13\tunsigned long mscbit[BITS_TO_LONGS(MSC_CNT)]; 14\tunsigned long ledbit[BITS_TO_LONGS(LED_CNT)]; 15\tunsigned long sndbit[BITS_TO_LONGS(SND_CNT)]; 16\tunsigned long ffbit[BITS_TO_LONGS(FF_CNT)]; 17\tunsigned long swbit[BITS_TO_LONGS(SW_CNT)]; 18 19\tunsigned int hint_events_per_packet; 20 21\tunsigned int keycodemax; 22\tunsigned int keycodesize; 23\tvoid *keycode; 24 25\tint (*setkeycode)(struct input_dev *dev, 26\tconst struct input_keymap_entry *ke, 27\tunsigned int *old_keycode); 28\tint (*getkeycode)(struct input_dev *dev, 29\tstruct input_keymap_entry *ke); 30 31\tstruct ff_device *ff; 32 33\tunsigned int repeat_key; 34\tstruct timer_list timer; 35 36\tint rep[REP_CNT]; 37 38\tstruct input_mt *mt; 39 40\tstruct input_absinfo *absinfo; 41 42\tunsigned long key[BITS_TO_LONGS(KEY_CNT)]; 43\tunsigned long led[BITS_TO_LONGS(LED_CNT)]; 44\tunsigned long snd[BITS_TO_LONGS(SND_CNT)]; 45\tunsigned long sw[BITS_TO_LONGS(SW_CNT)]; 46 47\tint (*open)(struct input_dev *dev); 48\tvoid (*close)(struct input_dev *dev); 49\tint (*flush)(struct input_dev *dev, struct file *file); 50\tint (*event)(struct input_dev *dev, unsigned int type, unsigned int code, int value); 51 52\tstruct input_handle __rcu *grab; 53 54\tspinlock_t event_lock; 55\tstruct mutex mutex; 56 57\tunsigned int users; 58\tbool going_away; 59 60\tstruct device dev; 61 62\tstruct list_head\th_list; 63\tstruct list_head\tnode; 64 65\tunsigned int num_vals; 66\tunsigned int max_vals; 67\tstruct input_value *vals; 68 69\tbool devres_managed; 70 }; 71#define to_input_dev(d) container_of(d, struct input_dev, dev) input子系统使用 input子系统分析  Makefile编写  1obj-$(CONFIG_INPUT)\t+= input-core.o 2input-core-y := input.o input-compat.o input-mt.o ff-core.o 开始判断下面的第一个文件 input.c  1subsys_initcall(input_init); 2module_exit(input_exit); 输入子系统的设备号\n1#define INPUT_MAJOR 13 安装驱动\n1 static int __init input_init(void) 2 { 3\tint err; 4\t/* 注册设备类 */ 5\terr = class_register(\u0026amp;input_class); 6\tif (err) { 7\tpr_err(\u0026#34;unable to register input_dev class\\n\u0026#34;); 8\treturn err; 9\t} 10 11\t/* 注册proc文件系统 */ 12\terr = input_proc_init(); 13\tif (err) 14\tgoto fail1; 15\t/* 注册设备 */ 16\terr = register_chrdev_region(MKDEV(INPUT_MAJOR, 0), 17\tINPUT_MAX_CHAR_DEVICES, \u0026#34;input\u0026#34;); 18\tif (err) { 19\tpr_err(\u0026#34;unable to register char major %d\u0026#34;, INPUT_MAJOR); 20\tgoto fail2; 21\t} 22 23\treturn 0; 24 25 fail2:\tinput_proc_exit(); 26 fail1:\tclass_unregister(\u0026amp;input_class); 27\treturn err; 28 } 卸载驱动\n1 static void __exit input_exit(void) 2 { 3\t/* 卸载proc文件系统 */ 4\tinput_proc_exit(); 5 6\t/* 注销设备号 */ 7\tunregister_chrdev_region(MKDEV(INPUT_MAJOR, 0), 8\tINPUT_MAX_CHAR_DEVICES); 9 10\t/* 注销CLass */ 11\tclass_unregister(\u0026amp;input_class); 12 } 设备类操作\n1\t/* 设备类型 */ 2\tstruct class input_class = { 3\t.name\t= \u0026#34;input\u0026#34;, 4\t.devnode\t= input_devnode, 5\t}; 6\tEXPORT_SYMBOL_GPL(input_class); 7 8\t/* 注册设备 */ 9\terr = class_register(\u0026amp;input_class); 10\tif (err) { 11\tpr_err(\u0026#34;unable to register input_dev class\\n\u0026#34;); 12\treturn err; 13\t} 14 15\t/* 卸载设备 */ 16 class_unregister(\u0026amp;input_class); Proc文件系统操作\nProc文件系统添加\n1static int __init input_proc_init(void) 2{ 3\tstruct proc_dir_entry *entry; 4 5\tproc_bus_input_dir = proc_mkdir(\u0026#34;bus/input\u0026#34;, NULL); 6\tif (!proc_bus_input_dir) 7\treturn -ENOMEM; 8 9\tentry = proc_create(\u0026#34;devices\u0026#34;, 0, proc_bus_input_dir, 10\t\u0026amp;input_devices_fileops); 11\tif (!entry) 12\tgoto fail1; 13 14\tentry = proc_create(\u0026#34;handlers\u0026#34;, 0, proc_bus_input_dir, 15\t\u0026amp;input_handlers_fileops); 16\tif (!entry) 17\tgoto fail2; 18 19\treturn 0; 20 21 fail2:\tremove_proc_entry(\u0026#34;devices\u0026#34;, proc_bus_input_dir); 22 fail1: remove_proc_entry(\u0026#34;bus/input\u0026#34;, NULL); 23\treturn -ENOMEM; 24} Proc文件系统卸载\n1static void input_proc_exit(void) 2{ 3\tremove_proc_entry(\u0026#34;devices\u0026#34;, proc_bus_input_dir); 4\tremove_proc_entry(\u0026#34;handlers\u0026#34;, proc_bus_input_dir); 5\tremove_proc_entry(\u0026#34;bus/input\u0026#34;, NULL); 6} 接口部分 Handler操作 1/** 2* 注册 input handler 3* input_register_handler - register a new input handler 4* @handler: handler to be registered 5* 6* This function registers a new input handler (interface) for input 7* devices in the system and attaches it to all input devices that 8* are compatible with the handler. 9*/ 10int input_register_handler(struct input_handler *handler) 11{ 12\tstruct input_dev *dev; 13\tint error; 14 15\terror = mutex_lock_interruptible(\u0026amp;input_mutex); 16\tif (error) 17\treturn error; 18 19\tINIT_LIST_HEAD(\u0026amp;handler-\u0026gt;h_list); 20 21\tlist_add_tail(\u0026amp;handler-\u0026gt;node, \u0026amp;input_handler_list); 22 23\tlist_for_each_entry(dev, \u0026amp;input_dev_list, node) 24\tinput_attach_handler(dev, handler); 25 26\tinput_wakeup_procfs_readers(); 27 28\tmutex_unlock(\u0026amp;input_mutex); 29\treturn 0; 30} 31EXPORT_SYMBOL(input_register_handler); 32 33/** 34* 解除注册 input handler 35* input_unregister_handler - unregisters an input handler 36* @handler: handler to be unregistered 37* 38* This function disconnects a handler from its input devices and 39* removes it from lists of known handlers. 40*/ 41void input_unregister_handler(struct input_handler *handler) 42{ 43\tstruct input_handle *handle, *next; 44 45\tmutex_lock(\u0026amp;input_mutex); 46 47\tlist_for_each_entry_safe(handle, next, \u0026amp;handler-\u0026gt;h_list, h_node) 48\thandler-\u0026gt;disconnect(handle); 49\tWARN_ON(!list_empty(\u0026amp;handler-\u0026gt;h_list)); 50 51\tlist_del_init(\u0026amp;handler-\u0026gt;node); 52 53\tinput_wakeup_procfs_readers(); 54 55\tmutex_unlock(\u0026amp;input_mutex); 56} 57EXPORT_SYMBOL(input_unregister_handler); 注册设备 1/** 2* 注册一个设备 3* input_register_device - register device with input core 4* @dev: device to be registered 5* 6* This function registers device with input core. The device must be 7* allocated with input_allocate_device() and all it\u0026#39;s capabilities 8* set up before registering. 9* If function fails the device must be freed with input_free_device(). 10* Once device has been successfully registered it can be unregistered 11* with input_unregister_device(); input_free_device() should not be 12* called in this case. 13* 14* Note that this function is also used to register managed input devices 15* (ones allocated with devm_input_allocate_device()). Such managed input 16* devices need not be explicitly unregistered or freed, their tear down 17* is controlled by the devres infrastructure. It is also worth noting 18* that tear down of managed input devices is internally a 2-step process: 19* registered managed input device is first unregistered, but stays in 20* memory and can still handle input_event() calls (although events will 21* not be delivered anywhere). The freeing of managed input device will 22* happen later, when devres stack is unwound to the point where device 23* allocation was made. 24*/ 25int input_register_device(struct input_dev *dev) 26{ 27\tstruct input_devres *devres = NULL; 28\tstruct input_handler *handler; 29\tunsigned int packet_size; 30\tconst char *path; 31\tint error; 32 33\tif (dev-\u0026gt;devres_managed) { 34\tdevres = devres_alloc(devm_input_device_unregister, 35\tsizeof(struct input_devres), GFP_KERNEL); 36\tif (!devres) 37\treturn -ENOMEM; 38 39\tdevres-\u0026gt;input = dev; 40\t} 41 42\t/* Every input device generates EV_SYN/SYN_REPORT events. */ 43\t__set_bit(EV_SYN, dev-\u0026gt;evbit); 44 45\t/* KEY_RESERVED is not supposed to be transmitted to userspace. */ 46\t__clear_bit(KEY_RESERVED, dev-\u0026gt;keybit); 47 48\t/* Make sure that bitmasks not mentioned in dev-\u0026gt;evbit are clean. */ 49\tinput_cleanse_bitmasks(dev); 50 51\tpacket_size = input_estimate_events_per_packet(dev); 52\tif (dev-\u0026gt;hint_events_per_packet \u0026lt; packet_size) 53\tdev-\u0026gt;hint_events_per_packet = packet_size; 54 55\tdev-\u0026gt;max_vals = dev-\u0026gt;hint_events_per_packet + 2; 56\tdev-\u0026gt;vals = kcalloc(dev-\u0026gt;max_vals, sizeof(*dev-\u0026gt;vals), GFP_KERNEL); 57\tif (!dev-\u0026gt;vals) { 58\terror = -ENOMEM; 59\tgoto err_devres_free; 60\t} 61 62\t/* 63* If delay and period are pre-set by the driver, then autorepeating 64* is handled by the driver itself and we don\u0026#39;t do it in input.c. 65*/ 66\tif (!dev-\u0026gt;rep[REP_DELAY] \u0026amp;\u0026amp; !dev-\u0026gt;rep[REP_PERIOD]) { 67\tdev-\u0026gt;timer.data = (long) dev; 68\tdev-\u0026gt;timer.function = input_repeat_key; 69\tdev-\u0026gt;rep[REP_DELAY] = 250; 70\tdev-\u0026gt;rep[REP_PERIOD] = 33; 71\t} 72 73\tif (!dev-\u0026gt;getkeycode) 74\tdev-\u0026gt;getkeycode = input_default_getkeycode; 75 76\tif (!dev-\u0026gt;setkeycode) 77\tdev-\u0026gt;setkeycode = input_default_setkeycode; 78 79\terror = device_add(\u0026amp;dev-\u0026gt;dev); 80\tif (error) 81\tgoto err_free_vals; 82 83\tpath = kobject_get_path(\u0026amp;dev-\u0026gt;dev.kobj, GFP_KERNEL); 84\tpr_info(\u0026#34;%s as %s\\n\u0026#34;, 85\tdev-\u0026gt;name ? dev-\u0026gt;name : \u0026#34;Unspecified device\u0026#34;, 86\tpath ? path : \u0026#34;N/A\u0026#34;); 87\tkfree(path); 88 89\terror = mutex_lock_interruptible(\u0026amp;input_mutex); 90\tif (error) 91\tgoto err_device_del; 92 93\tlist_add_tail(\u0026amp;dev-\u0026gt;node, \u0026amp;input_dev_list); 94 95\tlist_for_each_entry(handler, \u0026amp;input_handler_list, node) 96\tinput_attach_handler(dev, handler); 97 98\tinput_wakeup_procfs_readers(); 99 100\tmutex_unlock(\u0026amp;input_mutex); 101 102\tif (dev-\u0026gt;devres_managed) { 103\tdev_dbg(dev-\u0026gt;dev.parent, \u0026#34;%s: registering %s with devres.\\n\u0026#34;, 104\t__func__, dev_name(\u0026amp;dev-\u0026gt;dev)); 105\tdevres_add(dev-\u0026gt;dev.parent, devres); 106\t} 107\treturn 0; 108 109err_device_del: 110\tdevice_del(\u0026amp;dev-\u0026gt;dev); 111err_free_vals: 112\tkfree(dev-\u0026gt;vals); 113\tdev-\u0026gt;vals = NULL; 114err_devres_free: 115\tdevres_free(devres); 116\treturn error; 117} 118EXPORT_SYMBOL(input_register_device); 119 120/** 121* 解除注册设备 122* input_unregister_device - unregister previously registered device 123* @dev: device to be unregistered 124* 125* This function unregisters an input device. Once device is unregistered 126* the caller should not try to access it as it may get freed at any moment. 127*/ 128void input_unregister_device(struct input_dev *dev) 129{ 130\tif (dev-\u0026gt;devres_managed) { 131\tWARN_ON(devres_destroy(dev-\u0026gt;dev.parent, 132\tdevm_input_device_unregister, 133\tdevm_input_device_match, 134\tdev)); 135\t__input_unregister_device(dev); 136\t/* 137* We do not do input_put_device() here because it will be done 138* when 2nd devres fires up. 139*/ 140\t} else { 141\t__input_unregister_device(dev); 142\tinput_put_device(dev); 143\t} 144} 145EXPORT_SYMBOL(input_unregister_device); ","date":"Oct 5, 2021","img":"","permalink":"https://mengdemao.github.io/posts/input_drive/","series":null,"tags":[],"title":"输入子系统"},{"categories":[],"content":"","date":"Oct 4, 2021","img":"","permalink":"https://mengdemao.github.io/posts/slab/","series":null,"tags":[],"title":"Slab"},{"categories":[],"content":"","date":"Oct 4, 2021","img":"","permalink":"https://mengdemao.github.io/posts/mmu/","series":null,"tags":[],"title":"Mmu"},{"categories":[],"content":"","date":"Oct 4, 2021","img":"","permalink":"https://mengdemao.github.io/posts/page/","series":null,"tags":[],"title":"Page"},{"categories":[],"content":"","date":"Oct 4, 2021","img":"","permalink":"https://mengdemao.github.io/posts/fork/","series":null,"tags":[],"title":"Fork"},{"categories":[],"content":"","date":"Oct 4, 2021","img":"","permalink":"https://mengdemao.github.io/posts/task/","series":null,"tags":[],"title":"任务管理"},{"categories":["linux"],"content":"系统调度 调度核心函数实现 1/* 2* kernel/sched/core.c 3* 4* Core kernel scheduler code and related syscalls 5* 6* Copyright (C) 1991-2002 Linus Torvalds 7*/ 8#include \u0026#34;sched.h\u0026#34;9 10#include \u0026lt;linux/nospec.h\u0026gt;11 12#include \u0026lt;linux/kcov.h\u0026gt;13 14#include \u0026lt;asm/switch_to.h\u0026gt;15#include \u0026lt;asm/tlb.h\u0026gt;16 17#include \u0026#34;../workqueue_internal.h\u0026#34;18#include \u0026#34;../smpboot.h\u0026#34;19 20#include \u0026#34;pelt.h\u0026#34;21 22#define CREATE_TRACE_POINTS 23#include \u0026lt;trace/events/sched.h\u0026gt;24 25DEFINE_PER_CPU_SHARED_ALIGNED(struct rq, runqueues); 26 27#ifdef CONFIG_SCHED_DEBUG 28/* 29* Debugging: various feature bits 30* 31* If SCHED_DEBUG is disabled, each compilation unit has its own copy of 32* sysctl_sched_features, defined in sched.h, to allow constants propagation 33* at compile time and compiler optimization based on features default. 34*/ 35#define SCHED_FEAT(name, enabled)\t\\ 36(1UL \u0026lt;\u0026lt; __SCHED_FEAT_##name) * enabled | 37const_debug unsigned int sysctl_sched_features = 38#include \u0026#34;features.h\u0026#34;39\t0; 40#undef SCHED_FEAT 41#endif 42 43/* 44* Number of tasks to iterate in a single balance run. 45* Limited because this is done with IRQs disabled. 46*/ 47const_debug unsigned int sysctl_sched_nr_migrate = 32; 48 49/* 50* period over which we measure -rt task CPU usage in us. 51* default: 1s 52*/ 53unsigned int sysctl_sched_rt_period = 1000000; 54 55__read_mostly int scheduler_running; 56 57/* 58* part of the period that we allow rt tasks to run in us. 59* default: 0.95s 60*/ 61int sysctl_sched_rt_runtime = 950000; 62 63/* 64* __task_rq_lock - lock the rq @p resides on. 65*/ 66struct rq *__task_rq_lock(struct task_struct *p, struct rq_flags *rf) 67\t__acquires(rq-\u0026gt;lock) 68{ 69\tstruct rq *rq; 70 71\tlockdep_assert_held(\u0026amp;p-\u0026gt;pi_lock); 72 73\tfor (;;) { 74\trq = task_rq(p); 75\traw_spin_lock(\u0026amp;rq-\u0026gt;lock); 76\tif (likely(rq == task_rq(p) \u0026amp;\u0026amp; !task_on_rq_migrating(p))) { 77\trq_pin_lock(rq, rf); 78\treturn rq; 79\t} 80\traw_spin_unlock(\u0026amp;rq-\u0026gt;lock); 81 82\twhile (unlikely(task_on_rq_migrating(p))) 83\tcpu_relax(); 84\t} 85} 86 87/* 88* task_rq_lock - lock p-\u0026gt;pi_lock and lock the rq @p resides on. 89*/ 90struct rq *task_rq_lock(struct task_struct *p, struct rq_flags *rf) 91\t__acquires(p-\u0026gt;pi_lock) 92\t__acquires(rq-\u0026gt;lock) 93{ 94\tstruct rq *rq; 95 96\tfor (;;) { 97\traw_spin_lock_irqsave(\u0026amp;p-\u0026gt;pi_lock, rf-\u0026gt;flags); 98\trq = task_rq(p); 99\traw_spin_lock(\u0026amp;rq-\u0026gt;lock); 100\t/* 101*\tmove_queued_task()\ttask_rq_lock() 102* 103*\tACQUIRE (rq-\u0026gt;lock) 104*\t[S] -\u0026gt;on_rq = MIGRATING\t[L] rq = task_rq() 105*\tWMB (__set_task_cpu())\tACQUIRE (rq-\u0026gt;lock); 106*\t[S] -\u0026gt;cpu = new_cpu\t[L] task_rq() 107*\t[L] -\u0026gt;on_rq 108*\tRELEASE (rq-\u0026gt;lock) 109* 110* If we observe the old CPU in task_rq_lock(), the acquire of 111* the old rq-\u0026gt;lock will fully serialize against the stores. 112* 113* If we observe the new CPU in task_rq_lock(), the address 114* dependency headed by \u0026#39;[L] rq = task_rq()\u0026#39; and the acquire 115* will pair with the WMB to ensure we then also see migrating. 116*/ 117\tif (likely(rq == task_rq(p) \u0026amp;\u0026amp; !task_on_rq_migrating(p))) { 118\trq_pin_lock(rq, rf); 119\treturn rq; 120\t} 121\traw_spin_unlock(\u0026amp;rq-\u0026gt;lock); 122\traw_spin_unlock_irqrestore(\u0026amp;p-\u0026gt;pi_lock, rf-\u0026gt;flags); 123 124\twhile (unlikely(task_on_rq_migrating(p))) 125\tcpu_relax(); 126\t} 127} 128 129/* 130* RQ-clock updating methods: 131*/ 132 133static void update_rq_clock_task(struct rq *rq, s64 delta) 134{ 135/* 136* In theory, the compile should just see 0 here, and optimize out the call 137* to sched_rt_avg_update. But I don\u0026#39;t trust it... 138*/ 139\ts64 __maybe_unused steal = 0, irq_delta = 0; 140 141#ifdef CONFIG_IRQ_TIME_ACCOUNTING 142\tirq_delta = irq_time_read(cpu_of(rq)) - rq-\u0026gt;prev_irq_time; 143 144\t/* 145* Since irq_time is only updated on {soft,}irq_exit, we might run into 146* this case when a previous update_rq_clock() happened inside a 147* {soft,}irq region. 148* 149* When this happens, we stop -\u0026gt;clock_task and only update the 150* prev_irq_time stamp to account for the part that fit, so that a next 151* update will consume the rest. This ensures -\u0026gt;clock_task is 152* monotonic. 153* 154* It does however cause some slight miss-attribution of {soft,}irq 155* time, a more accurate solution would be to update the irq_time using 156* the current rq-\u0026gt;clock timestamp, except that would require using 157* atomic ops. 158*/ 159\tif (irq_delta \u0026gt; delta) 160\tirq_delta = delta; 161 162\trq-\u0026gt;prev_irq_time += irq_delta; 163\tdelta -= irq_delta; 164#endif 165#ifdef CONFIG_PARAVIRT_TIME_ACCOUNTING 166\tif (static_key_false((\u0026amp;paravirt_steal_rq_enabled))) { 167\tsteal = paravirt_steal_clock(cpu_of(rq)); 168\tsteal -= rq-\u0026gt;prev_steal_time_rq; 169 170\tif (unlikely(steal \u0026gt; delta)) 171\tsteal = delta; 172 173\trq-\u0026gt;prev_steal_time_rq += steal; 174\tdelta -= steal; 175\t} 176#endif 177 178\trq-\u0026gt;clock_task += delta; 179 180#ifdef CONFIG_HAVE_SCHED_AVG_IRQ 181\tif ((irq_delta + steal) \u0026amp;\u0026amp; sched_feat(NONTASK_CAPACITY)) 182\tupdate_irq_load_avg(rq, irq_delta + steal); 183#endif 184} 185 186void update_rq_clock(struct rq *rq) 187{ 188\ts64 delta; 189 190\tlockdep_assert_held(\u0026amp;rq-\u0026gt;lock); 191 192\tif (rq-\u0026gt;clock_update_flags \u0026amp; RQCF_ACT_SKIP) 193\treturn; 194 195#ifdef CONFIG_SCHED_DEBUG 196\tif (sched_feat(WARN_DOUBLE_CLOCK)) 197\tSCHED_WARN_ON(rq-\u0026gt;clock_update_flags \u0026amp; RQCF_UPDATED); 198\trq-\u0026gt;clock_update_flags |= RQCF_UPDATED; 199#endif 200 201\tdelta = sched_clock_cpu(cpu_of(rq)) - rq-\u0026gt;clock; 202\tif (delta \u0026lt; 0) 203\treturn; 204\trq-\u0026gt;clock += delta; 205\tupdate_rq_clock_task(rq, delta); 206} 207 208 209#ifdef CONFIG_SCHED_HRTICK 210/* 211* Use HR-timers to deliver accurate preemption points. 212*/ 213 214static void hrtick_clear(struct rq *rq) 215{ 216\tif (hrtimer_active(\u0026amp;rq-\u0026gt;hrtick_timer)) 217\thrtimer_cancel(\u0026amp;rq-\u0026gt;hrtick_timer); 218} 219 220/* 221* High-resolution timer tick. 222* Runs from hardirq context with interrupts disabled. 223*/ 224static enum hrtimer_restart hrtick(struct hrtimer *timer) 225{ 226\tstruct rq *rq = container_of(timer, struct rq, hrtick_timer); 227\tstruct rq_flags rf; 228 229\tWARN_ON_ONCE(cpu_of(rq) != smp_processor_id()); 230 231\trq_lock(rq, \u0026amp;rf); 232\tupdate_rq_clock(rq); 233\trq-\u0026gt;curr-\u0026gt;sched_class-\u0026gt;task_tick(rq, rq-\u0026gt;curr, 1); 234\trq_unlock(rq, \u0026amp;rf); 235 236\treturn HRTIMER_NORESTART; 237} 238 239#ifdef CONFIG_SMP 240 241static void __hrtick_restart(struct rq *rq) 242{ 243\tstruct hrtimer *timer = \u0026amp;rq-\u0026gt;hrtick_timer; 244 245\thrtimer_start_expires(timer, HRTIMER_MODE_ABS_PINNED); 246} 247 248/* 249* called from hardirq (IPI) context 250*/ 251static void __hrtick_start(void *arg) 252{ 253\tstruct rq *rq = arg; 254\tstruct rq_flags rf; 255 256\trq_lock(rq, \u0026amp;rf); 257\t__hrtick_restart(rq); 258\trq-\u0026gt;hrtick_csd_pending = 0; 259\trq_unlock(rq, \u0026amp;rf); 260} 261 262/* 263* Called to set the hrtick timer state. 264* 265* called with rq-\u0026gt;lock held and irqs disabled 266*/ 267void hrtick_start(struct rq *rq, u64 delay) 268{ 269\tstruct hrtimer *timer = \u0026amp;rq-\u0026gt;hrtick_timer; 270\tktime_t time; 271\ts64 delta; 272 273\t/* 274* Don\u0026#39;t schedule slices shorter than 10000ns, that just 275* doesn\u0026#39;t make sense and can cause timer DoS. 276*/ 277\tdelta = max_t(s64, delay, 10000LL); 278\ttime = ktime_add_ns(timer-\u0026gt;base-\u0026gt;get_time(), delta); 279 280\thrtimer_set_expires(timer, time); 281 282\tif (rq == this_rq()) { 283\t__hrtick_restart(rq); 284\t} else if (!rq-\u0026gt;hrtick_csd_pending) { 285\tsmp_call_function_single_async(cpu_of(rq), \u0026amp;rq-\u0026gt;hrtick_csd); 286\trq-\u0026gt;hrtick_csd_pending = 1; 287\t} 288} 289 290#else 291/* 292* Called to set the hrtick timer state. 293* 294* called with rq-\u0026gt;lock held and irqs disabled 295*/ 296void hrtick_start(struct rq *rq, u64 delay) 297{ 298\t/* 299* Don\u0026#39;t schedule slices shorter than 10000ns, that just 300* doesn\u0026#39;t make sense. Rely on vruntime for fairness. 301*/ 302\tdelay = max_t(u64, delay, 10000LL); 303\thrtimer_start(\u0026amp;rq-\u0026gt;hrtick_timer, ns_to_ktime(delay), 304\tHRTIMER_MODE_REL_PINNED); 305} 306#endif /* CONFIG_SMP */307 308static void hrtick_rq_init(struct rq *rq) 309{ 310#ifdef CONFIG_SMP 311\trq-\u0026gt;hrtick_csd_pending = 0; 312 313\trq-\u0026gt;hrtick_csd.flags = 0; 314\trq-\u0026gt;hrtick_csd.func = __hrtick_start; 315\trq-\u0026gt;hrtick_csd.info = rq; 316#endif 317 318\thrtimer_init(\u0026amp;rq-\u0026gt;hrtick_timer, CLOCK_MONOTONIC, HRTIMER_MODE_REL); 319\trq-\u0026gt;hrtick_timer.function = hrtick; 320} 321#else\t/* CONFIG_SCHED_HRTICK */322static inline void hrtick_clear(struct rq *rq) 323{ 324} 325 326static inline void hrtick_rq_init(struct rq *rq) 327{ 328} 329#endif\t/* CONFIG_SCHED_HRTICK */330 331/* 332* cmpxchg based fetch_or, macro so it works for different integer types 333*/ 334#define fetch_or(ptr, mask)\t\\ 335({\t\\ 336typeof(ptr) _ptr = (ptr);\t\\ 337typeof(mask) _mask = (mask);\t\\ 338typeof(*_ptr) _old, _val = *_ptr;\t\\ 339\\ 340for (;;) {\t\\ 341_old = cmpxchg(_ptr, _val, _val | _mask);\t\\ 342if (_old == _val)\t\\ 343break;\t\\ 344_val = _old;\t\\ 345}\t\\ 346_old;\t\\ 347}) 348 349#if defined(CONFIG_SMP) \u0026amp;\u0026amp; defined(TIF_POLLING_NRFLAG) 350/* 351* Atomically set TIF_NEED_RESCHED and test for TIF_POLLING_NRFLAG, 352* this avoids any races wrt polling state changes and thereby avoids 353* spurious IPIs. 354*/ 355static bool set_nr_and_not_polling(struct task_struct *p) 356{ 357\tstruct thread_info *ti = task_thread_info(p); 358\treturn !(fetch_or(\u0026amp;ti-\u0026gt;flags, _TIF_NEED_RESCHED) \u0026amp; _TIF_POLLING_NRFLAG); 359} 360 361/* 362* Atomically set TIF_NEED_RESCHED if TIF_POLLING_NRFLAG is set. 363* 364* If this returns true, then the idle task promises to call 365* sched_ttwu_pending() and reschedule soon. 366*/ 367static bool set_nr_if_polling(struct task_struct *p) 368{ 369\tstruct thread_info *ti = task_thread_info(p); 370\ttypeof(ti-\u0026gt;flags) old, val = READ_ONCE(ti-\u0026gt;flags); 371 372\tfor (;;) { 373\tif (!(val \u0026amp; _TIF_POLLING_NRFLAG)) 374\treturn false; 375\tif (val \u0026amp; _TIF_NEED_RESCHED) 376\treturn true; 377\told = cmpxchg(\u0026amp;ti-\u0026gt;flags, val, val | _TIF_NEED_RESCHED); 378\tif (old == val) 379\tbreak; 380\tval = old; 381\t} 382\treturn true; 383} 384 385#else 386static bool set_nr_and_not_polling(struct task_struct *p) 387{ 388\tset_tsk_need_resched(p); 389\treturn true; 390} 391 392#ifdef CONFIG_SMP 393static bool set_nr_if_polling(struct task_struct *p) 394{ 395\treturn false; 396} 397#endif 398#endif 399 400void wake_q_add(struct wake_q_head *head, struct task_struct *task) 401{ 402\tstruct wake_q_node *node = \u0026amp;task-\u0026gt;wake_q; 403 404\t/* 405* Atomically grab the task, if -\u0026gt;wake_q is !nil already it means 406* its already queued (either by us or someone else) and will get the 407* wakeup due to that. 408* 409* In order to ensure that a pending wakeup will observe our pending 410* state, even in the failed case, an explicit smp_mb() must be used. 411*/ 412\tsmp_mb__before_atomic(); 413\tif (cmpxchg_relaxed(\u0026amp;node-\u0026gt;next, NULL, WAKE_Q_TAIL)) 414\treturn; 415 416\tget_task_struct(task); 417 418\t/* 419* The head is context local, there can be no concurrency. 420*/ 421\t*head-\u0026gt;lastp = node; 422\thead-\u0026gt;lastp = \u0026amp;node-\u0026gt;next; 423} 424 425void wake_up_q(struct wake_q_head *head) 426{ 427\tstruct wake_q_node *node = head-\u0026gt;first; 428 429\twhile (node != WAKE_Q_TAIL) { 430\tstruct task_struct *task; 431 432\ttask = container_of(node, struct task_struct, wake_q); 433\tBUG_ON(!task); 434\t/* Task can safely be re-inserted now: */ 435\tnode = node-\u0026gt;next; 436\ttask-\u0026gt;wake_q.next = NULL; 437 438\t/* 439* wake_up_process() executes a full barrier, which pairs with 440* the queueing in wake_q_add() so as not to miss wakeups. 441*/ 442\twake_up_process(task); 443\tput_task_struct(task); 444\t} 445} 446 447/* 448* resched_curr - mark rq\u0026#39;s current task \u0026#39;to be rescheduled now\u0026#39;. 449* 450* On UP this means the setting of the need_resched flag, on SMP it 451* might also involve a cross-CPU call to trigger the scheduler on 452* the target CPU. 453*/ 454void resched_curr(struct rq *rq) 455{ 456\tstruct task_struct *curr = rq-\u0026gt;curr; 457\tint cpu; 458 459\tlockdep_assert_held(\u0026amp;rq-\u0026gt;lock); 460 461\tif (test_tsk_need_resched(curr)) 462\treturn; 463 464\tcpu = cpu_of(rq); 465 466\tif (cpu == smp_processor_id()) { 467\tset_tsk_need_resched(curr); 468\tset_preempt_need_resched(); 469\treturn; 470\t} 471 472\tif (set_nr_and_not_polling(curr)) 473\tsmp_send_reschedule(cpu); 474\telse 475\ttrace_sched_wake_idle_without_ipi(cpu); 476} 477 478void resched_cpu(int cpu) 479{ 480\tstruct rq *rq = cpu_rq(cpu); 481\tunsigned long flags; 482 483\traw_spin_lock_irqsave(\u0026amp;rq-\u0026gt;lock, flags); 484\tif (cpu_online(cpu) || cpu == smp_processor_id()) 485\tresched_curr(rq); 486\traw_spin_unlock_irqrestore(\u0026amp;rq-\u0026gt;lock, flags); 487} 488 489#ifdef CONFIG_SMP 490#ifdef CONFIG_NO_HZ_COMMON 491/* 492* In the semi idle case, use the nearest busy CPU for migrating timers 493* from an idle CPU. This is good for power-savings. 494* 495* We don\u0026#39;t do similar optimization for completely idle system, as 496* selecting an idle CPU will add more delays to the timers than intended 497* (as that CPU\u0026#39;s timer base may not be uptodate wrt jiffies etc). 498*/ 499int get_nohz_timer_target(void) 500{ 501\tint i, cpu = smp_processor_id(); 502\tstruct sched_domain *sd; 503 504\tif (!idle_cpu(cpu) \u0026amp;\u0026amp; housekeeping_cpu(cpu, HK_FLAG_TIMER)) 505\treturn cpu; 506 507\trcu_read_lock(); 508\tfor_each_domain(cpu, sd) { 509\tfor_each_cpu(i, sched_domain_span(sd)) { 510\tif (cpu == i) 511\tcontinue; 512 513\tif (!idle_cpu(i) \u0026amp;\u0026amp; housekeeping_cpu(i, HK_FLAG_TIMER)) { 514\tcpu = i; 515\tgoto unlock; 516\t} 517\t} 518\t} 519 520\tif (!housekeeping_cpu(cpu, HK_FLAG_TIMER)) 521\tcpu = housekeeping_any_cpu(HK_FLAG_TIMER); 522unlock: 523\trcu_read_unlock(); 524\treturn cpu; 525} 526 527/* 528* When add_timer_on() enqueues a timer into the timer wheel of an 529* idle CPU then this timer might expire before the next timer event 530* which is scheduled to wake up that CPU. In case of a completely 531* idle system the next event might even be infinite time into the 532* future. wake_up_idle_cpu() ensures that the CPU is woken up and 533* leaves the inner idle loop so the newly added timer is taken into 534* account when the CPU goes back to idle and evaluates the timer 535* wheel for the next timer event. 536*/ 537static void wake_up_idle_cpu(int cpu) 538{ 539\tstruct rq *rq = cpu_rq(cpu); 540 541\tif (cpu == smp_processor_id()) 542\treturn; 543 544\tif (set_nr_and_not_polling(rq-\u0026gt;idle)) 545\tsmp_send_reschedule(cpu); 546\telse 547\ttrace_sched_wake_idle_without_ipi(cpu); 548} 549 550static bool wake_up_full_nohz_cpu(int cpu) 551{ 552\t/* 553* We just need the target to call irq_exit() and re-evaluate 554* the next tick. The nohz full kick at least implies that. 555* If needed we can still optimize that later with an 556* empty IRQ. 557*/ 558\tif (cpu_is_offline(cpu)) 559\treturn true; /* Don\u0026#39;t try to wake offline CPUs. */ 560\tif (tick_nohz_full_cpu(cpu)) { 561\tif (cpu != smp_processor_id() || 562\ttick_nohz_tick_stopped()) 563\ttick_nohz_full_kick_cpu(cpu); 564\treturn true; 565\t} 566 567\treturn false; 568} 569 570/* 571* Wake up the specified CPU. If the CPU is going offline, it is the 572* caller\u0026#39;s responsibility to deal with the lost wakeup, for example, 573* by hooking into the CPU_DEAD notifier like timers and hrtimers do. 574*/ 575void wake_up_nohz_cpu(int cpu) 576{ 577\tif (!wake_up_full_nohz_cpu(cpu)) 578\twake_up_idle_cpu(cpu); 579} 580 581static inline bool got_nohz_idle_kick(void) 582{ 583\tint cpu = smp_processor_id(); 584 585\tif (!(atomic_read(nohz_flags(cpu)) \u0026amp; NOHZ_KICK_MASK)) 586\treturn false; 587 588\tif (idle_cpu(cpu) \u0026amp;\u0026amp; !need_resched()) 589\treturn true; 590 591\t/* 592* We can\u0026#39;t run Idle Load Balance on this CPU for this time so we 593* cancel it and clear NOHZ_BALANCE_KICK 594*/ 595\tatomic_andnot(NOHZ_KICK_MASK, nohz_flags(cpu)); 596\treturn false; 597} 598 599#else /* CONFIG_NO_HZ_COMMON */600 601static inline bool got_nohz_idle_kick(void) 602{ 603\treturn false; 604} 605 606#endif /* CONFIG_NO_HZ_COMMON */607 608#ifdef CONFIG_NO_HZ_FULL 609bool sched_can_stop_tick(struct rq *rq) 610{ 611\tint fifo_nr_running; 612 613\t/* Deadline tasks, even if single, need the tick */ 614\tif (rq-\u0026gt;dl.dl_nr_running) 615\treturn false; 616 617\t/* 618* If there are more than one RR tasks, we need the tick to effect the 619* actual RR behaviour. 620*/ 621\tif (rq-\u0026gt;rt.rr_nr_running) { 622\tif (rq-\u0026gt;rt.rr_nr_running == 1) 623\treturn true; 624\telse 625\treturn false; 626\t} 627 628\t/* 629* If there\u0026#39;s no RR tasks, but FIFO tasks, we can skip the tick, no 630* forced preemption between FIFO tasks. 631*/ 632\tfifo_nr_running = rq-\u0026gt;rt.rt_nr_running - rq-\u0026gt;rt.rr_nr_running; 633\tif (fifo_nr_running) 634\treturn true; 635 636\t/* 637* If there are no DL,RR/FIFO tasks, there must only be CFS tasks left; 638* if there\u0026#39;s more than one we need the tick for involuntary 639* preemption. 640*/ 641\tif (rq-\u0026gt;nr_running \u0026gt; 1) 642\treturn false; 643 644\treturn true; 645} 646#endif /* CONFIG_NO_HZ_FULL */647#endif /* CONFIG_SMP */648 649#if defined(CONFIG_RT_GROUP_SCHED) || (defined(CONFIG_FAIR_GROUP_SCHED) \u0026amp;\u0026amp; \\ 650(defined(CONFIG_SMP) || defined(CONFIG_CFS_BANDWIDTH))) 651/* 652* Iterate task_group tree rooted at *from, calling @down when first entering a 653* node and @up when leaving it for the final time. 654* 655* Caller must hold rcu_lock or sufficient equivalent. 656*/ 657int walk_tg_tree_from(struct task_group *from, 658\ttg_visitor down, tg_visitor up, void *data) 659{ 660\tstruct task_group *parent, *child; 661\tint ret; 662 663\tparent = from; 664 665down: 666\tret = (*down)(parent, data); 667\tif (ret) 668\tgoto out; 669\tlist_for_each_entry_rcu(child, \u0026amp;parent-\u0026gt;children, siblings) { 670\tparent = child; 671\tgoto down; 672 673up: 674\tcontinue; 675\t} 676\tret = (*up)(parent, data); 677\tif (ret || parent == from) 678\tgoto out; 679 680\tchild = parent; 681\tparent = parent-\u0026gt;parent; 682\tif (parent) 683\tgoto up; 684out: 685\treturn ret; 686} 687 688int tg_nop(struct task_group *tg, void *data) 689{ 690\treturn 0; 691} 692#endif 693 694static void set_load_weight(struct task_struct *p, bool update_load) 695{ 696\tint prio = p-\u0026gt;static_prio - MAX_RT_PRIO; 697\tstruct load_weight *load = \u0026amp;p-\u0026gt;se.load; 698 699\t/* 700* SCHED_IDLE tasks get minimal weight: 701*/ 702\tif (idle_policy(p-\u0026gt;policy)) { 703\tload-\u0026gt;weight = scale_load(WEIGHT_IDLEPRIO); 704\tload-\u0026gt;inv_weight = WMULT_IDLEPRIO; 705\treturn; 706\t} 707 708\t/* 709* SCHED_OTHER tasks have to update their load when changing their 710* weight 711*/ 712\tif (update_load \u0026amp;\u0026amp; p-\u0026gt;sched_class == \u0026amp;fair_sched_class) { 713\treweight_task(p, prio); 714\t} else { 715\tload-\u0026gt;weight = scale_load(sched_prio_to_weight[prio]); 716\tload-\u0026gt;inv_weight = sched_prio_to_wmult[prio]; 717\t} 718} 719 720static inline void enqueue_task(struct rq *rq, struct task_struct *p, int flags) 721{ 722\tif (!(flags \u0026amp; ENQUEUE_NOCLOCK)) 723\tupdate_rq_clock(rq); 724 725\tif (!(flags \u0026amp; ENQUEUE_RESTORE)) 726\tsched_info_queued(rq, p); 727 728\tp-\u0026gt;sched_class-\u0026gt;enqueue_task(rq, p, flags); 729} 730 731static inline void dequeue_task(struct rq *rq, struct task_struct *p, int flags) 732{ 733\tif (!(flags \u0026amp; DEQUEUE_NOCLOCK)) 734\tupdate_rq_clock(rq); 735 736\tif (!(flags \u0026amp; DEQUEUE_SAVE)) 737\tsched_info_dequeued(rq, p); 738 739\tp-\u0026gt;sched_class-\u0026gt;dequeue_task(rq, p, flags); 740} 741 742void activate_task(struct rq *rq, struct task_struct *p, int flags) 743{ 744\tif (task_contributes_to_load(p)) 745\trq-\u0026gt;nr_uninterruptible--; 746 747\tenqueue_task(rq, p, flags); 748} 749 750void deactivate_task(struct rq *rq, struct task_struct *p, int flags) 751{ 752\tif (task_contributes_to_load(p)) 753\trq-\u0026gt;nr_uninterruptible++; 754 755\tdequeue_task(rq, p, flags); 756} 757 758/* 759* __normal_prio - return the priority that is based on the static prio 760*/ 761static inline int __normal_prio(struct task_struct *p) 762{ 763\treturn p-\u0026gt;static_prio; 764} 765 766/* 767* Calculate the expected normal priority: i.e. priority 768* without taking RT-inheritance into account. Might be 769* boosted by interactivity modifiers. Changes upon fork, 770* setprio syscalls, and whenever the interactivity 771* estimator recalculates. 772*/ 773static inline int normal_prio(struct task_struct *p) 774{ 775\tint prio; 776 777\tif (task_has_dl_policy(p)) 778\tprio = MAX_DL_PRIO-1; 779\telse if (task_has_rt_policy(p)) 780\tprio = MAX_RT_PRIO-1 - p-\u0026gt;rt_priority; 781\telse 782\tprio = __normal_prio(p); 783\treturn prio; 784} 785 786/* 787* Calculate the current priority, i.e. the priority 788* taken into account by the scheduler. This value might 789* be boosted by RT tasks, or might be boosted by 790* interactivity modifiers. Will be RT if the task got 791* RT-boosted. If not then it returns p-\u0026gt;normal_prio. 792*/ 793static int effective_prio(struct task_struct *p) 794{ 795\tp-\u0026gt;normal_prio = normal_prio(p); 796\t/* 797* If we are RT tasks or we were boosted to RT priority, 798* keep the priority unchanged. Otherwise, update priority 799* to the normal priority: 800*/ 801\tif (!rt_prio(p-\u0026gt;prio)) 802\treturn p-\u0026gt;normal_prio; 803\treturn p-\u0026gt;prio; 804} 805 806/** 807* task_curr - is this task currently executing on a CPU? 808* @p: the task in question. 809* 810* Return: 1 if the task is currently executing. 0 otherwise. 811*/ 812inline int task_curr(const struct task_struct *p) 813{ 814\treturn cpu_curr(task_cpu(p)) == p; 815} 816 817/* 818* switched_from, switched_to and prio_changed must _NOT_ drop rq-\u0026gt;lock, 819* use the balance_callback list if you want balancing. 820* 821* this means any call to check_class_changed() must be followed by a call to 822* balance_callback(). 823*/ 824static inline void check_class_changed(struct rq *rq, struct task_struct *p, 825\tconst struct sched_class *prev_class, 826\tint oldprio) 827{ 828\tif (prev_class != p-\u0026gt;sched_class) { 829\tif (prev_class-\u0026gt;switched_from) 830\tprev_class-\u0026gt;switched_from(rq, p); 831 832\tp-\u0026gt;sched_class-\u0026gt;switched_to(rq, p); 833\t} else if (oldprio != p-\u0026gt;prio || dl_task(p)) 834\tp-\u0026gt;sched_class-\u0026gt;prio_changed(rq, p, oldprio); 835} 836 837void check_preempt_curr(struct rq *rq, struct task_struct *p, int flags) 838{ 839\tconst struct sched_class *class; 840 841\tif (p-\u0026gt;sched_class == rq-\u0026gt;curr-\u0026gt;sched_class) { 842\trq-\u0026gt;curr-\u0026gt;sched_class-\u0026gt;check_preempt_curr(rq, p, flags); 843\t} else { 844\tfor_each_class(class) { 845\tif (class == rq-\u0026gt;curr-\u0026gt;sched_class) 846\tbreak; 847\tif (class == p-\u0026gt;sched_class) { 848\tresched_curr(rq); 849\tbreak; 850\t} 851\t} 852\t} 853 854\t/* 855* A queue event has occurred, and we\u0026#39;re going to schedule. In 856* this case, we can save a useless back to back clock update. 857*/ 858\tif (task_on_rq_queued(rq-\u0026gt;curr) \u0026amp;\u0026amp; test_tsk_need_resched(rq-\u0026gt;curr)) 859\trq_clock_skip_update(rq); 860} 861 862#ifdef CONFIG_SMP 863 864static inline bool is_per_cpu_kthread(struct task_struct *p) 865{ 866\tif (!(p-\u0026gt;flags \u0026amp; PF_KTHREAD)) 867\treturn false; 868 869\tif (p-\u0026gt;nr_cpus_allowed != 1) 870\treturn false; 871 872\treturn true; 873} 874 875/* 876* Per-CPU kthreads are allowed to run on !actie \u0026amp;\u0026amp; online CPUs, see 877* __set_cpus_allowed_ptr() and select_fallback_rq(). 878*/ 879static inline bool is_cpu_allowed(struct task_struct *p, int cpu) 880{ 881\tif (!cpumask_test_cpu(cpu, \u0026amp;p-\u0026gt;cpus_allowed)) 882\treturn false; 883 884\tif (is_per_cpu_kthread(p)) 885\treturn cpu_online(cpu); 886 887\treturn cpu_active(cpu); 888} 889 890/* 891* This is how migration works: 892* 893* 1) we invoke migration_cpu_stop() on the target CPU using 894* stop_one_cpu(). 895* 2) stopper starts to run (implicitly forcing the migrated thread 896* off the CPU) 897* 3) it checks whether the migrated task is still in the wrong runqueue. 898* 4) if it\u0026#39;s in the wrong runqueue then the migration thread removes 899* it and puts it into the right queue. 900* 5) stopper completes and stop_one_cpu() returns and the migration 901* is done. 902*/ 903 904/* 905* move_queued_task - move a queued task to new rq. 906* 907* Returns (locked) new rq. Old rq\u0026#39;s lock is released. 908*/ 909static struct rq *move_queued_task(struct rq *rq, struct rq_flags *rf, 910\tstruct task_struct *p, int new_cpu) 911{ 912\tlockdep_assert_held(\u0026amp;rq-\u0026gt;lock); 913 914\tWRITE_ONCE(p-\u0026gt;on_rq, TASK_ON_RQ_MIGRATING); 915\tdequeue_task(rq, p, DEQUEUE_NOCLOCK); 916\tset_task_cpu(p, new_cpu); 917\trq_unlock(rq, rf); 918 919\trq = cpu_rq(new_cpu); 920 921\trq_lock(rq, rf); 922\tBUG_ON(task_cpu(p) != new_cpu); 923\tenqueue_task(rq, p, 0); 924\tp-\u0026gt;on_rq = TASK_ON_RQ_QUEUED; 925\tcheck_preempt_curr(rq, p, 0); 926 927\treturn rq; 928} 929 930struct migration_arg { 931\tstruct task_struct *task; 932\tint dest_cpu; 933}; 934 935/* 936* Move (not current) task off this CPU, onto the destination CPU. We\u0026#39;re doing 937* this because either it can\u0026#39;t run here any more (set_cpus_allowed() 938* away from this CPU, or CPU going down), or because we\u0026#39;re 939* attempting to rebalance this task on exec (sched_exec). 940* 941* So we race with normal scheduler movements, but that\u0026#39;s OK, as long 942* as the task is no longer on this CPU. 943*/ 944static struct rq *__migrate_task(struct rq *rq, struct rq_flags *rf, 945\tstruct task_struct *p, int dest_cpu) 946{ 947\t/* Affinity changed (again). */ 948\tif (!is_cpu_allowed(p, dest_cpu)) 949\treturn rq; 950 951\tupdate_rq_clock(rq); 952\trq = move_queued_task(rq, rf, p, dest_cpu); 953 954\treturn rq; 955} 956 957/* 958* migration_cpu_stop - this will be executed by a highprio stopper thread 959* and performs thread migration by bumping thread off CPU then 960* \u0026#39;pushing\u0026#39; onto another runqueue. 961*/ 962static int migration_cpu_stop(void *data) 963{ 964\tstruct migration_arg *arg = data; 965\tstruct task_struct *p = arg-\u0026gt;task; 966\tstruct rq *rq = this_rq(); 967\tstruct rq_flags rf; 968 969\t/* 970* The original target CPU might have gone down and we might 971* be on another CPU but it doesn\u0026#39;t matter. 972*/ 973\tlocal_irq_disable(); 974\t/* 975* We need to explicitly wake pending tasks before running 976* __migrate_task() such that we will not miss enforcing cpus_allowed 977* during wakeups, see set_cpus_allowed_ptr()\u0026#39;s TASK_WAKING test. 978*/ 979\tsched_ttwu_pending(); 980 981\traw_spin_lock(\u0026amp;p-\u0026gt;pi_lock); 982\trq_lock(rq, \u0026amp;rf); 983\t/* 984* If task_rq(p) != rq, it cannot be migrated here, because we\u0026#39;re 985* holding rq-\u0026gt;lock, if p-\u0026gt;on_rq == 0 it cannot get enqueued because 986* we\u0026#39;re holding p-\u0026gt;pi_lock. 987*/ 988\tif (task_rq(p) == rq) { 989\tif (task_on_rq_queued(p)) 990\trq = __migrate_task(rq, \u0026amp;rf, p, arg-\u0026gt;dest_cpu); 991\telse 992\tp-\u0026gt;wake_cpu = arg-\u0026gt;dest_cpu; 993\t} 994\trq_unlock(rq, \u0026amp;rf); 995\traw_spin_unlock(\u0026amp;p-\u0026gt;pi_lock); 996 997\tlocal_irq_enable(); 998\treturn 0; 999} 1000 1001/* 1002* sched_class::set_cpus_allowed must do the below, but is not required to 1003* actually call this function. 1004*/ 1005void set_cpus_allowed_common(struct task_struct *p, const struct cpumask *new_mask) 1006{ 1007\tcpumask_copy(\u0026amp;p-\u0026gt;cpus_allowed, new_mask); 1008\tp-\u0026gt;nr_cpus_allowed = cpumask_weight(new_mask); 1009} 1010 1011void do_set_cpus_allowed(struct task_struct *p, const struct cpumask *new_mask) 1012{ 1013\tstruct rq *rq = task_rq(p); 1014\tbool queued, running; 1015 1016\tlockdep_assert_held(\u0026amp;p-\u0026gt;pi_lock); 1017 1018\tqueued = task_on_rq_queued(p); 1019\trunning = task_current(rq, p); 1020 1021\tif (queued) { 1022\t/* 1023* Because __kthread_bind() calls this on blocked tasks without 1024* holding rq-\u0026gt;lock. 1025*/ 1026\tlockdep_assert_held(\u0026amp;rq-\u0026gt;lock); 1027\tdequeue_task(rq, p, DEQUEUE_SAVE | DEQUEUE_NOCLOCK); 1028\t} 1029\tif (running) 1030\tput_prev_task(rq, p); 1031 1032\tp-\u0026gt;sched_class-\u0026gt;set_cpus_allowed(p, new_mask); 1033 1034\tif (queued) 1035\tenqueue_task(rq, p, ENQUEUE_RESTORE | ENQUEUE_NOCLOCK); 1036\tif (running) 1037\tset_curr_task(rq, p); 1038} 1039 1040/* 1041* Change a given task\u0026#39;s CPU affinity. Migrate the thread to a 1042* proper CPU and schedule it away if the CPU it\u0026#39;s executing on 1043* is removed from the allowed bitmask. 1044* 1045* NOTE: the caller must have a valid reference to the task, the 1046* task must not exit() \u0026amp; deallocate itself prematurely. The 1047* call is not atomic; no spinlocks may be held. 1048*/ 1049static int __set_cpus_allowed_ptr(struct task_struct *p, 1050\tconst struct cpumask *new_mask, bool check) 1051{ 1052\tconst struct cpumask *cpu_valid_mask = cpu_active_mask; 1053\tunsigned int dest_cpu; 1054\tstruct rq_flags rf; 1055\tstruct rq *rq; 1056\tint ret = 0; 1057 1058\trq = task_rq_lock(p, \u0026amp;rf); 1059\tupdate_rq_clock(rq); 1060 1061\tif (p-\u0026gt;flags \u0026amp; PF_KTHREAD) { 1062\t/* 1063* Kernel threads are allowed on online \u0026amp;\u0026amp; !active CPUs 1064*/ 1065\tcpu_valid_mask = cpu_online_mask; 1066\t} 1067 1068\t/* 1069* Must re-check here, to close a race against __kthread_bind(), 1070* sched_setaffinity() is not guaranteed to observe the flag. 1071*/ 1072\tif (check \u0026amp;\u0026amp; (p-\u0026gt;flags \u0026amp; PF_NO_SETAFFINITY)) { 1073\tret = -EINVAL; 1074\tgoto out; 1075\t} 1076 1077\tif (cpumask_equal(\u0026amp;p-\u0026gt;cpus_allowed, new_mask)) 1078\tgoto out; 1079 1080\tdest_cpu = cpumask_any_and(cpu_valid_mask, new_mask); 1081\tif (dest_cpu \u0026gt;= nr_cpu_ids) { 1082\tret = -EINVAL; 1083\tgoto out; 1084\t} 1085 1086\tdo_set_cpus_allowed(p, new_mask); 1087 1088\tif (p-\u0026gt;flags \u0026amp; PF_KTHREAD) { 1089\t/* 1090* For kernel threads that do indeed end up on online \u0026amp;\u0026amp; 1091* !active we want to ensure they are strict per-CPU threads. 1092*/ 1093\tWARN_ON(cpumask_intersects(new_mask, cpu_online_mask) \u0026amp;\u0026amp; 1094\t!cpumask_intersects(new_mask, cpu_active_mask) \u0026amp;\u0026amp; 1095\tp-\u0026gt;nr_cpus_allowed != 1); 1096\t} 1097 1098\t/* Can the task run on the task\u0026#39;s current CPU? If so, we\u0026#39;re done */ 1099\tif (cpumask_test_cpu(task_cpu(p), new_mask)) 1100\tgoto out; 1101 1102\tif (task_running(rq, p) || p-\u0026gt;state == TASK_WAKING) { 1103\tstruct migration_arg arg = { p, dest_cpu }; 1104\t/* Need help from migration thread: drop lock and wait. */ 1105\ttask_rq_unlock(rq, p, \u0026amp;rf); 1106\tstop_one_cpu(cpu_of(rq), migration_cpu_stop, \u0026amp;arg); 1107\ttlb_migrate_finish(p-\u0026gt;mm); 1108\treturn 0; 1109\t} else if (task_on_rq_queued(p)) { 1110\t/* 1111* OK, since we\u0026#39;re going to drop the lock immediately 1112* afterwards anyway. 1113*/ 1114\trq = move_queued_task(rq, \u0026amp;rf, p, dest_cpu); 1115\t} 1116out: 1117\ttask_rq_unlock(rq, p, \u0026amp;rf); 1118 1119\treturn ret; 1120} 1121 1122int set_cpus_allowed_ptr(struct task_struct *p, const struct cpumask *new_mask) 1123{ 1124\treturn __set_cpus_allowed_ptr(p, new_mask, false); 1125} 1126EXPORT_SYMBOL_GPL(set_cpus_allowed_ptr); 1127 1128void set_task_cpu(struct task_struct *p, unsigned int new_cpu) 1129{ 1130#ifdef CONFIG_SCHED_DEBUG 1131\t/* 1132* We should never call set_task_cpu() on a blocked task, 1133* ttwu() will sort out the placement. 1134*/ 1135\tWARN_ON_ONCE(p-\u0026gt;state != TASK_RUNNING \u0026amp;\u0026amp; p-\u0026gt;state != TASK_WAKING \u0026amp;\u0026amp; 1136\t!p-\u0026gt;on_rq); 1137 1138\t/* 1139* Migrating fair class task must have p-\u0026gt;on_rq = TASK_ON_RQ_MIGRATING, 1140* because schedstat_wait_{start,end} rebase migrating task\u0026#39;s wait_start 1141* time relying on p-\u0026gt;on_rq. 1142*/ 1143\tWARN_ON_ONCE(p-\u0026gt;state == TASK_RUNNING \u0026amp;\u0026amp; 1144\tp-\u0026gt;sched_class == \u0026amp;fair_sched_class \u0026amp;\u0026amp; 1145\t(p-\u0026gt;on_rq \u0026amp;\u0026amp; !task_on_rq_migrating(p))); 1146 1147#ifdef CONFIG_LOCKDEP 1148\t/* 1149* The caller should hold either p-\u0026gt;pi_lock or rq-\u0026gt;lock, when changing 1150* a task\u0026#39;s CPU. -\u0026gt;pi_lock for waking tasks, rq-\u0026gt;lock for runnable tasks. 1151* 1152* sched_move_task() holds both and thus holding either pins the cgroup, 1153* see task_group(). 1154* 1155* Furthermore, all task_rq users should acquire both locks, see 1156* task_rq_lock(). 1157*/ 1158\tWARN_ON_ONCE(debug_locks \u0026amp;\u0026amp; !(lockdep_is_held(\u0026amp;p-\u0026gt;pi_lock) || 1159\tlockdep_is_held(\u0026amp;task_rq(p)-\u0026gt;lock))); 1160#endif 1161\t/* 1162* Clearly, migrating tasks to offline CPUs is a fairly daft thing. 1163*/ 1164\tWARN_ON_ONCE(!cpu_online(new_cpu)); 1165#endif 1166 1167\ttrace_sched_migrate_task(p, new_cpu); 1168 1169\tif (task_cpu(p) != new_cpu) { 1170\tif (p-\u0026gt;sched_class-\u0026gt;migrate_task_rq) 1171\tp-\u0026gt;sched_class-\u0026gt;migrate_task_rq(p, new_cpu); 1172\tp-\u0026gt;se.nr_migrations++; 1173\trseq_migrate(p); 1174\tperf_event_task_migrate(p); 1175\t} 1176 1177\t__set_task_cpu(p, new_cpu); 1178} 1179 1180#ifdef CONFIG_NUMA_BALANCING 1181static void __migrate_swap_task(struct task_struct *p, int cpu) 1182{ 1183\tif (task_on_rq_queued(p)) { 1184\tstruct rq *src_rq, *dst_rq; 1185\tstruct rq_flags srf, drf; 1186 1187\tsrc_rq = task_rq(p); 1188\tdst_rq = cpu_rq(cpu); 1189 1190\trq_pin_lock(src_rq, \u0026amp;srf); 1191\trq_pin_lock(dst_rq, \u0026amp;drf); 1192 1193\tp-\u0026gt;on_rq = TASK_ON_RQ_MIGRATING; 1194\tdeactivate_task(src_rq, p, 0); 1195\tset_task_cpu(p, cpu); 1196\tactivate_task(dst_rq, p, 0); 1197\tp-\u0026gt;on_rq = TASK_ON_RQ_QUEUED; 1198\tcheck_preempt_curr(dst_rq, p, 0); 1199 1200\trq_unpin_lock(dst_rq, \u0026amp;drf); 1201\trq_unpin_lock(src_rq, \u0026amp;srf); 1202 1203\t} else { 1204\t/* 1205* Task isn\u0026#39;t running anymore; make it appear like we migrated 1206* it before it went to sleep. This means on wakeup we make the 1207* previous CPU our target instead of where it really is. 1208*/ 1209\tp-\u0026gt;wake_cpu = cpu; 1210\t} 1211} 1212 1213struct migration_swap_arg { 1214\tstruct task_struct *src_task, *dst_task; 1215\tint src_cpu, dst_cpu; 1216}; 1217 1218static int migrate_swap_stop(void *data) 1219{ 1220\tstruct migration_swap_arg *arg = data; 1221\tstruct rq *src_rq, *dst_rq; 1222\tint ret = -EAGAIN; 1223 1224\tif (!cpu_active(arg-\u0026gt;src_cpu) || !cpu_active(arg-\u0026gt;dst_cpu)) 1225\treturn -EAGAIN; 1226 1227\tsrc_rq = cpu_rq(arg-\u0026gt;src_cpu); 1228\tdst_rq = cpu_rq(arg-\u0026gt;dst_cpu); 1229 1230\tdouble_raw_lock(\u0026amp;arg-\u0026gt;src_task-\u0026gt;pi_lock, 1231\t\u0026amp;arg-\u0026gt;dst_task-\u0026gt;pi_lock); 1232\tdouble_rq_lock(src_rq, dst_rq); 1233 1234\tif (task_cpu(arg-\u0026gt;dst_task) != arg-\u0026gt;dst_cpu) 1235\tgoto unlock; 1236 1237\tif (task_cpu(arg-\u0026gt;src_task) != arg-\u0026gt;src_cpu) 1238\tgoto unlock; 1239 1240\tif (!cpumask_test_cpu(arg-\u0026gt;dst_cpu, \u0026amp;arg-\u0026gt;src_task-\u0026gt;cpus_allowed)) 1241\tgoto unlock; 1242 1243\tif (!cpumask_test_cpu(arg-\u0026gt;src_cpu, \u0026amp;arg-\u0026gt;dst_task-\u0026gt;cpus_allowed)) 1244\tgoto unlock; 1245 1246\t__migrate_swap_task(arg-\u0026gt;src_task, arg-\u0026gt;dst_cpu); 1247\t__migrate_swap_task(arg-\u0026gt;dst_task, arg-\u0026gt;src_cpu); 1248 1249\tret = 0; 1250 1251unlock: 1252\tdouble_rq_unlock(src_rq, dst_rq); 1253\traw_spin_unlock(\u0026amp;arg-\u0026gt;dst_task-\u0026gt;pi_lock); 1254\traw_spin_unlock(\u0026amp;arg-\u0026gt;src_task-\u0026gt;pi_lock); 1255 1256\treturn ret; 1257} 1258 1259/* 1260* Cross migrate two tasks 1261*/ 1262int migrate_swap(struct task_struct *cur, struct task_struct *p, 1263\tint target_cpu, int curr_cpu) 1264{ 1265\tstruct migration_swap_arg arg; 1266\tint ret = -EINVAL; 1267 1268\targ = (struct migration_swap_arg){ 1269\t.src_task = cur, 1270\t.src_cpu = curr_cpu, 1271\t.dst_task = p, 1272\t.dst_cpu = target_cpu, 1273\t}; 1274 1275\tif (arg.src_cpu == arg.dst_cpu) 1276\tgoto out; 1277 1278\t/* 1279* These three tests are all lockless; this is OK since all of them 1280* will be re-checked with proper locks held further down the line. 1281*/ 1282\tif (!cpu_active(arg.src_cpu) || !cpu_active(arg.dst_cpu)) 1283\tgoto out; 1284 1285\tif (!cpumask_test_cpu(arg.dst_cpu, \u0026amp;arg.src_task-\u0026gt;cpus_allowed)) 1286\tgoto out; 1287 1288\tif (!cpumask_test_cpu(arg.src_cpu, \u0026amp;arg.dst_task-\u0026gt;cpus_allowed)) 1289\tgoto out; 1290 1291\ttrace_sched_swap_numa(cur, arg.src_cpu, p, arg.dst_cpu); 1292\tret = stop_two_cpus(arg.dst_cpu, arg.src_cpu, migrate_swap_stop, \u0026amp;arg); 1293 1294out: 1295\treturn ret; 1296} 1297#endif /* CONFIG_NUMA_BALANCING */1298 1299/* 1300* wait_task_inactive - wait for a thread to unschedule. 1301* 1302* If @match_state is nonzero, it\u0026#39;s the @p-\u0026gt;state value just checked and 1303* not expected to change. If it changes, i.e. @p might have woken up, 1304* then return zero. When we succeed in waiting for @p to be off its CPU, 1305* we return a positive number (its total switch count). If a second call 1306* a short while later returns the same number, the caller can be sure that 1307* @p has remained unscheduled the whole time. 1308* 1309* The caller must ensure that the task *will* unschedule sometime soon, 1310* else this function might spin for a *long* time. This function can\u0026#39;t 1311* be called with interrupts off, or it may introduce deadlock with 1312* smp_call_function() if an IPI is sent by the same process we are 1313* waiting to become inactive. 1314*/ 1315unsigned long wait_task_inactive(struct task_struct *p, long match_state) 1316{ 1317\tint running, queued; 1318\tstruct rq_flags rf; 1319\tunsigned long ncsw; 1320\tstruct rq *rq; 1321 1322\tfor (;;) { 1323\t/* 1324* We do the initial early heuristics without holding 1325* any task-queue locks at all. We\u0026#39;ll only try to get 1326* the runqueue lock when things look like they will 1327* work out! 1328*/ 1329\trq = task_rq(p); 1330 1331\t/* 1332* If the task is actively running on another CPU 1333* still, just relax and busy-wait without holding 1334* any locks. 1335* 1336* NOTE! Since we don\u0026#39;t hold any locks, it\u0026#39;s not 1337* even sure that \u0026#34;rq\u0026#34; stays as the right runqueue! 1338* But we don\u0026#39;t care, since \u0026#34;task_running()\u0026#34; will 1339* return false if the runqueue has changed and p 1340* is actually now running somewhere else! 1341*/ 1342\twhile (task_running(rq, p)) { 1343\tif (match_state \u0026amp;\u0026amp; unlikely(p-\u0026gt;state != match_state)) 1344\treturn 0; 1345\tcpu_relax(); 1346\t} 1347 1348\t/* 1349* Ok, time to look more closely! We need the rq 1350* lock now, to be *sure*. If we\u0026#39;re wrong, we\u0026#39;ll 1351* just go back and repeat. 1352*/ 1353\trq = task_rq_lock(p, \u0026amp;rf); 1354\ttrace_sched_wait_task(p); 1355\trunning = task_running(rq, p); 1356\tqueued = task_on_rq_queued(p); 1357\tncsw = 0; 1358\tif (!match_state || p-\u0026gt;state == match_state) 1359\tncsw = p-\u0026gt;nvcsw | LONG_MIN; /* sets MSB */ 1360\ttask_rq_unlock(rq, p, \u0026amp;rf); 1361 1362\t/* 1363* If it changed from the expected state, bail out now. 1364*/ 1365\tif (unlikely(!ncsw)) 1366\tbreak; 1367 1368\t/* 1369* Was it really running after all now that we 1370* checked with the proper locks actually held? 1371* 1372* Oops. Go back and try again.. 1373*/ 1374\tif (unlikely(running)) { 1375\tcpu_relax(); 1376\tcontinue; 1377\t} 1378 1379\t/* 1380* It\u0026#39;s not enough that it\u0026#39;s not actively running, 1381* it must be off the runqueue _entirely_, and not 1382* preempted! 1383* 1384* So if it was still runnable (but just not actively 1385* running right now), it\u0026#39;s preempted, and we should 1386* yield - it could be a while. 1387*/ 1388\tif (unlikely(queued)) { 1389\tktime_t to = NSEC_PER_SEC / HZ; 1390 1391\tset_current_state(TASK_UNINTERRUPTIBLE); 1392\tschedule_hrtimeout(\u0026amp;to, HRTIMER_MODE_REL); 1393\tcontinue; 1394\t} 1395 1396\t/* 1397* Ahh, all good. It wasn\u0026#39;t running, and it wasn\u0026#39;t 1398* runnable, which means that it will never become 1399* running in the future either. We\u0026#39;re all done! 1400*/ 1401\tbreak; 1402\t} 1403 1404\treturn ncsw; 1405} 1406 1407/*** 1408* kick_process - kick a running thread to enter/exit the kernel 1409* @p: the to-be-kicked thread 1410* 1411* Cause a process which is running on another CPU to enter 1412* kernel-mode, without any delay. (to get signals handled.) 1413* 1414* NOTE: this function doesn\u0026#39;t have to take the runqueue lock, 1415* because all it wants to ensure is that the remote task enters 1416* the kernel. If the IPI races and the task has been migrated 1417* to another CPU then no harm is done and the purpose has been 1418* achieved as well. 1419*/ 1420void kick_process(struct task_struct *p) 1421{ 1422\tint cpu; 1423 1424\tpreempt_disable(); 1425\tcpu = task_cpu(p); 1426\tif ((cpu != smp_processor_id()) \u0026amp;\u0026amp; task_curr(p)) 1427\tsmp_send_reschedule(cpu); 1428\tpreempt_enable(); 1429} 1430EXPORT_SYMBOL_GPL(kick_process); 1431 1432/* 1433* -\u0026gt;cpus_allowed is protected by both rq-\u0026gt;lock and p-\u0026gt;pi_lock 1434* 1435* A few notes on cpu_active vs cpu_online: 1436* 1437* - cpu_active must be a subset of cpu_online 1438* 1439* - on CPU-up we allow per-CPU kthreads on the online \u0026amp;\u0026amp; !active CPU, 1440* see __set_cpus_allowed_ptr(). At this point the newly online 1441* CPU isn\u0026#39;t yet part of the sched domains, and balancing will not 1442* see it. 1443* 1444* - on CPU-down we clear cpu_active() to mask the sched domains and 1445* avoid the load balancer to place new tasks on the to be removed 1446* CPU. Existing tasks will remain running there and will be taken 1447* off. 1448* 1449* This means that fallback selection must not select !active CPUs. 1450* And can assume that any active CPU must be online. Conversely 1451* select_task_rq() below may allow selection of !active CPUs in order 1452* to satisfy the above rules. 1453*/ 1454static int select_fallback_rq(int cpu, struct task_struct *p) 1455{ 1456\tint nid = cpu_to_node(cpu); 1457\tconst struct cpumask *nodemask = NULL; 1458\tenum { cpuset, possible, fail } state = cpuset; 1459\tint dest_cpu; 1460 1461\t/* 1462* If the node that the CPU is on has been offlined, cpu_to_node() 1463* will return -1. There is no CPU on the node, and we should 1464* select the CPU on the other node. 1465*/ 1466\tif (nid != -1) { 1467\tnodemask = cpumask_of_node(nid); 1468 1469\t/* Look for allowed, online CPU in same node. */ 1470\tfor_each_cpu(dest_cpu, nodemask) { 1471\tif (!cpu_active(dest_cpu)) 1472\tcontinue; 1473\tif (cpumask_test_cpu(dest_cpu, \u0026amp;p-\u0026gt;cpus_allowed)) 1474\treturn dest_cpu; 1475\t} 1476\t} 1477 1478\tfor (;;) { 1479\t/* Any allowed, online CPU? */ 1480\tfor_each_cpu(dest_cpu, \u0026amp;p-\u0026gt;cpus_allowed) { 1481\tif (!is_cpu_allowed(p, dest_cpu)) 1482\tcontinue; 1483 1484\tgoto out; 1485\t} 1486 1487\t/* No more Mr. Nice Guy. */ 1488\tswitch (state) { 1489\tcase cpuset: 1490\tif (IS_ENABLED(CONFIG_CPUSETS)) { 1491\tcpuset_cpus_allowed_fallback(p); 1492\tstate = possible; 1493\tbreak; 1494\t} 1495\t/* Fall-through */ 1496\tcase possible: 1497\tdo_set_cpus_allowed(p, cpu_possible_mask); 1498\tstate = fail; 1499\tbreak; 1500 1501\tcase fail: 1502\tBUG(); 1503\tbreak; 1504\t} 1505\t} 1506 1507out: 1508\tif (state != cpuset) { 1509\t/* 1510* Don\u0026#39;t tell them about moving exiting tasks or 1511* kernel threads (both mm NULL), since they never 1512* leave kernel. 1513*/ 1514\tif (p-\u0026gt;mm \u0026amp;\u0026amp; printk_ratelimit()) { 1515\tprintk_deferred(\u0026#34;process %d (%s) no longer affine to cpu%d\\n\u0026#34;, 1516\ttask_pid_nr(p), p-\u0026gt;comm, cpu); 1517\t} 1518\t} 1519 1520\treturn dest_cpu; 1521} 1522 1523/* 1524* The caller (fork, wakeup) owns p-\u0026gt;pi_lock, -\u0026gt;cpus_allowed is stable. 1525*/ 1526static inline 1527int select_task_rq(struct task_struct *p, int cpu, int sd_flags, int wake_flags) 1528{ 1529\tlockdep_assert_held(\u0026amp;p-\u0026gt;pi_lock); 1530 1531\tif (p-\u0026gt;nr_cpus_allowed \u0026gt; 1) 1532\tcpu = p-\u0026gt;sched_class-\u0026gt;select_task_rq(p, cpu, sd_flags, wake_flags); 1533\telse 1534\tcpu = cpumask_any(\u0026amp;p-\u0026gt;cpus_allowed); 1535 1536\t/* 1537* In order not to call set_task_cpu() on a blocking task we need 1538* to rely on ttwu() to place the task on a valid -\u0026gt;cpus_allowed 1539* CPU. 1540* 1541* Since this is common to all placement strategies, this lives here. 1542* 1543* [ this allows -\u0026gt;select_task() to simply return task_cpu(p) and 1544* not worry about this generic constraint ] 1545*/ 1546\tif (unlikely(!is_cpu_allowed(p, cpu))) 1547\tcpu = select_fallback_rq(task_cpu(p), p); 1548 1549\treturn cpu; 1550} 1551 1552static void update_avg(u64 *avg, u64 sample) 1553{ 1554\ts64 diff = sample - *avg; 1555\t*avg += diff \u0026gt;\u0026gt; 3; 1556} 1557 1558void sched_set_stop_task(int cpu, struct task_struct *stop) 1559{ 1560\tstruct sched_param param = { .sched_priority = MAX_RT_PRIO - 1 }; 1561\tstruct task_struct *old_stop = cpu_rq(cpu)-\u0026gt;stop; 1562 1563\tif (stop) { 1564\t/* 1565* Make it appear like a SCHED_FIFO task, its something 1566* userspace knows about and won\u0026#39;t get confused about. 1567* 1568* Also, it will make PI more or less work without too 1569* much confusion -- but then, stop work should not 1570* rely on PI working anyway. 1571*/ 1572\tsched_setscheduler_nocheck(stop, SCHED_FIFO, \u0026amp;param); 1573 1574\tstop-\u0026gt;sched_class = \u0026amp;stop_sched_class; 1575\t} 1576 1577\tcpu_rq(cpu)-\u0026gt;stop = stop; 1578 1579\tif (old_stop) { 1580\t/* 1581* Reset it back to a normal scheduling class so that 1582* it can die in pieces. 1583*/ 1584\told_stop-\u0026gt;sched_class = \u0026amp;rt_sched_class; 1585\t} 1586} 1587 1588#else 1589 1590static inline int __set_cpus_allowed_ptr(struct task_struct *p, 1591\tconst struct cpumask *new_mask, bool check) 1592{ 1593\treturn set_cpus_allowed_ptr(p, new_mask); 1594} 1595 1596#endif /* CONFIG_SMP */1597 1598static void 1599ttwu_stat(struct task_struct *p, int cpu, int wake_flags) 1600{ 1601\tstruct rq *rq; 1602 1603\tif (!schedstat_enabled()) 1604\treturn; 1605 1606\trq = this_rq(); 1607 1608#ifdef CONFIG_SMP 1609\tif (cpu == rq-\u0026gt;cpu) { 1610\t__schedstat_inc(rq-\u0026gt;ttwu_local); 1611\t__schedstat_inc(p-\u0026gt;se.statistics.nr_wakeups_local); 1612\t} else { 1613\tstruct sched_domain *sd; 1614 1615\t__schedstat_inc(p-\u0026gt;se.statistics.nr_wakeups_remote); 1616\trcu_read_lock(); 1617\tfor_each_domain(rq-\u0026gt;cpu, sd) { 1618\tif (cpumask_test_cpu(cpu, sched_domain_span(sd))) { 1619\t__schedstat_inc(sd-\u0026gt;ttwu_wake_remote); 1620\tbreak; 1621\t} 1622\t} 1623\trcu_read_unlock(); 1624\t} 1625 1626\tif (wake_flags \u0026amp; WF_MIGRATED) 1627\t__schedstat_inc(p-\u0026gt;se.statistics.nr_wakeups_migrate); 1628#endif /* CONFIG_SMP */1629 1630\t__schedstat_inc(rq-\u0026gt;ttwu_count); 1631\t__schedstat_inc(p-\u0026gt;se.statistics.nr_wakeups); 1632 1633\tif (wake_flags \u0026amp; WF_SYNC) 1634\t__schedstat_inc(p-\u0026gt;se.statistics.nr_wakeups_sync); 1635} 1636 1637static inline void ttwu_activate(struct rq *rq, struct task_struct *p, int en_flags) 1638{ 1639\tactivate_task(rq, p, en_flags); 1640\tp-\u0026gt;on_rq = TASK_ON_RQ_QUEUED; 1641 1642\t/* If a worker is waking up, notify the workqueue: */ 1643\tif (p-\u0026gt;flags \u0026amp; PF_WQ_WORKER) 1644\twq_worker_waking_up(p, cpu_of(rq)); 1645} 1646 1647/* 1648* Mark the task runnable and perform wakeup-preemption. 1649*/ 1650static void ttwu_do_wakeup(struct rq *rq, struct task_struct *p, int wake_flags, 1651\tstruct rq_flags *rf) 1652{ 1653\tcheck_preempt_curr(rq, p, wake_flags); 1654\tp-\u0026gt;state = TASK_RUNNING; 1655\ttrace_sched_wakeup(p); 1656 1657#ifdef CONFIG_SMP 1658\tif (p-\u0026gt;sched_class-\u0026gt;task_woken) { 1659\t/* 1660* Our task @p is fully woken up and running; so its safe to 1661* drop the rq-\u0026gt;lock, hereafter rq is only used for statistics. 1662*/ 1663\trq_unpin_lock(rq, rf); 1664\tp-\u0026gt;sched_class-\u0026gt;task_woken(rq, p); 1665\trq_repin_lock(rq, rf); 1666\t} 1667 1668\tif (rq-\u0026gt;idle_stamp) { 1669\tu64 delta = rq_clock(rq) - rq-\u0026gt;idle_stamp; 1670\tu64 max = 2*rq-\u0026gt;max_idle_balance_cost; 1671 1672\tupdate_avg(\u0026amp;rq-\u0026gt;avg_idle, delta); 1673 1674\tif (rq-\u0026gt;avg_idle \u0026gt; max) 1675\trq-\u0026gt;avg_idle = max; 1676 1677\trq-\u0026gt;idle_stamp = 0; 1678\t} 1679#endif 1680} 1681 1682static void 1683ttwu_do_activate(struct rq *rq, struct task_struct *p, int wake_flags, 1684\tstruct rq_flags *rf) 1685{ 1686\tint en_flags = ENQUEUE_WAKEUP | ENQUEUE_NOCLOCK; 1687 1688\tlockdep_assert_held(\u0026amp;rq-\u0026gt;lock); 1689 1690#ifdef CONFIG_SMP 1691\tif (p-\u0026gt;sched_contributes_to_load) 1692\trq-\u0026gt;nr_uninterruptible--; 1693 1694\tif (wake_flags \u0026amp; WF_MIGRATED) 1695\ten_flags |= ENQUEUE_MIGRATED; 1696#endif 1697 1698\tttwu_activate(rq, p, en_flags); 1699\tttwu_do_wakeup(rq, p, wake_flags, rf); 1700} 1701 1702/* 1703* Called in case the task @p isn\u0026#39;t fully descheduled from its runqueue, 1704* in this case we must do a remote wakeup. Its a \u0026#39;light\u0026#39; wakeup though, 1705* since all we need to do is flip p-\u0026gt;state to TASK_RUNNING, since 1706* the task is still -\u0026gt;on_rq. 1707*/ 1708static int ttwu_remote(struct task_struct *p, int wake_flags) 1709{ 1710\tstruct rq_flags rf; 1711\tstruct rq *rq; 1712\tint ret = 0; 1713 1714\trq = __task_rq_lock(p, \u0026amp;rf); 1715\tif (task_on_rq_queued(p)) { 1716\t/* check_preempt_curr() may use rq clock */ 1717\tupdate_rq_clock(rq); 1718\tttwu_do_wakeup(rq, p, wake_flags, \u0026amp;rf); 1719\tret = 1; 1720\t} 1721\t__task_rq_unlock(rq, \u0026amp;rf); 1722 1723\treturn ret; 1724} 1725 1726#ifdef CONFIG_SMP 1727void sched_ttwu_pending(void) 1728{ 1729\tstruct rq *rq = this_rq(); 1730\tstruct llist_node *llist = llist_del_all(\u0026amp;rq-\u0026gt;wake_list); 1731\tstruct task_struct *p, *t; 1732\tstruct rq_flags rf; 1733 1734\tif (!llist) 1735\treturn; 1736 1737\trq_lock_irqsave(rq, \u0026amp;rf); 1738\tupdate_rq_clock(rq); 1739 1740\tllist_for_each_entry_safe(p, t, llist, wake_entry) 1741\tttwu_do_activate(rq, p, p-\u0026gt;sched_remote_wakeup ? WF_MIGRATED : 0, \u0026amp;rf); 1742 1743\trq_unlock_irqrestore(rq, \u0026amp;rf); 1744} 1745 1746void scheduler_ipi(void) 1747{ 1748\t/* 1749* Fold TIF_NEED_RESCHED into the preempt_count; anybody setting 1750* TIF_NEED_RESCHED remotely (for the first time) will also send 1751* this IPI. 1752*/ 1753\tpreempt_fold_need_resched(); 1754 1755\tif (llist_empty(\u0026amp;this_rq()-\u0026gt;wake_list) \u0026amp;\u0026amp; !got_nohz_idle_kick()) 1756\treturn; 1757 1758\t/* 1759* Not all reschedule IPI handlers call irq_enter/irq_exit, since 1760* traditionally all their work was done from the interrupt return 1761* path. Now that we actually do some work, we need to make sure 1762* we do call them. 1763* 1764* Some archs already do call them, luckily irq_enter/exit nest 1765* properly. 1766* 1767* Arguably we should visit all archs and update all handlers, 1768* however a fair share of IPIs are still resched only so this would 1769* somewhat pessimize the simple resched case. 1770*/ 1771\tirq_enter(); 1772\tsched_ttwu_pending(); 1773 1774\t/* 1775* Check if someone kicked us for doing the nohz idle load balance. 1776*/ 1777\tif (unlikely(got_nohz_idle_kick())) { 1778\tthis_rq()-\u0026gt;idle_balance = 1; 1779\traise_softirq_irqoff(SCHED_SOFTIRQ); 1780\t} 1781\tirq_exit(); 1782} 1783 1784static void ttwu_queue_remote(struct task_struct *p, int cpu, int wake_flags) 1785{ 1786\tstruct rq *rq = cpu_rq(cpu); 1787 1788\tp-\u0026gt;sched_remote_wakeup = !!(wake_flags \u0026amp; WF_MIGRATED); 1789 1790\tif (llist_add(\u0026amp;p-\u0026gt;wake_entry, \u0026amp;cpu_rq(cpu)-\u0026gt;wake_list)) { 1791\tif (!set_nr_if_polling(rq-\u0026gt;idle)) 1792\tsmp_send_reschedule(cpu); 1793\telse 1794\ttrace_sched_wake_idle_without_ipi(cpu); 1795\t} 1796} 1797 1798void wake_up_if_idle(int cpu) 1799{ 1800\tstruct rq *rq = cpu_rq(cpu); 1801\tstruct rq_flags rf; 1802 1803\trcu_read_lock(); 1804 1805\tif (!is_idle_task(rcu_dereference(rq-\u0026gt;curr))) 1806\tgoto out; 1807 1808\tif (set_nr_if_polling(rq-\u0026gt;idle)) { 1809\ttrace_sched_wake_idle_without_ipi(cpu); 1810\t} else { 1811\trq_lock_irqsave(rq, \u0026amp;rf); 1812\tif (is_idle_task(rq-\u0026gt;curr)) 1813\tsmp_send_reschedule(cpu); 1814\t/* Else CPU is not idle, do nothing here: */ 1815\trq_unlock_irqrestore(rq, \u0026amp;rf); 1816\t} 1817 1818out: 1819\trcu_read_unlock(); 1820} 1821 1822bool cpus_share_cache(int this_cpu, int that_cpu) 1823{ 1824\treturn per_cpu(sd_llc_id, this_cpu) == per_cpu(sd_llc_id, that_cpu); 1825} 1826#endif /* CONFIG_SMP */1827 1828static void ttwu_queue(struct task_struct *p, int cpu, int wake_flags) 1829{ 1830\tstruct rq *rq = cpu_rq(cpu); 1831\tstruct rq_flags rf; 1832 1833#if defined(CONFIG_SMP) 1834\tif (sched_feat(TTWU_QUEUE) \u0026amp;\u0026amp; !cpus_share_cache(smp_processor_id(), cpu)) { 1835\tsched_clock_cpu(cpu); /* Sync clocks across CPUs */ 1836\tttwu_queue_remote(p, cpu, wake_flags); 1837\treturn; 1838\t} 1839#endif 1840 1841\trq_lock(rq, \u0026amp;rf); 1842\tupdate_rq_clock(rq); 1843\tttwu_do_activate(rq, p, wake_flags, \u0026amp;rf); 1844\trq_unlock(rq, \u0026amp;rf); 1845} 1846 1847/* 1848* Notes on Program-Order guarantees on SMP systems. 1849* 1850* MIGRATION 1851* 1852* The basic program-order guarantee on SMP systems is that when a task [t] 1853* migrates, all its activity on its old CPU [c0] happens-before any subsequent 1854* execution on its new CPU [c1]. 1855* 1856* For migration (of runnable tasks) this is provided by the following means: 1857* 1858* A) UNLOCK of the rq(c0)-\u0026gt;lock scheduling out task t 1859* B) migration for t is required to synchronize *both* rq(c0)-\u0026gt;lock and 1860* rq(c1)-\u0026gt;lock (if not at the same time, then in that order). 1861* C) LOCK of the rq(c1)-\u0026gt;lock scheduling in task 1862* 1863* Release/acquire chaining guarantees that B happens after A and C after B. 1864* Note: the CPU doing B need not be c0 or c1 1865* 1866* Example: 1867* 1868* CPU0 CPU1 CPU2 1869* 1870* LOCK rq(0)-\u0026gt;lock 1871* sched-out X 1872* sched-in Y 1873* UNLOCK rq(0)-\u0026gt;lock 1874* 1875* LOCK rq(0)-\u0026gt;lock // orders against CPU0 1876* dequeue X 1877* UNLOCK rq(0)-\u0026gt;lock 1878* 1879* LOCK rq(1)-\u0026gt;lock 1880* enqueue X 1881* UNLOCK rq(1)-\u0026gt;lock 1882* 1883* LOCK rq(1)-\u0026gt;lock // orders against CPU2 1884* sched-out Z 1885* sched-in X 1886* UNLOCK rq(1)-\u0026gt;lock 1887* 1888* 1889* BLOCKING -- aka. SLEEP + WAKEUP 1890* 1891* For blocking we (obviously) need to provide the same guarantee as for 1892* migration. However the means are completely different as there is no lock 1893* chain to provide order. Instead we do: 1894* 1895* 1) smp_store_release(X-\u0026gt;on_cpu, 0) 1896* 2) smp_cond_load_acquire(!X-\u0026gt;on_cpu) 1897* 1898* Example: 1899* 1900* CPU0 (schedule) CPU1 (try_to_wake_up) CPU2 (schedule) 1901* 1902* LOCK rq(0)-\u0026gt;lock LOCK X-\u0026gt;pi_lock 1903* dequeue X 1904* sched-out X 1905* smp_store_release(X-\u0026gt;on_cpu, 0); 1906* 1907* smp_cond_load_acquire(\u0026amp;X-\u0026gt;on_cpu, !VAL); 1908* X-\u0026gt;state = WAKING 1909* set_task_cpu(X,2) 1910* 1911* LOCK rq(2)-\u0026gt;lock 1912* enqueue X 1913* X-\u0026gt;state = RUNNING 1914* UNLOCK rq(2)-\u0026gt;lock 1915* 1916* LOCK rq(2)-\u0026gt;lock // orders against CPU1 1917* sched-out Z 1918* sched-in X 1919* UNLOCK rq(2)-\u0026gt;lock 1920* 1921* UNLOCK X-\u0026gt;pi_lock 1922* UNLOCK rq(0)-\u0026gt;lock 1923* 1924* 1925* However, for wakeups there is a second guarantee we must provide, namely we 1926* must ensure that CONDITION=1 done by the caller can not be reordered with 1927* accesses to the task state; see try_to_wake_up() and set_current_state(). 1928*/ 1929 1930/** 1931* try_to_wake_up - wake up a thread 1932* @p: the thread to be awakened 1933* @state: the mask of task states that can be woken 1934* @wake_flags: wake modifier flags (WF_*) 1935* 1936* If (@state \u0026amp; @p-\u0026gt;state) @p-\u0026gt;state = TASK_RUNNING. 1937* 1938* If the task was not queued/runnable, also place it back on a runqueue. 1939* 1940* Atomic against schedule() which would dequeue a task, also see 1941* set_current_state(). 1942* 1943* This function executes a full memory barrier before accessing the task 1944* state; see set_current_state(). 1945* 1946* Return: %true if @p-\u0026gt;state changes (an actual wakeup was done), 1947*\t%false otherwise. 1948*/ 1949static int 1950try_to_wake_up(struct task_struct *p, unsigned int state, int wake_flags) 1951{ 1952\tunsigned long flags; 1953\tint cpu, success = 0; 1954 1955\t/* 1956* If we are going to wake up a thread waiting for CONDITION we 1957* need to ensure that CONDITION=1 done by the caller can not be 1958* reordered with p-\u0026gt;state check below. This pairs with mb() in 1959* set_current_state() the waiting thread does. 1960*/ 1961\traw_spin_lock_irqsave(\u0026amp;p-\u0026gt;pi_lock, flags); 1962\tsmp_mb__after_spinlock(); 1963\tif (!(p-\u0026gt;state \u0026amp; state)) 1964\tgoto out; 1965 1966\ttrace_sched_waking(p); 1967 1968\t/* We\u0026#39;re going to change -\u0026gt;state: */ 1969\tsuccess = 1; 1970\tcpu = task_cpu(p); 1971 1972\t/* 1973* Ensure we load p-\u0026gt;on_rq _after_ p-\u0026gt;state, otherwise it would 1974* be possible to, falsely, observe p-\u0026gt;on_rq == 0 and get stuck 1975* in smp_cond_load_acquire() below. 1976* 1977* sched_ttwu_pending()\ttry_to_wake_up() 1978* STORE p-\u0026gt;on_rq = 1\tLOAD p-\u0026gt;state 1979* UNLOCK rq-\u0026gt;lock 1980* 1981* __schedule() (switch to task \u0026#39;p\u0026#39;) 1982* LOCK rq-\u0026gt;lock\tsmp_rmb(); 1983* smp_mb__after_spinlock(); 1984* UNLOCK rq-\u0026gt;lock 1985* 1986* [task p] 1987* STORE p-\u0026gt;state = UNINTERRUPTIBLE\tLOAD p-\u0026gt;on_rq 1988* 1989* Pairs with the LOCK+smp_mb__after_spinlock() on rq-\u0026gt;lock in 1990* __schedule(). See the comment for smp_mb__after_spinlock(). 1991*/ 1992\tsmp_rmb(); 1993\tif (p-\u0026gt;on_rq \u0026amp;\u0026amp; ttwu_remote(p, wake_flags)) 1994\tgoto stat; 1995 1996#ifdef CONFIG_SMP 1997\t/* 1998* Ensure we load p-\u0026gt;on_cpu _after_ p-\u0026gt;on_rq, otherwise it would be 1999* possible to, falsely, observe p-\u0026gt;on_cpu == 0. 2000* 2001* One must be running (-\u0026gt;on_cpu == 1) in order to remove oneself 2002* from the runqueue. 2003* 2004* __schedule() (switch to task \u0026#39;p\u0026#39;)\ttry_to_wake_up() 2005* STORE p-\u0026gt;on_cpu = 1\tLOAD p-\u0026gt;on_rq 2006* UNLOCK rq-\u0026gt;lock 2007* 2008* __schedule() (put \u0026#39;p\u0026#39; to sleep) 2009* LOCK rq-\u0026gt;lock\tsmp_rmb(); 2010* smp_mb__after_spinlock(); 2011* STORE p-\u0026gt;on_rq = 0\tLOAD p-\u0026gt;on_cpu 2012* 2013* Pairs with the LOCK+smp_mb__after_spinlock() on rq-\u0026gt;lock in 2014* __schedule(). See the comment for smp_mb__after_spinlock(). 2015*/ 2016\tsmp_rmb(); 2017 2018\t/* 2019* If the owning (remote) CPU is still in the middle of schedule() with 2020* this task as prev, wait until its done referencing the task. 2021* 2022* Pairs with the smp_store_release() in finish_task(). 2023* 2024* This ensures that tasks getting woken will be fully ordered against 2025* their previous state and preserve Program Order. 2026*/ 2027\tsmp_cond_load_acquire(\u0026amp;p-\u0026gt;on_cpu, !VAL); 2028 2029\tp-\u0026gt;sched_contributes_to_load = !!task_contributes_to_load(p); 2030\tp-\u0026gt;state = TASK_WAKING; 2031 2032\tif (p-\u0026gt;in_iowait) { 2033\tdelayacct_blkio_end(p); 2034\tatomic_dec(\u0026amp;task_rq(p)-\u0026gt;nr_iowait); 2035\t} 2036 2037\tcpu = select_task_rq(p, p-\u0026gt;wake_cpu, SD_BALANCE_WAKE, wake_flags); 2038\tif (task_cpu(p) != cpu) { 2039\twake_flags |= WF_MIGRATED; 2040\tset_task_cpu(p, cpu); 2041\t} 2042 2043#else /* CONFIG_SMP */2044 2045\tif (p-\u0026gt;in_iowait) { 2046\tdelayacct_blkio_end(p); 2047\tatomic_dec(\u0026amp;task_rq(p)-\u0026gt;nr_iowait); 2048\t} 2049 2050#endif /* CONFIG_SMP */2051 2052\tttwu_queue(p, cpu, wake_flags); 2053stat: 2054\tttwu_stat(p, cpu, wake_flags); 2055out: 2056\traw_spin_unlock_irqrestore(\u0026amp;p-\u0026gt;pi_lock, flags); 2057 2058\treturn success; 2059} 2060 2061/** 2062* try_to_wake_up_local - try to wake up a local task with rq lock held 2063* @p: the thread to be awakened 2064* @rf: request-queue flags for pinning 2065* 2066* Put @p on the run-queue if it\u0026#39;s not already there. The caller must 2067* ensure that this_rq() is locked, @p is bound to this_rq() and not 2068* the current task. 2069*/ 2070static void try_to_wake_up_local(struct task_struct *p, struct rq_flags *rf) 2071{ 2072\tstruct rq *rq = task_rq(p); 2073 2074\tif (WARN_ON_ONCE(rq != this_rq()) || 2075\tWARN_ON_ONCE(p == current)) 2076\treturn; 2077 2078\tlockdep_assert_held(\u0026amp;rq-\u0026gt;lock); 2079 2080\tif (!raw_spin_trylock(\u0026amp;p-\u0026gt;pi_lock)) { 2081\t/* 2082* This is OK, because current is on_cpu, which avoids it being 2083* picked for load-balance and preemption/IRQs are still 2084* disabled avoiding further scheduler activity on it and we\u0026#39;ve 2085* not yet picked a replacement task. 2086*/ 2087\trq_unlock(rq, rf); 2088\traw_spin_lock(\u0026amp;p-\u0026gt;pi_lock); 2089\trq_relock(rq, rf); 2090\t} 2091 2092\tif (!(p-\u0026gt;state \u0026amp; TASK_NORMAL)) 2093\tgoto out; 2094 2095\ttrace_sched_waking(p); 2096 2097\tif (!task_on_rq_queued(p)) { 2098\tif (p-\u0026gt;in_iowait) { 2099\tdelayacct_blkio_end(p); 2100\tatomic_dec(\u0026amp;rq-\u0026gt;nr_iowait); 2101\t} 2102\tttwu_activate(rq, p, ENQUEUE_WAKEUP | ENQUEUE_NOCLOCK); 2103\t} 2104 2105\tttwu_do_wakeup(rq, p, 0, rf); 2106\tttwu_stat(p, smp_processor_id(), 0); 2107out: 2108\traw_spin_unlock(\u0026amp;p-\u0026gt;pi_lock); 2109} 2110 2111/** 2112* wake_up_process - Wake up a specific process 2113* @p: The process to be woken up. 2114* 2115* Attempt to wake up the nominated process and move it to the set of runnable 2116* processes. 2117* 2118* Return: 1 if the process was woken up, 0 if it was already running. 2119* 2120* This function executes a full memory barrier before accessing the task state. 2121*/ 2122int wake_up_process(struct task_struct *p) 2123{ 2124\treturn try_to_wake_up(p, TASK_NORMAL, 0); 2125} 2126EXPORT_SYMBOL(wake_up_process); 2127 2128int wake_up_state(struct task_struct *p, unsigned int state) 2129{ 2130\treturn try_to_wake_up(p, state, 0); 2131} 2132 2133/* 2134* Perform scheduler related setup for a newly forked process p. 2135* p is forked by current. 2136* 2137* __sched_fork() is basic setup used by init_idle() too: 2138*/ 2139static void __sched_fork(unsigned long clone_flags, struct task_struct *p) 2140{ 2141\tp-\u0026gt;on_rq\t= 0; 2142 2143\tp-\u0026gt;se.on_rq\t= 0; 2144\tp-\u0026gt;se.exec_start\t= 0; 2145\tp-\u0026gt;se.sum_exec_runtime\t= 0; 2146\tp-\u0026gt;se.prev_sum_exec_runtime\t= 0; 2147\tp-\u0026gt;se.nr_migrations\t= 0; 2148\tp-\u0026gt;se.vruntime\t= 0; 2149\tINIT_LIST_HEAD(\u0026amp;p-\u0026gt;se.group_node); 2150 2151#ifdef CONFIG_FAIR_GROUP_SCHED 2152\tp-\u0026gt;se.cfs_rq\t= NULL; 2153#endif 2154 2155#ifdef CONFIG_SCHEDSTATS 2156\t/* Even if schedstat is disabled, there should not be garbage */ 2157\tmemset(\u0026amp;p-\u0026gt;se.statistics, 0, sizeof(p-\u0026gt;se.statistics)); 2158#endif 2159 2160\tRB_CLEAR_NODE(\u0026amp;p-\u0026gt;dl.rb_node); 2161\tinit_dl_task_timer(\u0026amp;p-\u0026gt;dl); 2162\tinit_dl_inactive_task_timer(\u0026amp;p-\u0026gt;dl); 2163\t__dl_clear_params(p); 2164 2165\tINIT_LIST_HEAD(\u0026amp;p-\u0026gt;rt.run_list); 2166\tp-\u0026gt;rt.timeout\t= 0; 2167\tp-\u0026gt;rt.time_slice\t= sched_rr_timeslice; 2168\tp-\u0026gt;rt.on_rq\t= 0; 2169\tp-\u0026gt;rt.on_list\t= 0; 2170 2171#ifdef CONFIG_PREEMPT_NOTIFIERS 2172\tINIT_HLIST_HEAD(\u0026amp;p-\u0026gt;preempt_notifiers); 2173#endif 2174 2175\tinit_numa_balancing(clone_flags, p); 2176} 2177 2178DEFINE_STATIC_KEY_FALSE(sched_numa_balancing); 2179 2180#ifdef CONFIG_NUMA_BALANCING 2181 2182void set_numabalancing_state(bool enabled) 2183{ 2184\tif (enabled) 2185\tstatic_branch_enable(\u0026amp;sched_numa_balancing); 2186\telse 2187\tstatic_branch_disable(\u0026amp;sched_numa_balancing); 2188} 2189 2190#ifdef CONFIG_PROC_SYSCTL 2191int sysctl_numa_balancing(struct ctl_table *table, int write, 2192\tvoid __user *buffer, size_t *lenp, loff_t *ppos) 2193{ 2194\tstruct ctl_table t; 2195\tint err; 2196\tint state = static_branch_likely(\u0026amp;sched_numa_balancing); 2197 2198\tif (write \u0026amp;\u0026amp; !capable(CAP_SYS_ADMIN)) 2199\treturn -EPERM; 2200 2201\tt = *table; 2202\tt.data = \u0026amp;state; 2203\terr = proc_dointvec_minmax(\u0026amp;t, write, buffer, lenp, ppos); 2204\tif (err \u0026lt; 0) 2205\treturn err; 2206\tif (write) 2207\tset_numabalancing_state(state); 2208\treturn err; 2209} 2210#endif 2211#endif 2212 2213#ifdef CONFIG_SCHEDSTATS 2214 2215DEFINE_STATIC_KEY_FALSE(sched_schedstats); 2216static bool __initdata __sched_schedstats = false; 2217 2218static void set_schedstats(bool enabled) 2219{ 2220\tif (enabled) 2221\tstatic_branch_enable(\u0026amp;sched_schedstats); 2222\telse 2223\tstatic_branch_disable(\u0026amp;sched_schedstats); 2224} 2225 2226void force_schedstat_enabled(void) 2227{ 2228\tif (!schedstat_enabled()) { 2229\tpr_info(\u0026#34;kernel profiling enabled schedstats, disable via kernel.sched_schedstats.\\n\u0026#34;); 2230\tstatic_branch_enable(\u0026amp;sched_schedstats); 2231\t} 2232} 2233 2234static int __init setup_schedstats(char *str) 2235{ 2236\tint ret = 0; 2237\tif (!str) 2238\tgoto out; 2239 2240\t/* 2241* This code is called before jump labels have been set up, so we can\u0026#39;t 2242* change the static branch directly just yet. Instead set a temporary 2243* variable so init_schedstats() can do it later. 2244*/ 2245\tif (!strcmp(str, \u0026#34;enable\u0026#34;)) { 2246\t__sched_schedstats = true; 2247\tret = 1; 2248\t} else if (!strcmp(str, \u0026#34;disable\u0026#34;)) { 2249\t__sched_schedstats = false; 2250\tret = 1; 2251\t} 2252out: 2253\tif (!ret) 2254\tpr_warn(\u0026#34;Unable to parse schedstats=\\n\u0026#34;); 2255 2256\treturn ret; 2257} 2258__setup(\u0026#34;schedstats=\u0026#34;, setup_schedstats); 2259 2260static void __init init_schedstats(void) 2261{ 2262\tset_schedstats(__sched_schedstats); 2263} 2264 2265#ifdef CONFIG_PROC_SYSCTL 2266int sysctl_schedstats(struct ctl_table *table, int write, 2267\tvoid __user *buffer, size_t *lenp, loff_t *ppos) 2268{ 2269\tstruct ctl_table t; 2270\tint err; 2271\tint state = static_branch_likely(\u0026amp;sched_schedstats); 2272 2273\tif (write \u0026amp;\u0026amp; !capable(CAP_SYS_ADMIN)) 2274\treturn -EPERM; 2275 2276\tt = *table; 2277\tt.data = \u0026amp;state; 2278\terr = proc_dointvec_minmax(\u0026amp;t, write, buffer, lenp, ppos); 2279\tif (err \u0026lt; 0) 2280\treturn err; 2281\tif (write) 2282\tset_schedstats(state); 2283\treturn err; 2284} 2285#endif /* CONFIG_PROC_SYSCTL */2286#else /* !CONFIG_SCHEDSTATS */2287static inline void init_schedstats(void) {} 2288#endif /* CONFIG_SCHEDSTATS */2289 2290/* 2291* fork()/clone()-time setup: 2292*/ 2293int sched_fork(unsigned long clone_flags, struct task_struct *p) 2294{ 2295\tunsigned long flags; 2296 2297\t__sched_fork(clone_flags, p); 2298\t/* 2299* We mark the process as NEW here. This guarantees that 2300* nobody will actually run it, and a signal or other external 2301* event cannot wake it up and insert it on the runqueue either. 2302*/ 2303\tp-\u0026gt;state = TASK_NEW; 2304 2305\t/* 2306* Make sure we do not leak PI boosting priority to the child. 2307*/ 2308\tp-\u0026gt;prio = current-\u0026gt;normal_prio; 2309 2310\t/* 2311* Revert to default priority/policy on fork if requested. 2312*/ 2313\tif (unlikely(p-\u0026gt;sched_reset_on_fork)) { 2314\tif (task_has_dl_policy(p) || task_has_rt_policy(p)) { 2315\tp-\u0026gt;policy = SCHED_NORMAL; 2316\tp-\u0026gt;static_prio = NICE_TO_PRIO(0); 2317\tp-\u0026gt;rt_priority = 0; 2318\t} else if (PRIO_TO_NICE(p-\u0026gt;static_prio) \u0026lt; 0) 2319\tp-\u0026gt;static_prio = NICE_TO_PRIO(0); 2320 2321\tp-\u0026gt;prio = p-\u0026gt;normal_prio = __normal_prio(p); 2322\tset_load_weight(p, false); 2323 2324\t/* 2325* We don\u0026#39;t need the reset flag anymore after the fork. It has 2326* fulfilled its duty: 2327*/ 2328\tp-\u0026gt;sched_reset_on_fork = 0; 2329\t} 2330 2331\tif (dl_prio(p-\u0026gt;prio)) 2332\treturn -EAGAIN; 2333\telse if (rt_prio(p-\u0026gt;prio)) 2334\tp-\u0026gt;sched_class = \u0026amp;rt_sched_class; 2335\telse 2336\tp-\u0026gt;sched_class = \u0026amp;fair_sched_class; 2337 2338\tinit_entity_runnable_average(\u0026amp;p-\u0026gt;se); 2339 2340\t/* 2341* The child is not yet in the pid-hash so no cgroup attach races, 2342* and the cgroup is pinned to this child due to cgroup_fork() 2343* is ran before sched_fork(). 2344* 2345* Silence PROVE_RCU. 2346*/ 2347\traw_spin_lock_irqsave(\u0026amp;p-\u0026gt;pi_lock, flags); 2348\trseq_migrate(p); 2349\t/* 2350* We\u0026#39;re setting the CPU for the first time, we don\u0026#39;t migrate, 2351* so use __set_task_cpu(). 2352*/ 2353\t__set_task_cpu(p, smp_processor_id()); 2354\tif (p-\u0026gt;sched_class-\u0026gt;task_fork) 2355\tp-\u0026gt;sched_class-\u0026gt;task_fork(p); 2356\traw_spin_unlock_irqrestore(\u0026amp;p-\u0026gt;pi_lock, flags); 2357 2358#ifdef CONFIG_SCHED_INFO 2359\tif (likely(sched_info_on())) 2360\tmemset(\u0026amp;p-\u0026gt;sched_info, 0, sizeof(p-\u0026gt;sched_info)); 2361#endif 2362#if defined(CONFIG_SMP) 2363\tp-\u0026gt;on_cpu = 0; 2364#endif 2365\tinit_task_preempt_count(p); 2366#ifdef CONFIG_SMP 2367\tplist_node_init(\u0026amp;p-\u0026gt;pushable_tasks, MAX_PRIO); 2368\tRB_CLEAR_NODE(\u0026amp;p-\u0026gt;pushable_dl_tasks); 2369#endif 2370\treturn 0; 2371} 2372 2373unsigned long to_ratio(u64 period, u64 runtime) 2374{ 2375\tif (runtime == RUNTIME_INF) 2376\treturn BW_UNIT; 2377 2378\t/* 2379* Doing this here saves a lot of checks in all 2380* the calling paths, and returning zero seems 2381* safe for them anyway. 2382*/ 2383\tif (period == 0) 2384\treturn 0; 2385 2386\treturn div64_u64(runtime \u0026lt;\u0026lt; BW_SHIFT, period); 2387} 2388 2389/* 2390* wake_up_new_task - wake up a newly created task for the first time. 2391* 2392* This function will do some initial scheduler statistics housekeeping 2393* that must be done for every newly created context, then puts the task 2394* on the runqueue and wakes it. 2395*/ 2396void wake_up_new_task(struct task_struct *p) 2397{ 2398\tstruct rq_flags rf; 2399\tstruct rq *rq; 2400 2401\traw_spin_lock_irqsave(\u0026amp;p-\u0026gt;pi_lock, rf.flags); 2402\tp-\u0026gt;state = TASK_RUNNING; 2403#ifdef CONFIG_SMP 2404\t/* 2405* Fork balancing, do it here and not earlier because: 2406* - cpus_allowed can change in the fork path 2407* - any previously selected CPU might disappear through hotplug 2408* 2409* Use __set_task_cpu() to avoid calling sched_class::migrate_task_rq, 2410* as we\u0026#39;re not fully set-up yet. 2411*/ 2412\tp-\u0026gt;recent_used_cpu = task_cpu(p); 2413\trseq_migrate(p); 2414\t__set_task_cpu(p, select_task_rq(p, task_cpu(p), SD_BALANCE_FORK, 0)); 2415#endif 2416\trq = __task_rq_lock(p, \u0026amp;rf); 2417\tupdate_rq_clock(rq); 2418\tpost_init_entity_util_avg(\u0026amp;p-\u0026gt;se); 2419 2420\tactivate_task(rq, p, ENQUEUE_NOCLOCK); 2421\tp-\u0026gt;on_rq = TASK_ON_RQ_QUEUED; 2422\ttrace_sched_wakeup_new(p); 2423\tcheck_preempt_curr(rq, p, WF_FORK); 2424#ifdef CONFIG_SMP 2425\tif (p-\u0026gt;sched_class-\u0026gt;task_woken) { 2426\t/* 2427* Nothing relies on rq-\u0026gt;lock after this, so its fine to 2428* drop it. 2429*/ 2430\trq_unpin_lock(rq, \u0026amp;rf); 2431\tp-\u0026gt;sched_class-\u0026gt;task_woken(rq, p); 2432\trq_repin_lock(rq, \u0026amp;rf); 2433\t} 2434#endif 2435\ttask_rq_unlock(rq, p, \u0026amp;rf); 2436} 2437 2438#ifdef CONFIG_PREEMPT_NOTIFIERS 2439 2440static DEFINE_STATIC_KEY_FALSE(preempt_notifier_key); 2441 2442void preempt_notifier_inc(void) 2443{ 2444\tstatic_branch_inc(\u0026amp;preempt_notifier_key); 2445} 2446EXPORT_SYMBOL_GPL(preempt_notifier_inc); 2447 2448void preempt_notifier_dec(void) 2449{ 2450\tstatic_branch_dec(\u0026amp;preempt_notifier_key); 2451} 2452EXPORT_SYMBOL_GPL(preempt_notifier_dec); 2453 2454/** 2455* preempt_notifier_register - tell me when current is being preempted \u0026amp; rescheduled 2456* @notifier: notifier struct to register 2457*/ 2458void preempt_notifier_register(struct preempt_notifier *notifier) 2459{ 2460\tif (!static_branch_unlikely(\u0026amp;preempt_notifier_key)) 2461\tWARN(1, \u0026#34;registering preempt_notifier while notifiers disabled\\n\u0026#34;); 2462 2463\thlist_add_head(\u0026amp;notifier-\u0026gt;link, \u0026amp;current-\u0026gt;preempt_notifiers); 2464} 2465EXPORT_SYMBOL_GPL(preempt_notifier_register); 2466 2467/** 2468* preempt_notifier_unregister - no longer interested in preemption notifications 2469* @notifier: notifier struct to unregister 2470* 2471* This is *not* safe to call from within a preemption notifier. 2472*/ 2473void preempt_notifier_unregister(struct preempt_notifier *notifier) 2474{ 2475\thlist_del(\u0026amp;notifier-\u0026gt;link); 2476} 2477EXPORT_SYMBOL_GPL(preempt_notifier_unregister); 2478 2479static void __fire_sched_in_preempt_notifiers(struct task_struct *curr) 2480{ 2481\tstruct preempt_notifier *notifier; 2482 2483\thlist_for_each_entry(notifier, \u0026amp;curr-\u0026gt;preempt_notifiers, link) 2484\tnotifier-\u0026gt;ops-\u0026gt;sched_in(notifier, raw_smp_processor_id()); 2485} 2486 2487static __always_inline void fire_sched_in_preempt_notifiers(struct task_struct *curr) 2488{ 2489\tif (static_branch_unlikely(\u0026amp;preempt_notifier_key)) 2490\t__fire_sched_in_preempt_notifiers(curr); 2491} 2492 2493static void 2494__fire_sched_out_preempt_notifiers(struct task_struct *curr, 2495\tstruct task_struct *next) 2496{ 2497\tstruct preempt_notifier *notifier; 2498 2499\thlist_for_each_entry(notifier, \u0026amp;curr-\u0026gt;preempt_notifiers, link) 2500\tnotifier-\u0026gt;ops-\u0026gt;sched_out(notifier, next); 2501} 2502 2503static __always_inline void 2504fire_sched_out_preempt_notifiers(struct task_struct *curr, 2505\tstruct task_struct *next) 2506{ 2507\tif (static_branch_unlikely(\u0026amp;preempt_notifier_key)) 2508\t__fire_sched_out_preempt_notifiers(curr, next); 2509} 2510 2511#else /* !CONFIG_PREEMPT_NOTIFIERS */2512 2513static inline void fire_sched_in_preempt_notifiers(struct task_struct *curr) 2514{ 2515} 2516 2517static inline void 2518fire_sched_out_preempt_notifiers(struct task_struct *curr, 2519\tstruct task_struct *next) 2520{ 2521} 2522 2523#endif /* CONFIG_PREEMPT_NOTIFIERS */2524 2525static inline void prepare_task(struct task_struct *next) 2526{ 2527#ifdef CONFIG_SMP 2528\t/* 2529* Claim the task as running, we do this before switching to it 2530* such that any running task will have this set. 2531*/ 2532\tnext-\u0026gt;on_cpu = 1; 2533#endif 2534} 2535 2536static inline void finish_task(struct task_struct *prev) 2537{ 2538#ifdef CONFIG_SMP 2539\t/* 2540* After -\u0026gt;on_cpu is cleared, the task can be moved to a different CPU. 2541* We must ensure this doesn\u0026#39;t happen until the switch is completely 2542* finished. 2543* 2544* In particular, the load of prev-\u0026gt;state in finish_task_switch() must 2545* happen before this. 2546* 2547* Pairs with the smp_cond_load_acquire() in try_to_wake_up(). 2548*/ 2549\tsmp_store_release(\u0026amp;prev-\u0026gt;on_cpu, 0); 2550#endif 2551} 2552 2553static inline void 2554prepare_lock_switch(struct rq *rq, struct task_struct *next, struct rq_flags *rf) 2555{ 2556\t/* 2557* Since the runqueue lock will be released by the next 2558* task (which is an invalid locking op but in the case 2559* of the scheduler it\u0026#39;s an obvious special-case), so we 2560* do an early lockdep release here: 2561*/ 2562\trq_unpin_lock(rq, rf); 2563\tspin_release(\u0026amp;rq-\u0026gt;lock.dep_map, 1, _THIS_IP_); 2564#ifdef CONFIG_DEBUG_SPINLOCK 2565\t/* this is a valid case when another task releases the spinlock */ 2566\trq-\u0026gt;lock.owner = next; 2567#endif 2568} 2569 2570static inline void finish_lock_switch(struct rq *rq) 2571{ 2572\t/* 2573* If we are tracking spinlock dependencies then we have to 2574* fix up the runqueue lock - which gets \u0026#39;carried over\u0026#39; from 2575* prev into current: 2576*/ 2577\tspin_acquire(\u0026amp;rq-\u0026gt;lock.dep_map, 0, 0, _THIS_IP_); 2578\traw_spin_unlock_irq(\u0026amp;rq-\u0026gt;lock); 2579} 2580 2581/* 2582* NOP if the arch has not defined these: 2583*/ 2584 2585#ifndef prepare_arch_switch 2586# define prepare_arch_switch(next)\tdo { } while (0) 2587#endif 2588 2589#ifndef finish_arch_post_lock_switch 2590# define finish_arch_post_lock_switch()\tdo { } while (0) 2591#endif 2592 2593/** 2594* prepare_task_switch - prepare to switch tasks 2595* @rq: the runqueue preparing to switch 2596* @prev: the current task that is being switched out 2597* @next: the task we are going to switch to. 2598* 2599* This is called with the rq lock held and interrupts off. It must 2600* be paired with a subsequent finish_task_switch after the context 2601* switch. 2602* 2603* prepare_task_switch sets up locking and calls architecture specific 2604* hooks. 2605*/ 2606static inline void 2607prepare_task_switch(struct rq *rq, struct task_struct *prev, 2608\tstruct task_struct *next) 2609{ 2610\tkcov_prepare_switch(prev); 2611\tsched_info_switch(rq, prev, next); 2612\tperf_event_task_sched_out(prev, next); 2613\trseq_preempt(prev); 2614\tfire_sched_out_preempt_notifiers(prev, next); 2615\tprepare_task(next); 2616\tprepare_arch_switch(next); 2617} 2618 2619/** 2620* finish_task_switch - clean up after a task-switch 2621* @prev: the thread we just switched away from. 2622* 2623* finish_task_switch must be called after the context switch, paired 2624* with a prepare_task_switch call before the context switch. 2625* finish_task_switch will reconcile locking set up by prepare_task_switch, 2626* and do any other architecture-specific cleanup actions. 2627* 2628* Note that we may have delayed dropping an mm in context_switch(). If 2629* so, we finish that here outside of the runqueue lock. (Doing it 2630* with the lock held can cause deadlocks; see schedule() for 2631* details.) 2632* 2633* The context switch have flipped the stack from under us and restored the 2634* local variables which were saved when this task called schedule() in the 2635* past. prev == current is still correct but we need to recalculate this_rq 2636* because prev may have moved to another CPU. 2637*/ 2638static struct rq *finish_task_switch(struct task_struct *prev) 2639\t__releases(rq-\u0026gt;lock) 2640{ 2641\tstruct rq *rq = this_rq(); 2642\tstruct mm_struct *mm = rq-\u0026gt;prev_mm; 2643\tlong prev_state; 2644 2645\t/* 2646* The previous task will have left us with a preempt_count of 2 2647* because it left us after: 2648* 2649*\tschedule() 2650*\tpreempt_disable();\t// 1 2651*\t__schedule() 2652*\traw_spin_lock_irq(\u0026amp;rq-\u0026gt;lock)\t// 2 2653* 2654* Also, see FORK_PREEMPT_COUNT. 2655*/ 2656\tif (WARN_ONCE(preempt_count() != 2*PREEMPT_DISABLE_OFFSET, 2657\t\u0026#34;corrupted preempt_count: %s/%d/0x%x\\n\u0026#34;, 2658\tcurrent-\u0026gt;comm, current-\u0026gt;pid, preempt_count())) 2659\tpreempt_count_set(FORK_PREEMPT_COUNT); 2660 2661\trq-\u0026gt;prev_mm = NULL; 2662 2663\t/* 2664* A task struct has one reference for the use as \u0026#34;current\u0026#34;. 2665* If a task dies, then it sets TASK_DEAD in tsk-\u0026gt;state and calls 2666* schedule one last time. The schedule call will never return, and 2667* the scheduled task must drop that reference. 2668* 2669* We must observe prev-\u0026gt;state before clearing prev-\u0026gt;on_cpu (in 2670* finish_task), otherwise a concurrent wakeup can get prev 2671* running on another CPU and we could rave with its RUNNING -\u0026gt; DEAD 2672* transition, resulting in a double drop. 2673*/ 2674\tprev_state = prev-\u0026gt;state; 2675\tvtime_task_switch(prev); 2676\tperf_event_task_sched_in(prev, current); 2677\tfinish_task(prev); 2678\tfinish_lock_switch(rq); 2679\tfinish_arch_post_lock_switch(); 2680\tkcov_finish_switch(current); 2681 2682\tfire_sched_in_preempt_notifiers(current); 2683\t/* 2684* When switching through a kernel thread, the loop in 2685* membarrier_{private,global}_expedited() may have observed that 2686* kernel thread and not issued an IPI. It is therefore possible to 2687* schedule between user-\u0026gt;kernel-\u0026gt;user threads without passing though 2688* switch_mm(). Membarrier requires a barrier after storing to 2689* rq-\u0026gt;curr, before returning to userspace, so provide them here: 2690* 2691* - a full memory barrier for {PRIVATE,GLOBAL}_EXPEDITED, implicitly 2692* provided by mmdrop(), 2693* - a sync_core for SYNC_CORE. 2694*/ 2695\tif (mm) { 2696\tmembarrier_mm_sync_core_before_usermode(mm); 2697\tmmdrop(mm); 2698\t} 2699\tif (unlikely(prev_state == TASK_DEAD)) { 2700\tif (prev-\u0026gt;sched_class-\u0026gt;task_dead) 2701\tprev-\u0026gt;sched_class-\u0026gt;task_dead(prev); 2702 2703\t/* 2704* Remove function-return probe instances associated with this 2705* task and put them back on the free list. 2706*/ 2707\tkprobe_flush_task(prev); 2708 2709\t/* Task is done with its stack. */ 2710\tput_task_stack(prev); 2711 2712\tput_task_struct(prev); 2713\t} 2714 2715\ttick_nohz_task_switch(); 2716\treturn rq; 2717} 2718 2719#ifdef CONFIG_SMP 2720 2721/* rq-\u0026gt;lock is NOT held, but preemption is disabled */ 2722static void __balance_callback(struct rq *rq) 2723{ 2724\tstruct callback_head *head, *next; 2725\tvoid (*func)(struct rq *rq); 2726\tunsigned long flags; 2727 2728\traw_spin_lock_irqsave(\u0026amp;rq-\u0026gt;lock, flags); 2729\thead = rq-\u0026gt;balance_callback; 2730\trq-\u0026gt;balance_callback = NULL; 2731\twhile (head) { 2732\tfunc = (void (*)(struct rq *))head-\u0026gt;func; 2733\tnext = head-\u0026gt;next; 2734\thead-\u0026gt;next = NULL; 2735\thead = next; 2736 2737\tfunc(rq); 2738\t} 2739\traw_spin_unlock_irqrestore(\u0026amp;rq-\u0026gt;lock, flags); 2740} 2741 2742static inline void balance_callback(struct rq *rq) 2743{ 2744\tif (unlikely(rq-\u0026gt;balance_callback)) 2745\t__balance_callback(rq); 2746} 2747 2748#else 2749 2750static inline void balance_callback(struct rq *rq) 2751{ 2752} 2753 2754#endif 2755 2756/** 2757* schedule_tail - first thing a freshly forked thread must call. 2758* @prev: the thread we just switched away from. 2759*/ 2760asmlinkage __visible void schedule_tail(struct task_struct *prev) 2761\t__releases(rq-\u0026gt;lock) 2762{ 2763\tstruct rq *rq; 2764 2765\t/* 2766* New tasks start with FORK_PREEMPT_COUNT, see there and 2767* finish_task_switch() for details. 2768* 2769* finish_task_switch() will drop rq-\u0026gt;lock() and lower preempt_count 2770* and the preempt_enable() will end up enabling preemption (on 2771* PREEMPT_COUNT kernels). 2772*/ 2773 2774\trq = finish_task_switch(prev); 2775\tbalance_callback(rq); 2776\tpreempt_enable(); 2777 2778\tif (current-\u0026gt;set_child_tid) 2779\tput_user(task_pid_vnr(current), current-\u0026gt;set_child_tid); 2780 2781\tcalculate_sigpending(); 2782} 2783 2784/* 2785* context_switch - switch to the new MM and the new thread\u0026#39;s register state. 2786*/ 2787static __always_inline struct rq * 2788context_switch(struct rq *rq, struct task_struct *prev, 2789\tstruct task_struct *next, struct rq_flags *rf) 2790{ 2791\tstruct mm_struct *mm, *oldmm; 2792 2793\tprepare_task_switch(rq, prev, next); 2794 2795\tmm = next-\u0026gt;mm; 2796\toldmm = prev-\u0026gt;active_mm; 2797\t/* 2798* For paravirt, this is coupled with an exit in switch_to to 2799* combine the page table reload and the switch backend into 2800* one hypercall. 2801*/ 2802\tarch_start_context_switch(prev); 2803 2804\t/* 2805* If mm is non-NULL, we pass through switch_mm(). If mm is 2806* NULL, we will pass through mmdrop() in finish_task_switch(). 2807* Both of these contain the full memory barrier required by 2808* membarrier after storing to rq-\u0026gt;curr, before returning to 2809* user-space. 2810*/ 2811\tif (!mm) { 2812\tnext-\u0026gt;active_mm = oldmm; 2813\tmmgrab(oldmm); 2814\tenter_lazy_tlb(oldmm, next); 2815\t} else 2816\tswitch_mm_irqs_off(oldmm, mm, next); 2817 2818\tif (!prev-\u0026gt;mm) { 2819\tprev-\u0026gt;active_mm = NULL; 2820\trq-\u0026gt;prev_mm = oldmm; 2821\t} 2822 2823\trq-\u0026gt;clock_update_flags \u0026amp;= ~(RQCF_ACT_SKIP|RQCF_REQ_SKIP); 2824 2825\tprepare_lock_switch(rq, next, rf); 2826 2827\t/* Here we just switch the register state and the stack. */ 2828\tswitch_to(prev, next, prev); 2829\tbarrier(); 2830 2831\treturn finish_task_switch(prev); 2832} 2833 2834/* 2835* nr_running and nr_context_switches: 2836* 2837* externally visible scheduler statistics: current number of runnable 2838* threads, total number of context switches performed since bootup. 2839*/ 2840unsigned long nr_running(void) 2841{ 2842\tunsigned long i, sum = 0; 2843 2844\tfor_each_online_cpu(i) 2845\tsum += cpu_rq(i)-\u0026gt;nr_running; 2846 2847\treturn sum; 2848} 2849 2850/* 2851* Check if only the current task is running on the CPU. 2852* 2853* Caution: this function does not check that the caller has disabled 2854* preemption, thus the result might have a time-of-check-to-time-of-use 2855* race. The caller is responsible to use it correctly, for example: 2856* 2857* - from a non-preemptable section (of course) 2858* 2859* - from a thread that is bound to a single CPU 2860* 2861* - in a loop with very short iterations (e.g. a polling loop) 2862*/ 2863bool single_task_running(void) 2864{ 2865\treturn raw_rq()-\u0026gt;nr_running == 1; 2866} 2867EXPORT_SYMBOL(single_task_running); 2868 2869unsigned long long nr_context_switches(void) 2870{ 2871\tint i; 2872\tunsigned long long sum = 0; 2873 2874\tfor_each_possible_cpu(i) 2875\tsum += cpu_rq(i)-\u0026gt;nr_switches; 2876 2877\treturn sum; 2878} 2879 2880/* 2881* IO-wait accounting, and how its mostly bollocks (on SMP). 2882* 2883* The idea behind IO-wait account is to account the idle time that we could 2884* have spend running if it were not for IO. That is, if we were to improve the 2885* storage performance, we\u0026#39;d have a proportional reduction in IO-wait time. 2886* 2887* This all works nicely on UP, where, when a task blocks on IO, we account 2888* idle time as IO-wait, because if the storage were faster, it could\u0026#39;ve been 2889* running and we\u0026#39;d not be idle. 2890* 2891* This has been extended to SMP, by doing the same for each CPU. This however 2892* is broken. 2893* 2894* Imagine for instance the case where two tasks block on one CPU, only the one 2895* CPU will have IO-wait accounted, while the other has regular idle. Even 2896* though, if the storage were faster, both could\u0026#39;ve ran at the same time, 2897* utilising both CPUs. 2898* 2899* This means, that when looking globally, the current IO-wait accounting on 2900* SMP is a lower bound, by reason of under accounting. 2901* 2902* Worse, since the numbers are provided per CPU, they are sometimes 2903* interpreted per CPU, and that is nonsensical. A blocked task isn\u0026#39;t strictly 2904* associated with any one particular CPU, it can wake to another CPU than it 2905* blocked on. This means the per CPU IO-wait number is meaningless. 2906* 2907* Task CPU affinities can make all that even more \u0026#39;interesting\u0026#39;. 2908*/ 2909 2910unsigned long nr_iowait(void) 2911{ 2912\tunsigned long i, sum = 0; 2913 2914\tfor_each_possible_cpu(i) 2915\tsum += atomic_read(\u0026amp;cpu_rq(i)-\u0026gt;nr_iowait); 2916 2917\treturn sum; 2918} 2919 2920/* 2921* Consumers of these two interfaces, like for example the cpufreq menu 2922* governor are using nonsensical data. Boosting frequency for a CPU that has 2923* IO-wait which might not even end up running the task when it does become 2924* runnable. 2925*/ 2926 2927unsigned long nr_iowait_cpu(int cpu) 2928{ 2929\tstruct rq *this = cpu_rq(cpu); 2930\treturn atomic_read(\u0026amp;this-\u0026gt;nr_iowait); 2931} 2932 2933void get_iowait_load(unsigned long *nr_waiters, unsigned long *load) 2934{ 2935\tstruct rq *rq = this_rq(); 2936\t*nr_waiters = atomic_read(\u0026amp;rq-\u0026gt;nr_iowait); 2937\t*load = rq-\u0026gt;load.weight; 2938} 2939 2940#ifdef CONFIG_SMP 2941 2942/* 2943* sched_exec - execve() is a valuable balancing opportunity, because at 2944* this point the task has the smallest effective memory and cache footprint. 2945*/ 2946void sched_exec(void) 2947{ 2948\tstruct task_struct *p = current; 2949\tunsigned long flags; 2950\tint dest_cpu; 2951 2952\traw_spin_lock_irqsave(\u0026amp;p-\u0026gt;pi_lock, flags); 2953\tdest_cpu = p-\u0026gt;sched_class-\u0026gt;select_task_rq(p, task_cpu(p), SD_BALANCE_EXEC, 0); 2954\tif (dest_cpu == smp_processor_id()) 2955\tgoto unlock; 2956 2957\tif (likely(cpu_active(dest_cpu))) { 2958\tstruct migration_arg arg = { p, dest_cpu }; 2959 2960\traw_spin_unlock_irqrestore(\u0026amp;p-\u0026gt;pi_lock, flags); 2961\tstop_one_cpu(task_cpu(p), migration_cpu_stop, \u0026amp;arg); 2962\treturn; 2963\t} 2964unlock: 2965\traw_spin_unlock_irqrestore(\u0026amp;p-\u0026gt;pi_lock, flags); 2966} 2967 2968#endif 2969 2970DEFINE_PER_CPU(struct kernel_stat, kstat); 2971DEFINE_PER_CPU(struct kernel_cpustat, kernel_cpustat); 2972 2973EXPORT_PER_CPU_SYMBOL(kstat); 2974EXPORT_PER_CPU_SYMBOL(kernel_cpustat); 2975 2976/* 2977* The function fair_sched_class.update_curr accesses the struct curr 2978* and its field curr-\u0026gt;exec_start; when called from task_sched_runtime(), 2979* we observe a high rate of cache misses in practice. 2980* Prefetching this data results in improved performance. 2981*/ 2982static inline void prefetch_curr_exec_start(struct task_struct *p) 2983{ 2984#ifdef CONFIG_FAIR_GROUP_SCHED 2985\tstruct sched_entity *curr = (\u0026amp;p-\u0026gt;se)-\u0026gt;cfs_rq-\u0026gt;curr; 2986#else 2987\tstruct sched_entity *curr = (\u0026amp;task_rq(p)-\u0026gt;cfs)-\u0026gt;curr; 2988#endif 2989\tprefetch(curr); 2990\tprefetch(\u0026amp;curr-\u0026gt;exec_start); 2991} 2992 2993/* 2994* Return accounted runtime for the task. 2995* In case the task is currently running, return the runtime plus current\u0026#39;s 2996* pending runtime that have not been accounted yet. 2997*/ 2998unsigned long long task_sched_runtime(struct task_struct *p) 2999{ 3000\tstruct rq_flags rf; 3001\tstruct rq *rq; 3002\tu64 ns; 3003 3004#if defined(CONFIG_64BIT) \u0026amp;\u0026amp; defined(CONFIG_SMP) 3005\t/* 3006* 64-bit doesn\u0026#39;t need locks to atomically read a 64-bit value. 3007* So we have a optimization chance when the task\u0026#39;s delta_exec is 0. 3008* Reading -\u0026gt;on_cpu is racy, but this is ok. 3009* 3010* If we race with it leaving CPU, we\u0026#39;ll take a lock. So we\u0026#39;re correct. 3011* If we race with it entering CPU, unaccounted time is 0. This is 3012* indistinguishable from the read occurring a few cycles earlier. 3013* If we see -\u0026gt;on_cpu without -\u0026gt;on_rq, the task is leaving, and has 3014* been accounted, so we\u0026#39;re correct here as well. 3015*/ 3016\tif (!p-\u0026gt;on_cpu || !task_on_rq_queued(p)) 3017\treturn p-\u0026gt;se.sum_exec_runtime; 3018#endif 3019 3020\trq = task_rq_lock(p, \u0026amp;rf); 3021\t/* 3022* Must be -\u0026gt;curr _and_ -\u0026gt;on_rq. If dequeued, we would 3023* project cycles that may never be accounted to this 3024* thread, breaking clock_gettime(). 3025*/ 3026\tif (task_current(rq, p) \u0026amp;\u0026amp; task_on_rq_queued(p)) { 3027\tprefetch_curr_exec_start(p); 3028\tupdate_rq_clock(rq); 3029\tp-\u0026gt;sched_class-\u0026gt;update_curr(rq); 3030\t} 3031\tns = p-\u0026gt;se.sum_exec_runtime; 3032\ttask_rq_unlock(rq, p, \u0026amp;rf); 3033 3034\treturn ns; 3035} 3036 3037/* 3038* This function gets called by the timer code, with HZ frequency. 3039* We call it with interrupts disabled. 3040*/ 3041void scheduler_tick(void) 3042{ 3043\tint cpu = smp_processor_id(); 3044\tstruct rq *rq = cpu_rq(cpu); 3045\tstruct task_struct *curr = rq-\u0026gt;curr; 3046\tstruct rq_flags rf; 3047 3048\tsched_clock_tick(); 3049 3050\trq_lock(rq, \u0026amp;rf); 3051 3052\tupdate_rq_clock(rq); 3053\tcurr-\u0026gt;sched_class-\u0026gt;task_tick(rq, curr, 0); 3054\tcpu_load_update_active(rq); 3055\tcalc_global_load_tick(rq); 3056 3057\trq_unlock(rq, \u0026amp;rf); 3058 3059\tperf_event_task_tick(); 3060 3061#ifdef CONFIG_SMP 3062\trq-\u0026gt;idle_balance = idle_cpu(cpu); 3063\ttrigger_load_balance(rq); 3064#endif 3065} 3066 3067#ifdef CONFIG_NO_HZ_FULL 3068 3069struct tick_work { 3070\tint\tcpu; 3071\tatomic_t\tstate; 3072\tstruct delayed_work\twork; 3073}; 3074/* Values for -\u0026gt;state, see diagram below. */ 3075#define TICK_SCHED_REMOTE_OFFLINE\t0 3076#define TICK_SCHED_REMOTE_OFFLINING\t1 3077#define TICK_SCHED_REMOTE_RUNNING\t2 3078 3079/* 3080* State diagram for -\u0026gt;state: 3081* 3082* 3083* TICK_SCHED_REMOTE_OFFLINE 3084* | ^ 3085* | | 3086* | | sched_tick_remote() 3087* | | 3088* | | 3089* +--TICK_SCHED_REMOTE_OFFLINING 3090* | ^ 3091* | | 3092* sched_tick_start() | | sched_tick_stop() 3093* | | 3094* V | 3095* TICK_SCHED_REMOTE_RUNNING 3096* 3097* 3098* Other transitions get WARN_ON_ONCE(), except that sched_tick_remote() 3099* and sched_tick_start() are happy to leave the state in RUNNING. 3100*/ 3101 3102static struct tick_work __percpu *tick_work_cpu; 3103 3104static void sched_tick_remote(struct work_struct *work) 3105{ 3106\tstruct delayed_work *dwork = to_delayed_work(work); 3107\tstruct tick_work *twork = container_of(dwork, struct tick_work, work); 3108\tint cpu = twork-\u0026gt;cpu; 3109\tstruct rq *rq = cpu_rq(cpu); 3110\tstruct task_struct *curr; 3111\tstruct rq_flags rf; 3112\tu64 delta; 3113\tint os; 3114 3115\t/* 3116* Handle the tick only if it appears the remote CPU is running in full 3117* dynticks mode. The check is racy by nature, but missing a tick or 3118* having one too much is no big deal because the scheduler tick updates 3119* statistics and checks timeslices in a time-independent way, regardless 3120* of when exactly it is running. 3121*/ 3122\tif (idle_cpu(cpu) || !tick_nohz_tick_stopped_cpu(cpu)) 3123\tgoto out_requeue; 3124 3125\trq_lock_irq(rq, \u0026amp;rf); 3126\tcurr = rq-\u0026gt;curr; 3127\tif (is_idle_task(curr) || cpu_is_offline(cpu)) 3128\tgoto out_unlock; 3129 3130\tupdate_rq_clock(rq); 3131\tdelta = rq_clock_task(rq) - curr-\u0026gt;se.exec_start; 3132 3133\t/* 3134* Make sure the next tick runs within a reasonable 3135* amount of time. 3136*/ 3137\tWARN_ON_ONCE(delta \u0026gt; (u64)NSEC_PER_SEC * 3); 3138\tcurr-\u0026gt;sched_class-\u0026gt;task_tick(rq, curr, 0); 3139 3140out_unlock: 3141\trq_unlock_irq(rq, \u0026amp;rf); 3142 3143out_requeue: 3144\t/* 3145* Run the remote tick once per second (1Hz). This arbitrary 3146* frequency is large enough to avoid overload but short enough 3147* to keep scheduler internal stats reasonably up to date. But 3148* first update state to reflect hotplug activity if required. 3149*/ 3150\tos = atomic_fetch_add_unless(\u0026amp;twork-\u0026gt;state, -1, TICK_SCHED_REMOTE_RUNNING); 3151\tWARN_ON_ONCE(os == TICK_SCHED_REMOTE_OFFLINE); 3152\tif (os == TICK_SCHED_REMOTE_RUNNING) 3153\tqueue_delayed_work(system_unbound_wq, dwork, HZ); 3154} 3155 3156static void sched_tick_start(int cpu) 3157{ 3158\tint os; 3159\tstruct tick_work *twork; 3160 3161\tif (housekeeping_cpu(cpu, HK_FLAG_TICK)) 3162\treturn; 3163 3164\tWARN_ON_ONCE(!tick_work_cpu); 3165 3166\ttwork = per_cpu_ptr(tick_work_cpu, cpu); 3167\tos = atomic_xchg(\u0026amp;twork-\u0026gt;state, TICK_SCHED_REMOTE_RUNNING); 3168\tWARN_ON_ONCE(os == TICK_SCHED_REMOTE_RUNNING); 3169\tif (os == TICK_SCHED_REMOTE_OFFLINE) { 3170\ttwork-\u0026gt;cpu = cpu; 3171\tINIT_DELAYED_WORK(\u0026amp;twork-\u0026gt;work, sched_tick_remote); 3172\tqueue_delayed_work(system_unbound_wq, \u0026amp;twork-\u0026gt;work, HZ); 3173\t} 3174} 3175 3176#ifdef CONFIG_HOTPLUG_CPU 3177static void sched_tick_stop(int cpu) 3178{ 3179\tstruct tick_work *twork; 3180\tint os; 3181 3182\tif (housekeeping_cpu(cpu, HK_FLAG_TICK)) 3183\treturn; 3184 3185\tWARN_ON_ONCE(!tick_work_cpu); 3186 3187\ttwork = per_cpu_ptr(tick_work_cpu, cpu); 3188\t/* There cannot be competing actions, but don\u0026#39;t rely on stop-machine. */ 3189\tos = atomic_xchg(\u0026amp;twork-\u0026gt;state, TICK_SCHED_REMOTE_OFFLINING); 3190\tWARN_ON_ONCE(os != TICK_SCHED_REMOTE_RUNNING); 3191\t/* Don\u0026#39;t cancel, as this would mess up the state machine. */ 3192} 3193#endif /* CONFIG_HOTPLUG_CPU */3194 3195int __init sched_tick_offload_init(void) 3196{ 3197\ttick_work_cpu = alloc_percpu(struct tick_work); 3198\tBUG_ON(!tick_work_cpu); 3199\treturn 0; 3200} 3201 3202#else /* !CONFIG_NO_HZ_FULL */3203static inline void sched_tick_start(int cpu) { } 3204static inline void sched_tick_stop(int cpu) { } 3205#endif 3206 3207#if defined(CONFIG_PREEMPT) \u0026amp;\u0026amp; (defined(CONFIG_DEBUG_PREEMPT) || \\ 3208defined(CONFIG_TRACE_PREEMPT_TOGGLE)) 3209/* 3210* If the value passed in is equal to the current preempt count 3211* then we just disabled preemption. Start timing the latency. 3212*/ 3213static inline void preempt_latency_start(int val) 3214{ 3215\tif (preempt_count() == val) { 3216\tunsigned long ip = get_lock_parent_ip(); 3217#ifdef CONFIG_DEBUG_PREEMPT 3218\tcurrent-\u0026gt;preempt_disable_ip = ip; 3219#endif 3220\ttrace_preempt_off(CALLER_ADDR0, ip); 3221\t} 3222} 3223 3224void preempt_count_add(int val) 3225{ 3226#ifdef CONFIG_DEBUG_PREEMPT 3227\t/* 3228* Underflow? 3229*/ 3230\tif (DEBUG_LOCKS_WARN_ON((preempt_count() \u0026lt; 0))) 3231\treturn; 3232#endif 3233\t__preempt_count_add(val); 3234#ifdef CONFIG_DEBUG_PREEMPT 3235\t/* 3236* Spinlock count overflowing soon? 3237*/ 3238\tDEBUG_LOCKS_WARN_ON((preempt_count() \u0026amp; PREEMPT_MASK) \u0026gt;= 3239\tPREEMPT_MASK - 10); 3240#endif 3241\tpreempt_latency_start(val); 3242} 3243EXPORT_SYMBOL(preempt_count_add); 3244NOKPROBE_SYMBOL(preempt_count_add); 3245 3246/* 3247* If the value passed in equals to the current preempt count 3248* then we just enabled preemption. Stop timing the latency. 3249*/ 3250static inline void preempt_latency_stop(int val) 3251{ 3252\tif (preempt_count() == val) 3253\ttrace_preempt_on(CALLER_ADDR0, get_lock_parent_ip()); 3254} 3255 3256void preempt_count_sub(int val) 3257{ 3258#ifdef CONFIG_DEBUG_PREEMPT 3259\t/* 3260* Underflow? 3261*/ 3262\tif (DEBUG_LOCKS_WARN_ON(val \u0026gt; preempt_count())) 3263\treturn; 3264\t/* 3265* Is the spinlock portion underflowing? 3266*/ 3267\tif (DEBUG_LOCKS_WARN_ON((val \u0026lt; PREEMPT_MASK) \u0026amp;\u0026amp; 3268\t!(preempt_count() \u0026amp; PREEMPT_MASK))) 3269\treturn; 3270#endif 3271 3272\tpreempt_latency_stop(val); 3273\t__preempt_count_sub(val); 3274} 3275EXPORT_SYMBOL(preempt_count_sub); 3276NOKPROBE_SYMBOL(preempt_count_sub); 3277 3278#else 3279static inline void preempt_latency_start(int val) { } 3280static inline void preempt_latency_stop(int val) { } 3281#endif 3282 3283static inline unsigned long get_preempt_disable_ip(struct task_struct *p) 3284{ 3285#ifdef CONFIG_DEBUG_PREEMPT 3286\treturn p-\u0026gt;preempt_disable_ip; 3287#else 3288\treturn 0; 3289#endif 3290} 3291 3292/* 3293* Print scheduling while atomic bug: 3294*/ 3295static noinline void __schedule_bug(struct task_struct *prev) 3296{ 3297\t/* Save this before calling printk(), since that will clobber it */ 3298\tunsigned long preempt_disable_ip = get_preempt_disable_ip(current); 3299 3300\tif (oops_in_progress) 3301\treturn; 3302 3303\tprintk(KERN_ERR \u0026#34;BUG: scheduling while atomic: %s/%d/0x%08x\\n\u0026#34;, 3304\tprev-\u0026gt;comm, prev-\u0026gt;pid, preempt_count()); 3305 3306\tdebug_show_held_locks(prev); 3307\tprint_modules(); 3308\tif (irqs_disabled()) 3309\tprint_irqtrace_events(prev); 3310\tif (IS_ENABLED(CONFIG_DEBUG_PREEMPT) 3311\t\u0026amp;\u0026amp; in_atomic_preempt_off()) { 3312\tpr_err(\u0026#34;Preemption disabled at:\u0026#34;); 3313\tprint_ip_sym(preempt_disable_ip); 3314\tpr_cont(\u0026#34;\\n\u0026#34;); 3315\t} 3316\tif (panic_on_warn) 3317\tpanic(\u0026#34;scheduling while atomic\\n\u0026#34;); 3318 3319\tdump_stack(); 3320\tadd_taint(TAINT_WARN, LOCKDEP_STILL_OK); 3321} 3322 3323/* 3324* Various schedule()-time debugging checks and statistics: 3325*/ 3326static inline void schedule_debug(struct task_struct *prev) 3327{ 3328#ifdef CONFIG_SCHED_STACK_END_CHECK 3329\tif (task_stack_end_corrupted(prev)) 3330\tpanic(\u0026#34;corrupted stack end detected inside scheduler\\n\u0026#34;); 3331#endif 3332 3333\tif (unlikely(in_atomic_preempt_off())) { 3334\t__schedule_bug(prev); 3335\tpreempt_count_set(PREEMPT_DISABLED); 3336\t} 3337\trcu_sleep_check(); 3338 3339\tprofile_hit(SCHED_PROFILING, __builtin_return_address(0)); 3340 3341\tschedstat_inc(this_rq()-\u0026gt;sched_count); 3342} 3343 3344/* 3345* Pick up the highest-prio task: 3346*/ 3347static inline struct task_struct * 3348pick_next_task(struct rq *rq, struct task_struct *prev, struct rq_flags *rf) 3349{ 3350\tconst struct sched_class *class; 3351\tstruct task_struct *p; 3352 3353\t/* 3354* Optimization: we know that if all tasks are in the fair class we can 3355* call that function directly, but only if the @prev task wasn\u0026#39;t of a 3356* higher scheduling class, because otherwise those loose the 3357* opportunity to pull in more work from other CPUs. 3358*/ 3359\tif (likely((prev-\u0026gt;sched_class == \u0026amp;idle_sched_class || 3360\tprev-\u0026gt;sched_class == \u0026amp;fair_sched_class) \u0026amp;\u0026amp; 3361\trq-\u0026gt;nr_running == rq-\u0026gt;cfs.h_nr_running)) { 3362 3363\tp = fair_sched_class.pick_next_task(rq, prev, rf); 3364\tif (unlikely(p == RETRY_TASK)) 3365\tgoto again; 3366 3367\t/* Assumes fair_sched_class-\u0026gt;next == idle_sched_class */ 3368\tif (unlikely(!p)) 3369\tp = idle_sched_class.pick_next_task(rq, prev, rf); 3370 3371\treturn p; 3372\t} 3373 3374again: 3375\tfor_each_class(class) { 3376\tp = class-\u0026gt;pick_next_task(rq, prev, rf); 3377\tif (p) { 3378\tif (unlikely(p == RETRY_TASK)) 3379\tgoto again; 3380\treturn p; 3381\t} 3382\t} 3383 3384\t/* The idle class should always have a runnable task: */ 3385\tBUG(); 3386} 3387 3388/* 3389* __schedule() is the main scheduler function. 3390* 3391* The main means of driving the scheduler and thus entering this function are: 3392* 3393* 1. Explicit blocking: mutex, semaphore, waitqueue, etc. 3394* 3395* 2. TIF_NEED_RESCHED flag is checked on interrupt and userspace return 3396* paths. For example, see arch/x86/entry_64.S. 3397* 3398* To drive preemption between tasks, the scheduler sets the flag in timer 3399* interrupt handler scheduler_tick(). 3400* 3401* 3. Wakeups don\u0026#39;t really cause entry into schedule(). They add a 3402* task to the run-queue and that\u0026#39;s it. 3403* 3404* Now, if the new task added to the run-queue preempts the current 3405* task, then the wakeup sets TIF_NEED_RESCHED and schedule() gets 3406* called on the nearest possible occasion: 3407* 3408* - If the kernel is preemptible (CONFIG_PREEMPT=y): 3409* 3410* - in syscall or exception context, at the next outmost 3411* preempt_enable(). (this might be as soon as the wake_up()\u0026#39;s 3412* spin_unlock()!) 3413* 3414* - in IRQ context, return from interrupt-handler to 3415* preemptible context 3416* 3417* - If the kernel is not preemptible (CONFIG_PREEMPT is not set) 3418* then at the next: 3419* 3420* - cond_resched() call 3421* - explicit schedule() call 3422* - return from syscall or exception to user-space 3423* - return from interrupt-handler to user-space 3424* 3425* WARNING: must be called with preemption disabled! 3426*/ 3427static void __sched notrace __schedule(bool preempt) 3428{ 3429\tstruct task_struct *prev, *next; 3430\tunsigned long *switch_count; 3431\tstruct rq_flags rf; 3432\tstruct rq *rq; 3433\tint cpu; 3434 3435\tcpu = smp_processor_id(); 3436\trq = cpu_rq(cpu); 3437\tprev = rq-\u0026gt;curr; 3438 3439\tschedule_debug(prev); 3440 3441\tif (sched_feat(HRTICK)) 3442\thrtick_clear(rq); 3443 3444\tlocal_irq_disable(); 3445\trcu_note_context_switch(preempt); 3446 3447\t/* 3448* Make sure that signal_pending_state()-\u0026gt;signal_pending() below 3449* can\u0026#39;t be reordered with __set_current_state(TASK_INTERRUPTIBLE) 3450* done by the caller to avoid the race with signal_wake_up(). 3451* 3452* The membarrier system call requires a full memory barrier 3453* after coming from user-space, before storing to rq-\u0026gt;curr. 3454*/ 3455\trq_lock(rq, \u0026amp;rf); 3456\tsmp_mb__after_spinlock(); 3457 3458\t/* Promote REQ to ACT */ 3459\trq-\u0026gt;clock_update_flags \u0026lt;\u0026lt;= 1; 3460\tupdate_rq_clock(rq); 3461 3462\tswitch_count = \u0026amp;prev-\u0026gt;nivcsw; 3463\tif (!preempt \u0026amp;\u0026amp; prev-\u0026gt;state) { 3464\tif (unlikely(signal_pending_state(prev-\u0026gt;state, prev))) { 3465\tprev-\u0026gt;state = TASK_RUNNING; 3466\t} else { 3467\tdeactivate_task(rq, prev, DEQUEUE_SLEEP | DEQUEUE_NOCLOCK); 3468\tprev-\u0026gt;on_rq = 0; 3469 3470\tif (prev-\u0026gt;in_iowait) { 3471\tatomic_inc(\u0026amp;rq-\u0026gt;nr_iowait); 3472\tdelayacct_blkio_start(); 3473\t} 3474 3475\t/* 3476* If a worker went to sleep, notify and ask workqueue 3477* whether it wants to wake up a task to maintain 3478* concurrency. 3479*/ 3480\tif (prev-\u0026gt;flags \u0026amp; PF_WQ_WORKER) { 3481\tstruct task_struct *to_wakeup; 3482 3483\tto_wakeup = wq_worker_sleeping(prev); 3484\tif (to_wakeup) 3485\ttry_to_wake_up_local(to_wakeup, \u0026amp;rf); 3486\t} 3487\t} 3488\tswitch_count = \u0026amp;prev-\u0026gt;nvcsw; 3489\t} 3490 3491\tnext = pick_next_task(rq, prev, \u0026amp;rf); 3492\tclear_tsk_need_resched(prev); 3493\tclear_preempt_need_resched(); 3494 3495\tif (likely(prev != next)) { 3496\trq-\u0026gt;nr_switches++; 3497\trq-\u0026gt;curr = next; 3498\t/* 3499* The membarrier system call requires each architecture 3500* to have a full memory barrier after updating 3501* rq-\u0026gt;curr, before returning to user-space. 3502* 3503* Here are the schemes providing that barrier on the 3504* various architectures: 3505* - mm ? switch_mm() : mmdrop() for x86, s390, sparc, PowerPC. 3506* switch_mm() rely on membarrier_arch_switch_mm() on PowerPC. 3507* - finish_lock_switch() for weakly-ordered 3508* architectures where spin_unlock is a full barrier, 3509* - switch_to() for arm64 (weakly-ordered, spin_unlock 3510* is a RELEASE barrier), 3511*/ 3512\t++*switch_count; 3513 3514\ttrace_sched_switch(preempt, prev, next); 3515 3516\t/* Also unlocks the rq: */ 3517\trq = context_switch(rq, prev, next, \u0026amp;rf); 3518\t} else { 3519\trq-\u0026gt;clock_update_flags \u0026amp;= ~(RQCF_ACT_SKIP|RQCF_REQ_SKIP); 3520\trq_unlock_irq(rq, \u0026amp;rf); 3521\t} 3522 3523\tbalance_callback(rq); 3524} 3525 3526void __noreturn do_task_dead(void) 3527{ 3528\t/* Causes final put_task_struct in finish_task_switch(): */ 3529\tset_special_state(TASK_DEAD); 3530 3531\t/* Tell freezer to ignore us: */ 3532\tcurrent-\u0026gt;flags |= PF_NOFREEZE; 3533 3534\t__schedule(false); 3535\tBUG(); 3536 3537\t/* Avoid \u0026#34;noreturn function does return\u0026#34; - but don\u0026#39;t continue if BUG() is a NOP: */ 3538\tfor (;;) 3539\tcpu_relax(); 3540} 3541 3542static inline void sched_submit_work(struct task_struct *tsk) 3543{ 3544\tif (!tsk-\u0026gt;state || tsk_is_pi_blocked(tsk)) 3545\treturn; 3546\t/* 3547* If we are going to sleep and we have plugged IO queued, 3548* make sure to submit it to avoid deadlocks. 3549*/ 3550\tif (blk_needs_flush_plug(tsk)) 3551\tblk_schedule_flush_plug(tsk); 3552} 3553 3554asmlinkage __visible void __sched schedule(void) 3555{ 3556\tstruct task_struct *tsk = current; 3557 3558\tsched_submit_work(tsk); 3559\tdo { 3560\tpreempt_disable(); 3561\t__schedule(false); 3562\tsched_preempt_enable_no_resched(); 3563\t} while (need_resched()); 3564} 3565EXPORT_SYMBOL(schedule); 3566 3567/* 3568* synchronize_rcu_tasks() makes sure that no task is stuck in preempted 3569* state (have scheduled out non-voluntarily) by making sure that all 3570* tasks have either left the run queue or have gone into user space. 3571* As idle tasks do not do either, they must not ever be preempted 3572* (schedule out non-voluntarily). 3573* 3574* schedule_idle() is similar to schedule_preempt_disable() except that it 3575* never enables preemption because it does not call sched_submit_work(). 3576*/ 3577void __sched schedule_idle(void) 3578{ 3579\t/* 3580* As this skips calling sched_submit_work(), which the idle task does 3581* regardless because that function is a nop when the task is in a 3582* TASK_RUNNING state, make sure this isn\u0026#39;t used someplace that the 3583* current task can be in any other state. Note, idle is always in the 3584* TASK_RUNNING state. 3585*/ 3586\tWARN_ON_ONCE(current-\u0026gt;state); 3587\tdo { 3588\t__schedule(false); 3589\t} while (need_resched()); 3590} 3591 3592#ifdef CONFIG_CONTEXT_TRACKING 3593asmlinkage __visible void __sched schedule_user(void) 3594{ 3595\t/* 3596* If we come here after a random call to set_need_resched(), 3597* or we have been woken up remotely but the IPI has not yet arrived, 3598* we haven\u0026#39;t yet exited the RCU idle mode. Do it here manually until 3599* we find a better solution. 3600* 3601* NB: There are buggy callers of this function. Ideally we 3602* should warn if prev_state != CONTEXT_USER, but that will trigger 3603* too frequently to make sense yet. 3604*/ 3605\tenum ctx_state prev_state = exception_enter(); 3606\tschedule(); 3607\texception_exit(prev_state); 3608} 3609#endif 3610 3611/** 3612* schedule_preempt_disabled - called with preemption disabled 3613* 3614* Returns with preemption disabled. Note: preempt_count must be 1 3615*/ 3616void __sched schedule_preempt_disabled(void) 3617{ 3618\tsched_preempt_enable_no_resched(); 3619\tschedule(); 3620\tpreempt_disable(); 3621} 3622 3623static void __sched notrace preempt_schedule_common(void) 3624{ 3625\tdo { 3626\t/* 3627* Because the function tracer can trace preempt_count_sub() 3628* and it also uses preempt_enable/disable_notrace(), if 3629* NEED_RESCHED is set, the preempt_enable_notrace() called 3630* by the function tracer will call this function again and 3631* cause infinite recursion. 3632* 3633* Preemption must be disabled here before the function 3634* tracer can trace. Break up preempt_disable() into two 3635* calls. One to disable preemption without fear of being 3636* traced. The other to still record the preemption latency, 3637* which can also be traced by the function tracer. 3638*/ 3639\tpreempt_disable_notrace(); 3640\tpreempt_latency_start(1); 3641\t__schedule(true); 3642\tpreempt_latency_stop(1); 3643\tpreempt_enable_no_resched_notrace(); 3644 3645\t/* 3646* Check again in case we missed a preemption opportunity 3647* between schedule and now. 3648*/ 3649\t} while (need_resched()); 3650} 3651 3652#ifdef CONFIG_PREEMPT 3653/* 3654* this is the entry point to schedule() from in-kernel preemption 3655* off of preempt_enable. Kernel preemptions off return from interrupt 3656* occur there and call schedule directly. 3657*/ 3658asmlinkage __visible void __sched notrace preempt_schedule(void) 3659{ 3660\t/* 3661* If there is a non-zero preempt_count or interrupts are disabled, 3662* we do not want to preempt the current task. Just return.. 3663*/ 3664\tif (likely(!preemptible())) 3665\treturn; 3666 3667\tpreempt_schedule_common(); 3668} 3669NOKPROBE_SYMBOL(preempt_schedule); 3670EXPORT_SYMBOL(preempt_schedule); 3671 3672/** 3673* preempt_schedule_notrace - preempt_schedule called by tracing 3674* 3675* The tracing infrastructure uses preempt_enable_notrace to prevent 3676* recursion and tracing preempt enabling caused by the tracing 3677* infrastructure itself. But as tracing can happen in areas coming 3678* from userspace or just about to enter userspace, a preempt enable 3679* can occur before user_exit() is called. This will cause the scheduler 3680* to be called when the system is still in usermode. 3681* 3682* To prevent this, the preempt_enable_notrace will use this function 3683* instead of preempt_schedule() to exit user context if needed before 3684* calling the scheduler. 3685*/ 3686asmlinkage __visible void __sched notrace preempt_schedule_notrace(void) 3687{ 3688\tenum ctx_state prev_ctx; 3689 3690\tif (likely(!preemptible())) 3691\treturn; 3692 3693\tdo { 3694\t/* 3695* Because the function tracer can trace preempt_count_sub() 3696* and it also uses preempt_enable/disable_notrace(), if 3697* NEED_RESCHED is set, the preempt_enable_notrace() called 3698* by the function tracer will call this function again and 3699* cause infinite recursion. 3700* 3701* Preemption must be disabled here before the function 3702* tracer can trace. Break up preempt_disable() into two 3703* calls. One to disable preemption without fear of being 3704* traced. The other to still record the preemption latency, 3705* which can also be traced by the function tracer. 3706*/ 3707\tpreempt_disable_notrace(); 3708\tpreempt_latency_start(1); 3709\t/* 3710* Needs preempt disabled in case user_exit() is traced 3711* and the tracer calls preempt_enable_notrace() causing 3712* an infinite recursion. 3713*/ 3714\tprev_ctx = exception_enter(); 3715\t__schedule(true); 3716\texception_exit(prev_ctx); 3717 3718\tpreempt_latency_stop(1); 3719\tpreempt_enable_no_resched_notrace(); 3720\t} while (need_resched()); 3721} 3722EXPORT_SYMBOL_GPL(preempt_schedule_notrace); 3723 3724#endif /* CONFIG_PREEMPT */3725 3726/* 3727* this is the entry point to schedule() from kernel preemption 3728* off of irq context. 3729* Note, that this is called and return with irqs disabled. This will 3730* protect us against recursive calling from irq. 3731*/ 3732asmlinkage __visible void __sched preempt_schedule_irq(void) 3733{ 3734\tenum ctx_state prev_state; 3735 3736\t/* Catch callers which need to be fixed */ 3737\tBUG_ON(preempt_count() || !irqs_disabled()); 3738 3739\tprev_state = exception_enter(); 3740 3741\tdo { 3742\tpreempt_disable(); 3743\tlocal_irq_enable(); 3744\t__schedule(true); 3745\tlocal_irq_disable(); 3746\tsched_preempt_enable_no_resched(); 3747\t} while (need_resched()); 3748 3749\texception_exit(prev_state); 3750} 3751 3752int default_wake_function(wait_queue_entry_t *curr, unsigned mode, int wake_flags, 3753\tvoid *key) 3754{ 3755\treturn try_to_wake_up(curr-\u0026gt;private, mode, wake_flags); 3756} 3757EXPORT_SYMBOL(default_wake_function); 3758 3759#ifdef CONFIG_RT_MUTEXES 3760 3761static inline int __rt_effective_prio(struct task_struct *pi_task, int prio) 3762{ 3763\tif (pi_task) 3764\tprio = min(prio, pi_task-\u0026gt;prio); 3765 3766\treturn prio; 3767} 3768 3769static inline int rt_effective_prio(struct task_struct *p, int prio) 3770{ 3771\tstruct task_struct *pi_task = rt_mutex_get_top_task(p); 3772 3773\treturn __rt_effective_prio(pi_task, prio); 3774} 3775 3776/* 3777* rt_mutex_setprio - set the current priority of a task 3778* @p: task to boost 3779* @pi_task: donor task 3780* 3781* This function changes the \u0026#39;effective\u0026#39; priority of a task. It does 3782* not touch -\u0026gt;normal_prio like __setscheduler(). 3783* 3784* Used by the rt_mutex code to implement priority inheritance 3785* logic. Call site only calls if the priority of the task changed. 3786*/ 3787void rt_mutex_setprio(struct task_struct *p, struct task_struct *pi_task) 3788{ 3789\tint prio, oldprio, queued, running, queue_flag = 3790\tDEQUEUE_SAVE | DEQUEUE_MOVE | DEQUEUE_NOCLOCK; 3791\tconst struct sched_class *prev_class; 3792\tstruct rq_flags rf; 3793\tstruct rq *rq; 3794 3795\t/* XXX used to be waiter-\u0026gt;prio, not waiter-\u0026gt;task-\u0026gt;prio */ 3796\tprio = __rt_effective_prio(pi_task, p-\u0026gt;normal_prio); 3797 3798\t/* 3799* If nothing changed; bail early. 3800*/ 3801\tif (p-\u0026gt;pi_top_task == pi_task \u0026amp;\u0026amp; prio == p-\u0026gt;prio \u0026amp;\u0026amp; !dl_prio(prio)) 3802\treturn; 3803 3804\trq = __task_rq_lock(p, \u0026amp;rf); 3805\tupdate_rq_clock(rq); 3806\t/* 3807* Set under pi_lock \u0026amp;\u0026amp; rq-\u0026gt;lock, such that the value can be used under 3808* either lock. 3809* 3810* Note that there is loads of tricky to make this pointer cache work 3811* right. rt_mutex_slowunlock()+rt_mutex_postunlock() work together to 3812* ensure a task is de-boosted (pi_task is set to NULL) before the 3813* task is allowed to run again (and can exit). This ensures the pointer 3814* points to a blocked task -- which guaratees the task is present. 3815*/ 3816\tp-\u0026gt;pi_top_task = pi_task; 3817 3818\t/* 3819* For FIFO/RR we only need to set prio, if that matches we\u0026#39;re done. 3820*/ 3821\tif (prio == p-\u0026gt;prio \u0026amp;\u0026amp; !dl_prio(prio)) 3822\tgoto out_unlock; 3823 3824\t/* 3825* Idle task boosting is a nono in general. There is one 3826* exception, when PREEMPT_RT and NOHZ is active: 3827* 3828* The idle task calls get_next_timer_interrupt() and holds 3829* the timer wheel base-\u0026gt;lock on the CPU and another CPU wants 3830* to access the timer (probably to cancel it). We can safely 3831* ignore the boosting request, as the idle CPU runs this code 3832* with interrupts disabled and will complete the lock 3833* protected section without being interrupted. So there is no 3834* real need to boost. 3835*/ 3836\tif (unlikely(p == rq-\u0026gt;idle)) { 3837\tWARN_ON(p != rq-\u0026gt;curr); 3838\tWARN_ON(p-\u0026gt;pi_blocked_on); 3839\tgoto out_unlock; 3840\t} 3841 3842\ttrace_sched_pi_setprio(p, pi_task); 3843\toldprio = p-\u0026gt;prio; 3844 3845\tif (oldprio == prio) 3846\tqueue_flag \u0026amp;= ~DEQUEUE_MOVE; 3847 3848\tprev_class = p-\u0026gt;sched_class; 3849\tqueued = task_on_rq_queued(p); 3850\trunning = task_current(rq, p); 3851\tif (queued) 3852\tdequeue_task(rq, p, queue_flag); 3853\tif (running) 3854\tput_prev_task(rq, p); 3855 3856\t/* 3857* Boosting condition are: 3858* 1. -rt task is running and holds mutex A 3859* --\u0026gt; -dl task blocks on mutex A 3860* 3861* 2. -dl task is running and holds mutex A 3862* --\u0026gt; -dl task blocks on mutex A and could preempt the 3863* running task 3864*/ 3865\tif (dl_prio(prio)) { 3866\tif (!dl_prio(p-\u0026gt;normal_prio) || 3867\t(pi_task \u0026amp;\u0026amp; dl_prio(pi_task-\u0026gt;prio) \u0026amp;\u0026amp; 3868\tdl_entity_preempt(\u0026amp;pi_task-\u0026gt;dl, \u0026amp;p-\u0026gt;dl))) { 3869\tp-\u0026gt;dl.dl_boosted = 1; 3870\tqueue_flag |= ENQUEUE_REPLENISH; 3871\t} else 3872\tp-\u0026gt;dl.dl_boosted = 0; 3873\tp-\u0026gt;sched_class = \u0026amp;dl_sched_class; 3874\t} else if (rt_prio(prio)) { 3875\tif (dl_prio(oldprio)) 3876\tp-\u0026gt;dl.dl_boosted = 0; 3877\tif (oldprio \u0026lt; prio) 3878\tqueue_flag |= ENQUEUE_HEAD; 3879\tp-\u0026gt;sched_class = \u0026amp;rt_sched_class; 3880\t} else { 3881\tif (dl_prio(oldprio)) 3882\tp-\u0026gt;dl.dl_boosted = 0; 3883\tif (rt_prio(oldprio)) 3884\tp-\u0026gt;rt.timeout = 0; 3885\tp-\u0026gt;sched_class = \u0026amp;fair_sched_class; 3886\t} 3887 3888\tp-\u0026gt;prio = prio; 3889 3890\tif (queued) 3891\tenqueue_task(rq, p, queue_flag); 3892\tif (running) 3893\tset_curr_task(rq, p); 3894 3895\tcheck_class_changed(rq, p, prev_class, oldprio); 3896out_unlock: 3897\t/* Avoid rq from going away on us: */ 3898\tpreempt_disable(); 3899\t__task_rq_unlock(rq, \u0026amp;rf); 3900 3901\tbalance_callback(rq); 3902\tpreempt_enable(); 3903} 3904#else 3905static inline int rt_effective_prio(struct task_struct *p, int prio) 3906{ 3907\treturn prio; 3908} 3909#endif 3910 3911void set_user_nice(struct task_struct *p, long nice) 3912{ 3913\tbool queued, running; 3914\tint old_prio, delta; 3915\tstruct rq_flags rf; 3916\tstruct rq *rq; 3917 3918\tif (task_nice(p) == nice || nice \u0026lt; MIN_NICE || nice \u0026gt; MAX_NICE) 3919\treturn; 3920\t/* 3921* We have to be careful, if called from sys_setpriority(), 3922* the task might be in the middle of scheduling on another CPU. 3923*/ 3924\trq = task_rq_lock(p, \u0026amp;rf); 3925\tupdate_rq_clock(rq); 3926 3927\t/* 3928* The RT priorities are set via sched_setscheduler(), but we still 3929* allow the \u0026#39;normal\u0026#39; nice value to be set - but as expected 3930* it wont have any effect on scheduling until the task is 3931* SCHED_DEADLINE, SCHED_FIFO or SCHED_RR: 3932*/ 3933\tif (task_has_dl_policy(p) || task_has_rt_policy(p)) { 3934\tp-\u0026gt;static_prio = NICE_TO_PRIO(nice); 3935\tgoto out_unlock; 3936\t} 3937\tqueued = task_on_rq_queued(p); 3938\trunning = task_current(rq, p); 3939\tif (queued) 3940\tdequeue_task(rq, p, DEQUEUE_SAVE | DEQUEUE_NOCLOCK); 3941\tif (running) 3942\tput_prev_task(rq, p); 3943 3944\tp-\u0026gt;static_prio = NICE_TO_PRIO(nice); 3945\tset_load_weight(p, true); 3946\told_prio = p-\u0026gt;prio; 3947\tp-\u0026gt;prio = effective_prio(p); 3948\tdelta = p-\u0026gt;prio - old_prio; 3949 3950\tif (queued) { 3951\tenqueue_task(rq, p, ENQUEUE_RESTORE | ENQUEUE_NOCLOCK); 3952\t/* 3953* If the task increased its priority or is running and 3954* lowered its priority, then reschedule its CPU: 3955*/ 3956\tif (delta \u0026lt; 0 || (delta \u0026gt; 0 \u0026amp;\u0026amp; task_running(rq, p))) 3957\tresched_curr(rq); 3958\t} 3959\tif (running) 3960\tset_curr_task(rq, p); 3961out_unlock: 3962\ttask_rq_unlock(rq, p, \u0026amp;rf); 3963} 3964EXPORT_SYMBOL(set_user_nice); 3965 3966/* 3967* can_nice - check if a task can reduce its nice value 3968* @p: task 3969* @nice: nice value 3970*/ 3971int can_nice(const struct task_struct *p, const int nice) 3972{ 3973\t/* Convert nice value [19,-20] to rlimit style value [1,40]: */ 3974\tint nice_rlim = nice_to_rlimit(nice); 3975 3976\treturn (nice_rlim \u0026lt;= task_rlimit(p, RLIMIT_NICE) || 3977\tcapable(CAP_SYS_NICE)); 3978} 3979 3980#ifdef __ARCH_WANT_SYS_NICE 3981 3982/* 3983* sys_nice - change the priority of the current process. 3984* @increment: priority increment 3985* 3986* sys_setpriority is a more generic, but much slower function that 3987* does similar things. 3988*/ 3989SYSCALL_DEFINE1(nice, int, increment) 3990{ 3991\tlong nice, retval; 3992 3993\t/* 3994* Setpriority might change our priority at the same moment. 3995* We don\u0026#39;t have to worry. Conceptually one call occurs first 3996* and we have a single winner. 3997*/ 3998\tincrement = clamp(increment, -NICE_WIDTH, NICE_WIDTH); 3999\tnice = task_nice(current) + increment; 4000 4001\tnice = clamp_val(nice, MIN_NICE, MAX_NICE); 4002\tif (increment \u0026lt; 0 \u0026amp;\u0026amp; !can_nice(current, nice)) 4003\treturn -EPERM; 4004 4005\tretval = security_task_setnice(current, nice); 4006\tif (retval) 4007\treturn retval; 4008 4009\tset_user_nice(current, nice); 4010\treturn 0; 4011} 4012 4013#endif 4014 4015/** 4016* task_prio - return the priority value of a given task. 4017* @p: the task in question. 4018* 4019* Return: The priority value as seen by users in /proc. 4020* RT tasks are offset by -200. Normal tasks are centered 4021* around 0, value goes from -16 to +15. 4022*/ 4023int task_prio(const struct task_struct *p) 4024{ 4025\treturn p-\u0026gt;prio - MAX_RT_PRIO; 4026} 4027 4028/** 4029* idle_cpu - is a given CPU idle currently? 4030* @cpu: the processor in question. 4031* 4032* Return: 1 if the CPU is currently idle. 0 otherwise. 4033*/ 4034int idle_cpu(int cpu) 4035{ 4036\tstruct rq *rq = cpu_rq(cpu); 4037 4038\tif (rq-\u0026gt;curr != rq-\u0026gt;idle) 4039\treturn 0; 4040 4041\tif (rq-\u0026gt;nr_running) 4042\treturn 0; 4043 4044#ifdef CONFIG_SMP 4045\tif (!llist_empty(\u0026amp;rq-\u0026gt;wake_list)) 4046\treturn 0; 4047#endif 4048 4049\treturn 1; 4050} 4051 4052/** 4053* available_idle_cpu - is a given CPU idle for enqueuing work. 4054* @cpu: the CPU in question. 4055* 4056* Return: 1 if the CPU is currently idle. 0 otherwise. 4057*/ 4058int available_idle_cpu(int cpu) 4059{ 4060\tif (!idle_cpu(cpu)) 4061\treturn 0; 4062 4063\tif (vcpu_is_preempted(cpu)) 4064\treturn 0; 4065 4066\treturn 1; 4067} 4068 4069/** 4070* idle_task - return the idle task for a given CPU. 4071* @cpu: the processor in question. 4072* 4073* Return: The idle task for the CPU @cpu. 4074*/ 4075struct task_struct *idle_task(int cpu) 4076{ 4077\treturn cpu_rq(cpu)-\u0026gt;idle; 4078} 4079 4080/** 4081* find_process_by_pid - find a process with a matching PID value. 4082* @pid: the pid in question. 4083* 4084* The task of @pid, if found. %NULL otherwise. 4085*/ 4086static struct task_struct *find_process_by_pid(pid_t pid) 4087{ 4088\treturn pid ? find_task_by_vpid(pid) : current; 4089} 4090 4091/* 4092* sched_setparam() passes in -1 for its policy, to let the functions 4093* it calls know not to change it. 4094*/ 4095#define SETPARAM_POLICY\t-1 4096 4097static void __setscheduler_params(struct task_struct *p, 4098\tconst struct sched_attr *attr) 4099{ 4100\tint policy = attr-\u0026gt;sched_policy; 4101 4102\tif (policy == SETPARAM_POLICY) 4103\tpolicy = p-\u0026gt;policy; 4104 4105\tp-\u0026gt;policy = policy; 4106 4107\tif (dl_policy(policy)) 4108\t__setparam_dl(p, attr); 4109\telse if (fair_policy(policy)) 4110\tp-\u0026gt;static_prio = NICE_TO_PRIO(attr-\u0026gt;sched_nice); 4111 4112\t/* 4113* __sched_setscheduler() ensures attr-\u0026gt;sched_priority == 0 when 4114* !rt_policy. Always setting this ensures that things like 4115* getparam()/getattr() don\u0026#39;t report silly values for !rt tasks. 4116*/ 4117\tp-\u0026gt;rt_priority = attr-\u0026gt;sched_priority; 4118\tp-\u0026gt;normal_prio = normal_prio(p); 4119\tset_load_weight(p, true); 4120} 4121 4122/* Actually do priority change: must hold pi \u0026amp; rq lock. */ 4123static void __setscheduler(struct rq *rq, struct task_struct *p, 4124\tconst struct sched_attr *attr, bool keep_boost) 4125{ 4126\t__setscheduler_params(p, attr); 4127 4128\t/* 4129* Keep a potential priority boosting if called from 4130* sched_setscheduler(). 4131*/ 4132\tp-\u0026gt;prio = normal_prio(p); 4133\tif (keep_boost) 4134\tp-\u0026gt;prio = rt_effective_prio(p, p-\u0026gt;prio); 4135 4136\tif (dl_prio(p-\u0026gt;prio)) 4137\tp-\u0026gt;sched_class = \u0026amp;dl_sched_class; 4138\telse if (rt_prio(p-\u0026gt;prio)) 4139\tp-\u0026gt;sched_class = \u0026amp;rt_sched_class; 4140\telse 4141\tp-\u0026gt;sched_class = \u0026amp;fair_sched_class; 4142} 4143 4144/* 4145* Check the target process has a UID that matches the current process\u0026#39;s: 4146*/ 4147static bool check_same_owner(struct task_struct *p) 4148{ 4149\tconst struct cred *cred = current_cred(), *pcred; 4150\tbool match; 4151 4152\trcu_read_lock(); 4153\tpcred = __task_cred(p); 4154\tmatch = (uid_eq(cred-\u0026gt;euid, pcred-\u0026gt;euid) || 4155\tuid_eq(cred-\u0026gt;euid, pcred-\u0026gt;uid)); 4156\trcu_read_unlock(); 4157\treturn match; 4158} 4159 4160static int __sched_setscheduler(struct task_struct *p, 4161\tconst struct sched_attr *attr, 4162\tbool user, bool pi) 4163{ 4164\tint newprio = dl_policy(attr-\u0026gt;sched_policy) ? MAX_DL_PRIO - 1 : 4165\tMAX_RT_PRIO - 1 - attr-\u0026gt;sched_priority; 4166\tint retval, oldprio, oldpolicy = -1, queued, running; 4167\tint new_effective_prio, policy = attr-\u0026gt;sched_policy; 4168\tconst struct sched_class *prev_class; 4169\tstruct rq_flags rf; 4170\tint reset_on_fork; 4171\tint queue_flags = DEQUEUE_SAVE | DEQUEUE_MOVE | DEQUEUE_NOCLOCK; 4172\tstruct rq *rq; 4173 4174\t/* The pi code expects interrupts enabled */ 4175\tBUG_ON(pi \u0026amp;\u0026amp; in_interrupt()); 4176recheck: 4177\t/* Double check policy once rq lock held: */ 4178\tif (policy \u0026lt; 0) { 4179\treset_on_fork = p-\u0026gt;sched_reset_on_fork; 4180\tpolicy = oldpolicy = p-\u0026gt;policy; 4181\t} else { 4182\treset_on_fork = !!(attr-\u0026gt;sched_flags \u0026amp; SCHED_FLAG_RESET_ON_FORK); 4183 4184\tif (!valid_policy(policy)) 4185\treturn -EINVAL; 4186\t} 4187 4188\tif (attr-\u0026gt;sched_flags \u0026amp; ~(SCHED_FLAG_ALL | SCHED_FLAG_SUGOV)) 4189\treturn -EINVAL; 4190 4191\t/* 4192* Valid priorities for SCHED_FIFO and SCHED_RR are 4193* 1..MAX_USER_RT_PRIO-1, valid priority for SCHED_NORMAL, 4194* SCHED_BATCH and SCHED_IDLE is 0. 4195*/ 4196\tif ((p-\u0026gt;mm \u0026amp;\u0026amp; attr-\u0026gt;sched_priority \u0026gt; MAX_USER_RT_PRIO-1) || 4197\t(!p-\u0026gt;mm \u0026amp;\u0026amp; attr-\u0026gt;sched_priority \u0026gt; MAX_RT_PRIO-1)) 4198\treturn -EINVAL; 4199\tif ((dl_policy(policy) \u0026amp;\u0026amp; !__checkparam_dl(attr)) || 4200\t(rt_policy(policy) != (attr-\u0026gt;sched_priority != 0))) 4201\treturn -EINVAL; 4202 4203\t/* 4204* Allow unprivileged RT tasks to decrease priority: 4205*/ 4206\tif (user \u0026amp;\u0026amp; !capable(CAP_SYS_NICE)) { 4207\tif (fair_policy(policy)) { 4208\tif (attr-\u0026gt;sched_nice \u0026lt; task_nice(p) \u0026amp;\u0026amp; 4209\t!can_nice(p, attr-\u0026gt;sched_nice)) 4210\treturn -EPERM; 4211\t} 4212 4213\tif (rt_policy(policy)) { 4214\tunsigned long rlim_rtprio = 4215\ttask_rlimit(p, RLIMIT_RTPRIO); 4216 4217\t/* Can\u0026#39;t set/change the rt policy: */ 4218\tif (policy != p-\u0026gt;policy \u0026amp;\u0026amp; !rlim_rtprio) 4219\treturn -EPERM; 4220 4221\t/* Can\u0026#39;t increase priority: */ 4222\tif (attr-\u0026gt;sched_priority \u0026gt; p-\u0026gt;rt_priority \u0026amp;\u0026amp; 4223\tattr-\u0026gt;sched_priority \u0026gt; rlim_rtprio) 4224\treturn -EPERM; 4225\t} 4226 4227\t/* 4228* Can\u0026#39;t set/change SCHED_DEADLINE policy at all for now 4229* (safest behavior); in the future we would like to allow 4230* unprivileged DL tasks to increase their relative deadline 4231* or reduce their runtime (both ways reducing utilization) 4232*/ 4233\tif (dl_policy(policy)) 4234\treturn -EPERM; 4235 4236\t/* 4237* Treat SCHED_IDLE as nice 20. Only allow a switch to 4238* SCHED_NORMAL if the RLIMIT_NICE would normally permit it. 4239*/ 4240\tif (idle_policy(p-\u0026gt;policy) \u0026amp;\u0026amp; !idle_policy(policy)) { 4241\tif (!can_nice(p, task_nice(p))) 4242\treturn -EPERM; 4243\t} 4244 4245\t/* Can\u0026#39;t change other user\u0026#39;s priorities: */ 4246\tif (!check_same_owner(p)) 4247\treturn -EPERM; 4248 4249\t/* Normal users shall not reset the sched_reset_on_fork flag: */ 4250\tif (p-\u0026gt;sched_reset_on_fork \u0026amp;\u0026amp; !reset_on_fork) 4251\treturn -EPERM; 4252\t} 4253 4254\tif (user) { 4255\tif (attr-\u0026gt;sched_flags \u0026amp; SCHED_FLAG_SUGOV) 4256\treturn -EINVAL; 4257 4258\tretval = security_task_setscheduler(p); 4259\tif (retval) 4260\treturn retval; 4261\t} 4262 4263\t/* 4264* Make sure no PI-waiters arrive (or leave) while we are 4265* changing the priority of the task: 4266* 4267* To be able to change p-\u0026gt;policy safely, the appropriate 4268* runqueue lock must be held. 4269*/ 4270\trq = task_rq_lock(p, \u0026amp;rf); 4271\tupdate_rq_clock(rq); 4272 4273\t/* 4274* Changing the policy of the stop threads its a very bad idea: 4275*/ 4276\tif (p == rq-\u0026gt;stop) { 4277\ttask_rq_unlock(rq, p, \u0026amp;rf); 4278\treturn -EINVAL; 4279\t} 4280 4281\t/* 4282* If not changing anything there\u0026#39;s no need to proceed further, 4283* but store a possible modification of reset_on_fork. 4284*/ 4285\tif (unlikely(policy == p-\u0026gt;policy)) { 4286\tif (fair_policy(policy) \u0026amp;\u0026amp; attr-\u0026gt;sched_nice != task_nice(p)) 4287\tgoto change; 4288\tif (rt_policy(policy) \u0026amp;\u0026amp; attr-\u0026gt;sched_priority != p-\u0026gt;rt_priority) 4289\tgoto change; 4290\tif (dl_policy(policy) \u0026amp;\u0026amp; dl_param_changed(p, attr)) 4291\tgoto change; 4292 4293\tp-\u0026gt;sched_reset_on_fork = reset_on_fork; 4294\ttask_rq_unlock(rq, p, \u0026amp;rf); 4295\treturn 0; 4296\t} 4297change: 4298 4299\tif (user) { 4300#ifdef CONFIG_RT_GROUP_SCHED 4301\t/* 4302* Do not allow realtime tasks into groups that have no runtime 4303* assigned. 4304*/ 4305\tif (rt_bandwidth_enabled() \u0026amp;\u0026amp; rt_policy(policy) \u0026amp;\u0026amp; 4306\ttask_group(p)-\u0026gt;rt_bandwidth.rt_runtime == 0 \u0026amp;\u0026amp; 4307\t!task_group_is_autogroup(task_group(p))) { 4308\ttask_rq_unlock(rq, p, \u0026amp;rf); 4309\treturn -EPERM; 4310\t} 4311#endif 4312#ifdef CONFIG_SMP 4313\tif (dl_bandwidth_enabled() \u0026amp;\u0026amp; dl_policy(policy) \u0026amp;\u0026amp; 4314\t!(attr-\u0026gt;sched_flags \u0026amp; SCHED_FLAG_SUGOV)) { 4315\tcpumask_t *span = rq-\u0026gt;rd-\u0026gt;span; 4316 4317\t/* 4318* Don\u0026#39;t allow tasks with an affinity mask smaller than 4319* the entire root_domain to become SCHED_DEADLINE. We 4320* will also fail if there\u0026#39;s no bandwidth available. 4321*/ 4322\tif (!cpumask_subset(span, \u0026amp;p-\u0026gt;cpus_allowed) || 4323\trq-\u0026gt;rd-\u0026gt;dl_bw.bw == 0) { 4324\ttask_rq_unlock(rq, p, \u0026amp;rf); 4325\treturn -EPERM; 4326\t} 4327\t} 4328#endif 4329\t} 4330 4331\t/* Re-check policy now with rq lock held: */ 4332\tif (unlikely(oldpolicy != -1 \u0026amp;\u0026amp; oldpolicy != p-\u0026gt;policy)) { 4333\tpolicy = oldpolicy = -1; 4334\ttask_rq_unlock(rq, p, \u0026amp;rf); 4335\tgoto recheck; 4336\t} 4337 4338\t/* 4339* If setscheduling to SCHED_DEADLINE (or changing the parameters 4340* of a SCHED_DEADLINE task) we need to check if enough bandwidth 4341* is available. 4342*/ 4343\tif ((dl_policy(policy) || dl_task(p)) \u0026amp;\u0026amp; sched_dl_overflow(p, policy, attr)) { 4344\ttask_rq_unlock(rq, p, \u0026amp;rf); 4345\treturn -EBUSY; 4346\t} 4347 4348\tp-\u0026gt;sched_reset_on_fork = reset_on_fork; 4349\toldprio = p-\u0026gt;prio; 4350 4351\tif (pi) { 4352\t/* 4353* Take priority boosted tasks into account. If the new 4354* effective priority is unchanged, we just store the new 4355* normal parameters and do not touch the scheduler class and 4356* the runqueue. This will be done when the task deboost 4357* itself. 4358*/ 4359\tnew_effective_prio = rt_effective_prio(p, newprio); 4360\tif (new_effective_prio == oldprio) 4361\tqueue_flags \u0026amp;= ~DEQUEUE_MOVE; 4362\t} 4363 4364\tqueued = task_on_rq_queued(p); 4365\trunning = task_current(rq, p); 4366\tif (queued) 4367\tdequeue_task(rq, p, queue_flags); 4368\tif (running) 4369\tput_prev_task(rq, p); 4370 4371\tprev_class = p-\u0026gt;sched_class; 4372\t__setscheduler(rq, p, attr, pi); 4373 4374\tif (queued) { 4375\t/* 4376* We enqueue to tail when the priority of a task is 4377* increased (user space view). 4378*/ 4379\tif (oldprio \u0026lt; p-\u0026gt;prio) 4380\tqueue_flags |= ENQUEUE_HEAD; 4381 4382\tenqueue_task(rq, p, queue_flags); 4383\t} 4384\tif (running) 4385\tset_curr_task(rq, p); 4386 4387\tcheck_class_changed(rq, p, prev_class, oldprio); 4388 4389\t/* Avoid rq from going away on us: */ 4390\tpreempt_disable(); 4391\ttask_rq_unlock(rq, p, \u0026amp;rf); 4392 4393\tif (pi) 4394\trt_mutex_adjust_pi(p); 4395 4396\t/* Run balance callbacks after we\u0026#39;ve adjusted the PI chain: */ 4397\tbalance_callback(rq); 4398\tpreempt_enable(); 4399 4400\treturn 0; 4401} 4402 4403static int _sched_setscheduler(struct task_struct *p, int policy, 4404\tconst struct sched_param *param, bool check) 4405{ 4406\tstruct sched_attr attr = { 4407\t.sched_policy = policy, 4408\t.sched_priority = param-\u0026gt;sched_priority, 4409\t.sched_nice\t= PRIO_TO_NICE(p-\u0026gt;static_prio), 4410\t}; 4411 4412\t/* Fixup the legacy SCHED_RESET_ON_FORK hack. */ 4413\tif ((policy != SETPARAM_POLICY) \u0026amp;\u0026amp; (policy \u0026amp; SCHED_RESET_ON_FORK)) { 4414\tattr.sched_flags |= SCHED_FLAG_RESET_ON_FORK; 4415\tpolicy \u0026amp;= ~SCHED_RESET_ON_FORK; 4416\tattr.sched_policy = policy; 4417\t} 4418 4419\treturn __sched_setscheduler(p, \u0026amp;attr, check, true); 4420} 4421/** 4422* sched_setscheduler - change the scheduling policy and/or RT priority of a thread. 4423* @p: the task in question. 4424* @policy: new policy. 4425* @param: structure containing the new RT priority. 4426* 4427* Return: 0 on success. An error code otherwise. 4428* 4429* NOTE that the task may be already dead. 4430*/ 4431int sched_setscheduler(struct task_struct *p, int policy, 4432\tconst struct sched_param *param) 4433{ 4434\treturn _sched_setscheduler(p, policy, param, true); 4435} 4436EXPORT_SYMBOL_GPL(sched_setscheduler); 4437 4438int sched_setattr(struct task_struct *p, const struct sched_attr *attr) 4439{ 4440\treturn __sched_setscheduler(p, attr, true, true); 4441} 4442EXPORT_SYMBOL_GPL(sched_setattr); 4443 4444int sched_setattr_nocheck(struct task_struct *p, const struct sched_attr *attr) 4445{ 4446\treturn __sched_setscheduler(p, attr, false, true); 4447} 4448 4449/** 4450* sched_setscheduler_nocheck - change the scheduling policy and/or RT priority of a thread from kernelspace. 4451* @p: the task in question. 4452* @policy: new policy. 4453* @param: structure containing the new RT priority. 4454* 4455* Just like sched_setscheduler, only don\u0026#39;t bother checking if the 4456* current context has permission. For example, this is needed in 4457* stop_machine(): we create temporary high priority worker threads, 4458* but our caller might not have that capability. 4459* 4460* Return: 0 on success. An error code otherwise. 4461*/ 4462int sched_setscheduler_nocheck(struct task_struct *p, int policy, 4463\tconst struct sched_param *param) 4464{ 4465\treturn _sched_setscheduler(p, policy, param, false); 4466} 4467EXPORT_SYMBOL_GPL(sched_setscheduler_nocheck); 4468 4469static int 4470do_sched_setscheduler(pid_t pid, int policy, struct sched_param __user *param) 4471{ 4472\tstruct sched_param lparam; 4473\tstruct task_struct *p; 4474\tint retval; 4475 4476\tif (!param || pid \u0026lt; 0) 4477\treturn -EINVAL; 4478\tif (copy_from_user(\u0026amp;lparam, param, sizeof(struct sched_param))) 4479\treturn -EFAULT; 4480 4481\trcu_read_lock(); 4482\tretval = -ESRCH; 4483\tp = find_process_by_pid(pid); 4484\tif (p != NULL) 4485\tretval = sched_setscheduler(p, policy, \u0026amp;lparam); 4486\trcu_read_unlock(); 4487 4488\treturn retval; 4489} 4490 4491/* 4492* Mimics kernel/events/core.c perf_copy_attr(). 4493*/ 4494static int sched_copy_attr(struct sched_attr __user *uattr, struct sched_attr *attr) 4495{ 4496\tu32 size; 4497\tint ret; 4498 4499\tif (!access_ok(VERIFY_WRITE, uattr, SCHED_ATTR_SIZE_VER0)) 4500\treturn -EFAULT; 4501 4502\t/* Zero the full structure, so that a short copy will be nice: */ 4503\tmemset(attr, 0, sizeof(*attr)); 4504 4505\tret = get_user(size, \u0026amp;uattr-\u0026gt;size); 4506\tif (ret) 4507\treturn ret; 4508 4509\t/* Bail out on silly large: */ 4510\tif (size \u0026gt; PAGE_SIZE) 4511\tgoto err_size; 4512 4513\t/* ABI compatibility quirk: */ 4514\tif (!size) 4515\tsize = SCHED_ATTR_SIZE_VER0; 4516 4517\tif (size \u0026lt; SCHED_ATTR_SIZE_VER0) 4518\tgoto err_size; 4519 4520\t/* 4521* If we\u0026#39;re handed a bigger struct than we know of, 4522* ensure all the unknown bits are 0 - i.e. new 4523* user-space does not rely on any kernel feature 4524* extensions we dont know about yet. 4525*/ 4526\tif (size \u0026gt; sizeof(*attr)) { 4527\tunsigned char __user *addr; 4528\tunsigned char __user *end; 4529\tunsigned char val; 4530 4531\taddr = (void __user *)uattr + sizeof(*attr); 4532\tend = (void __user *)uattr + size; 4533 4534\tfor (; addr \u0026lt; end; addr++) { 4535\tret = get_user(val, addr); 4536\tif (ret) 4537\treturn ret; 4538\tif (val) 4539\tgoto err_size; 4540\t} 4541\tsize = sizeof(*attr); 4542\t} 4543 4544\tret = copy_from_user(attr, uattr, size); 4545\tif (ret) 4546\treturn -EFAULT; 4547 4548\t/* 4549* XXX: Do we want to be lenient like existing syscalls; or do we want 4550* to be strict and return an error on out-of-bounds values? 4551*/ 4552\tattr-\u0026gt;sched_nice = clamp(attr-\u0026gt;sched_nice, MIN_NICE, MAX_NICE); 4553 4554\treturn 0; 4555 4556err_size: 4557\tput_user(sizeof(*attr), \u0026amp;uattr-\u0026gt;size); 4558\treturn -E2BIG; 4559} 4560 4561/** 4562* sys_sched_setscheduler - set/change the scheduler policy and RT priority 4563* @pid: the pid in question. 4564* @policy: new policy. 4565* @param: structure containing the new RT priority. 4566* 4567* Return: 0 on success. An error code otherwise. 4568*/ 4569SYSCALL_DEFINE3(sched_setscheduler, pid_t, pid, int, policy, struct sched_param __user *, param) 4570{ 4571\tif (policy \u0026lt; 0) 4572\treturn -EINVAL; 4573 4574\treturn do_sched_setscheduler(pid, policy, param); 4575} 4576 4577/** 4578* sys_sched_setparam - set/change the RT priority of a thread 4579* @pid: the pid in question. 4580* @param: structure containing the new RT priority. 4581* 4582* Return: 0 on success. An error code otherwise. 4583*/ 4584SYSCALL_DEFINE2(sched_setparam, pid_t, pid, struct sched_param __user *, param) 4585{ 4586\treturn do_sched_setscheduler(pid, SETPARAM_POLICY, param); 4587} 4588 4589/** 4590* sys_sched_setattr - same as above, but with extended sched_attr 4591* @pid: the pid in question. 4592* @uattr: structure containing the extended parameters. 4593* @flags: for future extension. 4594*/ 4595SYSCALL_DEFINE3(sched_setattr, pid_t, pid, struct sched_attr __user *, uattr, 4596\tunsigned int, flags) 4597{ 4598\tstruct sched_attr attr; 4599\tstruct task_struct *p; 4600\tint retval; 4601 4602\tif (!uattr || pid \u0026lt; 0 || flags) 4603\treturn -EINVAL; 4604 4605\tretval = sched_copy_attr(uattr, \u0026amp;attr); 4606\tif (retval) 4607\treturn retval; 4608 4609\tif ((int)attr.sched_policy \u0026lt; 0) 4610\treturn -EINVAL; 4611 4612\trcu_read_lock(); 4613\tretval = -ESRCH; 4614\tp = find_process_by_pid(pid); 4615\tif (p != NULL) 4616\tretval = sched_setattr(p, \u0026amp;attr); 4617\trcu_read_unlock(); 4618 4619\treturn retval; 4620} 4621 4622/** 4623* sys_sched_getscheduler - get the policy (scheduling class) of a thread 4624* @pid: the pid in question. 4625* 4626* Return: On success, the policy of the thread. Otherwise, a negative error 4627* code. 4628*/ 4629SYSCALL_DEFINE1(sched_getscheduler, pid_t, pid) 4630{ 4631\tstruct task_struct *p; 4632\tint retval; 4633 4634\tif (pid \u0026lt; 0) 4635\treturn -EINVAL; 4636 4637\tretval = -ESRCH; 4638\trcu_read_lock(); 4639\tp = find_process_by_pid(pid); 4640\tif (p) { 4641\tretval = security_task_getscheduler(p); 4642\tif (!retval) 4643\tretval = p-\u0026gt;policy 4644\t| (p-\u0026gt;sched_reset_on_fork ? SCHED_RESET_ON_FORK : 0); 4645\t} 4646\trcu_read_unlock(); 4647\treturn retval; 4648} 4649 4650/** 4651* sys_sched_getparam - get the RT priority of a thread 4652* @pid: the pid in question. 4653* @param: structure containing the RT priority. 4654* 4655* Return: On success, 0 and the RT priority is in @param. Otherwise, an error 4656* code. 4657*/ 4658SYSCALL_DEFINE2(sched_getparam, pid_t, pid, struct sched_param __user *, param) 4659{ 4660\tstruct sched_param lp = { .sched_priority = 0 }; 4661\tstruct task_struct *p; 4662\tint retval; 4663 4664\tif (!param || pid \u0026lt; 0) 4665\treturn -EINVAL; 4666 4667\trcu_read_lock(); 4668\tp = find_process_by_pid(pid); 4669\tretval = -ESRCH; 4670\tif (!p) 4671\tgoto out_unlock; 4672 4673\tretval = security_task_getscheduler(p); 4674\tif (retval) 4675\tgoto out_unlock; 4676 4677\tif (task_has_rt_policy(p)) 4678\tlp.sched_priority = p-\u0026gt;rt_priority; 4679\trcu_read_unlock(); 4680 4681\t/* 4682* This one might sleep, we cannot do it with a spinlock held ... 4683*/ 4684\tretval = copy_to_user(param, \u0026amp;lp, sizeof(*param)) ? -EFAULT : 0; 4685 4686\treturn retval; 4687 4688out_unlock: 4689\trcu_read_unlock(); 4690\treturn retval; 4691} 4692 4693static int sched_read_attr(struct sched_attr __user *uattr, 4694\tstruct sched_attr *attr, 4695\tunsigned int usize) 4696{ 4697\tint ret; 4698 4699\tif (!access_ok(VERIFY_WRITE, uattr, usize)) 4700\treturn -EFAULT; 4701 4702\t/* 4703* If we\u0026#39;re handed a smaller struct than we know of, 4704* ensure all the unknown bits are 0 - i.e. old 4705* user-space does not get uncomplete information. 4706*/ 4707\tif (usize \u0026lt; sizeof(*attr)) { 4708\tunsigned char *addr; 4709\tunsigned char *end; 4710 4711\taddr = (void *)attr + usize; 4712\tend = (void *)attr + sizeof(*attr); 4713 4714\tfor (; addr \u0026lt; end; addr++) { 4715\tif (*addr) 4716\treturn -EFBIG; 4717\t} 4718 4719\tattr-\u0026gt;size = usize; 4720\t} 4721 4722\tret = copy_to_user(uattr, attr, attr-\u0026gt;size); 4723\tif (ret) 4724\treturn -EFAULT; 4725 4726\treturn 0; 4727} 4728 4729/** 4730* sys_sched_getattr - similar to sched_getparam, but with sched_attr 4731* @pid: the pid in question. 4732* @uattr: structure containing the extended parameters. 4733* @size: sizeof(attr) for fwd/bwd comp. 4734* @flags: for future extension. 4735*/ 4736SYSCALL_DEFINE4(sched_getattr, pid_t, pid, struct sched_attr __user *, uattr, 4737\tunsigned int, size, unsigned int, flags) 4738{ 4739\tstruct sched_attr attr = { 4740\t.size = sizeof(struct sched_attr), 4741\t}; 4742\tstruct task_struct *p; 4743\tint retval; 4744 4745\tif (!uattr || pid \u0026lt; 0 || size \u0026gt; PAGE_SIZE || 4746\tsize \u0026lt; SCHED_ATTR_SIZE_VER0 || flags) 4747\treturn -EINVAL; 4748 4749\trcu_read_lock(); 4750\tp = find_process_by_pid(pid); 4751\tretval = -ESRCH; 4752\tif (!p) 4753\tgoto out_unlock; 4754 4755\tretval = security_task_getscheduler(p); 4756\tif (retval) 4757\tgoto out_unlock; 4758 4759\tattr.sched_policy = p-\u0026gt;policy; 4760\tif (p-\u0026gt;sched_reset_on_fork) 4761\tattr.sched_flags |= SCHED_FLAG_RESET_ON_FORK; 4762\tif (task_has_dl_policy(p)) 4763\t__getparam_dl(p, \u0026amp;attr); 4764\telse if (task_has_rt_policy(p)) 4765\tattr.sched_priority = p-\u0026gt;rt_priority; 4766\telse 4767\tattr.sched_nice = task_nice(p); 4768 4769\trcu_read_unlock(); 4770 4771\tretval = sched_read_attr(uattr, \u0026amp;attr, size); 4772\treturn retval; 4773 4774out_unlock: 4775\trcu_read_unlock(); 4776\treturn retval; 4777} 4778 4779long sched_setaffinity(pid_t pid, const struct cpumask *in_mask) 4780{ 4781\tcpumask_var_t cpus_allowed, new_mask; 4782\tstruct task_struct *p; 4783\tint retval; 4784 4785\trcu_read_lock(); 4786 4787\tp = find_process_by_pid(pid); 4788\tif (!p) { 4789\trcu_read_unlock(); 4790\treturn -ESRCH; 4791\t} 4792 4793\t/* Prevent p going away */ 4794\tget_task_struct(p); 4795\trcu_read_unlock(); 4796 4797\tif (p-\u0026gt;flags \u0026amp; PF_NO_SETAFFINITY) { 4798\tretval = -EINVAL; 4799\tgoto out_put_task; 4800\t} 4801\tif (!alloc_cpumask_var(\u0026amp;cpus_allowed, GFP_KERNEL)) { 4802\tretval = -ENOMEM; 4803\tgoto out_put_task; 4804\t} 4805\tif (!alloc_cpumask_var(\u0026amp;new_mask, GFP_KERNEL)) { 4806\tretval = -ENOMEM; 4807\tgoto out_free_cpus_allowed; 4808\t} 4809\tretval = -EPERM; 4810\tif (!check_same_owner(p)) { 4811\trcu_read_lock(); 4812\tif (!ns_capable(__task_cred(p)-\u0026gt;user_ns, CAP_SYS_NICE)) { 4813\trcu_read_unlock(); 4814\tgoto out_free_new_mask; 4815\t} 4816\trcu_read_unlock(); 4817\t} 4818 4819\tretval = security_task_setscheduler(p); 4820\tif (retval) 4821\tgoto out_free_new_mask; 4822 4823 4824\tcpuset_cpus_allowed(p, cpus_allowed); 4825\tcpumask_and(new_mask, in_mask, cpus_allowed); 4826 4827\t/* 4828* Since bandwidth control happens on root_domain basis, 4829* if admission test is enabled, we only admit -deadline 4830* tasks allowed to run on all the CPUs in the task\u0026#39;s 4831* root_domain. 4832*/ 4833#ifdef CONFIG_SMP 4834\tif (task_has_dl_policy(p) \u0026amp;\u0026amp; dl_bandwidth_enabled()) { 4835\trcu_read_lock(); 4836\tif (!cpumask_subset(task_rq(p)-\u0026gt;rd-\u0026gt;span, new_mask)) { 4837\tretval = -EBUSY; 4838\trcu_read_unlock(); 4839\tgoto out_free_new_mask; 4840\t} 4841\trcu_read_unlock(); 4842\t} 4843#endif 4844again: 4845\tretval = __set_cpus_allowed_ptr(p, new_mask, true); 4846 4847\tif (!retval) { 4848\tcpuset_cpus_allowed(p, cpus_allowed); 4849\tif (!cpumask_subset(new_mask, cpus_allowed)) { 4850\t/* 4851* We must have raced with a concurrent cpuset 4852* update. Just reset the cpus_allowed to the 4853* cpuset\u0026#39;s cpus_allowed 4854*/ 4855\tcpumask_copy(new_mask, cpus_allowed); 4856\tgoto again; 4857\t} 4858\t} 4859out_free_new_mask: 4860\tfree_cpumask_var(new_mask); 4861out_free_cpus_allowed: 4862\tfree_cpumask_var(cpus_allowed); 4863out_put_task: 4864\tput_task_struct(p); 4865\treturn retval; 4866} 4867 4868static int get_user_cpu_mask(unsigned long __user *user_mask_ptr, unsigned len, 4869\tstruct cpumask *new_mask) 4870{ 4871\tif (len \u0026lt; cpumask_size()) 4872\tcpumask_clear(new_mask); 4873\telse if (len \u0026gt; cpumask_size()) 4874\tlen = cpumask_size(); 4875 4876\treturn copy_from_user(new_mask, user_mask_ptr, len) ? -EFAULT : 0; 4877} 4878 4879/** 4880* sys_sched_setaffinity - set the CPU affinity of a process 4881* @pid: pid of the process 4882* @len: length in bytes of the bitmask pointed to by user_mask_ptr 4883* @user_mask_ptr: user-space pointer to the new CPU mask 4884* 4885* Return: 0 on success. An error code otherwise. 4886*/ 4887SYSCALL_DEFINE3(sched_setaffinity, pid_t, pid, unsigned int, len, 4888\tunsigned long __user *, user_mask_ptr) 4889{ 4890\tcpumask_var_t new_mask; 4891\tint retval; 4892 4893\tif (!alloc_cpumask_var(\u0026amp;new_mask, GFP_KERNEL)) 4894\treturn -ENOMEM; 4895 4896\tretval = get_user_cpu_mask(user_mask_ptr, len, new_mask); 4897\tif (retval == 0) 4898\tretval = sched_setaffinity(pid, new_mask); 4899\tfree_cpumask_var(new_mask); 4900\treturn retval; 4901} 4902 4903long sched_getaffinity(pid_t pid, struct cpumask *mask) 4904{ 4905\tstruct task_struct *p; 4906\tunsigned long flags; 4907\tint retval; 4908 4909\trcu_read_lock(); 4910 4911\tretval = -ESRCH; 4912\tp = find_process_by_pid(pid); 4913\tif (!p) 4914\tgoto out_unlock; 4915 4916\tretval = security_task_getscheduler(p); 4917\tif (retval) 4918\tgoto out_unlock; 4919 4920\traw_spin_lock_irqsave(\u0026amp;p-\u0026gt;pi_lock, flags); 4921\tcpumask_and(mask, \u0026amp;p-\u0026gt;cpus_allowed, cpu_active_mask); 4922\traw_spin_unlock_irqrestore(\u0026amp;p-\u0026gt;pi_lock, flags); 4923 4924out_unlock: 4925\trcu_read_unlock(); 4926 4927\treturn retval; 4928} 4929 4930/** 4931* sys_sched_getaffinity - get the CPU affinity of a process 4932* @pid: pid of the process 4933* @len: length in bytes of the bitmask pointed to by user_mask_ptr 4934* @user_mask_ptr: user-space pointer to hold the current CPU mask 4935* 4936* Return: size of CPU mask copied to user_mask_ptr on success. An 4937* error code otherwise. 4938*/ 4939SYSCALL_DEFINE3(sched_getaffinity, pid_t, pid, unsigned int, len, 4940\tunsigned long __user *, user_mask_ptr) 4941{ 4942\tint ret; 4943\tcpumask_var_t mask; 4944 4945\tif ((len * BITS_PER_BYTE) \u0026lt; nr_cpu_ids) 4946\treturn -EINVAL; 4947\tif (len \u0026amp; (sizeof(unsigned long)-1)) 4948\treturn -EINVAL; 4949 4950\tif (!alloc_cpumask_var(\u0026amp;mask, GFP_KERNEL)) 4951\treturn -ENOMEM; 4952 4953\tret = sched_getaffinity(pid, mask); 4954\tif (ret == 0) { 4955\tunsigned int retlen = min(len, cpumask_size()); 4956 4957\tif (copy_to_user(user_mask_ptr, mask, retlen)) 4958\tret = -EFAULT; 4959\telse 4960\tret = retlen; 4961\t} 4962\tfree_cpumask_var(mask); 4963 4964\treturn ret; 4965} 4966 4967/** 4968* sys_sched_yield - yield the current processor to other threads. 4969* 4970* This function yields the current CPU to other tasks. If there are no 4971* other threads running on this CPU then this function will return. 4972* 4973* Return: 0. 4974*/ 4975static void do_sched_yield(void) 4976{ 4977\tstruct rq_flags rf; 4978\tstruct rq *rq; 4979 4980\tlocal_irq_disable(); 4981\trq = this_rq(); 4982\trq_lock(rq, \u0026amp;rf); 4983 4984\tschedstat_inc(rq-\u0026gt;yld_count); 4985\tcurrent-\u0026gt;sched_class-\u0026gt;yield_task(rq); 4986 4987\tpreempt_disable(); 4988\trq_unlock_irq(rq, \u0026amp;rf); 4989\tsched_preempt_enable_no_resched(); 4990 4991\tschedule(); 4992} 4993 4994SYSCALL_DEFINE0(sched_yield) 4995{ 4996\tdo_sched_yield(); 4997\treturn 0; 4998} 4999 5000#ifndef CONFIG_PREEMPT 5001int __sched _cond_resched(void) 5002{ 5003\tif (should_resched(0)) { 5004\tpreempt_schedule_common(); 5005\treturn 1; 5006\t} 5007\trcu_all_qs(); 5008\treturn 0; 5009} 5010EXPORT_SYMBOL(_cond_resched); 5011#endif 5012 5013/* 5014* __cond_resched_lock() - if a reschedule is pending, drop the given lock, 5015* call schedule, and on return reacquire the lock. 5016* 5017* This works OK both with and without CONFIG_PREEMPT. We do strange low-level 5018* operations here to prevent schedule() from being called twice (once via 5019* spin_unlock(), once by hand). 5020*/ 5021int __cond_resched_lock(spinlock_t *lock) 5022{ 5023\tint resched = should_resched(PREEMPT_LOCK_OFFSET); 5024\tint ret = 0; 5025 5026\tlockdep_assert_held(lock); 5027 5028\tif (spin_needbreak(lock) || resched) { 5029\tspin_unlock(lock); 5030\tif (resched) 5031\tpreempt_schedule_common(); 5032\telse 5033\tcpu_relax(); 5034\tret = 1; 5035\tspin_lock(lock); 5036\t} 5037\treturn ret; 5038} 5039EXPORT_SYMBOL(__cond_resched_lock); 5040 5041/** 5042* yield - yield the current processor to other threads. 5043* 5044* Do not ever use this function, there\u0026#39;s a 99% chance you\u0026#39;re doing it wrong. 5045* 5046* The scheduler is at all times free to pick the calling task as the most 5047* eligible task to run, if removing the yield() call from your code breaks 5048* it, its already broken. 5049* 5050* Typical broken usage is: 5051* 5052* while (!event) 5053*\tyield(); 5054* 5055* where one assumes that yield() will let \u0026#39;the other\u0026#39; process run that will 5056* make event true. If the current task is a SCHED_FIFO task that will never 5057* happen. Never use yield() as a progress guarantee!! 5058* 5059* If you want to use yield() to wait for something, use wait_event(). 5060* If you want to use yield() to be \u0026#39;nice\u0026#39; for others, use cond_resched(). 5061* If you still want to use yield(), do not! 5062*/ 5063void __sched yield(void) 5064{ 5065\tset_current_state(TASK_RUNNING); 5066\tdo_sched_yield(); 5067} 5068EXPORT_SYMBOL(yield); 5069 5070/** 5071* yield_to - yield the current processor to another thread in 5072* your thread group, or accelerate that thread toward the 5073* processor it\u0026#39;s on. 5074* @p: target task 5075* @preempt: whether task preemption is allowed or not 5076* 5077* It\u0026#39;s the caller\u0026#39;s job to ensure that the target task struct 5078* can\u0026#39;t go away on us before we can do any checks. 5079* 5080* Return: 5081*\ttrue (\u0026gt;0) if we indeed boosted the target task. 5082*\tfalse (0) if we failed to boost the target. 5083*\t-ESRCH if there\u0026#39;s no task to yield to. 5084*/ 5085int __sched yield_to(struct task_struct *p, bool preempt) 5086{ 5087\tstruct task_struct *curr = current; 5088\tstruct rq *rq, *p_rq; 5089\tunsigned long flags; 5090\tint yielded = 0; 5091 5092\tlocal_irq_save(flags); 5093\trq = this_rq(); 5094 5095again: 5096\tp_rq = task_rq(p); 5097\t/* 5098* If we\u0026#39;re the only runnable task on the rq and target rq also 5099* has only one task, there\u0026#39;s absolutely no point in yielding. 5100*/ 5101\tif (rq-\u0026gt;nr_running == 1 \u0026amp;\u0026amp; p_rq-\u0026gt;nr_running == 1) { 5102\tyielded = -ESRCH; 5103\tgoto out_irq; 5104\t} 5105 5106\tdouble_rq_lock(rq, p_rq); 5107\tif (task_rq(p) != p_rq) { 5108\tdouble_rq_unlock(rq, p_rq); 5109\tgoto again; 5110\t} 5111 5112\tif (!curr-\u0026gt;sched_class-\u0026gt;yield_to_task) 5113\tgoto out_unlock; 5114 5115\tif (curr-\u0026gt;sched_class != p-\u0026gt;sched_class) 5116\tgoto out_unlock; 5117 5118\tif (task_running(p_rq, p) || p-\u0026gt;state) 5119\tgoto out_unlock; 5120 5121\tyielded = curr-\u0026gt;sched_class-\u0026gt;yield_to_task(rq, p, preempt); 5122\tif (yielded) { 5123\tschedstat_inc(rq-\u0026gt;yld_count); 5124\t/* 5125* Make p\u0026#39;s CPU reschedule; pick_next_entity takes care of 5126* fairness. 5127*/ 5128\tif (preempt \u0026amp;\u0026amp; rq != p_rq) 5129\tresched_curr(p_rq); 5130\t} 5131 5132out_unlock: 5133\tdouble_rq_unlock(rq, p_rq); 5134out_irq: 5135\tlocal_irq_restore(flags); 5136 5137\tif (yielded \u0026gt; 0) 5138\tschedule(); 5139 5140\treturn yielded; 5141} 5142EXPORT_SYMBOL_GPL(yield_to); 5143 5144int io_schedule_prepare(void) 5145{ 5146\tint old_iowait = current-\u0026gt;in_iowait; 5147 5148\tcurrent-\u0026gt;in_iowait = 1; 5149\tblk_schedule_flush_plug(current); 5150 5151\treturn old_iowait; 5152} 5153 5154void io_schedule_finish(int token) 5155{ 5156\tcurrent-\u0026gt;in_iowait = token; 5157} 5158 5159/* 5160* This task is about to go to sleep on IO. Increment rq-\u0026gt;nr_iowait so 5161* that process accounting knows that this is a task in IO wait state. 5162*/ 5163long __sched io_schedule_timeout(long timeout) 5164{ 5165\tint token; 5166\tlong ret; 5167 5168\ttoken = io_schedule_prepare(); 5169\tret = schedule_timeout(timeout); 5170\tio_schedule_finish(token); 5171 5172\treturn ret; 5173} 5174EXPORT_SYMBOL(io_schedule_timeout); 5175 5176void __sched io_schedule(void) 5177{ 5178\tint token; 5179 5180\ttoken = io_schedule_prepare(); 5181\tschedule(); 5182\tio_schedule_finish(token); 5183} 5184EXPORT_SYMBOL(io_schedule); 5185 5186/** 5187* sys_sched_get_priority_max - return maximum RT priority. 5188* @policy: scheduling class. 5189* 5190* Return: On success, this syscall returns the maximum 5191* rt_priority that can be used by a given scheduling class. 5192* On failure, a negative error code is returned. 5193*/ 5194SYSCALL_DEFINE1(sched_get_priority_max, int, policy) 5195{ 5196\tint ret = -EINVAL; 5197 5198\tswitch (policy) { 5199\tcase SCHED_FIFO: 5200\tcase SCHED_RR: 5201\tret = MAX_USER_RT_PRIO-1; 5202\tbreak; 5203\tcase SCHED_DEADLINE: 5204\tcase SCHED_NORMAL: 5205\tcase SCHED_BATCH: 5206\tcase SCHED_IDLE: 5207\tret = 0; 5208\tbreak; 5209\t} 5210\treturn ret; 5211} 5212 5213/** 5214* sys_sched_get_priority_min - return minimum RT priority. 5215* @policy: scheduling class. 5216* 5217* Return: On success, this syscall returns the minimum 5218* rt_priority that can be used by a given scheduling class. 5219* On failure, a negative error code is returned. 5220*/ 5221SYSCALL_DEFINE1(sched_get_priority_min, int, policy) 5222{ 5223\tint ret = -EINVAL; 5224 5225\tswitch (policy) { 5226\tcase SCHED_FIFO: 5227\tcase SCHED_RR: 5228\tret = 1; 5229\tbreak; 5230\tcase SCHED_DEADLINE: 5231\tcase SCHED_NORMAL: 5232\tcase SCHED_BATCH: 5233\tcase SCHED_IDLE: 5234\tret = 0; 5235\t} 5236\treturn ret; 5237} 5238 5239static int sched_rr_get_interval(pid_t pid, struct timespec64 *t) 5240{ 5241\tstruct task_struct *p; 5242\tunsigned int time_slice; 5243\tstruct rq_flags rf; 5244\tstruct rq *rq; 5245\tint retval; 5246 5247\tif (pid \u0026lt; 0) 5248\treturn -EINVAL; 5249 5250\tretval = -ESRCH; 5251\trcu_read_lock(); 5252\tp = find_process_by_pid(pid); 5253\tif (!p) 5254\tgoto out_unlock; 5255 5256\tretval = security_task_getscheduler(p); 5257\tif (retval) 5258\tgoto out_unlock; 5259 5260\trq = task_rq_lock(p, \u0026amp;rf); 5261\ttime_slice = 0; 5262\tif (p-\u0026gt;sched_class-\u0026gt;get_rr_interval) 5263\ttime_slice = p-\u0026gt;sched_class-\u0026gt;get_rr_interval(rq, p); 5264\ttask_rq_unlock(rq, p, \u0026amp;rf); 5265 5266\trcu_read_unlock(); 5267\tjiffies_to_timespec64(time_slice, t); 5268\treturn 0; 5269 5270out_unlock: 5271\trcu_read_unlock(); 5272\treturn retval; 5273} 5274 5275/** 5276* sys_sched_rr_get_interval - return the default timeslice of a process. 5277* @pid: pid of the process. 5278* @interval: userspace pointer to the timeslice value. 5279* 5280* this syscall writes the default timeslice value of a given process 5281* into the user-space timespec buffer. A value of \u0026#39;0\u0026#39; means infinity. 5282* 5283* Return: On success, 0 and the timeslice is in @interval. Otherwise, 5284* an error code. 5285*/ 5286SYSCALL_DEFINE2(sched_rr_get_interval, pid_t, pid, 5287\tstruct timespec __user *, interval) 5288{ 5289\tstruct timespec64 t; 5290\tint retval = sched_rr_get_interval(pid, \u0026amp;t); 5291 5292\tif (retval == 0) 5293\tretval = put_timespec64(\u0026amp;t, interval); 5294 5295\treturn retval; 5296} 5297 5298#ifdef CONFIG_COMPAT 5299COMPAT_SYSCALL_DEFINE2(sched_rr_get_interval, 5300\tcompat_pid_t, pid, 5301\tstruct compat_timespec __user *, interval) 5302{ 5303\tstruct timespec64 t; 5304\tint retval = sched_rr_get_interval(pid, \u0026amp;t); 5305 5306\tif (retval == 0) 5307\tretval = compat_put_timespec64(\u0026amp;t, interval); 5308\treturn retval; 5309} 5310#endif 5311 5312void sched_show_task(struct task_struct *p) 5313{ 5314\tunsigned long free = 0; 5315\tint ppid; 5316 5317\tif (!try_get_task_stack(p)) 5318\treturn; 5319 5320\tprintk(KERN_INFO \u0026#34;%-15.15s %c\u0026#34;, p-\u0026gt;comm, task_state_to_char(p)); 5321 5322\tif (p-\u0026gt;state == TASK_RUNNING) 5323\tprintk(KERN_CONT \u0026#34; running task \u0026#34;); 5324#ifdef CONFIG_DEBUG_STACK_USAGE 5325\tfree = stack_not_used(p); 5326#endif 5327\tppid = 0; 5328\trcu_read_lock(); 5329\tif (pid_alive(p)) 5330\tppid = task_pid_nr(rcu_dereference(p-\u0026gt;real_parent)); 5331\trcu_read_unlock(); 5332\tprintk(KERN_CONT \u0026#34;%5lu %5d %6d 0x%08lx\\n\u0026#34;, free, 5333\ttask_pid_nr(p), ppid, 5334\t(unsigned long)task_thread_info(p)-\u0026gt;flags); 5335 5336\tprint_worker_info(KERN_INFO, p); 5337\tshow_stack(p, NULL); 5338\tput_task_stack(p); 5339} 5340EXPORT_SYMBOL_GPL(sched_show_task); 5341 5342static inline bool 5343state_filter_match(unsigned long state_filter, struct task_struct *p) 5344{ 5345\t/* no filter, everything matches */ 5346\tif (!state_filter) 5347\treturn true; 5348 5349\t/* filter, but doesn\u0026#39;t match */ 5350\tif (!(p-\u0026gt;state \u0026amp; state_filter)) 5351\treturn false; 5352 5353\t/* 5354* When looking for TASK_UNINTERRUPTIBLE skip TASK_IDLE (allows 5355* TASK_KILLABLE). 5356*/ 5357\tif (state_filter == TASK_UNINTERRUPTIBLE \u0026amp;\u0026amp; p-\u0026gt;state == TASK_IDLE) 5358\treturn false; 5359 5360\treturn true; 5361} 5362 5363 5364void show_state_filter(unsigned long state_filter) 5365{ 5366\tstruct task_struct *g, *p; 5367 5368#if BITS_PER_LONG == 32 5369\tprintk(KERN_INFO 5370\t\u0026#34; task PC stack pid father\\n\u0026#34;); 5371#else 5372\tprintk(KERN_INFO 5373\t\u0026#34; task PC stack pid father\\n\u0026#34;); 5374#endif 5375\trcu_read_lock(); 5376\tfor_each_process_thread(g, p) { 5377\t/* 5378* reset the NMI-timeout, listing all files on a slow 5379* console might take a lot of time: 5380* Also, reset softlockup watchdogs on all CPUs, because 5381* another CPU might be blocked waiting for us to process 5382* an IPI. 5383*/ 5384\ttouch_nmi_watchdog(); 5385\ttouch_all_softlockup_watchdogs(); 5386\tif (state_filter_match(state_filter, p)) 5387\tsched_show_task(p); 5388\t} 5389 5390#ifdef CONFIG_SCHED_DEBUG 5391\tif (!state_filter) 5392\tsysrq_sched_debug_show(); 5393#endif 5394\trcu_read_unlock(); 5395\t/* 5396* Only show locks if all tasks are dumped: 5397*/ 5398\tif (!state_filter) 5399\tdebug_show_all_locks(); 5400} 5401 5402/** 5403* init_idle - set up an idle thread for a given CPU 5404* @idle: task in question 5405* @cpu: CPU the idle task belongs to 5406* 5407* NOTE: this function does not set the idle thread\u0026#39;s NEED_RESCHED 5408* flag, to make booting more robust. 5409*/ 5410void init_idle(struct task_struct *idle, int cpu) 5411{ 5412\tstruct rq *rq = cpu_rq(cpu); 5413\tunsigned long flags; 5414 5415\t__sched_fork(0, idle); 5416 5417\traw_spin_lock_irqsave(\u0026amp;idle-\u0026gt;pi_lock, flags); 5418\traw_spin_lock(\u0026amp;rq-\u0026gt;lock); 5419 5420\tidle-\u0026gt;state = TASK_RUNNING; 5421\tidle-\u0026gt;se.exec_start = sched_clock(); 5422\tidle-\u0026gt;flags |= PF_IDLE; 5423 5424\tkasan_unpoison_task_stack(idle); 5425 5426#ifdef CONFIG_SMP 5427\t/* 5428* Its possible that init_idle() gets called multiple times on a task, 5429* in that case do_set_cpus_allowed() will not do the right thing. 5430* 5431* And since this is boot we can forgo the serialization. 5432*/ 5433\tset_cpus_allowed_common(idle, cpumask_of(cpu)); 5434#endif 5435\t/* 5436* We\u0026#39;re having a chicken and egg problem, even though we are 5437* holding rq-\u0026gt;lock, the CPU isn\u0026#39;t yet set to this CPU so the 5438* lockdep check in task_group() will fail. 5439* 5440* Similar case to sched_fork(). / Alternatively we could 5441* use task_rq_lock() here and obtain the other rq-\u0026gt;lock. 5442* 5443* Silence PROVE_RCU 5444*/ 5445\trcu_read_lock(); 5446\t__set_task_cpu(idle, cpu); 5447\trcu_read_unlock(); 5448 5449\trq-\u0026gt;curr = rq-\u0026gt;idle = idle; 5450\tidle-\u0026gt;on_rq = TASK_ON_RQ_QUEUED; 5451#ifdef CONFIG_SMP 5452\tidle-\u0026gt;on_cpu = 1; 5453#endif 5454\traw_spin_unlock(\u0026amp;rq-\u0026gt;lock); 5455\traw_spin_unlock_irqrestore(\u0026amp;idle-\u0026gt;pi_lock, flags); 5456 5457\t/* Set the preempt count _outside_ the spinlocks! */ 5458\tinit_idle_preempt_count(idle, cpu); 5459 5460\t/* 5461* The idle tasks have their own, simple scheduling class: 5462*/ 5463\tidle-\u0026gt;sched_class = \u0026amp;idle_sched_class; 5464\tftrace_graph_init_idle_task(idle, cpu); 5465\tvtime_init_idle(idle, cpu); 5466#ifdef CONFIG_SMP 5467\tsprintf(idle-\u0026gt;comm, \u0026#34;%s/%d\u0026#34;, INIT_TASK_COMM, cpu); 5468#endif 5469} 5470 5471#ifdef CONFIG_SMP 5472 5473int cpuset_cpumask_can_shrink(const struct cpumask *cur, 5474\tconst struct cpumask *trial) 5475{ 5476\tint ret = 1; 5477 5478\tif (!cpumask_weight(cur)) 5479\treturn ret; 5480 5481\tret = dl_cpuset_cpumask_can_shrink(cur, trial); 5482 5483\treturn ret; 5484} 5485 5486int task_can_attach(struct task_struct *p, 5487\tconst struct cpumask *cs_cpus_allowed) 5488{ 5489\tint ret = 0; 5490 5491\t/* 5492* Kthreads which disallow setaffinity shouldn\u0026#39;t be moved 5493* to a new cpuset; we don\u0026#39;t want to change their CPU 5494* affinity and isolating such threads by their set of 5495* allowed nodes is unnecessary. Thus, cpusets are not 5496* applicable for such threads. This prevents checking for 5497* success of set_cpus_allowed_ptr() on all attached tasks 5498* before cpus_allowed may be changed. 5499*/ 5500\tif (p-\u0026gt;flags \u0026amp; PF_NO_SETAFFINITY) { 5501\tret = -EINVAL; 5502\tgoto out; 5503\t} 5504 5505\tif (dl_task(p) \u0026amp;\u0026amp; !cpumask_intersects(task_rq(p)-\u0026gt;rd-\u0026gt;span, 5506\tcs_cpus_allowed)) 5507\tret = dl_task_can_attach(p, cs_cpus_allowed); 5508 5509out: 5510\treturn ret; 5511} 5512 5513bool sched_smp_initialized __read_mostly; 5514 5515#ifdef CONFIG_NUMA_BALANCING 5516/* Migrate current task p to target_cpu */ 5517int migrate_task_to(struct task_struct *p, int target_cpu) 5518{ 5519\tstruct migration_arg arg = { p, target_cpu }; 5520\tint curr_cpu = task_cpu(p); 5521 5522\tif (curr_cpu == target_cpu) 5523\treturn 0; 5524 5525\tif (!cpumask_test_cpu(target_cpu, \u0026amp;p-\u0026gt;cpus_allowed)) 5526\treturn -EINVAL; 5527 5528\t/* TODO: This is not properly updating schedstats */ 5529 5530\ttrace_sched_move_numa(p, curr_cpu, target_cpu); 5531\treturn stop_one_cpu(curr_cpu, migration_cpu_stop, \u0026amp;arg); 5532} 5533 5534/* 5535* Requeue a task on a given node and accurately track the number of NUMA 5536* tasks on the runqueues 5537*/ 5538void sched_setnuma(struct task_struct *p, int nid) 5539{ 5540\tbool queued, running; 5541\tstruct rq_flags rf; 5542\tstruct rq *rq; 5543 5544\trq = task_rq_lock(p, \u0026amp;rf); 5545\tqueued = task_on_rq_queued(p); 5546\trunning = task_current(rq, p); 5547 5548\tif (queued) 5549\tdequeue_task(rq, p, DEQUEUE_SAVE); 5550\tif (running) 5551\tput_prev_task(rq, p); 5552 5553\tp-\u0026gt;numa_preferred_nid = nid; 5554 5555\tif (queued) 5556\tenqueue_task(rq, p, ENQUEUE_RESTORE | ENQUEUE_NOCLOCK); 5557\tif (running) 5558\tset_curr_task(rq, p); 5559\ttask_rq_unlock(rq, p, \u0026amp;rf); 5560} 5561#endif /* CONFIG_NUMA_BALANCING */5562 5563#ifdef CONFIG_HOTPLUG_CPU 5564/* 5565* Ensure that the idle task is using init_mm right before its CPU goes 5566* offline. 5567*/ 5568void idle_task_exit(void) 5569{ 5570\tstruct mm_struct *mm = current-\u0026gt;active_mm; 5571 5572\tBUG_ON(cpu_online(smp_processor_id())); 5573\tBUG_ON(current != this_rq()-\u0026gt;idle); 5574 5575\tif (mm != \u0026amp;init_mm) { 5576\tswitch_mm(mm, \u0026amp;init_mm, current); 5577\tfinish_arch_post_lock_switch(); 5578\t} 5579 5580\t/* finish_cpu(), as ran on the BP, will clean up the active_mm state */ 5581} 5582 5583/* 5584* Since this CPU is going \u0026#39;away\u0026#39; for a while, fold any nr_active delta 5585* we might have. Assumes we\u0026#39;re called after migrate_tasks() so that the 5586* nr_active count is stable. We need to take the teardown thread which 5587* is calling this into account, so we hand in adjust = 1 to the load 5588* calculation. 5589* 5590* Also see the comment \u0026#34;Global load-average calculations\u0026#34;. 5591*/ 5592static void calc_load_migrate(struct rq *rq) 5593{ 5594\tlong delta = calc_load_fold_active(rq, 1); 5595\tif (delta) 5596\tatomic_long_add(delta, \u0026amp;calc_load_tasks); 5597} 5598 5599static void put_prev_task_fake(struct rq *rq, struct task_struct *prev) 5600{ 5601} 5602 5603static const struct sched_class fake_sched_class = { 5604\t.put_prev_task = put_prev_task_fake, 5605}; 5606 5607static struct task_struct fake_task = { 5608\t/* 5609* Avoid pull_{rt,dl}_task() 5610*/ 5611\t.prio = MAX_PRIO + 1, 5612\t.sched_class = \u0026amp;fake_sched_class, 5613}; 5614 5615/* 5616* Migrate all tasks from the rq, sleeping tasks will be migrated by 5617* try_to_wake_up()-\u0026gt;select_task_rq(). 5618* 5619* Called with rq-\u0026gt;lock held even though we\u0026#39;er in stop_machine() and 5620* there\u0026#39;s no concurrency possible, we hold the required locks anyway 5621* because of lock validation efforts. 5622*/ 5623static void migrate_tasks(struct rq *dead_rq, struct rq_flags *rf) 5624{ 5625\tstruct rq *rq = dead_rq; 5626\tstruct task_struct *next, *stop = rq-\u0026gt;stop; 5627\tstruct rq_flags orf = *rf; 5628\tint dest_cpu; 5629 5630\t/* 5631* Fudge the rq selection such that the below task selection loop 5632* doesn\u0026#39;t get stuck on the currently eligible stop task. 5633* 5634* We\u0026#39;re currently inside stop_machine() and the rq is either stuck 5635* in the stop_machine_cpu_stop() loop, or we\u0026#39;re executing this code, 5636* either way we should never end up calling schedule() until we\u0026#39;re 5637* done here. 5638*/ 5639\trq-\u0026gt;stop = NULL; 5640 5641\t/* 5642* put_prev_task() and pick_next_task() sched 5643* class method both need to have an up-to-date 5644* value of rq-\u0026gt;clock[_task] 5645*/ 5646\tupdate_rq_clock(rq); 5647 5648\tfor (;;) { 5649\t/* 5650* There\u0026#39;s this thread running, bail when that\u0026#39;s the only 5651* remaining thread: 5652*/ 5653\tif (rq-\u0026gt;nr_running == 1) 5654\tbreak; 5655 5656\t/* 5657* pick_next_task() assumes pinned rq-\u0026gt;lock: 5658*/ 5659\tnext = pick_next_task(rq, \u0026amp;fake_task, rf); 5660\tBUG_ON(!next); 5661\tput_prev_task(rq, next); 5662 5663\t/* 5664* Rules for changing task_struct::cpus_allowed are holding 5665* both pi_lock and rq-\u0026gt;lock, such that holding either 5666* stabilizes the mask. 5667* 5668* Drop rq-\u0026gt;lock is not quite as disastrous as it usually is 5669* because !cpu_active at this point, which means load-balance 5670* will not interfere. Also, stop-machine. 5671*/ 5672\trq_unlock(rq, rf); 5673\traw_spin_lock(\u0026amp;next-\u0026gt;pi_lock); 5674\trq_relock(rq, rf); 5675 5676\t/* 5677* Since we\u0026#39;re inside stop-machine, _nothing_ should have 5678* changed the task, WARN if weird stuff happened, because in 5679* that case the above rq-\u0026gt;lock drop is a fail too. 5680*/ 5681\tif (WARN_ON(task_rq(next) != rq || !task_on_rq_queued(next))) { 5682\traw_spin_unlock(\u0026amp;next-\u0026gt;pi_lock); 5683\tcontinue; 5684\t} 5685 5686\t/* Find suitable destination for @next, with force if needed. */ 5687\tdest_cpu = select_fallback_rq(dead_rq-\u0026gt;cpu, next); 5688\trq = __migrate_task(rq, rf, next, dest_cpu); 5689\tif (rq != dead_rq) { 5690\trq_unlock(rq, rf); 5691\trq = dead_rq; 5692\t*rf = orf; 5693\trq_relock(rq, rf); 5694\t} 5695\traw_spin_unlock(\u0026amp;next-\u0026gt;pi_lock); 5696\t} 5697 5698\trq-\u0026gt;stop = stop; 5699} 5700#endif /* CONFIG_HOTPLUG_CPU */5701 5702void set_rq_online(struct rq *rq) 5703{ 5704\tif (!rq-\u0026gt;online) { 5705\tconst struct sched_class *class; 5706 5707\tcpumask_set_cpu(rq-\u0026gt;cpu, rq-\u0026gt;rd-\u0026gt;online); 5708\trq-\u0026gt;online = 1; 5709 5710\tfor_each_class(class) { 5711\tif (class-\u0026gt;rq_online) 5712\tclass-\u0026gt;rq_online(rq); 5713\t} 5714\t} 5715} 5716 5717void set_rq_offline(struct rq *rq) 5718{ 5719\tif (rq-\u0026gt;online) { 5720\tconst struct sched_class *class; 5721 5722\tfor_each_class(class) { 5723\tif (class-\u0026gt;rq_offline) 5724\tclass-\u0026gt;rq_offline(rq); 5725\t} 5726 5727\tcpumask_clear_cpu(rq-\u0026gt;cpu, rq-\u0026gt;rd-\u0026gt;online); 5728\trq-\u0026gt;online = 0; 5729\t} 5730} 5731 5732/* 5733* used to mark begin/end of suspend/resume: 5734*/ 5735static int num_cpus_frozen; 5736 5737/* 5738* Update cpusets according to cpu_active mask. If cpusets are 5739* disabled, cpuset_update_active_cpus() becomes a simple wrapper 5740* around partition_sched_domains(). 5741* 5742* If we come here as part of a suspend/resume, don\u0026#39;t touch cpusets because we 5743* want to restore it back to its original state upon resume anyway. 5744*/ 5745static void cpuset_cpu_active(void) 5746{ 5747\tif (cpuhp_tasks_frozen) { 5748\t/* 5749* num_cpus_frozen tracks how many CPUs are involved in suspend 5750* resume sequence. As long as this is not the last online 5751* operation in the resume sequence, just build a single sched 5752* domain, ignoring cpusets. 5753*/ 5754\tpartition_sched_domains(1, NULL, NULL); 5755\tif (--num_cpus_frozen) 5756\treturn; 5757\t/* 5758* This is the last CPU online operation. So fall through and 5759* restore the original sched domains by considering the 5760* cpuset configurations. 5761*/ 5762\tcpuset_force_rebuild(); 5763\t} 5764\tcpuset_update_active_cpus(); 5765} 5766 5767static int cpuset_cpu_inactive(unsigned int cpu) 5768{ 5769\tif (!cpuhp_tasks_frozen) { 5770\tif (dl_cpu_busy(cpu)) 5771\treturn -EBUSY; 5772\tcpuset_update_active_cpus(); 5773\t} else { 5774\tnum_cpus_frozen++; 5775\tpartition_sched_domains(1, NULL, NULL); 5776\t} 5777\treturn 0; 5778} 5779 5780int sched_cpu_activate(unsigned int cpu) 5781{ 5782\tstruct rq *rq = cpu_rq(cpu); 5783\tstruct rq_flags rf; 5784 5785#ifdef CONFIG_SCHED_SMT 5786\t/* 5787* When going up, increment the number of cores with SMT present. 5788*/ 5789\tif (cpumask_weight(cpu_smt_mask(cpu)) == 2) 5790\tstatic_branch_inc_cpuslocked(\u0026amp;sched_smt_present); 5791#endif 5792\tset_cpu_active(cpu, true); 5793 5794\tif (sched_smp_initialized) { 5795\tsched_domains_numa_masks_set(cpu); 5796\tcpuset_cpu_active(); 5797\t} 5798 5799\t/* 5800* Put the rq online, if not already. This happens: 5801* 5802* 1) In the early boot process, because we build the real domains 5803* after all CPUs have been brought up. 5804* 5805* 2) At runtime, if cpuset_cpu_active() fails to rebuild the 5806* domains. 5807*/ 5808\trq_lock_irqsave(rq, \u0026amp;rf); 5809\tif (rq-\u0026gt;rd) { 5810\tBUG_ON(!cpumask_test_cpu(cpu, rq-\u0026gt;rd-\u0026gt;span)); 5811\tset_rq_online(rq); 5812\t} 5813\trq_unlock_irqrestore(rq, \u0026amp;rf); 5814 5815\tupdate_max_interval(); 5816 5817\treturn 0; 5818} 5819 5820int sched_cpu_deactivate(unsigned int cpu) 5821{ 5822\tint ret; 5823 5824\tset_cpu_active(cpu, false); 5825\t/* 5826* We\u0026#39;ve cleared cpu_active_mask, wait for all preempt-disabled and RCU 5827* users of this state to go away such that all new such users will 5828* observe it. 5829* 5830* Do sync before park smpboot threads to take care the rcu boost case. 5831*/ 5832\tsynchronize_rcu_mult(call_rcu, call_rcu_sched); 5833 5834#ifdef CONFIG_SCHED_SMT 5835\t/* 5836* When going down, decrement the number of cores with SMT present. 5837*/ 5838\tif (cpumask_weight(cpu_smt_mask(cpu)) == 2) 5839\tstatic_branch_dec_cpuslocked(\u0026amp;sched_smt_present); 5840#endif 5841 5842\tif (!sched_smp_initialized) 5843\treturn 0; 5844 5845\tret = cpuset_cpu_inactive(cpu); 5846\tif (ret) { 5847\tset_cpu_active(cpu, true); 5848\treturn ret; 5849\t} 5850\tsched_domains_numa_masks_clear(cpu); 5851\treturn 0; 5852} 5853 5854static void sched_rq_cpu_starting(unsigned int cpu) 5855{ 5856\tstruct rq *rq = cpu_rq(cpu); 5857 5858\trq-\u0026gt;calc_load_update = calc_load_update; 5859\tupdate_max_interval(); 5860} 5861 5862int sched_cpu_starting(unsigned int cpu) 5863{ 5864\tsched_rq_cpu_starting(cpu); 5865\tsched_tick_start(cpu); 5866\treturn 0; 5867} 5868 5869#ifdef CONFIG_HOTPLUG_CPU 5870int sched_cpu_dying(unsigned int cpu) 5871{ 5872\tstruct rq *rq = cpu_rq(cpu); 5873\tstruct rq_flags rf; 5874 5875\t/* Handle pending wakeups and then migrate everything off */ 5876\tsched_ttwu_pending(); 5877\tsched_tick_stop(cpu); 5878 5879\trq_lock_irqsave(rq, \u0026amp;rf); 5880\tif (rq-\u0026gt;rd) { 5881\tBUG_ON(!cpumask_test_cpu(cpu, rq-\u0026gt;rd-\u0026gt;span)); 5882\tset_rq_offline(rq); 5883\t} 5884\tmigrate_tasks(rq, \u0026amp;rf); 5885\tBUG_ON(rq-\u0026gt;nr_running != 1); 5886\trq_unlock_irqrestore(rq, \u0026amp;rf); 5887 5888\tcalc_load_migrate(rq); 5889\tupdate_max_interval(); 5890\tnohz_balance_exit_idle(rq); 5891\thrtick_clear(rq); 5892\treturn 0; 5893} 5894#endif 5895 5896void __init sched_init_smp(void) 5897{ 5898\tsched_init_numa(); 5899 5900\t/* 5901* There\u0026#39;s no userspace yet to cause hotplug operations; hence all the 5902* CPU masks are stable and all blatant races in the below code cannot 5903* happen. The hotplug lock is nevertheless taken to satisfy lockdep, 5904* but there won\u0026#39;t be any contention on it. 5905*/ 5906\tcpus_read_lock(); 5907\tmutex_lock(\u0026amp;sched_domains_mutex); 5908\tsched_init_domains(cpu_active_mask); 5909\tmutex_unlock(\u0026amp;sched_domains_mutex); 5910\tcpus_read_unlock(); 5911 5912\t/* Move init over to a non-isolated CPU */ 5913\tif (set_cpus_allowed_ptr(current, housekeeping_cpumask(HK_FLAG_DOMAIN)) \u0026lt; 0) 5914\tBUG(); 5915\tsched_init_granularity(); 5916 5917\tinit_sched_rt_class(); 5918\tinit_sched_dl_class(); 5919 5920\tsched_smp_initialized = true; 5921} 5922 5923static int __init migration_init(void) 5924{ 5925\tsched_rq_cpu_starting(smp_processor_id()); 5926\treturn 0; 5927} 5928early_initcall(migration_init); 5929 5930#else 5931void __init sched_init_smp(void) 5932{ 5933\tsched_init_granularity(); 5934} 5935#endif /* CONFIG_SMP */5936 5937int in_sched_functions(unsigned long addr) 5938{ 5939\treturn in_lock_functions(addr) || 5940\t(addr \u0026gt;= (unsigned long)__sched_text_start 5941\t\u0026amp;\u0026amp; addr \u0026lt; (unsigned long)__sched_text_end); 5942} 5943 5944#ifdef CONFIG_CGROUP_SCHED 5945/* 5946* Default task group. 5947* Every task in system belongs to this group at bootup. 5948*/ 5949struct task_group root_task_group; 5950LIST_HEAD(task_groups); 5951 5952/* Cacheline aligned slab cache for task_group */ 5953static struct kmem_cache *task_group_cache __read_mostly; 5954#endif 5955 5956DECLARE_PER_CPU(cpumask_var_t, load_balance_mask); 5957DECLARE_PER_CPU(cpumask_var_t, select_idle_mask); 5958 5959void __init sched_init(void) 5960{ 5961\tint i, j; 5962\tunsigned long alloc_size = 0, ptr; 5963 5964\twait_bit_init(); 5965 5966#ifdef CONFIG_FAIR_GROUP_SCHED 5967\talloc_size += 2 * nr_cpu_ids * sizeof(void **); 5968#endif 5969#ifdef CONFIG_RT_GROUP_SCHED 5970\talloc_size += 2 * nr_cpu_ids * sizeof(void **); 5971#endif 5972\tif (alloc_size) { 5973\tptr = (unsigned long)kzalloc(alloc_size, GFP_NOWAIT); 5974 5975#ifdef CONFIG_FAIR_GROUP_SCHED 5976\troot_task_group.se = (struct sched_entity **)ptr; 5977\tptr += nr_cpu_ids * sizeof(void **); 5978 5979\troot_task_group.cfs_rq = (struct cfs_rq **)ptr; 5980\tptr += nr_cpu_ids * sizeof(void **); 5981 5982#endif /* CONFIG_FAIR_GROUP_SCHED */5983#ifdef CONFIG_RT_GROUP_SCHED 5984\troot_task_group.rt_se = (struct sched_rt_entity **)ptr; 5985\tptr += nr_cpu_ids * sizeof(void **); 5986 5987\troot_task_group.rt_rq = (struct rt_rq **)ptr; 5988\tptr += nr_cpu_ids * sizeof(void **); 5989 5990#endif /* CONFIG_RT_GROUP_SCHED */5991\t} 5992#ifdef CONFIG_CPUMASK_OFFSTACK 5993\tfor_each_possible_cpu(i) { 5994\tper_cpu(load_balance_mask, i) = (cpumask_var_t)kzalloc_node( 5995\tcpumask_size(), GFP_KERNEL, cpu_to_node(i)); 5996\tper_cpu(select_idle_mask, i) = (cpumask_var_t)kzalloc_node( 5997\tcpumask_size(), GFP_KERNEL, cpu_to_node(i)); 5998\t} 5999#endif /* CONFIG_CPUMASK_OFFSTACK */6000 6001\tinit_rt_bandwidth(\u0026amp;def_rt_bandwidth, global_rt_period(), global_rt_runtime()); 6002\tinit_dl_bandwidth(\u0026amp;def_dl_bandwidth, global_rt_period(), global_rt_runtime()); 6003 6004#ifdef CONFIG_SMP 6005\tinit_defrootdomain(); 6006#endif 6007 6008#ifdef CONFIG_RT_GROUP_SCHED 6009\tinit_rt_bandwidth(\u0026amp;root_task_group.rt_bandwidth, 6010\tglobal_rt_period(), global_rt_runtime()); 6011#endif /* CONFIG_RT_GROUP_SCHED */6012 6013#ifdef CONFIG_CGROUP_SCHED 6014\ttask_group_cache = KMEM_CACHE(task_group, 0); 6015 6016\tlist_add(\u0026amp;root_task_group.list, \u0026amp;task_groups); 6017\tINIT_LIST_HEAD(\u0026amp;root_task_group.children); 6018\tINIT_LIST_HEAD(\u0026amp;root_task_group.siblings); 6019\tautogroup_init(\u0026amp;init_task); 6020#endif /* CONFIG_CGROUP_SCHED */6021 6022\tfor_each_possible_cpu(i) { 6023\tstruct rq *rq; 6024 6025\trq = cpu_rq(i); 6026\traw_spin_lock_init(\u0026amp;rq-\u0026gt;lock); 6027\trq-\u0026gt;nr_running = 0; 6028\trq-\u0026gt;calc_load_active = 0; 6029\trq-\u0026gt;calc_load_update = jiffies + LOAD_FREQ; 6030\tinit_cfs_rq(\u0026amp;rq-\u0026gt;cfs); 6031\tinit_rt_rq(\u0026amp;rq-\u0026gt;rt); 6032\tinit_dl_rq(\u0026amp;rq-\u0026gt;dl); 6033#ifdef CONFIG_FAIR_GROUP_SCHED 6034\troot_task_group.shares = ROOT_TASK_GROUP_LOAD; 6035\tINIT_LIST_HEAD(\u0026amp;rq-\u0026gt;leaf_cfs_rq_list); 6036\trq-\u0026gt;tmp_alone_branch = \u0026amp;rq-\u0026gt;leaf_cfs_rq_list; 6037\t/* 6038* How much CPU bandwidth does root_task_group get? 6039* 6040* In case of task-groups formed thr\u0026#39; the cgroup filesystem, it 6041* gets 100% of the CPU resources in the system. This overall 6042* system CPU resource is divided among the tasks of 6043* root_task_group and its child task-groups in a fair manner, 6044* based on each entity\u0026#39;s (task or task-group\u0026#39;s) weight 6045* (se-\u0026gt;load.weight). 6046* 6047* In other words, if root_task_group has 10 tasks of weight 6048* 1024) and two child groups A0 and A1 (of weight 1024 each), 6049* then A0\u0026#39;s share of the CPU resource is: 6050* 6051*\tA0\u0026#39;s bandwidth = 1024 / (10*1024 + 1024 + 1024) = 8.33% 6052* 6053* We achieve this by letting root_task_group\u0026#39;s tasks sit 6054* directly in rq-\u0026gt;cfs (i.e root_task_group-\u0026gt;se[] = NULL). 6055*/ 6056\tinit_cfs_bandwidth(\u0026amp;root_task_group.cfs_bandwidth); 6057\tinit_tg_cfs_entry(\u0026amp;root_task_group, \u0026amp;rq-\u0026gt;cfs, NULL, i, NULL); 6058#endif /* CONFIG_FAIR_GROUP_SCHED */6059 6060\trq-\u0026gt;rt.rt_runtime = def_rt_bandwidth.rt_runtime; 6061#ifdef CONFIG_RT_GROUP_SCHED 6062\tinit_tg_rt_entry(\u0026amp;root_task_group, \u0026amp;rq-\u0026gt;rt, NULL, i, NULL); 6063#endif 6064 6065\tfor (j = 0; j \u0026lt; CPU_LOAD_IDX_MAX; j++) 6066\trq-\u0026gt;cpu_load[j] = 0; 6067 6068#ifdef CONFIG_SMP 6069\trq-\u0026gt;sd = NULL; 6070\trq-\u0026gt;rd = NULL; 6071\trq-\u0026gt;cpu_capacity = rq-\u0026gt;cpu_capacity_orig = SCHED_CAPACITY_SCALE; 6072\trq-\u0026gt;balance_callback = NULL; 6073\trq-\u0026gt;active_balance = 0; 6074\trq-\u0026gt;next_balance = jiffies; 6075\trq-\u0026gt;push_cpu = 0; 6076\trq-\u0026gt;cpu = i; 6077\trq-\u0026gt;online = 0; 6078\trq-\u0026gt;idle_stamp = 0; 6079\trq-\u0026gt;avg_idle = 2*sysctl_sched_migration_cost; 6080\trq-\u0026gt;max_idle_balance_cost = sysctl_sched_migration_cost; 6081 6082\tINIT_LIST_HEAD(\u0026amp;rq-\u0026gt;cfs_tasks); 6083 6084\trq_attach_root(rq, \u0026amp;def_root_domain); 6085#ifdef CONFIG_NO_HZ_COMMON 6086\trq-\u0026gt;last_load_update_tick = jiffies; 6087\trq-\u0026gt;last_blocked_load_update_tick = jiffies; 6088\tatomic_set(\u0026amp;rq-\u0026gt;nohz_flags, 0); 6089#endif 6090#endif /* CONFIG_SMP */6091\thrtick_rq_init(rq); 6092\tatomic_set(\u0026amp;rq-\u0026gt;nr_iowait, 0); 6093\t} 6094 6095\tset_load_weight(\u0026amp;init_task, false); 6096 6097\t/* 6098* The boot idle thread does lazy MMU switching as well: 6099*/ 6100\tmmgrab(\u0026amp;init_mm); 6101\tenter_lazy_tlb(\u0026amp;init_mm, current); 6102 6103\t/* 6104* Make us the idle thread. Technically, schedule() should not be 6105* called from this thread, however somewhere below it might be, 6106* but because we are the idle thread, we just pick up running again 6107* when this runqueue becomes \u0026#34;idle\u0026#34;. 6108*/ 6109\tinit_idle(current, smp_processor_id()); 6110 6111\tcalc_load_update = jiffies + LOAD_FREQ; 6112 6113#ifdef CONFIG_SMP 6114\tidle_thread_set_boot_cpu(); 6115#endif 6116\tinit_sched_fair_class(); 6117 6118\tinit_schedstats(); 6119 6120\tscheduler_running = 1; 6121} 6122 6123#ifdef CONFIG_DEBUG_ATOMIC_SLEEP 6124static inline int preempt_count_equals(int preempt_offset) 6125{ 6126\tint nested = preempt_count() + rcu_preempt_depth(); 6127 6128\treturn (nested == preempt_offset); 6129} 6130 6131void __might_sleep(const char *file, int line, int preempt_offset) 6132{ 6133\t/* 6134* Blocking primitives will set (and therefore destroy) current-\u0026gt;state, 6135* since we will exit with TASK_RUNNING make sure we enter with it, 6136* otherwise we will destroy state. 6137*/ 6138\tWARN_ONCE(current-\u0026gt;state != TASK_RUNNING \u0026amp;\u0026amp; current-\u0026gt;task_state_change, 6139\t\u0026#34;do not call blocking ops when !TASK_RUNNING; \u0026#34; 6140\t\u0026#34;state=%lx set at [\u0026lt;%p\u0026gt;] %pS\\n\u0026#34;, 6141\tcurrent-\u0026gt;state, 6142\t(void *)current-\u0026gt;task_state_change, 6143\t(void *)current-\u0026gt;task_state_change); 6144 6145\t___might_sleep(file, line, preempt_offset); 6146} 6147EXPORT_SYMBOL(__might_sleep); 6148 6149void ___might_sleep(const char *file, int line, int preempt_offset) 6150{ 6151\t/* Ratelimiting timestamp: */ 6152\tstatic unsigned long prev_jiffy; 6153 6154\tunsigned long preempt_disable_ip; 6155 6156\t/* WARN_ON_ONCE() by default, no rate limit required: */ 6157\trcu_sleep_check(); 6158 6159\tif ((preempt_count_equals(preempt_offset) \u0026amp;\u0026amp; !irqs_disabled() \u0026amp;\u0026amp; 6160\t!is_idle_task(current)) || 6161\tsystem_state == SYSTEM_BOOTING || system_state \u0026gt; SYSTEM_RUNNING || 6162\toops_in_progress) 6163\treturn; 6164 6165\tif (time_before(jiffies, prev_jiffy + HZ) \u0026amp;\u0026amp; prev_jiffy) 6166\treturn; 6167\tprev_jiffy = jiffies; 6168 6169\t/* Save this before calling printk(), since that will clobber it: */ 6170\tpreempt_disable_ip = get_preempt_disable_ip(current); 6171 6172\tprintk(KERN_ERR 6173\t\u0026#34;BUG: sleeping function called from invalid context at %s:%d\\n\u0026#34;, 6174\tfile, line); 6175\tprintk(KERN_ERR 6176\t\u0026#34;in_atomic(): %d, irqs_disabled(): %d, pid: %d, name: %s\\n\u0026#34;, 6177\tin_atomic(), irqs_disabled(), 6178\tcurrent-\u0026gt;pid, current-\u0026gt;comm); 6179 6180\tif (task_stack_end_corrupted(current)) 6181\tprintk(KERN_EMERG \u0026#34;Thread overran stack, or stack corrupted\\n\u0026#34;); 6182 6183\tdebug_show_held_locks(current); 6184\tif (irqs_disabled()) 6185\tprint_irqtrace_events(current); 6186\tif (IS_ENABLED(CONFIG_DEBUG_PREEMPT) 6187\t\u0026amp;\u0026amp; !preempt_count_equals(preempt_offset)) { 6188\tpr_err(\u0026#34;Preemption disabled at:\u0026#34;); 6189\tprint_ip_sym(preempt_disable_ip); 6190\tpr_cont(\u0026#34;\\n\u0026#34;); 6191\t} 6192\tdump_stack(); 6193\tadd_taint(TAINT_WARN, LOCKDEP_STILL_OK); 6194} 6195EXPORT_SYMBOL(___might_sleep); 6196#endif 6197 6198#ifdef CONFIG_MAGIC_SYSRQ 6199void normalize_rt_tasks(void) 6200{ 6201\tstruct task_struct *g, *p; 6202\tstruct sched_attr attr = { 6203\t.sched_policy = SCHED_NORMAL, 6204\t}; 6205 6206\tread_lock(\u0026amp;tasklist_lock); 6207\tfor_each_process_thread(g, p) { 6208\t/* 6209* Only normalize user tasks: 6210*/ 6211\tif (p-\u0026gt;flags \u0026amp; PF_KTHREAD) 6212\tcontinue; 6213 6214\tp-\u0026gt;se.exec_start = 0; 6215\tschedstat_set(p-\u0026gt;se.statistics.wait_start, 0); 6216\tschedstat_set(p-\u0026gt;se.statistics.sleep_start, 0); 6217\tschedstat_set(p-\u0026gt;se.statistics.block_start, 0); 6218 6219\tif (!dl_task(p) \u0026amp;\u0026amp; !rt_task(p)) { 6220\t/* 6221* Renice negative nice level userspace 6222* tasks back to 0: 6223*/ 6224\tif (task_nice(p) \u0026lt; 0) 6225\tset_user_nice(p, 0); 6226\tcontinue; 6227\t} 6228 6229\t__sched_setscheduler(p, \u0026amp;attr, false, false); 6230\t} 6231\tread_unlock(\u0026amp;tasklist_lock); 6232} 6233 6234#endif /* CONFIG_MAGIC_SYSRQ */6235 6236#if defined(CONFIG_IA64) || defined(CONFIG_KGDB_KDB) 6237/* 6238* These functions are only useful for the IA64 MCA handling, or kdb. 6239* 6240* They can only be called when the whole system has been 6241* stopped - every CPU needs to be quiescent, and no scheduling 6242* activity can take place. Using them for anything else would 6243* be a serious bug, and as a result, they aren\u0026#39;t even visible 6244* under any other configuration. 6245*/ 6246 6247/** 6248* curr_task - return the current task for a given CPU. 6249* @cpu: the processor in question. 6250* 6251* ONLY VALID WHEN THE WHOLE SYSTEM IS STOPPED! 6252* 6253* Return: The current task for @cpu. 6254*/ 6255struct task_struct *curr_task(int cpu) 6256{ 6257\treturn cpu_curr(cpu); 6258} 6259 6260#endif /* defined(CONFIG_IA64) || defined(CONFIG_KGDB_KDB) */6261 6262#ifdef CONFIG_IA64 6263/** 6264* set_curr_task - set the current task for a given CPU. 6265* @cpu: the processor in question. 6266* @p: the task pointer to set. 6267* 6268* Description: This function must only be used when non-maskable interrupts 6269* are serviced on a separate stack. It allows the architecture to switch the 6270* notion of the current task on a CPU in a non-blocking manner. This function 6271* must be called with all CPU\u0026#39;s synchronized, and interrupts disabled, the 6272* and caller must save the original value of the current task (see 6273* curr_task() above) and restore that value before reenabling interrupts and 6274* re-starting the system. 6275* 6276* ONLY VALID WHEN THE WHOLE SYSTEM IS STOPPED! 6277*/ 6278void ia64_set_curr_task(int cpu, struct task_struct *p) 6279{ 6280\tcpu_curr(cpu) = p; 6281} 6282 6283#endif 6284 6285#ifdef CONFIG_CGROUP_SCHED 6286/* task_group_lock serializes the addition/removal of task groups */ 6287static DEFINE_SPINLOCK(task_group_lock); 6288 6289static void sched_free_group(struct task_group *tg) 6290{ 6291\tfree_fair_sched_group(tg); 6292\tfree_rt_sched_group(tg); 6293\tautogroup_free(tg); 6294\tkmem_cache_free(task_group_cache, tg); 6295} 6296 6297/* allocate runqueue etc for a new task group */ 6298struct task_group *sched_create_group(struct task_group *parent) 6299{ 6300\tstruct task_group *tg; 6301 6302\ttg = kmem_cache_alloc(task_group_cache, GFP_KERNEL | __GFP_ZERO); 6303\tif (!tg) 6304\treturn ERR_PTR(-ENOMEM); 6305 6306\tif (!alloc_fair_sched_group(tg, parent)) 6307\tgoto err; 6308 6309\tif (!alloc_rt_sched_group(tg, parent)) 6310\tgoto err; 6311 6312\treturn tg; 6313 6314err: 6315\tsched_free_group(tg); 6316\treturn ERR_PTR(-ENOMEM); 6317} 6318 6319void sched_online_group(struct task_group *tg, struct task_group *parent) 6320{ 6321\tunsigned long flags; 6322 6323\tspin_lock_irqsave(\u0026amp;task_group_lock, flags); 6324\tlist_add_rcu(\u0026amp;tg-\u0026gt;list, \u0026amp;task_groups); 6325 6326\t/* Root should already exist: */ 6327\tWARN_ON(!parent); 6328 6329\ttg-\u0026gt;parent = parent; 6330\tINIT_LIST_HEAD(\u0026amp;tg-\u0026gt;children); 6331\tlist_add_rcu(\u0026amp;tg-\u0026gt;siblings, \u0026amp;parent-\u0026gt;children); 6332\tspin_unlock_irqrestore(\u0026amp;task_group_lock, flags); 6333 6334\tonline_fair_sched_group(tg); 6335} 6336 6337/* rcu callback to free various structures associated with a task group */ 6338static void sched_free_group_rcu(struct rcu_head *rhp) 6339{ 6340\t/* Now it should be safe to free those cfs_rqs: */ 6341\tsched_free_group(container_of(rhp, struct task_group, rcu)); 6342} 6343 6344void sched_destroy_group(struct task_group *tg) 6345{ 6346\t/* Wait for possible concurrent references to cfs_rqs complete: */ 6347\tcall_rcu(\u0026amp;tg-\u0026gt;rcu, sched_free_group_rcu); 6348} 6349 6350void sched_offline_group(struct task_group *tg) 6351{ 6352\tunsigned long flags; 6353 6354\t/* End participation in shares distribution: */ 6355\tunregister_fair_sched_group(tg); 6356 6357\tspin_lock_irqsave(\u0026amp;task_group_lock, flags); 6358\tlist_del_rcu(\u0026amp;tg-\u0026gt;list); 6359\tlist_del_rcu(\u0026amp;tg-\u0026gt;siblings); 6360\tspin_unlock_irqrestore(\u0026amp;task_group_lock, flags); 6361} 6362 6363static void sched_change_group(struct task_struct *tsk, int type) 6364{ 6365\tstruct task_group *tg; 6366 6367\t/* 6368* All callers are synchronized by task_rq_lock(); we do not use RCU 6369* which is pointless here. Thus, we pass \u0026#34;true\u0026#34; to task_css_check() 6370* to prevent lockdep warnings. 6371*/ 6372\ttg = container_of(task_css_check(tsk, cpu_cgrp_id, true), 6373\tstruct task_group, css); 6374\ttg = autogroup_task_group(tsk, tg); 6375\ttsk-\u0026gt;sched_task_group = tg; 6376 6377#ifdef CONFIG_FAIR_GROUP_SCHED 6378\tif (tsk-\u0026gt;sched_class-\u0026gt;task_change_group) 6379\ttsk-\u0026gt;sched_class-\u0026gt;task_change_group(tsk, type); 6380\telse 6381#endif 6382\tset_task_rq(tsk, task_cpu(tsk)); 6383} 6384 6385/* 6386* Change task\u0026#39;s runqueue when it moves between groups. 6387* 6388* The caller of this function should have put the task in its new group by 6389* now. This function just updates tsk-\u0026gt;se.cfs_rq and tsk-\u0026gt;se.parent to reflect 6390* its new group. 6391*/ 6392void sched_move_task(struct task_struct *tsk) 6393{ 6394\tint queued, running, queue_flags = 6395\tDEQUEUE_SAVE | DEQUEUE_MOVE | DEQUEUE_NOCLOCK; 6396\tstruct rq_flags rf; 6397\tstruct rq *rq; 6398 6399\trq = task_rq_lock(tsk, \u0026amp;rf); 6400\tupdate_rq_clock(rq); 6401 6402\trunning = task_current(rq, tsk); 6403\tqueued = task_on_rq_queued(tsk); 6404 6405\tif (queued) 6406\tdequeue_task(rq, tsk, queue_flags); 6407\tif (running) 6408\tput_prev_task(rq, tsk); 6409 6410\tsched_change_group(tsk, TASK_MOVE_GROUP); 6411 6412\tif (queued) 6413\tenqueue_task(rq, tsk, queue_flags); 6414\tif (running) 6415\tset_curr_task(rq, tsk); 6416 6417\ttask_rq_unlock(rq, tsk, \u0026amp;rf); 6418} 6419 6420static inline struct task_group *css_tg(struct cgroup_subsys_state *css) 6421{ 6422\treturn css ? container_of(css, struct task_group, css) : NULL; 6423} 6424 6425static struct cgroup_subsys_state * 6426cpu_cgroup_css_alloc(struct cgroup_subsys_state *parent_css) 6427{ 6428\tstruct task_group *parent = css_tg(parent_css); 6429\tstruct task_group *tg; 6430 6431\tif (!parent) { 6432\t/* This is early initialization for the top cgroup */ 6433\treturn \u0026amp;root_task_group.css; 6434\t} 6435 6436\ttg = sched_create_group(parent); 6437\tif (IS_ERR(tg)) 6438\treturn ERR_PTR(-ENOMEM); 6439 6440\treturn \u0026amp;tg-\u0026gt;css; 6441} 6442 6443/* Expose task group only after completing cgroup initialization */ 6444static int cpu_cgroup_css_online(struct cgroup_subsys_state *css) 6445{ 6446\tstruct task_group *tg = css_tg(css); 6447\tstruct task_group *parent = css_tg(css-\u0026gt;parent); 6448 6449\tif (parent) 6450\tsched_online_group(tg, parent); 6451\treturn 0; 6452} 6453 6454static void cpu_cgroup_css_released(struct cgroup_subsys_state *css) 6455{ 6456\tstruct task_group *tg = css_tg(css); 6457 6458\tsched_offline_group(tg); 6459} 6460 6461static void cpu_cgroup_css_free(struct cgroup_subsys_state *css) 6462{ 6463\tstruct task_group *tg = css_tg(css); 6464 6465\t/* 6466* Relies on the RCU grace period between css_released() and this. 6467*/ 6468\tsched_free_group(tg); 6469} 6470 6471/* 6472* This is called before wake_up_new_task(), therefore we really only 6473* have to set its group bits, all the other stuff does not apply. 6474*/ 6475static void cpu_cgroup_fork(struct task_struct *task) 6476{ 6477\tstruct rq_flags rf; 6478\tstruct rq *rq; 6479 6480\trq = task_rq_lock(task, \u0026amp;rf); 6481 6482\tupdate_rq_clock(rq); 6483\tsched_change_group(task, TASK_SET_GROUP); 6484 6485\ttask_rq_unlock(rq, task, \u0026amp;rf); 6486} 6487 6488static int cpu_cgroup_can_attach(struct cgroup_taskset *tset) 6489{ 6490\tstruct task_struct *task; 6491\tstruct cgroup_subsys_state *css; 6492\tint ret = 0; 6493 6494\tcgroup_taskset_for_each(task, css, tset) { 6495#ifdef CONFIG_RT_GROUP_SCHED 6496\tif (!sched_rt_can_attach(css_tg(css), task)) 6497\treturn -EINVAL; 6498#endif 6499\t/* 6500* Serialize against wake_up_new_task() such that if its 6501* running, we\u0026#39;re sure to observe its full state. 6502*/ 6503\traw_spin_lock_irq(\u0026amp;task-\u0026gt;pi_lock); 6504\t/* 6505* Avoid calling sched_move_task() before wake_up_new_task() 6506* has happened. This would lead to problems with PELT, due to 6507* move wanting to detach+attach while we\u0026#39;re not attached yet. 6508*/ 6509\tif (task-\u0026gt;state == TASK_NEW) 6510\tret = -EINVAL; 6511\traw_spin_unlock_irq(\u0026amp;task-\u0026gt;pi_lock); 6512 6513\tif (ret) 6514\tbreak; 6515\t} 6516\treturn ret; 6517} 6518 6519static void cpu_cgroup_attach(struct cgroup_taskset *tset) 6520{ 6521\tstruct task_struct *task; 6522\tstruct cgroup_subsys_state *css; 6523 6524\tcgroup_taskset_for_each(task, css, tset) 6525\tsched_move_task(task); 6526} 6527 6528#ifdef CONFIG_FAIR_GROUP_SCHED 6529static int cpu_shares_write_u64(struct cgroup_subsys_state *css, 6530\tstruct cftype *cftype, u64 shareval) 6531{ 6532\tif (shareval \u0026gt; scale_load_down(ULONG_MAX)) 6533\tshareval = MAX_SHARES; 6534\treturn sched_group_set_shares(css_tg(css), scale_load(shareval)); 6535} 6536 6537static u64 cpu_shares_read_u64(struct cgroup_subsys_state *css, 6538\tstruct cftype *cft) 6539{ 6540\tstruct task_group *tg = css_tg(css); 6541 6542\treturn (u64) scale_load_down(tg-\u0026gt;shares); 6543} 6544 6545#ifdef CONFIG_CFS_BANDWIDTH 6546static DEFINE_MUTEX(cfs_constraints_mutex); 6547 6548const u64 max_cfs_quota_period = 1 * NSEC_PER_SEC; /* 1s */ 6549const u64 min_cfs_quota_period = 1 * NSEC_PER_MSEC; /* 1ms */ 6550 6551static int __cfs_schedulable(struct task_group *tg, u64 period, u64 runtime); 6552 6553static int tg_set_cfs_bandwidth(struct task_group *tg, u64 period, u64 quota) 6554{ 6555\tint i, ret = 0, runtime_enabled, runtime_was_enabled; 6556\tstruct cfs_bandwidth *cfs_b = \u0026amp;tg-\u0026gt;cfs_bandwidth; 6557 6558\tif (tg == \u0026amp;root_task_group) 6559\treturn -EINVAL; 6560 6561\t/* 6562* Ensure we have at some amount of bandwidth every period. This is 6563* to prevent reaching a state of large arrears when throttled via 6564* entity_tick() resulting in prolonged exit starvation. 6565*/ 6566\tif (quota \u0026lt; min_cfs_quota_period || period \u0026lt; min_cfs_quota_period) 6567\treturn -EINVAL; 6568 6569\t/* 6570* Likewise, bound things on the otherside by preventing insane quota 6571* periods. This also allows us to normalize in computing quota 6572* feasibility. 6573*/ 6574\tif (period \u0026gt; max_cfs_quota_period) 6575\treturn -EINVAL; 6576 6577\t/* 6578* Prevent race between setting of cfs_rq-\u0026gt;runtime_enabled and 6579* unthrottle_offline_cfs_rqs(). 6580*/ 6581\tget_online_cpus(); 6582\tmutex_lock(\u0026amp;cfs_constraints_mutex); 6583\tret = __cfs_schedulable(tg, period, quota); 6584\tif (ret) 6585\tgoto out_unlock; 6586 6587\truntime_enabled = quota != RUNTIME_INF; 6588\truntime_was_enabled = cfs_b-\u0026gt;quota != RUNTIME_INF; 6589\t/* 6590* If we need to toggle cfs_bandwidth_used, off-\u0026gt;on must occur 6591* before making related changes, and on-\u0026gt;off must occur afterwards 6592*/ 6593\tif (runtime_enabled \u0026amp;\u0026amp; !runtime_was_enabled) 6594\tcfs_bandwidth_usage_inc(); 6595\traw_spin_lock_irq(\u0026amp;cfs_b-\u0026gt;lock); 6596\tcfs_b-\u0026gt;period = ns_to_ktime(period); 6597\tcfs_b-\u0026gt;quota = quota; 6598 6599\t__refill_cfs_bandwidth_runtime(cfs_b); 6600 6601\t/* Restart the period timer (if active) to handle new period expiry: */ 6602\tif (runtime_enabled) 6603\tstart_cfs_bandwidth(cfs_b); 6604 6605\traw_spin_unlock_irq(\u0026amp;cfs_b-\u0026gt;lock); 6606 6607\tfor_each_online_cpu(i) { 6608\tstruct cfs_rq *cfs_rq = tg-\u0026gt;cfs_rq[i]; 6609\tstruct rq *rq = cfs_rq-\u0026gt;rq; 6610\tstruct rq_flags rf; 6611 6612\trq_lock_irq(rq, \u0026amp;rf); 6613\tcfs_rq-\u0026gt;runtime_enabled = runtime_enabled; 6614\tcfs_rq-\u0026gt;runtime_remaining = 0; 6615 6616\tif (cfs_rq-\u0026gt;throttled) 6617\tunthrottle_cfs_rq(cfs_rq); 6618\trq_unlock_irq(rq, \u0026amp;rf); 6619\t} 6620\tif (runtime_was_enabled \u0026amp;\u0026amp; !runtime_enabled) 6621\tcfs_bandwidth_usage_dec(); 6622out_unlock: 6623\tmutex_unlock(\u0026amp;cfs_constraints_mutex); 6624\tput_online_cpus(); 6625 6626\treturn ret; 6627} 6628 6629int tg_set_cfs_quota(struct task_group *tg, long cfs_quota_us) 6630{ 6631\tu64 quota, period; 6632 6633\tperiod = ktime_to_ns(tg-\u0026gt;cfs_bandwidth.period); 6634\tif (cfs_quota_us \u0026lt; 0) 6635\tquota = RUNTIME_INF; 6636\telse if ((u64)cfs_quota_us \u0026lt;= U64_MAX / NSEC_PER_USEC) 6637\tquota = (u64)cfs_quota_us * NSEC_PER_USEC; 6638\telse 6639\treturn -EINVAL; 6640 6641\treturn tg_set_cfs_bandwidth(tg, period, quota); 6642} 6643 6644long tg_get_cfs_quota(struct task_group *tg) 6645{ 6646\tu64 quota_us; 6647 6648\tif (tg-\u0026gt;cfs_bandwidth.quota == RUNTIME_INF) 6649\treturn -1; 6650 6651\tquota_us = tg-\u0026gt;cfs_bandwidth.quota; 6652\tdo_div(quota_us, NSEC_PER_USEC); 6653 6654\treturn quota_us; 6655} 6656 6657int tg_set_cfs_period(struct task_group *tg, long cfs_period_us) 6658{ 6659\tu64 quota, period; 6660 6661\tif ((u64)cfs_period_us \u0026gt; U64_MAX / NSEC_PER_USEC) 6662\treturn -EINVAL; 6663 6664\tperiod = (u64)cfs_period_us * NSEC_PER_USEC; 6665\tquota = tg-\u0026gt;cfs_bandwidth.quota; 6666 6667\treturn tg_set_cfs_bandwidth(tg, period, quota); 6668} 6669 6670long tg_get_cfs_period(struct task_group *tg) 6671{ 6672\tu64 cfs_period_us; 6673 6674\tcfs_period_us = ktime_to_ns(tg-\u0026gt;cfs_bandwidth.period); 6675\tdo_div(cfs_period_us, NSEC_PER_USEC); 6676 6677\treturn cfs_period_us; 6678} 6679 6680static s64 cpu_cfs_quota_read_s64(struct cgroup_subsys_state *css, 6681\tstruct cftype *cft) 6682{ 6683\treturn tg_get_cfs_quota(css_tg(css)); 6684} 6685 6686static int cpu_cfs_quota_write_s64(struct cgroup_subsys_state *css, 6687\tstruct cftype *cftype, s64 cfs_quota_us) 6688{ 6689\treturn tg_set_cfs_quota(css_tg(css), cfs_quota_us); 6690} 6691 6692static u64 cpu_cfs_period_read_u64(struct cgroup_subsys_state *css, 6693\tstruct cftype *cft) 6694{ 6695\treturn tg_get_cfs_period(css_tg(css)); 6696} 6697 6698static int cpu_cfs_period_write_u64(struct cgroup_subsys_state *css, 6699\tstruct cftype *cftype, u64 cfs_period_us) 6700{ 6701\treturn tg_set_cfs_period(css_tg(css), cfs_period_us); 6702} 6703 6704struct cfs_schedulable_data { 6705\tstruct task_group *tg; 6706\tu64 period, quota; 6707}; 6708 6709/* 6710* normalize group quota/period to be quota/max_period 6711* note: units are usecs 6712*/ 6713static u64 normalize_cfs_quota(struct task_group *tg, 6714\tstruct cfs_schedulable_data *d) 6715{ 6716\tu64 quota, period; 6717 6718\tif (tg == d-\u0026gt;tg) { 6719\tperiod = d-\u0026gt;period; 6720\tquota = d-\u0026gt;quota; 6721\t} else { 6722\tperiod = tg_get_cfs_period(tg); 6723\tquota = tg_get_cfs_quota(tg); 6724\t} 6725 6726\t/* note: these should typically be equivalent */ 6727\tif (quota == RUNTIME_INF || quota == -1) 6728\treturn RUNTIME_INF; 6729 6730\treturn to_ratio(period, quota); 6731} 6732 6733static int tg_cfs_schedulable_down(struct task_group *tg, void *data) 6734{ 6735\tstruct cfs_schedulable_data *d = data; 6736\tstruct cfs_bandwidth *cfs_b = \u0026amp;tg-\u0026gt;cfs_bandwidth; 6737\ts64 quota = 0, parent_quota = -1; 6738 6739\tif (!tg-\u0026gt;parent) { 6740\tquota = RUNTIME_INF; 6741\t} else { 6742\tstruct cfs_bandwidth *parent_b = \u0026amp;tg-\u0026gt;parent-\u0026gt;cfs_bandwidth; 6743 6744\tquota = normalize_cfs_quota(tg, d); 6745\tparent_quota = parent_b-\u0026gt;hierarchical_quota; 6746 6747\t/* 6748* Ensure max(child_quota) \u0026lt;= parent_quota. On cgroup2, 6749* always take the min. On cgroup1, only inherit when no 6750* limit is set: 6751*/ 6752\tif (cgroup_subsys_on_dfl(cpu_cgrp_subsys)) { 6753\tquota = min(quota, parent_quota); 6754\t} else { 6755\tif (quota == RUNTIME_INF) 6756\tquota = parent_quota; 6757\telse if (parent_quota != RUNTIME_INF \u0026amp;\u0026amp; quota \u0026gt; parent_quota) 6758\treturn -EINVAL; 6759\t} 6760\t} 6761\tcfs_b-\u0026gt;hierarchical_quota = quota; 6762 6763\treturn 0; 6764} 6765 6766static int __cfs_schedulable(struct task_group *tg, u64 period, u64 quota) 6767{ 6768\tint ret; 6769\tstruct cfs_schedulable_data data = { 6770\t.tg = tg, 6771\t.period = period, 6772\t.quota = quota, 6773\t}; 6774 6775\tif (quota != RUNTIME_INF) { 6776\tdo_div(data.period, NSEC_PER_USEC); 6777\tdo_div(data.quota, NSEC_PER_USEC); 6778\t} 6779 6780\trcu_read_lock(); 6781\tret = walk_tg_tree(tg_cfs_schedulable_down, tg_nop, \u0026amp;data); 6782\trcu_read_unlock(); 6783 6784\treturn ret; 6785} 6786 6787static int cpu_cfs_stat_show(struct seq_file *sf, void *v) 6788{ 6789\tstruct task_group *tg = css_tg(seq_css(sf)); 6790\tstruct cfs_bandwidth *cfs_b = \u0026amp;tg-\u0026gt;cfs_bandwidth; 6791 6792\tseq_printf(sf, \u0026#34;nr_periods %d\\n\u0026#34;, cfs_b-\u0026gt;nr_periods); 6793\tseq_printf(sf, \u0026#34;nr_throttled %d\\n\u0026#34;, cfs_b-\u0026gt;nr_throttled); 6794\tseq_printf(sf, \u0026#34;throttled_time %llu\\n\u0026#34;, cfs_b-\u0026gt;throttled_time); 6795 6796\tif (schedstat_enabled() \u0026amp;\u0026amp; tg != \u0026amp;root_task_group) { 6797\tu64 ws = 0; 6798\tint i; 6799 6800\tfor_each_possible_cpu(i) 6801\tws += schedstat_val(tg-\u0026gt;se[i]-\u0026gt;statistics.wait_sum); 6802 6803\tseq_printf(sf, \u0026#34;wait_sum %llu\\n\u0026#34;, ws); 6804\t} 6805 6806\treturn 0; 6807} 6808#endif /* CONFIG_CFS_BANDWIDTH */6809#endif /* CONFIG_FAIR_GROUP_SCHED */6810 6811#ifdef CONFIG_RT_GROUP_SCHED 6812static int cpu_rt_runtime_write(struct cgroup_subsys_state *css, 6813\tstruct cftype *cft, s64 val) 6814{ 6815\treturn sched_group_set_rt_runtime(css_tg(css), val); 6816} 6817 6818static s64 cpu_rt_runtime_read(struct cgroup_subsys_state *css, 6819\tstruct cftype *cft) 6820{ 6821\treturn sched_group_rt_runtime(css_tg(css)); 6822} 6823 6824static int cpu_rt_period_write_uint(struct cgroup_subsys_state *css, 6825\tstruct cftype *cftype, u64 rt_period_us) 6826{ 6827\treturn sched_group_set_rt_period(css_tg(css), rt_period_us); 6828} 6829 6830static u64 cpu_rt_period_read_uint(struct cgroup_subsys_state *css, 6831\tstruct cftype *cft) 6832{ 6833\treturn sched_group_rt_period(css_tg(css)); 6834} 6835#endif /* CONFIG_RT_GROUP_SCHED */6836 6837static struct cftype cpu_legacy_files[] = { 6838#ifdef CONFIG_FAIR_GROUP_SCHED 6839\t{ 6840\t.name = \u0026#34;shares\u0026#34;, 6841\t.read_u64 = cpu_shares_read_u64, 6842\t.write_u64 = cpu_shares_write_u64, 6843\t}, 6844#endif 6845#ifdef CONFIG_CFS_BANDWIDTH 6846\t{ 6847\t.name = \u0026#34;cfs_quota_us\u0026#34;, 6848\t.read_s64 = cpu_cfs_quota_read_s64, 6849\t.write_s64 = cpu_cfs_quota_write_s64, 6850\t}, 6851\t{ 6852\t.name = \u0026#34;cfs_period_us\u0026#34;, 6853\t.read_u64 = cpu_cfs_period_read_u64, 6854\t.write_u64 = cpu_cfs_period_write_u64, 6855\t}, 6856\t{ 6857\t.name = \u0026#34;stat\u0026#34;, 6858\t.seq_show = cpu_cfs_stat_show, 6859\t}, 6860#endif 6861#ifdef CONFIG_RT_GROUP_SCHED 6862\t{ 6863\t.name = \u0026#34;rt_runtime_us\u0026#34;, 6864\t.read_s64 = cpu_rt_runtime_read, 6865\t.write_s64 = cpu_rt_runtime_write, 6866\t}, 6867\t{ 6868\t.name = \u0026#34;rt_period_us\u0026#34;, 6869\t.read_u64 = cpu_rt_period_read_uint, 6870\t.write_u64 = cpu_rt_period_write_uint, 6871\t}, 6872#endif 6873\t{ }\t/* Terminate */ 6874}; 6875 6876static int cpu_extra_stat_show(struct seq_file *sf, 6877\tstruct cgroup_subsys_state *css) 6878{ 6879#ifdef CONFIG_CFS_BANDWIDTH 6880\t{ 6881\tstruct task_group *tg = css_tg(css); 6882\tstruct cfs_bandwidth *cfs_b = \u0026amp;tg-\u0026gt;cfs_bandwidth; 6883\tu64 throttled_usec; 6884 6885\tthrottled_usec = cfs_b-\u0026gt;throttled_time; 6886\tdo_div(throttled_usec, NSEC_PER_USEC); 6887 6888\tseq_printf(sf, \u0026#34;nr_periods %d\\n\u0026#34; 6889\t\u0026#34;nr_throttled %d\\n\u0026#34; 6890\t\u0026#34;throttled_usec %llu\\n\u0026#34;, 6891\tcfs_b-\u0026gt;nr_periods, cfs_b-\u0026gt;nr_throttled, 6892\tthrottled_usec); 6893\t} 6894#endif 6895\treturn 0; 6896} 6897 6898#ifdef CONFIG_FAIR_GROUP_SCHED 6899static u64 cpu_weight_read_u64(struct cgroup_subsys_state *css, 6900\tstruct cftype *cft) 6901{ 6902\tstruct task_group *tg = css_tg(css); 6903\tu64 weight = scale_load_down(tg-\u0026gt;shares); 6904 6905\treturn DIV_ROUND_CLOSEST_ULL(weight * CGROUP_WEIGHT_DFL, 1024); 6906} 6907 6908static int cpu_weight_write_u64(struct cgroup_subsys_state *css, 6909\tstruct cftype *cft, u64 weight) 6910{ 6911\t/* 6912* cgroup weight knobs should use the common MIN, DFL and MAX 6913* values which are 1, 100 and 10000 respectively. While it loses 6914* a bit of range on both ends, it maps pretty well onto the shares 6915* value used by scheduler and the round-trip conversions preserve 6916* the original value over the entire range. 6917*/ 6918\tif (weight \u0026lt; CGROUP_WEIGHT_MIN || weight \u0026gt; CGROUP_WEIGHT_MAX) 6919\treturn -ERANGE; 6920 6921\tweight = DIV_ROUND_CLOSEST_ULL(weight * 1024, CGROUP_WEIGHT_DFL); 6922 6923\treturn sched_group_set_shares(css_tg(css), scale_load(weight)); 6924} 6925 6926static s64 cpu_weight_nice_read_s64(struct cgroup_subsys_state *css, 6927\tstruct cftype *cft) 6928{ 6929\tunsigned long weight = scale_load_down(css_tg(css)-\u0026gt;shares); 6930\tint last_delta = INT_MAX; 6931\tint prio, delta; 6932 6933\t/* find the closest nice value to the current weight */ 6934\tfor (prio = 0; prio \u0026lt; ARRAY_SIZE(sched_prio_to_weight); prio++) { 6935\tdelta = abs(sched_prio_to_weight[prio] - weight); 6936\tif (delta \u0026gt;= last_delta) 6937\tbreak; 6938\tlast_delta = delta; 6939\t} 6940 6941\treturn PRIO_TO_NICE(prio - 1 + MAX_RT_PRIO); 6942} 6943 6944static int cpu_weight_nice_write_s64(struct cgroup_subsys_state *css, 6945\tstruct cftype *cft, s64 nice) 6946{ 6947\tunsigned long weight; 6948\tint idx; 6949 6950\tif (nice \u0026lt; MIN_NICE || nice \u0026gt; MAX_NICE) 6951\treturn -ERANGE; 6952 6953\tidx = NICE_TO_PRIO(nice) - MAX_RT_PRIO; 6954\tidx = array_index_nospec(idx, 40); 6955\tweight = sched_prio_to_weight[idx]; 6956 6957\treturn sched_group_set_shares(css_tg(css), scale_load(weight)); 6958} 6959#endif 6960 6961static void __maybe_unused cpu_period_quota_print(struct seq_file *sf, 6962\tlong period, long quota) 6963{ 6964\tif (quota \u0026lt; 0) 6965\tseq_puts(sf, \u0026#34;max\u0026#34;); 6966\telse 6967\tseq_printf(sf, \u0026#34;%ld\u0026#34;, quota); 6968 6969\tseq_printf(sf, \u0026#34; %ld\\n\u0026#34;, period); 6970} 6971 6972/* caller should put the current value in *@periodp before calling */ 6973static int __maybe_unused cpu_period_quota_parse(char *buf, 6974\tu64 *periodp, u64 *quotap) 6975{ 6976\tchar tok[21];\t/* U64_MAX */ 6977 6978\tif (sscanf(buf, \u0026#34;%20s %llu\u0026#34;, tok, periodp) \u0026lt; 1) 6979\treturn -EINVAL; 6980 6981\t*periodp *= NSEC_PER_USEC; 6982 6983\tif (sscanf(tok, \u0026#34;%llu\u0026#34;, quotap)) 6984\t*quotap *= NSEC_PER_USEC; 6985\telse if (!strcmp(tok, \u0026#34;max\u0026#34;)) 6986\t*quotap = RUNTIME_INF; 6987\telse 6988\treturn -EINVAL; 6989 6990\treturn 0; 6991} 6992 6993#ifdef CONFIG_CFS_BANDWIDTH 6994static int cpu_max_show(struct seq_file *sf, void *v) 6995{ 6996\tstruct task_group *tg = css_tg(seq_css(sf)); 6997 6998\tcpu_period_quota_print(sf, tg_get_cfs_period(tg), tg_get_cfs_quota(tg)); 6999\treturn 0; 7000} 7001 7002static ssize_t cpu_max_write(struct kernfs_open_file *of, 7003\tchar *buf, size_t nbytes, loff_t off) 7004{ 7005\tstruct task_group *tg = css_tg(of_css(of)); 7006\tu64 period = tg_get_cfs_period(tg); 7007\tu64 quota; 7008\tint ret; 7009 7010\tret = cpu_period_quota_parse(buf, \u0026amp;period, \u0026amp;quota); 7011\tif (!ret) 7012\tret = tg_set_cfs_bandwidth(tg, period, quota); 7013\treturn ret ?: nbytes; 7014} 7015#endif 7016 7017static struct cftype cpu_files[] = { 7018#ifdef CONFIG_FAIR_GROUP_SCHED 7019\t{ 7020\t.name = \u0026#34;weight\u0026#34;, 7021\t.flags = CFTYPE_NOT_ON_ROOT, 7022\t.read_u64 = cpu_weight_read_u64, 7023\t.write_u64 = cpu_weight_write_u64, 7024\t}, 7025\t{ 7026\t.name = \u0026#34;weight.nice\u0026#34;, 7027\t.flags = CFTYPE_NOT_ON_ROOT, 7028\t.read_s64 = cpu_weight_nice_read_s64, 7029\t.write_s64 = cpu_weight_nice_write_s64, 7030\t}, 7031#endif 7032#ifdef CONFIG_CFS_BANDWIDTH 7033\t{ 7034\t.name = \u0026#34;max\u0026#34;, 7035\t.flags = CFTYPE_NOT_ON_ROOT, 7036\t.seq_show = cpu_max_show, 7037\t.write = cpu_max_write, 7038\t}, 7039#endif 7040\t{ }\t/* terminate */ 7041}; 7042 7043struct cgroup_subsys cpu_cgrp_subsys = { 7044\t.css_alloc\t= cpu_cgroup_css_alloc, 7045\t.css_online\t= cpu_cgroup_css_online, 7046\t.css_released\t= cpu_cgroup_css_released, 7047\t.css_free\t= cpu_cgroup_css_free, 7048\t.css_extra_stat_show = cpu_extra_stat_show, 7049\t.fork\t= cpu_cgroup_fork, 7050\t.can_attach\t= cpu_cgroup_can_attach, 7051\t.attach\t= cpu_cgroup_attach, 7052\t.legacy_cftypes\t= cpu_legacy_files, 7053\t.dfl_cftypes\t= cpu_files, 7054\t.early_init\t= true, 7055\t.threaded\t= true, 7056}; 7057 7058#endif\t/* CONFIG_CGROUP_SCHED */7059 7060void dump_cpu_task(int cpu) 7061{ 7062\tpr_info(\u0026#34;Task dump for CPU %d:\\n\u0026#34;, cpu); 7063\tsched_show_task(cpu_curr(cpu)); 7064} 7065 7066/* 7067* Nice levels are multiplicative, with a gentle 10% change for every 7068* nice level changed. I.e. when a CPU-bound task goes from nice 0 to 7069* nice 1, it will get ~10% less CPU time than another CPU-bound task 7070* that remained on nice 0. 7071* 7072* The \u0026#34;10% effect\u0026#34; is relative and cumulative: from _any_ nice level, 7073* if you go up 1 level, it\u0026#39;s -10% CPU usage, if you go down 1 level 7074* it\u0026#39;s +10% CPU usage. (to achieve that we use a multiplier of 1.25. 7075* If a task goes up by ~10% and another task goes down by ~10% then 7076* the relative distance between them is ~25%.) 7077*/ 7078const int sched_prio_to_weight[40] = { 7079 /* -20 */ 88761, 71755, 56483, 46273, 36291, 7080 /* -15 */ 29154, 23254, 18705, 14949, 11916, 7081 /* -10 */ 9548, 7620, 6100, 4904, 3906, 7082 /* -5 */ 3121, 2501, 1991, 1586, 1277, 7083 /* 0 */ 1024, 820, 655, 526, 423, 7084 /* 5 */ 335, 272, 215, 172, 137, 7085 /* 10 */ 110, 87, 70, 56, 45, 7086 /* 15 */ 36, 29, 23, 18, 15, 7087}; 7088 7089/* 7090* Inverse (2^32/x) values of the sched_prio_to_weight[] array, precalculated. 7091* 7092* In cases where the weight does not change often, we can use the 7093* precalculated inverse to speed up arithmetics by turning divisions 7094* into multiplications: 7095*/ 7096const u32 sched_prio_to_wmult[40] = { 7097 /* -20 */ 48388, 59856, 76040, 92818, 118348, 7098 /* -15 */ 147320, 184698, 229616, 287308, 360437, 7099 /* -10 */ 449829, 563644, 704093, 875809, 1099582, 7100 /* -5 */ 1376151, 1717300, 2157191, 2708050, 3363326, 7101 /* 0 */ 4194304, 5237765, 6557202, 8165337, 10153587, 7102 /* 5 */ 12820798, 15790321, 19976592, 24970740, 31350126, 7103 /* 10 */ 39045157, 49367440, 61356676, 76695844, 95443717, 7104 /* 15 */ 119304647, 148102320, 186737708, 238609294, 286331153, 7105}; 7106 7107#undef CREATE_TRACE_POINTS ","date":"Oct 4, 2021","img":"","permalink":"https://mengdemao.github.io/posts/sched/","series":null,"tags":["kernel"],"title":"系统调度"},{"categories":[],"content":"","date":"Oct 3, 2021","img":"","permalink":"https://mengdemao.github.io/posts/rime/","series":null,"tags":[],"title":"Rime"},{"categories":[],"content":"WASM笔记 ","date":"Oct 3, 2021","img":"","permalink":"https://mengdemao.github.io/posts/wasm/","series":null,"tags":[],"title":"Wasm"},{"categories":[],"content":"jupyter 安装 1conda install jupyter 基本配置 生成配置 1jupyter notebook --generate-config 自动生成配置文件 ~/.jupyter/jupyter_notebook_config.py\n设置密码 1jupyter notebook password  ","date":"Oct 3, 2021","img":"","permalink":"https://mengdemao.github.io/posts/jupyter/","series":null,"tags":[],"title":"Jupyter"},{"categories":[],"content":"","date":"Oct 3, 2021","img":"","permalink":"https://mengdemao.github.io/posts/conda/","series":null,"tags":[],"title":"Conda"},{"categories":[],"content":"RUST学习笔记\n====\n安装  添加环境变量.bashrc/profile  1set RUSTUP_DIST_SERVER=https://mirrors.ustc.edu.cn/rust-static 2set RUSTUP_UPDATE_ROOT=https://mirrors.ustc.edu.cn/rust-static/rustup 安装工具链  1curl https://mirrors.ustc.edu.cn/rust-static/rustup/rustup-init.sh | sh 设置rust的环境变量.bashrc/profile  1source ~/.cargo/env 2set PATH=~/.cargo/bin;$PATH 入门基础 HelloWorld  创建工程  1cargo new hello_world 编译  1cargo build 运行  1cargo run 数据类型    长度 有符号 无符号     8-bit i8 u8   16-bit i16 u16   32-bit i32 u32   64-bit i64 u64   128-bit i128 u128   arch isize usize    1// 创建变量 2let_xi32: i32 =5;3let_xu32: u32 =5;4let_xi64: i64 =10;5let_xu64: u64 =10;6let_xi128: i128=5;7let_xu128: u128=5;8let_xisize: isize =10;9let_xusize: usize =10;函数 1// 有返回值 2fn function_return()-\u0026gt; i32 {3println!(\u0026#34;Hello, World!\u0026#34;);4return0;5}67// 无返回值 8fn function_noreturn(){9println!(\u0026#34;Hello, World!\u0026#34;);10} 必须明确表示是否存在返回值 语法校验比较严格,  条件语句 1if\u0026lt;cond\u0026gt;{2do;3}1if\u0026lt;cond\u0026gt;{2Do1;3}else{4Do2;5}1if\u0026lt;cond1\u0026gt;{2Do1;3}elseif\u0026lt;cond2\u0026gt;{4Do2;5}else{6Do3;7}","date":"Oct 3, 2021","img":"","permalink":"https://mengdemao.github.io/posts/rust/","series":null,"tags":[],"title":"Rust"},{"categories":[],"content":"C++基础语法 第一个程序 1#include \u0026lt;iostream\u0026gt;2 3using namespace std; 4 5int main(int argc, char *argv[]) 6{ 7\tcout \u0026lt;\u0026lt; \u0026#34;Hello World\u0026#34; \u0026lt;\u0026lt; endl; 8\treturn 0; 9} 注释类型  单行注释  1// 这个是一个单行注释  多行注释  1/* 2这个里面是一个注释 3*/ 变量 变量的存在意义:方便我们管理内存\n变量创建的语法\n1数据类型 变量名 = 变量初始化; 常量 作用: 记录程序中不可以改变的数据\n define 宏常量(预编译期) const 修饰变量(编译期)  关键字    关键字        asm else new this   auto enum operator throw   bool explicit private true   break export protected try   case extern public typedef   catch false register typeid   char float reinterpret_cast typename   class for return union   const friend short unsigned   const_cast goto signed using   continue if sizeof virtual   default inline static void   delete int static_cast volatile   do long struct wchar_t   double mutable switch while   dynamic_cast namespace template     标识符命名规则  标识符不可以是关键字 只能由字母、数字、下划线构成 第一个字母只能是字母或者是下划线 区分大小写  数据类型 指定类型,分配内存\n整形 浮点型  单精度float 双精度double  字符型 转义字符 字符串  C风格  1char 变量名[] = \u0026#34;字符串值\u0026#34;; C++风格  1string 变量名 = \u0026#34;字符串值\u0026#34;; 布尔类型 1bool A = true; 2bool B = false; 运算符 基本运算符 取模运算 就是取余数\n自增自减运算 1a1++; 2a2--; 赋值运算    运算符 术语 示例 结果     =      +=      -=      *=      /=      %=       比较运算符 逻辑运算符 程序流程结构 顺序结构 if语句 三目运算符 1表达式1? 表达式2:表达式3 选择结构 1switch(condition) 2{ 3case 条件1: 4\tbreak; 5case 条件2: 6\tbreak; 7default: 8\tbreak; 9} 循环结构 while循环 1while(条件) 2{ 3\t循环体; 4} dowhile循环 1do { 2 3} while(条件) for循环 1for (起始表达式; 条件表达式; 末尾循环体) 2{ 3\t循环体; 4} 跳转语句 break continue goto 数组 函数定义  返回值类型 函数名 参数列表 函数体语句 return表达式  1返回值类型 函数名字(参数列表) 2{ 3\t函数体语句; 4\treturn 表达式; 5} 值传递 类似数值拷贝\n函数的常见样式  无参无返 有参无返 无参有反 有参有返  函数的声明 作用: 告诉编译器函数名以及调用方式,函数实体可以单独实现;\n函数的分文件编写 指针 指针的定义和使用 指针所占用空间 空指针 含义: 指针指向内存空间为0的指针; 用途: 初始化指针变量 注意: 空指针的地址是不可以访问的\n野指针 指针指向非法的内存空间\nconst与指针  const修饰指针 const修饰常量 const既修饰指针又修饰常量  1const int *p = \u0026amp;a; 2 3int const *p = \u0026amp;a; 4 5const int *const p = \u0026amp;a; 指针与数组 指针与函数 结构体 结构体数组 结构体指针 结构体嵌套 C++核心编程 本阶段主要对面向对象进行详细讲解\nC++内存分区 c++程序在运行时,将内存分为4个区域\n 代码区: 存放程序的二进制代码,由操作系统管理 全局区: 存放全局变量、静态变量和常量 栈区: 编译器自动分配 堆区: 程序负责分配和释放  new/delete操作符 C++利用new操作符在堆区开辟内存\n引用 作用: 给变量起别名 语法: 数据类型 \u0026amp;别名 = 原名;\n引用做参数 1#include \u0026lt;iostream\u0026gt;2void swap(int \u0026amp;a, int \u0026amp;b) 3{ 4 int t; t = a;a = b;b = t; 5} 6int main(int argc, char *argv[]) 7{ 8 int a = 10;int b = 12; 9 std::cout \u0026lt;\u0026lt; \u0026#34;交换前\u0026#34; \u0026lt;\u0026lt; a \u0026lt;\u0026lt; \u0026#39;\\t\u0026#39; \u0026lt;\u0026lt; b \u0026lt;\u0026lt; std::endl; 10 swap(a, b); 11 std::cout \u0026lt;\u0026lt; \u0026#34;交换后\u0026#34; \u0026lt;\u0026lt; a \u0026lt;\u0026lt; \u0026#39;\\t\u0026#39; \u0026lt;\u0026lt; b \u0026lt;\u0026lt; std::endl; 12 return 0; 13} 执行结果\n引用做返回值 引用的本质 引用的本质是C++内部实现的一个指针常量\n常量引用 1const int \u0026amp;ref = 10; 函数提高 函数默认值  某个位置有默认值，那么后面的参数也必须由默认值 如果声明了默认值，那么实现不可以有默认值(默认参数会产生冲突)  1void test_default_param(int a = 0, int b = 0, int c = 0) 2{ 3 std::cout \u0026lt;\u0026lt; a + b + c \u0026lt;\u0026lt; std::endl; 4} 函数的占位参数 占位参数还可以有默认值\n1void test(int a, int = 10) { 2 std::cout \u0026lt;\u0026lt; a \u0026lt;\u0026lt; std::endl; 3} 函数重载 作用:函数名相同,提高复用性\n重载的条件:\n  相同作用域\n  函数名相同\n  参数不同(类型, 个数,顺序)\n  注意事项:\n 引用作为重载条件 函数重载碰到默认参数  类和对象 类的访问属性\n public: protected: private:  class与struct的区别 class默认权限是private struct默认权限是public\n构造函数和析构函数 对象的初始化和清理\n 构造函数有参数 析构函数没有参数 二者都没有返回值  拷贝构造函数 1class Person { 2public: 3 /* 构造函数 */ 4 Person(std::string name, int age) { 5 std::cout \u0026lt;\u0026lt; \u0026#34;构造函数\u0026#34; \u0026lt;\u0026lt; std::endl; 6 } 7 /* 析构函数 */ 8 ~Person() { 9 std::cout \u0026lt;\u0026lt; \u0026#34;析构函数\u0026#34; \u0026lt;\u0026lt; std::endl; 10 } 11 /* 拷贝构造函数 */ 12 Person(const Person \u0026amp;p) { 13 std::cout \u0026lt;\u0026lt; \u0026#34;拷贝构造函数\u0026#34; \u0026lt;\u0026lt; std::endl; 14 } 15};  调用无参构造函数的时候不可以添加();否则就会产生函数声明的效果  1Person testPerson();\t// 表面上是执行构造函数 2int func();\t// 类似函数声明 拷贝构造函数的调用时机  使用一个已经创建完毕的对象初始化一个新对象 值传递的方式给函数进行参数传递 以值的方式返回局部对象  构造函数的调用规则 默认情况下:C++编译器至少给一个类添加3个函数\n 默认构造函数(无参) 默认析构函数(无参) 默认拷贝函数,对属性值进行拷贝  构造函数构造规则如下:\n 用户定义有参构造,C++默认不提供无参构造，但是提供默认拷贝构造 用户定义拷贝构造,C++不提供其他构造函数  深拷贝和浅拷贝  浅拷贝: 简单的复制操作 深拷贝: 在堆区重新申请空间，进行复制操作  初始化列表 作用:C++提供了初始化列表语法,用来初始化属性;\n语法:\n1构造函数(): 属性1(值1),属性2(值2),属性3(值3) 2{ 3\t/* 函数体 */ 4} 类对象作为类成员 静态成员 静态成员就是在静态成员变量和成员函数前加上static,称为静态成员;\n 静态成员变量  所有对象共享一份数据 编译阶段分配内存 类内声明,类外初始化   静态成员函数  所有对象共享同一个函数 静态成员函数只能访问静态成员变量    1class Person { 2public: 3 static int age; 4 static void func() 5 { 6 std::cout \u0026lt;\u0026lt; \u0026#34;静态成员函数\u0026#34; \u0026lt;\u0026lt; std::endl; 7 } 8}; 9/* 通过对象访问 */ 10Person p; 11p.func(); 12/* 通过类访问 */ 13Person::func(); 成员变量和成员函数分开存储  非静态成员,\t属于类的对象 静态成员,\t不属于类的对象 非静态成员函数,\t不属于类的对象 静态成员函数, 不属于类的对象  空对象大小为1\nC++对象模型 this指针 this指针指向被调用成员函数所属的对象 this指针本质：指针常量\n空指针访问成员函数 C++空指针也是可以访问成员函数的,但是要注意的this指针;\nconst修饰成员函数 常函数:\n 常函数不可以修改成员属性 成员属性加上mutable,常函数也可以修改 ** 常对象** 对象之前加const表示常对象 常对象只能调用函数  执行原理\n1this ==\u0026gt; Person * const this; 2后面新追加的const则会造成 3const Person * const this; 1class Person { 2public: 3 int m_A; 4 mutable int m_B; 5 void showPerson() const 6 { 7 m_A = 10; /* 错误,不可修改 */ 8 m_B = 10; /* 正确,可以修改 */ 9 } 10}; 友元  全局函数 全局类 成员函数  运算符重载 重载的原理:对已有的运算符进行重新定义,赋予新的功能含义;\n通过成员函数重载运算符 1class Person { 2public: 3 int m_A; 4 int m_B; 5 6 /* 使用成员函数实现 */ 7 Person PersonAddPerson(Person \u0026amp;p) 8 { 9 Person t; 10 t.m_A = this-\u0026gt;m_A + p.m_A; 11 t.m_B = this-\u0026gt;m_B + p.m_B; 12 return t; 13 } 14 15 /* 重载+ */ 16 Person operator+(Person \u0026amp;p) 17 { 18 Person t; 19 t.m_A = this-\u0026gt;m_A + p.m_A; 20 t.m_B = this-\u0026gt;m_B + p.m_B; 21 return t; 22 } 23}; 24 25int main(int argc, char *argv[]) 26{ 27 Person p1; 28 Person p2; 29 30 Person p3 = p1.PersonAddPerson(p2); 31 32 Person p4 = p1.operator+(p2); 33 34 Person p5 = p1 + p2; 35 36 return 0; 37} 通过全局函数重载 1Person operator+(Person \u0026amp;p1, Person \u0026amp;p2) 2{ 3 Person t; 4 t.m_A = p1.m_A + p2.m_A; 5 t.m_B = p2.m_B + p2.m_B; 6 return t; 7} 重载左移运算符 1std::ostream \u0026amp;operator\u0026lt;\u0026lt;(std::ostream \u0026amp;cout, Person \u0026amp;p) 2{ 3 cout \u0026lt;\u0026lt; p.m_A \u0026lt;\u0026lt; p.m_B; 4 return cout; 5} 递增重载++ 注意:\n 前置递增 p++ 后置递增 ++p  重载例子(复数) 1#include \u0026lt;iostream\u0026gt;2 3class Complex { 4 friend std::ostream \u0026amp;operator\u0026lt;\u0026lt;(std::ostream \u0026amp;cout, Complex p); 5 6public: 7 Complex(int i, int j); 8 9 Complex(); 10 11 /* 重载+ */ 12 Complex operator+(Complex \u0026amp;p) 13 { 14 Complex t; 15 t.i = this-\u0026gt;i + p.i; 16 t.j = this-\u0026gt;j + p.j; 17 return t; 18 } 19 /* 重载前置++ */ 20 Complex\u0026amp; operator++() 21 { 22 this-\u0026gt;i++; 23 this-\u0026gt;j++; 24 return *this; 25 } 26 27 /* 重载后置++ */ 28 Complex operator++(int) 29 { 30 Complex t; 31 32 /* 记录 */ 33 t.i = this-\u0026gt;i; 34 t.j = this-\u0026gt;j; 35 36 /* 递增 */ 37 this-\u0026gt;i++; 38 this-\u0026gt;j++; 39 40 return t; 41 } 42 43 /* 重载= */ 44 Complex\u0026amp; operator=(Complex \u0026amp;p) 45 { 46 this-\u0026gt;i = p.i; 47 this-\u0026gt;j = p.j; 48 49 return *this; 50 } 51private: 52 int i; /* 实部 */ 53 int j; /* 虚部 */ 54}; 55 56/* 构造函数 */ 57Complex::Complex(int i, int j) 58{ 59 this-\u0026gt;i = i; 60 this-\u0026gt;j = j; 61} 62 63Complex::Complex() 64{ 65 this-\u0026gt;i = 0; 66 this-\u0026gt;j = 0; 67} 68 69std::ostream \u0026amp;operator\u0026lt;\u0026lt;(std::ostream \u0026amp;cout, Complex p) 70{ 71 cout \u0026lt;\u0026lt; p.i \u0026lt;\u0026lt; \u0026#34;+\u0026#34; \u0026lt;\u0026lt; p.j \u0026lt;\u0026lt; \u0026#34;i\u0026#34;; 72 return cout; 73} 74 75int main(int argc, char *argv[]) 76{ 77 Complex p1(1, 2); 78 Complex p2(3, 4); 79 80 std::cout \u0026lt;\u0026lt; p1 \u0026lt;\u0026lt; std::endl; 81 std::cout \u0026lt;\u0026lt; p2 \u0026lt;\u0026lt; std::endl; 82 std::cout \u0026lt;\u0026lt; p1 + p2 \u0026lt;\u0026lt; std::endl; 83 84 std::cout \u0026lt;\u0026lt; ++p1 \u0026lt;\u0026lt; std::endl; 85 std::cout \u0026lt;\u0026lt; p2++ \u0026lt;\u0026lt; std::endl; 86 87 Complex p3 = p2 = p1; 88 std::cout \u0026lt;\u0026lt; p1 \u0026lt;\u0026lt; \u0026#34; \u0026#34; \u0026lt;\u0026lt; p2 \u0026lt;\u0026lt; \u0026#34; \u0026#34; \u0026lt;\u0026lt; p3 \u0026lt;\u0026lt; std::endl; 89 90 return 0; 91} 继承 减少重复代码\n1class 子类 : 继承方式 父类 父类:基类 子类:派生类\n继承方式  公共继承 保护继承 私有继承  继承中的对象模型 构造和析构的顺序  先构造父类再构造子类 先析构子类再析构父类\n 继承中同名成员处理  访问子类中同名成员,直接访问即可 s.m_A 访问父类中同名成员,需要加上作用域 s.Base:m_A  多重继承 C++允许一个类继承多个基类\n1class 子类 : 继承方式 父类1, 继承方式 父类2...  冲突解决：加上类名\n 菱形继承 孙子类继承了子类1和子类2,但是继承了两次父类。\n 多重继承数据会产生二义性 数据只需要一份即可  1/* 动物类 */ 2class Animal { 3public: 4 int m_age; 5}; 6class Sheep : public Animal {}; /* 羊类 */ 7class Camel : public Animal {}; /* 驼类 */ 8class Alpaca : public Sheep, public Camel {}; /* 羊驼 */ 9int main(int argc, char *argv[]) 10{ 11 Alpaca a; 12 a.Sheep::m_age = 18; 13 a.Camel::m_age = 18; 14 return 0; 15}  虚继承\n 1class Sheep : virtual public Animal {}; /* 羊类 */ 2class Camel : virtual public Animal {}; /* 驼类 */ 虚基类指针(vbptr) vbptr \u0026ndash;\u0026gt; vbtable\n多态  分类  静态多态: 重载 动态多态:虚函数   区别  静态多态函数地址早绑定:编译期确定函数地址 动态多态函数地址晚绑定:运行期确定函数地址    父类接收子类的对象,在程序运行期间确定具体改调用那个函数;\n  有继承关系\n  子类重写父类的虚函数 重写：函数完全一致\n  纯虚函数  只要有一个纯虚函数，就称为抽象类\n  抽象类无法直接实例化对象 抽象子类必须重写父类的纯虚函数,否则也是抽象类  原理 虚析构和纯虚析构 C++提高编程  泛型编程/STL\n 模版 建立通用的模板,提高复用；\nC++提供两种模版机制:函数模版和类模板\n函数模版 1template \u0026lt;typename T\u0026gt; 2函数声明和定义  template \u0026ndash; 声明创建模版 typename \u0026ndash; 表明后面的符号是数据类型可以用class代替 T \u0026ndash; 通用的数据类型  实例 1/* 两个数据交换 */ 2template \u0026lt;typename T\u0026gt; 3void swap(T \u0026amp;a, T \u0026amp;b) 4{ 5 T t= a; a = b; b = t; 6} 注意事项  自动类型推导,必须导出类型一致的T才可以使用 模版必须要确定T的数据类型,才可以使用  普通函数和函数模版的区别  普通函数可以发生隐式类型转换 函数模板: 用自动类型推导，不可以发生隐式转换 函数模板: 用显示类型推导，可以发生隐式转换  模版函数\n1template \u0026lt;typename T\u0026gt; 2T add(T a, T b) 3{ 4 return a + b; 5} 调用方法\n1/* 自动推导 */ 2std::cout \u0026lt;\u0026lt; add(10, 20) \u0026lt;\u0026lt; std::endl; 3/* 显示指定 */ 4std::cout \u0026lt;\u0026lt; add\u0026lt;int\u0026gt;(10, 3.14) \u0026lt;\u0026lt; std::endl; 普通函数和模版函数调用规则  普通函数和模版函数都可以调用,有限调用普通函数 强制通过空模版参数强制调用函数模版:函数名\u0026lt;\u0026gt;(参数列表) 函数模版也可以重载 函数模版更好的匹配,选择函数模版  类模板 1template \u0026lt;class T\u0026gt; 2类 例子\n1template \u0026lt;class NameType, class AgeType\u0026gt; 2class Person { 3public: 4 Person(NameType Name, AgeType Age) 5 { 6 m_Name = Name; 7 m_Age = Age; 8 } 9 NameType m_Name; 10 AgeType m_Age; 11}; 调用\n1Person\u0026lt;std::string, int\u0026gt; p(\u0026#34;Hello\u0026#34;, 99); ","date":"Oct 3, 2021","img":"","permalink":"https://mengdemao.github.io/posts/c++/","series":null,"tags":[],"title":"C++笔记"},{"categories":[],"content":"doxygen教程 开始 ","date":"Oct 3, 2021","img":"","permalink":"https://mengdemao.github.io/posts/doxygen/","series":null,"tags":[],"title":"Doxygen"},{"categories":[],"content":"","date":"Oct 3, 2021","img":"","permalink":"https://mengdemao.github.io/posts/engineering_compiler/","series":null,"tags":["compiler"],"title":"Engineering_compiler"},{"categories":[],"content":"简述  驱动模型\n    顶级kobject 解释     block 块设备链接\u0026ndash;\u0026gt;/sys/deives相关文件   bus 存放各种总线文件   class 各种设备类   dev 存放(字符/块)设备主副设备号链接文件\u0026ndash;\u0026gt;/sys/deives   devices 设备的具体存放文件   firmware 固件存放   fs 文件类型   kernel kernel子系统   module 模块信息   power 能源管理    底层机制 kobject  内核对象:kobject/kobject_type/kset 为模块提供一个底层抽象,其中文件存放于/sys文件下面\n 数据结构 1struct kobject { 2\tconst char\t*name;\t/* 名字 */ 3\tstruct list_head\tentry;\t/* 链表:链接进入kset */ 4\tstruct kobject\t*parent;\t/* 指向父对象,建立层次结构 */ 5\tstruct kset\t*kset;\t/* 对象集合 */ 6\tstruct kobj_type\t*ktype;\t/* 对象类型 */ 7\tstruct kernfs_node\t*sd; /* sysfs directory entry */ 8\tstruct kref\tkref;\t/* 引用计数 */ 9 10 #ifdef CONFIG_DEBUG_KOBJECT_RELEASE 11\tstruct delayed_work\trelease; 12\t#endif 13\t14 unsigned int state_initialized:1;\t/* 标志位:初始化 */ 15\tunsigned int state_in_sysfs:1;\t/* 标志位:在sysfs中 */ 16\tunsigned int state_add_uevent_sent:1;\t/* 标志位:发出KOBJ_ADD uevent */ 17\tunsigned int state_remove_uevent_sent:1;\t/* 标志位:发出KOBJ_REMOVE uevent */ 18\tunsigned int uevent_suppress:1;\t/* 标志位:禁止发出uevent */ 19}; 初始化 1/** 2* kobject_init - initialize a kobject structure 3* @kobj: pointer to the kobject to initialize 4* @ktype: pointer to the ktype for this kobject. 5* 6* This function will properly initialize a kobject such that it can then 7* be passed to the kobject_add() call. 8* 9* After this function is called, the kobject MUST be cleaned up by a call 10* to kobject_put(), not by a call to kfree directly to ensure that all of 11* the memory is cleaned up properly. 12*/ 13void kobject_init(struct kobject *kobj, struct kobj_type *ktype) 14{ 15\tchar *err_str;\t/** 错误信息 */ 16 17\t/** 校验参数NULL */ 18\tif (!kobj) { 19\terr_str = \u0026#34;invalid kobject pointer!\u0026#34;; 20\tgoto error; 21\t} 22\tif (!ktype) { 23\terr_str = \u0026#34;must have a ktype to be initialized properly!\\n\u0026#34;; 24\tgoto error; 25\t} 26 27\t/** kobject是否已经初始化 */ 28\tif (kobj-\u0026gt;state_initialized) { 29\t/* do not error out as sometimes we can recover */ 30\tpr_err(\u0026#34;kobject (%p): tried to init an initialized object, something is seriously wrong.\\n\u0026#34;, 31\tkobj); 32\tdump_stack(); /** 回溯堆栈 */ 33\t} 34 35\t/** 调用具体初始化函数 */ 36\tkobject_init_internal(kobj); 37 38\t/* 设置类型 */ 39\tkobj-\u0026gt;ktype = ktype; 40\treturn; 41 42error: 43\tpr_err(\u0026#34;kobject (%p): %s\\n\u0026#34;, kobj, err_str); 44\tdump_stack(); 45} 46EXPORT_SYMBOL(kobject_init); 添加 1int kobject_add(struct kobject *kobj, /* 需要添加kobject */ 2\tstruct kobject *parent, /* 父指针 */ 3\tconst char *fmt, ...) /* 命名 */ 4{ 5\tva_list args; 6\tint retval; 7\t8\t/* 校验kobject */ 9\tif (!kobj) 10\treturn -EINVAL; 11\t12\t/* 是否已经初始化 */ 13\tif (!kobj-\u0026gt;state_initialized) { 14\tpr_err(\u0026#34;kobject \u0026#39;%s\u0026#39; (%p): tried to add an uninitialized object, something is seriously wrong.\\n\u0026#34;, 15\tkobject_name(kobj), kobj); 16\tdump_stack(); 17\treturn -EINVAL; 18\t} 19\tva_start(args, fmt); 20\t/* 设置名字并且将父指针添加到parent */ 21\tretval = kobject_add_varg(kobj, parent, fmt, args); 22\tva_end(args); 23\t24\treturn retval; 25} 最终调用添加函数\n1static int kobject_add_internal(struct kobject *kobj) 2{ 3\tint error = 0; 4\tstruct kobject *parent; 5\t6 /* 判断参数NULL */ 7\tif (!kobj) 8\treturn -ENOENT; 9\t10 /* 判断名字是否有效 */ 11\tif (!kobj-\u0026gt;name || !kobj-\u0026gt;name[0]) { 12\tWARN(1, 13\t\u0026#34;kobject: (%p): attempted to be registered with empty name!\\n\u0026#34;, 14\tkobj); 15\treturn -EINVAL; 16\t} 17\t18 /** 获取父指针 */ 19\tparent = kobject_get(kobj-\u0026gt;parent); 20 21\t/* join kset if set, use it as parent if we do not already have one */ 22\tif (kobj-\u0026gt;kset) {\t/* kset已经设置 */ 23\tif (!parent)\t/* 不存在父指针 */ 24\t/* kset的kobject作为父指针 */ 25 parent = kobject_get(\u0026amp;kobj-\u0026gt;kset-\u0026gt;kobj); 26\t/* 将kobject加入kset */ 27 kobj_kset_join(kobj); 28\t29 /* 保存父指针 */ 30 kobj-\u0026gt;parent = parent; 31\t} 32 33\tpr_debug(\u0026#34;kobject: \u0026#39;%s\u0026#39; (%p): %s: parent: \u0026#39;%s\u0026#39;, set: \u0026#39;%s\u0026#39;\\n\u0026#34;, 34\tkobject_name(kobj), kobj, __func__, 35\tparent ? kobject_name(parent) : \u0026#34;\u0026lt;NULL\u0026gt;\u0026#34;, 36\tkobj-\u0026gt;kset ? kobject_name(\u0026amp;kobj-\u0026gt;kset-\u0026gt;kobj) : \u0026#34;\u0026lt;NULL\u0026gt;\u0026#34;); 37\t38 /* 创建dir */ 39\terror = create_dir(kobj); 40\tif (error) { /* 出错,清理 */ 41\tkobj_kset_leave(kobj); 42\tkobject_put(parent); 43\tkobj-\u0026gt;parent = NULL; 44 45\t/* be noisy on error issues */ 46\tif (error == -EEXIST) 47\tpr_err(\u0026#34;%s failed for %s with -EEXIST, don\u0026#39;t try to register things with the same name in the same directory.\\n\u0026#34;, 48\t__func__, kobject_name(kobj)); 49\telse 50\tpr_err(\u0026#34;%s failed for %s (error: %d parent: %s)\\n\u0026#34;, 51\t__func__, kobject_name(kobj), error, 52\tparent ? kobject_name(parent) : \u0026#34;\u0026#39;none\u0026#39;\u0026#34;); 53\t} else 54\tkobj-\u0026gt;state_in_sysfs = 1; /* 添加到sysfs中 */ 55 56\treturn error; 57} sysfs文件夹生成 1static int create_dir(struct kobject *kobj) 2{ 3\tconst struct kobj_ns_type_operations *ops; 4\tint error; 5 6\terror = sysfs_create_dir_ns(kobj, kobject_namespace(kobj)); 7\tif (error) 8\treturn error; 9 10\terror = populate_dir(kobj); 11\tif (error) { 12\tsysfs_remove_dir(kobj); 13\treturn error; 14\t} 15 16\t/* 17* @kobj-\u0026gt;sd may be deleted by an ancestor going away. Hold an 18* extra reference so that it stays until @kobj is gone. 19*/ 20\tsysfs_get(kobj-\u0026gt;sd); 21 22\t/* 23* If @kobj has ns_ops, its children need to be filtered based on 24* their namespace tags. Enable namespace support on @kobj-\u0026gt;sd. 25*/ 26\tops = kobj_child_ns_ops(kobj); 27\tif (ops) { 28\tBUG_ON(ops-\u0026gt;type \u0026lt;= KOBJ_NS_TYPE_NONE); 29\tBUG_ON(ops-\u0026gt;type \u0026gt;= KOBJ_NS_TYPES); 30\tBUG_ON(!kobj_ns_type_registered(ops-\u0026gt;type)); 31 32\tsysfs_enable_ns(kobj-\u0026gt;sd); 33\t} 34 35\treturn 0; 36} 删除 1void kobject_del(struct kobject *kobj) 2{ 3\tstruct kernfs_node *sd; 4 5\tif (!kobj) 6\treturn; 7 8\tsd = kobj-\u0026gt;sd; 9\tsysfs_remove_dir(kobj); 10\tsysfs_put(sd); 11 12\tkobj-\u0026gt;state_in_sysfs = 0; 13\tkobj_kset_leave(kobj); 14\tkobject_put(kobj-\u0026gt;parent); 15\tkobj-\u0026gt;parent = NULL; 16} 引用计数 1struct kobject *kobject_get(struct kobject *kobj) 2{ 3\tif (kobj) { 4\tif (!kobj-\u0026gt;state_initialized) 5\tWARN(1, KERN_WARNING 6\t\u0026#34;kobject: \u0026#39;%s\u0026#39; (%p): is not initialized, yet kobject_get() is being called.\\n\u0026#34;, 7\tkobject_name(kobj), kobj); 8\tkref_get(\u0026amp;kobj-\u0026gt;kref); 9\t} 10\treturn kobj; 11} 12 13void kobject_put(struct kobject *kobj) 14{ 15\tif (kobj) { 16\tif (!kobj-\u0026gt;state_initialized) 17\tWARN(1, KERN_WARNING 18\t\u0026#34;kobject: \u0026#39;%s\u0026#39; (%p): is not initialized, yet kobject_put() is being called.\\n\u0026#34;, 19\tkobject_name(kobj), kobj); 20\tkref_put(\u0026amp;kobj-\u0026gt;kref, kobject_release); 21\t} 22} kset 数据结构 1struct kset { 2\tstruct list_head list; 3\tspinlock_t list_lock; 4\tstruct kobject kobj; 5\tconst struct kset_uevent_ops *uevent_ops; 6} __randomize_layout; ktype 数据结构 1struct kobj_type { 2\tvoid (*release)(struct kobject *kobj); 3\tconst struct sysfs_ops *sysfs_ops; 4\tstruct attribute **default_attrs; 5\tconst struct kobj_ns_type_operations *(*child_ns_type)(struct kobject *kobj); 6\tconst void *(*namespace)(struct kobject *kobj); 7\tvoid (*get_ownership)(struct kobject *kobj, kuid_t *uid, kgid_t *gid); 8}; class  设备类描述\n 1struct class { 2\tconst char\t*name; 3\tstruct module\t*owner; 4 5\tconst struct attribute_group\t**class_groups; 6\tconst struct attribute_group\t**dev_groups; 7\tstruct kobject\t*dev_kobj; 8 9\tint (*dev_uevent)(struct device *dev, struct kobj_uevent_env *env); 10\tchar *(*devnode)(struct device *dev, umode_t *mode); 11 12\tvoid (*class_release)(struct class *class); 13\tvoid (*dev_release)(struct device *dev); 14 15\tint (*shutdown_pre)(struct device *dev); 16 17\tconst struct kobj_ns_type_operations *ns_type; 18\tconst void *(*namespace)(struct device *dev); 19 20\tvoid (*get_ownership)(struct device *dev, kuid_t *uid, kgid_t *gid); 21 22\tconst struct dev_pm_ops *pm; 23 24\tstruct subsys_private *p; 25}; bus  设备总线描述\n 总线类型 1# ls 2amba cpu nvmem platform virtio 3clockevents event_source pci scsi workqueue 4clocksource gpio pci_express serio 5container hid pcmcia spi 其中每一个总线具有如下信息\n1# ls 2devices drivers_autoprobe uevent 3drivers drivers_probe 1struct bus_type { 2\tconst char\t*name; 3\tconst char\t*dev_name; 4\tstruct device\t*dev_root; 5\tconst struct attribute_group **bus_groups; 6\tconst struct attribute_group **dev_groups; 7\tconst struct attribute_group **drv_groups; 8 9\tint (*match)(struct device *dev, struct device_driver *drv); 10\tint (*uevent)(struct device *dev, struct kobj_uevent_env *env); 11\tint (*probe)(struct device *dev); 12\tint (*remove)(struct device *dev); 13\tvoid (*shutdown)(struct device *dev); 14 15\tint (*online)(struct device *dev); 16\tint (*offline)(struct device *dev); 17 18\tint (*suspend)(struct device *dev, pm_message_t state); 19\tint (*resume)(struct device *dev); 20 21\tint (*num_vf)(struct device *dev); 22 23\tint (*dma_configure)(struct device *dev); 24 25\tconst struct dev_pm_ops *pm; 26 27\tconst struct iommu_ops *iommu_ops; 28 29\tstruct subsys_private *p; 30\tstruct lock_class_key lock_key; 31 32\tbool need_parent_lock; 33}; 设备总线注册 1int bus_register(struct bus_type *bus) 2{ 3\tint retval; 4\tstruct subsys_private *priv; 5\tstruct lock_class_key *key = \u0026amp;bus-\u0026gt;lock_key; 6 7\tpriv = kzalloc(sizeof(struct subsys_private), GFP_KERNEL); 8\tif (!priv) 9\treturn -ENOMEM; 10 11\tpriv-\u0026gt;bus = bus; 12\tbus-\u0026gt;p = priv; 13 14\tBLOCKING_INIT_NOTIFIER_HEAD(\u0026amp;priv-\u0026gt;bus_notifier); 15 16\tretval = kobject_set_name(\u0026amp;priv-\u0026gt;subsys.kobj, \u0026#34;%s\u0026#34;, bus-\u0026gt;name); 17\tif (retval) 18\tgoto out; 19 20\tpriv-\u0026gt;subsys.kobj.kset = bus_kset; 21\tpriv-\u0026gt;subsys.kobj.ktype = \u0026amp;bus_ktype; 22\tpriv-\u0026gt;drivers_autoprobe = 1; 23 24\tretval = kset_register(\u0026amp;priv-\u0026gt;subsys); 25\tif (retval) 26\tgoto out; 27 28\tretval = bus_create_file(bus, \u0026amp;bus_attr_uevent); 29\tif (retval) 30\tgoto bus_uevent_fail; 31 32\tpriv-\u0026gt;devices_kset = kset_create_and_add(\u0026#34;devices\u0026#34;, NULL, 33\t\u0026amp;priv-\u0026gt;subsys.kobj); 34\tif (!priv-\u0026gt;devices_kset) { 35\tretval = -ENOMEM; 36\tgoto bus_devices_fail; 37\t} 38 39\tpriv-\u0026gt;drivers_kset = kset_create_and_add(\u0026#34;drivers\u0026#34;, NULL, 40\t\u0026amp;priv-\u0026gt;subsys.kobj); 41\tif (!priv-\u0026gt;drivers_kset) { 42\tretval = -ENOMEM; 43\tgoto bus_drivers_fail; 44\t} 45 46\tINIT_LIST_HEAD(\u0026amp;priv-\u0026gt;interfaces); 47\t__mutex_init(\u0026amp;priv-\u0026gt;mutex, \u0026#34;subsys mutex\u0026#34;, key); 48\tklist_init(\u0026amp;priv-\u0026gt;klist_devices, klist_devices_get, klist_devices_put); 49\tklist_init(\u0026amp;priv-\u0026gt;klist_drivers, NULL, NULL); 50 51\tretval = add_probe_files(bus); 52\tif (retval) 53\tgoto bus_probe_files_fail; 54 55\tretval = bus_add_groups(bus, bus-\u0026gt;bus_groups); 56\tif (retval) 57\tgoto bus_groups_fail; 58 59\tpr_debug(\u0026#34;bus: \u0026#39;%s\u0026#39;: registered\\n\u0026#34;, bus-\u0026gt;name); 60\treturn 0; 61 62bus_groups_fail: 63\tremove_probe_files(bus); 64bus_probe_files_fail: 65\tkset_unregister(bus-\u0026gt;p-\u0026gt;drivers_kset); 66bus_drivers_fail: 67\tkset_unregister(bus-\u0026gt;p-\u0026gt;devices_kset); 68bus_devices_fail: 69\tbus_remove_file(bus, \u0026amp;bus_attr_uevent); 70bus_uevent_fail: 71\tkset_unregister(\u0026amp;bus-\u0026gt;p-\u0026gt;subsys); 72out: 73\tkfree(bus-\u0026gt;p); 74\tbus-\u0026gt;p = NULL; 75\treturn retval; 76} 77 设备总线卸载 1void bus_unregister(struct bus_type *bus) 2{ 3\tpr_debug(\u0026#34;bus: \u0026#39;%s\u0026#39;: unregistering\\n\u0026#34;, bus-\u0026gt;name); 4\tif (bus-\u0026gt;dev_root) 5\tdevice_unregister(bus-\u0026gt;dev_root); 6\tbus_remove_groups(bus, bus-\u0026gt;bus_groups); 7\tremove_probe_files(bus); 8\tkset_unregister(bus-\u0026gt;p-\u0026gt;drivers_kset); 9\tkset_unregister(bus-\u0026gt;p-\u0026gt;devices_kset); 10\tbus_remove_file(bus, \u0026amp;bus_attr_uevent); 11\tkset_unregister(\u0026amp;bus-\u0026gt;p-\u0026gt;subsys); 12} devices  设备文件具体描述: device(设备描述) device_driver(驱动描述) bus_type(总线信息)\n 1|-- breakpoint 2|-- kprobe 3|-- platform 4|-- software 5|-- system 6|-- tracepoint 7|-- uprobe 8`-- virtual  device  描述设备\n 1struct device { 2\tstruct device\t*parent; 3 4\tstruct device_private\t*p; 5 6\tstruct kobject kobj; 7\tconst char\t*init_name; /* initial name of the device */ 8\tconst struct device_type *type; 9 10\tstruct mutex\tmutex;\t/* mutex to synchronize calls to 11* its driver. 12*/ 13 14\tstruct bus_type\t*bus;\t/* type of bus device is on */ 15\tstruct device_driver *driver;\t/* which driver has allocated this 16device */ 17\tvoid\t*platform_data;\t/* Platform specific data, device 18core doesn\u0026#39;t touch it */ 19\tvoid\t*driver_data;\t/* Driver data, set and get with 20dev_set/get_drvdata */ 21\tstruct dev_links_info\tlinks; 22\tstruct dev_pm_info\tpower; 23\tstruct dev_pm_domain\t*pm_domain; 24 25#ifdef CONFIG_GENERIC_MSI_IRQ_DOMAIN 26\tstruct irq_domain\t*msi_domain; 27#endif 28#ifdef CONFIG_PINCTRL 29\tstruct dev_pin_info\t*pins; 30#endif 31#ifdef CONFIG_GENERIC_MSI_IRQ 32\tstruct list_head\tmsi_list; 33#endif 34 35#ifdef CONFIG_NUMA 36\tint\tnuma_node;\t/* NUMA node this device is close to */ 37#endif 38\tconst struct dma_map_ops *dma_ops; 39\tu64\t*dma_mask;\t/* dma mask (if dma\u0026#39;able device) */ 40\tu64\tcoherent_dma_mask;/* Like dma_mask, but for 41alloc_coherent mappings as 42not all hardware supports 4364 bit addresses for consistent 44allocations such descriptors. */ 45\tu64\tbus_dma_mask;\t/* upstream dma_mask constraint */ 46\tunsigned long\tdma_pfn_offset; 47 48\tstruct device_dma_parameters *dma_parms; 49 50\tstruct list_head\tdma_pools;\t/* dma pools (if dma\u0026#39;ble) */ 51 52\tstruct dma_coherent_mem\t*dma_mem; /* internal for coherent mem 53override */ 54#ifdef CONFIG_DMA_CMA 55\tstruct cma *cma_area;\t/* contiguous memory area for dma 56allocations */ 57#endif 58\t/* arch specific additions */ 59\tstruct dev_archdata\tarchdata; 60 61\tstruct device_node\t*of_node; /* associated device tree node */ 62\tstruct fwnode_handle\t*fwnode; /* firmware device node */ 63 64\tdev_t\tdevt;\t/* dev_t, creates the sysfs \u0026#34;dev\u0026#34; */ 65\tu32\tid;\t/* device instance */ 66 67\tspinlock_t\tdevres_lock; 68\tstruct list_head\tdevres_head; 69 70\tstruct klist_node\tknode_class; 71\tstruct class\t*class; 72\tconst struct attribute_group **groups;\t/* optional groups */ 73 74\tvoid\t(*release)(struct device *dev); 75\tstruct iommu_group\t*iommu_group; 76\tstruct iommu_fwspec\t*iommu_fwspec; 77 78\tbool\toffline_disabled:1; 79\tbool\toffline:1; 80\tbool\tof_node_reused:1; 81}; driver  描述驱动\n 1struct device_driver { 2\tconst char\t*name; 3\tstruct bus_type\t*bus; 4 5\tstruct module\t*owner; 6\tconst char\t*mod_name;\t/* used for built-in modules */ 7 8\tbool suppress_bind_attrs;\t/* disables bind/unbind via sysfs */ 9\tenum probe_type probe_type; 10 11\tconst struct of_device_id\t*of_match_table; 12\tconst struct acpi_device_id\t*acpi_match_table; 13 14\tint (*probe) (struct device *dev); 15\tint (*remove) (struct device *dev); 16\tvoid (*shutdown) (struct device *dev); 17\tint (*suspend) (struct device *dev, pm_message_t state); 18\tint (*resume) (struct device *dev); 19\tconst struct attribute_group **groups; 20 21\tconst struct dev_pm_ops *pm; 22\tvoid (*coredump) (struct device *dev); 23 24\tstruct driver_private *p; 25}; kernel  kernel子系统\n module  模块信息\n ","date":"Oct 1, 2021","img":"","permalink":"https://mengdemao.github.io/posts/drivermodel/","series":null,"tags":["kernel"],"title":"驱动模型"},{"categories":null,"content":"LuaJIT Lua语法 基本语法 1\tprint(\u0026#34;Hello World\u0026#34;) 表(table) LuaJIT分析 LuaJIT主函数 1int main(int argc, char **argv) 2{ 3\tint status; /* 返回值 */ 4\tlua_State *L = lua_open(); /* 创建LUA状态机 */ 5\tif (L == NULL) { 6\tl_message(argv[0], \u0026#34;cannot create state: not enough memory\u0026#34;); 7\treturn EXIT_FAILURE; 8\t} 9\t10\t/* smain只存在三个参数,主要作用是向pmain传递数据 */ 11\tsmain.argc = argc; 12\tsmain.argv = argv; 13\t14\tstatus = lua_cpcall(L, pmain, NULL);\t/* 启动函数调用 */ 15\t16\treport(L, status); /* 提取报错参数 */ 17\t18\tlua_close(L);\t/* 销毁状态机 */ 19\t20\treturn (status || smain.status \u0026gt; 0) ? EXIT_FAILURE : EXIT_SUCCESS; 21} Lua状态机 1struct lua_State { 2\tGCObject*next; 3 4 lu_byte tt; 5 lu_byte marked; 6\tlu_byte status; 7\t8 StkId top; 9\tStkId base; 10\t11 global_State *l_G;\t/* 全局状态信息 */ 12\t13 CallInfo*ci; 14\t15 const Instruction*savedpc; 16\tStkId stack_last; 17\tStkId stack; 18\t19 CallInfo*end_ci; 20\tCallInfo*base_ci; 21\t22 int stacksize; 23\tint size_ci; 24\tunsigned short nCcalls; 25\tunsigned short baseCcalls; 26\t27 lu_byte hookmask; 28\tlu_byte allowhook; 29\t30 int basehookcount; 31\tint hookcount; 32\t33 lua_Hook hook; 34\t35 TValue l_gt; 36\tTValue env; 37\t38 GCObject*openupval; 39\tGCObject*gclist; 40\t41 struct lua_longjmp*errorJmp; 42\t43 ptrdiff_t errfunc; 44}; 创建状态 1/* 此函数实际不存在,程序内部使用的是宏定义 */ 2void lua_open(void); 3 4/* 实际调用位置 */ 5LUALIB_API lua_State *luaL_newstate(void); 6 7/* 根据编译期64位信息选择调用 */ 8#if LJ_64 \u0026amp;\u0026amp; !LJ_GC64 \u0026amp;\u0026amp; !(defined(LUAJIT_USE_VALGRIND) \u0026amp;\u0026amp; defined(LUAJIT_USE_SYSMALLOC)) 9lua_State *lj_state_newstate(lua_Alloc allocf, void *allocd); 10#else 11LUA_API lua_State *lua_newstate(lua_Alloc allocf, void *allocd); 12#endif 函数调用 1LUA_API int lua_cpcall(lua_State *L, lua_CFunction func, void *ud); 2LUA_API int lua_pcall(lua_State *L, int nargs, int nresults, int errfunc); 3LUA_API void lua_call(lua_State *L, int nargs, int nresults); lua_cpcall函数调用\n执行原理 FFI分析 ","date":"Sep 4, 2021","img":"","permalink":"https://mengdemao.github.io/posts/luajit/","series":null,"tags":null,"title":"LuaJIT"},{"categories":null,"content":"编译原理 基础概念 词法分析 RE NFA DFA 语法分析 上下文无关文法(CFG) 自上而下(Top Down) 自下而上(Bottom Up) 语义分析 中间代码 目标代码 ","date":"Sep 4, 2021","img":"","permalink":"https://mengdemao.github.io/posts/compilerprinciples/","series":null,"tags":null,"title":"编译原理"},{"categories":null,"content":"页面分配器 核心函数: __alloc_pages_nodemask\n gfp_mask : 分配掩码 order : 分配阶数 preferred_nid nodemask  核心函数 1struct page *__alloc_pages_nodemask(gfp_t gfp_mask, unsigned int order, int preferred_nid, nodemask_t *nodemask) 2{ 3\tstruct page *page;\t// 分配变量 4\tunsigned int alloc_flags = ALLOC_WMARK_LOW;\t// 分配标志 5\tgfp_t alloc_mask; // 真实分配掩码 6\tstruct alloc_context ac = { };\t// 保存相关参数 7 8\t/* 9* There are several places where we assume that the order value is sane 10* so bail out early if the request is out of bound. 11* 限制分配的大小 12*/ 13\tif (unlikely(order \u0026gt;= MAX_ORDER)) { 14\tWARN_ON_ONCE(!(gfp_mask \u0026amp; __GFP_NOWARN)); 15\treturn NULL; 16\t} 17 18\tgfp_mask \u0026amp;= gfp_allowed_mask; 19\talloc_mask = gfp_mask; 20\tif (!prepare_alloc_pages(gfp_mask, order, preferred_nid, nodemask, \u0026amp;ac, \u0026amp;alloc_mask, \u0026amp;alloc_flags)) 21\treturn NULL; 22 23\tfinalise_ac(gfp_mask, \u0026amp;ac); 24 25\t/* First allocation attempt */ 26\tpage = get_page_from_freelist(alloc_mask, order, alloc_flags, \u0026amp;ac); 27\tif (likely(page)) 28\tgoto out; 29 30\t/* 31* Apply scoped allocation constraints. This is mainly about GFP_NOFS 32* resp. GFP_NOIO which has to be inherited for all allocation requests 33* from a particular context which has been marked by 34* memalloc_no{fs,io}_{save,restore}. 35*/ 36\talloc_mask = current_gfp_context(gfp_mask); 37\tac.spread_dirty_pages = false; 38 39\t/* 40* Restore the original nodemask if it was potentially replaced with 41* \u0026amp;cpuset_current_mems_allowed to optimize the fast-path attempt. 42*/ 43\tif (unlikely(ac.nodemask != nodemask)) 44\tac.nodemask = nodemask; 45 46\tpage = __alloc_pages_slowpath(alloc_mask, order, \u0026amp;ac); 47 48out: 49\tif (memcg_kmem_enabled() \u0026amp;\u0026amp; (gfp_mask \u0026amp; __GFP_ACCOUNT) \u0026amp;\u0026amp; page \u0026amp;\u0026amp; 50\tunlikely(memcg_kmem_charge(page, gfp_mask, order) != 0)) { 51\t__free_pages(page, order); 52\tpage = NULL; 53\t} 54 55\ttrace_mm_page_alloc(page, order, alloc_mask, ac.migratetype); 56 57\treturn page; 58} prepare_alloc_pages 1static inline bool prepare_alloc_pages(gfp_t gfp_mask, unsigned int order, 2\tint preferred_nid, nodemask_t *nodemask, 3\tstruct alloc_context *ac, gfp_t *alloc_mask, 4\tunsigned int *alloc_flags) 5{ 6\tac-\u0026gt;high_zoneidx = gfp_zone(gfp_mask); 7\tac-\u0026gt;zonelist = node_zonelist(preferred_nid, gfp_mask); 8\tac-\u0026gt;nodemask = nodemask; 9\tac-\u0026gt;migratetype = gfpflags_to_migratetype(gfp_mask); 10 11\tif (cpusets_enabled()) { 12\t*alloc_mask |= __GFP_HARDWALL; 13\tif (!ac-\u0026gt;nodemask) 14\tac-\u0026gt;nodemask = \u0026amp;cpuset_current_mems_allowed; 15\telse 16\t*alloc_flags |= ALLOC_CPUSET; 17\t} 18 19\tfs_reclaim_acquire(gfp_mask); 20\tfs_reclaim_release(gfp_mask); 21 22\tmight_sleep_if(gfp_mask \u0026amp; __GFP_DIRECT_RECLAIM); 23 24\tif (should_fail_alloc_page(gfp_mask, order)) 25\treturn false; 26 27\tif (IS_ENABLED(CONFIG_CMA) \u0026amp;\u0026amp; ac-\u0026gt;migratetype == MIGRATE_MOVABLE) 28\t*alloc_flags |= ALLOC_CMA; 29 30\treturn true; 31} get_page_from_freelist 1static struct page * 2get_page_from_freelist(gfp_t gfp_mask, unsigned int order, int alloc_flags, 3\tconst struct alloc_context *ac) 4{ 5\tstruct zoneref *z = ac-\u0026gt;preferred_zoneref; 6\tstruct zone *zone; 7\tstruct pglist_data *last_pgdat_dirty_limit = NULL; 8 9\t/* 10* Scan zonelist, looking for a zone with enough free. 11* See also __cpuset_node_allowed() comment in kernel/cpuset.c. 12*/ 13\tfor_next_zone_zonelist_nodemask(zone, z, ac-\u0026gt;zonelist, ac-\u0026gt;high_zoneidx, 14\tac-\u0026gt;nodemask) { 15\tstruct page *page; 16\tunsigned long mark; 17 18\tif (cpusets_enabled() \u0026amp;\u0026amp; 19\t(alloc_flags \u0026amp; ALLOC_CPUSET) \u0026amp;\u0026amp; 20\t!__cpuset_zone_allowed(zone, gfp_mask)) 21\tcontinue; 22\t/* 23* When allocating a page cache page for writing, we 24* want to get it from a node that is within its dirty 25* limit, such that no single node holds more than its 26* proportional share of globally allowed dirty pages. 27* The dirty limits take into account the node\u0026#39;s 28* lowmem reserves and high watermark so that kswapd 29* should be able to balance it without having to 30* write pages from its LRU list. 31* 32* XXX: For now, allow allocations to potentially 33* exceed the per-node dirty limit in the slowpath 34* (spread_dirty_pages unset) before going into reclaim, 35* which is important when on a NUMA setup the allowed 36* nodes are together not big enough to reach the 37* global limit. The proper fix for these situations 38* will require awareness of nodes in the 39* dirty-throttling and the flusher threads. 40*/ 41\tif (ac-\u0026gt;spread_dirty_pages) { 42\tif (last_pgdat_dirty_limit == zone-\u0026gt;zone_pgdat) 43\tcontinue; 44 45\tif (!node_dirty_ok(zone-\u0026gt;zone_pgdat)) { 46\tlast_pgdat_dirty_limit = zone-\u0026gt;zone_pgdat; 47\tcontinue; 48\t} 49\t} 50 51\tmark = zone-\u0026gt;watermark[alloc_flags \u0026amp; ALLOC_WMARK_MASK]; 52\tif (!zone_watermark_fast(zone, order, mark, 53\tac_classzone_idx(ac), alloc_flags)) { 54\tint ret; 55 56#ifdef CONFIG_DEFERRED_STRUCT_PAGE_INIT 57\t/* 58* Watermark failed for this zone, but see if we can 59* grow this zone if it contains deferred pages. 60*/ 61\tif (static_branch_unlikely(\u0026amp;deferred_pages)) { 62\tif (_deferred_grow_zone(zone, order)) 63\tgoto try_this_zone; 64\t} 65#endif 66\t/* Checked here to keep the fast path fast */ 67\tBUILD_BUG_ON(ALLOC_NO_WATERMARKS \u0026lt; NR_WMARK); 68\tif (alloc_flags \u0026amp; ALLOC_NO_WATERMARKS) 69\tgoto try_this_zone; 70 71\tif (node_reclaim_mode == 0 || 72\t!zone_allows_reclaim(ac-\u0026gt;preferred_zoneref-\u0026gt;zone, zone)) 73\tcontinue; 74 75\tret = node_reclaim(zone-\u0026gt;zone_pgdat, gfp_mask, order); 76\tswitch (ret) { 77\tcase NODE_RECLAIM_NOSCAN: 78\t/* did not scan */ 79\tcontinue; 80\tcase NODE_RECLAIM_FULL: 81\t/* scanned but unreclaimable */ 82\tcontinue; 83\tdefault: 84\t/* did we reclaim enough */ 85\tif (zone_watermark_ok(zone, order, mark, 86\tac_classzone_idx(ac), alloc_flags)) 87\tgoto try_this_zone; 88 89\tcontinue; 90\t} 91\t} 92 93try_this_zone: 94\tpage = rmqueue(ac-\u0026gt;preferred_zoneref-\u0026gt;zone, zone, order, 95\tgfp_mask, alloc_flags, ac-\u0026gt;migratetype); 96\tif (page) { 97\tprep_new_page(page, order, gfp_mask, alloc_flags); 98 99\t/* 100* If this is a high-order atomic allocation then check 101* if the pageblock should be reserved for the future 102*/ 103\tif (unlikely(order \u0026amp;\u0026amp; (alloc_flags \u0026amp; ALLOC_HARDER))) 104\treserve_highatomic_pageblock(page, zone, order); 105 106\treturn page; 107\t} else { 108#ifdef CONFIG_DEFERRED_STRUCT_PAGE_INIT 109\t/* Try again if zone has deferred pages */ 110\tif (static_branch_unlikely(\u0026amp;deferred_pages)) { 111\tif (_deferred_grow_zone(zone, order)) 112\tgoto try_this_zone; 113\t} 114#endif 115\t} 116\t} 117 118\treturn NULL; 119} ","date":"May 9, 2021","img":"","permalink":"https://mengdemao.github.io/posts/page_allocator/","series":null,"tags":null,"title":"Page_allocator"},{"categories":null,"content":"等待事件是建立在调度的基础之上的一种同步机制\n使用 等待队列头 1struct __wait_queue_head { 2\twq_lock_t lock; 3\tstruct list_head task_list; 4}; 5typedef struct __wait_queue_head wait_queue_head_t; 等待队列实体 1struct __wait_queue { 2\tunsigned int flags; 3\tstruct task_struct * task; 4\tstruct list_head task_list; 5}; 6typedef struct __wait_queue wait_queue_t; 初始化等待队列头 1void __init_waitqueue_head(struct wait_queue_head *wq_head, 2\tconst char *name, struct lock_class_key *); 3void init_waitqueue_head(struct wait_queue_head *wq_head); 初始化等待队列 1#define __WAITQUEUE_INITIALIZER(name, tsk) \\ 2{\t\\ 3.private\t= tsk,\t\\ 4.func\t= default_wake_function,\t\\ 5.entry\t= { NULL, NULL }\t\\ 6} 7 8#define DECLARE_WAITQUEUE(name, tsk) struct wait_queue_entry name = __WAITQUEUE_INITIALIZER(name, tsk) 9 10// 但是，一般直接 11DECLARE_WAITQUEUE(wait, current);  等待队列入口 等待的任务  等待队列操作 1void add_wait_queue(struct wait_queue_head *wq_head, 2\tstruct wait_queue_entry *wq_entry); 3void remove_wait_queue(struct wait_queue_head *wq_head, 4\tstruct wait_queue_entry *wq_entry);  等待队列头 等待队列实体  等待事件 1void wait_event(wq, condition); 2void wait_event_interruptible(wq, condition); 唤醒队列  wake_up wake_up_all wake_up_interruptible wake_up_interruptible_all wake_up_sync wake_up_interruptible_sync  例子 写端 1ssize_t wait_write(struct file *file, const char __user *data, size_t len, loff_t *ppos) 2{ 3\tDECLARE_WAITQUEUE(wait, current);\t/* 声明等待队列 */ 4\tint ret = -1; 5\tPTRACE; 6 7\tmutex_lock(\u0026amp;wait_device.mutex); 8\t/* 非阻塞模式直接写入 */ 9\tif (file-\u0026gt;f_flags \u0026amp; O_NONBLOCK) { 10\tpr_err(\u0026#34;write in O_NONBLOCK Mode\u0026#34;); 11\tgoto pure_write; 12\t} 13 14\tadd_wait_queue(\u0026amp;wait_device.wait_w, \u0026amp;wait); 15\twhile (wait_device.wait_flag == true) { 16\tpr_err(\u0026#34;Write INTERRUPTIBLE\u0026#34;); 17\t__set_current_state(TASK_INTERRUPTIBLE); 18\tmutex_unlock(\u0026amp;wait_device.mutex); 19\tschedule(); 20\tif (signal_pending(current)) { 21\tret = -ERESTARTSYS; 22\tremove_wait_queue(\u0026amp;wait_device.wait_w, \u0026amp;wait); 23\t__set_current_state(TASK_RUNNING); 24\tgoto out; 25\t} 26\t} 27\tremove_wait_queue(\u0026amp;wait_device.wait_w, \u0026amp;wait); 28 29pure_write: 30\twait_device.wait_flag = true; 31\tpr_err(\u0026#34;Write Successful\u0026#34;); 32 33\twake_up_interruptible(\u0026amp;wait_device.wait_r); 34\tpr_err(\u0026#34;Wakeup Read\u0026#34;); 35\tgoto out; 36 37out: 38\tmutex_unlock(\u0026amp;wait_device.mutex); 39\treturn ret; 40} 读端 1 ssize_t wait_read(struct file *file, char __user *buf, size_t len, loff_t * ppos) 2{ 3\tDECLARE_WAITQUEUE(wait, current);\t/* 声明等待队列 */ 4\tint ret = 0; 5\tPTRACE; 6 7\tmutex_lock(\u0026amp;wait_device.mutex); 8\t/* 非阻塞模式直接写入 */ 9\tif (file-\u0026gt;f_flags \u0026amp; O_NONBLOCK) { 10\tpr_err(\u0026#34;write in O_NONBLOCK Mode\u0026#34;); 11\tgoto pure_read; 12\t} 13 14\tadd_wait_queue(\u0026amp;wait_device.wait_r, \u0026amp;wait); 15\twhile (wait_device.wait_flag == false) { 16\tpr_err(\u0026#34;Write INTERRUPTIBLE\u0026#34;); 17\t__set_current_state(TASK_INTERRUPTIBLE); 18\tmutex_unlock(\u0026amp;wait_device.mutex); 19\tschedule(); 20\tif (signal_pending(current)) { 21\tret = -ERESTARTSYS; 22\tremove_wait_queue(\u0026amp;wait_device.wait_r, \u0026amp;wait); 23\t__set_current_state(TASK_RUNNING); 24\tgoto out; 25\t} 26\t} 27\tremove_wait_queue(\u0026amp;wait_device.wait_r, \u0026amp;wait); 28 29pure_read: 30\twait_device.wait_flag = false; 31\tpr_err(\u0026#34;Read Successful\u0026#34;); 32 33\twake_up_interruptible(\u0026amp;wait_device.wait_w); 34\tpr_err(\u0026#34;Wakeup Write\u0026#34;); 35 36\tgoto out; 37 38out: 39\tmutex_unlock(\u0026amp;wait_device.mutex); 40\treturn 0; 41} 原理 ","date":"May 4, 2021","img":"","permalink":"https://mengdemao.github.io/posts/wait_queue/","series":null,"tags":null,"title":"Linux等待队列实现"},{"categories":null,"content":"简介  ANTLR是一款强大的语法分析器生成工具,用于读取、处理、执行和翻译结构化的文本或二进制文件.\n 类似于flex/bison,根据描述文件，自动生成词法语法分析器\n安装说明  下载antlr  设置path和classpath 编写相关脚本  语法设计 错误处理 解析器 测试程序 antlr4编译器 1#!/bin/sh 2antlr4 Expr.g4 编译生成的java文件 1javac *.java 运行编译的结果 1grun Expr prog -tree 1(prog (stat (expr (expr (expr 1) + (expr 2)) + (expr 3)) \\r\\n)) 1grun Expr prog -gui\t1grun Expr prog -tokens 1[@0,0:0=\u0026#39;1\u0026#39;,\u0026lt;INT\u0026gt;,1:0] 2[@1,1:1=\u0026#39;+\u0026#39;,\u0026lt;\u0026#39;+\u0026#39;\u0026gt;,1:1] 3[@2,2:2=\u0026#39;2\u0026#39;,\u0026lt;INT\u0026gt;,1:2] 4[@3,3:3=\u0026#39;+\u0026#39;,\u0026lt;\u0026#39;+\u0026#39;\u0026gt;,1:3] 5[@4,4:4=\u0026#39;3\u0026#39;,\u0026lt;INT\u0026gt;,1:4] 6[@5,5:6=\u0026#39;\\r\\n\u0026#39;,\u0026lt;NEWLINE\u0026gt;,1:5] 7[@6,7:6=\u0026#39;\u0026lt;EOF\u0026gt;\u0026#39;,\u0026lt;EOF\u0026gt;,2:0] antlr语法详解 Hello 1// antlr4 Hello.g42// javac *.java3// grun Hello r -gui4grammarHello;// 定义一个Hello的grammer5r:\u0026#39;hello\u0026#39;ID;// 开头是hello后面接着一个ID6ID:[a-z]+;// ID由小写字母组成7WS:[\\t\\r\\n]+-\u0026gt;skip;// 控制符清除ArrayInit 1// antlr4 ArrayInit.g42// javac *.java3// grun ArrayInit init -gui4grammarArrayInit;// 定义一个ArrayInit的grammer5init:\u0026#39;{\u0026#39;value(\u0026#39;,\u0026#39;value)*\u0026#39;}\u0026#39;;// 6value:init// 嵌套定义7|INT// 定义整数8;9INT:[0-9]+;10WS:[\\t\\r\\n]+-\u0026gt;skip;()* \u0026ndash;\u0026gt; 相当于扩展\nExpr 1// antlr4 Expr.g42// javac *.java3// grun Expr prog -gui4grammarExpr;56prog:stat+;78stat:exprNEWLINE#printExpr9|ID\u0026#39;=\u0026#39;exprNEWLINE#assign10|NEWLINE#blank11;1213expr:exprop=(\u0026#39;*\u0026#39;|\u0026#39;/\u0026#39;)expr#MulDiv14|exprop=(\u0026#39;+\u0026#39;|\u0026#39;-\u0026#39;)expr#AddSub15|INT#int16|ID#id17|\u0026#39;(\u0026#39;expr\u0026#39;)\u0026#39;#parens18;1920MUL:\u0026#39;*\u0026#39;;// assigns token name to \u0026#39;*\u0026#39; used above in grammar21DIV:\u0026#39;/\u0026#39;;22ADD:\u0026#39;+\u0026#39;;23SUB:\u0026#39;-\u0026#39;;24ID:[a-zA-Z]+;25INT:[0-9]+;26NEWLINE:\u0026#39;\\r\u0026#39;?\u0026#39;\\n\u0026#39;;27WS:[\\t]+-\u0026gt;skip;json  在词法规则中那些不会被语法规则直接调用的词法规则可以用一个fragment关键字来标识， fragment标识的规则只能为其它词法规则提供基础\n 1grammar JSON;\t// 声明一个grammar 2 3json 4 : value\t// 一个value候选 5 ; 6 7obj\t// 对象类型 8 : \u0026#39;{\u0026#39; pair (\u0026#39;,\u0026#39; pair)* \u0026#39;}\u0026#39; 9 | \u0026#39;{\u0026#39; \u0026#39;}\u0026#39; 10 ; 11 12pair 13 : STRING \u0026#39;:\u0026#39; value 14 ; 15 16arr 17 : \u0026#39;[\u0026#39; value (\u0026#39;,\u0026#39; value)* \u0026#39;]\u0026#39; 18 | \u0026#39;[\u0026#39; \u0026#39;]\u0026#39; 19 ; 20 21value 22 : STRING 23 | NUMBER 24 | obj 25 | arr 26 | \u0026#39;true\u0026#39; 27 | \u0026#39;false\u0026#39; 28 | \u0026#39;null\u0026#39; 29 ; 30 31 32STRING 33 : \u0026#39;\u0026#34;\u0026#39; (ESC | SAFECODEPOINT)* \u0026#39;\u0026#34;\u0026#39; 34 ; 35 36 37fragment ESC 38 : \u0026#39;\\\\\u0026#39; ([\u0026#34;\\\\/bfnrt] | UNICODE) 39 ; 40 41 42fragment UNICODE 43 : \u0026#39;u\u0026#39; HEX HEX HEX HEX 44 ; 45 46 47fragment HEX 48 : [0-9a-fA-F] 49 ; 50 51 52fragment SAFECODEPOINT 53 : ~ [\u0026#34;\\\\\\u0000-\\u001F] 54 ; 55 56 57NUMBER 58 : \u0026#39;-\u0026#39;? INT (\u0026#39;.\u0026#39; [0-9] +)? EXP? 59 ; 60 61 62fragment INT 63 : \u0026#39;0\u0026#39; | [1-9] [0-9]* 64 ; 65 66// no leading zeros 67 68fragment EXP 69 : [Ee] [+\\-]? INT 70 ; 71 72// \\- since - means \u0026#34;range\u0026#34; inside [...] 73 74WS 75 : [ \\t\\n\\r] + -\u0026gt; skip 76 ; 测试例子\n1{ 2 \u0026#34;glossary\u0026#34;: { 3 \u0026#34;title\u0026#34;: \u0026#34;example glossary\u0026#34;, 4\t\u0026#34;GlossDiv\u0026#34;: { 5 \u0026#34;title\u0026#34;: \u0026#34;S\u0026#34;, 6\t\u0026#34;GlossList\u0026#34;: { 7 \u0026#34;GlossEntry\u0026#34;: { 8 \u0026#34;ID\u0026#34;: \u0026#34;SGML\u0026#34;, 9\t\u0026#34;SortAs\u0026#34;: \u0026#34;SGML\u0026#34;, 10\t\u0026#34;GlossTerm\u0026#34;: \u0026#34;Standard Generalized Markup Language\u0026#34;, 11\t\u0026#34;Acronym\u0026#34;: \u0026#34;SGML\u0026#34;, 12\t\u0026#34;Abbrev\u0026#34;: \u0026#34;ISO 8879:1986\u0026#34;, 13\t\u0026#34;GlossDef\u0026#34;: { 14 \u0026#34;para\u0026#34;: \u0026#34;A meta-markup language\u0026#34;, 15\t\u0026#34;GlossSeeAlso\u0026#34;: [\u0026#34;GML\u0026#34;, \u0026#34;XML\u0026#34;] 16 }, 17\t\u0026#34;GlossSee\u0026#34;: \u0026#34;markup\u0026#34; 18 } 19 } 20 } 21 } 22} 显示结果：\nXML  孤岛语法:\n dot 1grammarDOT;23graph4:STRICT?(GRAPH|DIGRAPH)id_?\u0026#39;{\u0026#39;stmt_list\u0026#39;}\u0026#39;5;67stmt_list8:(stmt\u0026#39;;\u0026#39;?)*9;1011stmt12:node_stmt|edge_stmt|attr_stmt|id_\u0026#39;=\u0026#39;id_|subgraph13;1415attr_stmt16:(GRAPH|NODE|EDGE)attr_list17;1819attr_list20:(\u0026#39;[\u0026#39;a_list?\u0026#39;]\u0026#39;)+21;2223a_list24:(id_(\u0026#39;=\u0026#39;id_)?\u0026#39;,\u0026#39;?)+25;2627edge_stmt28:(node_id|subgraph)edgeRHSattr_list?29;3031edgeRHS32:(edgeop(node_id|subgraph))+33;3435edgeop36:\u0026#39;-\u0026gt;\u0026#39;|\u0026#39;--\u0026#39;37;3839node_stmt40:node_idattr_list?41;4243node_id44:id_port?45;4647port48:\u0026#39;:\u0026#39;id_(\u0026#39;:\u0026#39;id_)?49;5051subgraph52:(SUBGRAPHid_?)?\u0026#39;{\u0026#39;stmt_list\u0026#39;}\u0026#39;53;5455id_56:ID|STRING|HTML_STRING|NUMBER57;5859// \u0026#34;The keywords node, edge, graph, digraph, subgraph, and strict are60// case-independent\u0026#34;6162STRICT63:[Ss][Tt][Rr][Ii][Cc][Tt]64;656667GRAPH68:[Gg][Rr][Aa][Pp][Hh]69;707172DIGRAPH73:[Dd][Ii][Gg][Rr][Aa][Pp][Hh]74;757677NODE78:[Nn][Oo][Dd][Ee]79;808182EDGE83:[Ee][Dd][Gg][Ee]84;858687SUBGRAPH88:[Ss][Uu][Bb][Gg][Rr][Aa][Pp][Hh]89;909192/** \u0026#34;a numeral [-]?(.[0-9]+ | [0-9]+(.[0-9]*)? )\u0026#34; */NUMBER93:\u0026#39;-\u0026#39;?(\u0026#39;.\u0026#39;DIGIT+|DIGIT+(\u0026#39;.\u0026#39;DIGIT*)?)94;959697fragmentDIGIT98:[0-9]99;100101102/** \u0026#34;any double-quoted string (\u0026#34;...\u0026#34;) possibly containing escaped quotes\u0026#34; */STRING103:\u0026#39;\u0026#34;\u0026#39;(\u0026#39;\\\\\u0026#34;\u0026#39;|.)*?\u0026#39;\u0026#34;\u0026#39;104;105106107/** \u0026#34;Any string of alphabetic ([a-zA-Z\\200-\\377]) characters, underscores 108* (\u0026#39;_\u0026#39;) or digits ([0-9]), not beginning with a digit\u0026#34; 109*/ID110:LETTER(LETTER|DIGIT)*111;112113114fragmentLETTER115:[a-zA-Z\\u0080-\\u00FF_]116;117118119/** \u0026#34;HTML strings, angle brackets must occur in matched pairs, and 120* unescaped newlines are allowed.\u0026#34; 121*/HTML_STRING122:\u0026#39;\u0026lt;\u0026#39;(TAG|~[\u0026lt;\u0026gt;])*\u0026#39;\u0026gt;\u0026#39;123;124125126fragmentTAG127:\u0026#39;\u0026lt;\u0026#39;.*?\u0026#39;\u0026gt;\u0026#39;128;129130131COMMENT132:\u0026#39;/*\u0026#39;.*?\u0026#39;*/\u0026#39;-\u0026gt;skip133;134135136LINE_COMMENT137:\u0026#39;//\u0026#39;.*?\u0026#39;\\r\u0026#39;?\u0026#39;\\n\u0026#39;-\u0026gt;skip138;139140141/** \u0026#34;a \u0026#39;#\u0026#39; character is considered a line output from a C preprocessor (e.g., 142* # 34 to indicate line 34 ) and discarded\u0026#34; 143*/PREPROC144:\u0026#39;#\u0026#39;~[\\r\\n]*-\u0026gt;skip145;146147148WS149:[\\t\\n\\r]+-\u0026gt;skip150;","date":"May 4, 2021","img":"","permalink":"https://mengdemao.github.io/posts/antlr/","series":null,"tags":["编译原理"],"title":"Antlr教程"},{"categories":null,"content":"基本操作 安装hugo 在linux/windows上只能通过直接release 下载,\n创建网站 1hugo new site 路径 添加主题  将主题直接添加到theme文件下面 将主题作为一个submodule  创建文档 1hugo new posts/hugo.md 设置预览 1 hugo server -D --disableFastRender 文件结构 1. 2├── archetypes 3├── config 4├── content 5├── data 6├── layouts 7├── static 8├── themes 9├── static 10└── resources 目录结构说明 以下是每个目录的高级概述，其中包含指向 Hugo 文档中每个相应部分的链接。\narchetypes hugo模板,在创建文件时作为模板自动生成\nassets 存储所有需要HugoPipes处理的文件;只有使用了.Permalink 或 .RelPermalink的文件才会发布到公共目录. 注意：默认情况下不创建该目录\nconfig Hugo配置目录\ncontent 此目录存在所有的网站内容,Hugo中的每个顶级文件夹都被视为一个内容部分.\ndata 该目录用于存储 Hugo 在生成网站时可以使用的配置文件\nlayouts 以 .html文件的形式存储模板.\nstatic 存储所有静态内容:图像、CSS、JavaScript等。当Hugo构建您的站点时,静态目录中的所有资产都按原样复制\n编写工具 typora 使用typora作为markdown编写工具\npicgo ","date":"May 4, 2021","img":"","permalink":"https://mengdemao.github.io/posts/hugo/","series":null,"tags":["技巧"],"title":"Hugo教程"},{"categories":null,"content":"nfs服务 安装 1sudo apt-get install nfs-kernel-server 设置导出 1/home/exports *(rw,nohide,insecure,no_subtree_check,async,no_root_squash) 开启服务 1sudo /etc/init.d/nfs-kernel-server restart 测试 1sudo mount -t nfs -o nolock,vers=3 127.0.0.1:/home/exports /mnt 2ls /mnt ","date":"May 3, 2021","img":"","permalink":"https://mengdemao.github.io/posts/nfs/","series":null,"tags":["nfs"],"title":"Nfs"}]