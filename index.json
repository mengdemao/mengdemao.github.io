[{"categories":[],"content":"bsdiff  bsdiff and bspatch are tools for building and applying patches to binary files. By using suffix \u0026gt; sorting (specifically, Larsson and Sadakane\u0026rsquo;s qsufsort) and taking advantage of how executable files change, bsdiff routinely produces binary patches 50-80% smaller than those produced by Xdelta, and 15% smaller than those produced by .RTPatch (a $2750/seat commercial patch tool) \u0026ndash; 直接摘抄自官网\n 详情可以查看bsdiff官网 ,存在这个详细描述.\n测试 新建立两个文件old.c与new.c,二者的差别是\n1// new.c 2#include \u0026lt;stdio.h\u0026gt;3int main(void) 4{ 5\tprintf(\u0026#34;Hello World\\r\\n\u0026#34;); 6\treturn 0; 7} 1#include \u0026lt;stdio.h\u0026gt;2int main(void) 3{ 4\treturn 0; 5} 测试的情况\n1# 执行编译 2gcc old.c -o old \u0026amp;\u0026amp; ./old 3gcc new.c -o new \u0026amp;\u0026amp; ./new 计算md5\n1md5sum old 2# d08fd167e74f279522fe8aa64d8e27dd old 3 4md5sum new 5# b0b4be993de61064a118d32a692bf795 new 6 7md5sum mid 8# b0b4be993de61064a118d32a692bf795 mid 生成补丁并且测试\n1# 生成diff 2bsdiff old new test.diff 3 4# 打入补丁--\u0026gt;mid 5bspatch old mid test.diff 分析 接口分析 1struct bsdiff_stream 2{ 3\tvoid* opaque;\t/* bzip文件 */ 4 5\tvoid* (*malloc)(size_t size);\t/* 内存申请接口 */ 6\tvoid (*free)(void* ptr);\t/* 内存释放接口 */ 7\tint (*write)(struct bsdiff_stream* stream, /* 写文件接口 */ 8 const void* buffer, int size); 9}; 10 11struct bspatch_stream 12{ 13\tvoid* opaque;\t/* bzip文件 */ 14\tint (*read)(const struct bspatch_stream* stream, /* 读取文件接口 */ 15 void* buffer, int length); 16}; 17 18int bsdiff(const uint8_t* old, int64_t oldsize, 19 const uint8_t* new, int64_t newsize, 20 struct bsdiff_stream* stream); 21 22int bspatch(const uint8_t* old, int64_t oldsize, 23 uint8_t* new, int64_t newsize, 24 struct bspatch_stream* stream); 25 diff算法核心 bsdiff更新数据由四部分组成:Header,ctrl block,diff block,extra block\n Header描述了文件基本信息 ctrl 包含了ADD和INSERT指令的控制文件  每一个ADD指令指定了旧文件中的偏移位置和长度，从旧文件中读取相应数量的字节内容并且从差异文件中读取相同字节的内容添加进去。 INSERT指令仅仅制定一个长度，用于从额外文件中读取指定数量的字节内容   diff 含了概率匹配中不同字节内容的差异文件 extra 包含了不属于概略匹配中内容的额外的文件  Header的结构:    start/bytes length/bytes content     0 8 \u0026ldquo;BSDIFF40\u0026rdquo;   8 8 the length of ctrl block   16 8 the length of diff block   24 8 新文件的大小     BSDIFF40 0x93 \u0026ndash;\u0026gt; 147 0x313 \u0026ndash;\u0026gt; 787 0x3ED0 \u0026ndash;\u0026gt; 16080 可以匹配新生成的文件  代码分析 偏移信息与字符串信息相互转化 1/* 一般情况下,buf的长度为8个字节 */ 2static int64_t offtin(uint8_t *buf) 3{ 4\tint64_t y; 5 6\ty=buf[7]\u0026amp;0x7F; /* 提取绝对值 */ 7 8\ty=y*256;y+=buf[6]; 9\ty=y*256;y+=buf[5]; 10\ty=y*256;y+=buf[4]; 11\ty=y*256;y+=buf[3]; 12\ty=y*256;y+=buf[2]; 13\ty=y*256;y+=buf[1]; 14\ty=y*256;y+=buf[0]; 15 16 /* 根据最高位置确定正负 */ 17\tif (buf[7] \u0026amp; 0x80) { 18 y=-y; 19 } 20 21\treturn y; 22} 23static void offtout(int64_t x,uint8_t *buf) 24{ 25\tint64_t y; 26 27 /* 保证 y = |x| */ 28\tif (x \u0026lt; 0) { 29 y = -x; 30 } 31 else { 32 y = x; 33 } 34 35\tbuf[0]=y%256;y-=buf[0]; 36\ty=y/256;buf[1]=y%256;y-=buf[1]; 37\ty=y/256;buf[2]=y%256;y-=buf[2]; 38\ty=y/256;buf[3]=y%256;y-=buf[3]; 39\ty=y/256;buf[4]=y%256;y-=buf[4]; 40\ty=y/256;buf[5]=y%256;y-=buf[5]; 41\ty=y/256;buf[6]=y%256;y-=buf[6]; 42\ty=y/256;buf[7]=y%256; 43 44\tif(x\u0026lt;0) buf[7]|=0x80; 45} patch代码分析 总体的执行路径 1int bspatch(const uint8_t* old, int64_t oldsize, 2 uint8_t* new, int64_t newsize, 3 struct bspatch_stream* stream) 4{ 5\tuint8_t buf[8]; 6\tint64_t oldpos; 7 int64_t newpos; 8\tint64_t ctrl[3]; 9\tint64_t i; 10 11\toldpos=0; 12 newpos=0; 13\twhile (newpos \u0026lt; newsize) { 14\t/* Read control data 3*8个为一组,每次生成3个控制数据 */ 15\tfor(i=0;i\u0026lt;=2;i++) { 16\tif (stream-\u0026gt;read(stream, buf, 8)) 17\treturn -1; 18\tctrl[i]=offtin(buf); 19\t}; 20 21\t/* 健壮性检查 */ 22\tif (ctrl[0]\u0026lt;0 || ctrl[0]\u0026gt;INT_MAX || 23\tctrl[1]\u0026lt;0 || ctrl[1]\u0026gt;INT_MAX || 24\tnewpos+ctrl[0]\u0026gt;newsize) 25\treturn -1; 26 27\t/* Read diff string */ 28\tif (stream-\u0026gt;read(stream, new + newpos, ctrl[0])) 29\treturn -1; 30 31\t/* Add old data to diff string */ 32\tfor(i=0;i\u0026lt;ctrl[0];i++) 33\tif((oldpos+i\u0026gt;=0) \u0026amp;\u0026amp; (oldpos+i\u0026lt;oldsize)) 34\tnew[newpos+i]+=old[oldpos+i]; 35 36\t/* Adjust pointers */ 37\tnewpos+=ctrl[0]; 38\toldpos+=ctrl[0]; 39 40\t/* Sanity-check */ 41\tif(newpos+ctrl[1]\u0026gt;newsize) 42\treturn -1; 43 44\t/* Read extra string */ 45\tif (stream-\u0026gt;read(stream, new + newpos, ctrl[1])) 46\treturn -1; 47 48\t/* Adjust pointers */ 49\tnewpos+=ctrl[1]; 50\toldpos+=ctrl[2]; 51\t}; 52 53\treturn 0; 54} 主要函数的调用路线\nqsufsort  调用qsufsort该函数生成后缀数组,但是后缀数组是什么? 我现在还没有理解\n 参数分析:\n I 后缀数组 V 辅助信息 old 原始文件 oldsiz原始文件大小  1static void qsufsort(int64_t *I,int64_t *V,const uint8_t *old,int64_t oldsize) 2{ 3\tint64_t buckets[256]; 4\tint64_t i,h,len; 5 6\tfor(i=0;i\u0026lt;256;i++) buckets[i]=0; 7\tfor(i=0;i\u0026lt;oldsize;i++) buckets[old[i]]++; 8\tfor(i=1;i\u0026lt;256;i++) buckets[i]+=buckets[i-1]; 9\tfor(i=255;i\u0026gt;0;i--) buckets[i]=buckets[i-1]; 10\tbuckets[0]=0; 11 12\tfor(i=0;i\u0026lt;oldsize;i++) I[++buckets[old[i]]]=i; 13\tI[0]=oldsize; 14\tfor(i=0;i\u0026lt;oldsize;i++) V[i]=buckets[old[i]]; 15\tV[oldsize]=0; 16\tfor(i=1;i\u0026lt;256;i++) if(buckets[i]==buckets[i-1]+1) I[buckets[i]]=-1; 17\tI[0]=-1; 18 19\tfor(h=1;I[0]!=-(oldsize+1);h+=h) { 20\tlen=0; 21\tfor(i=0;i\u0026lt;oldsize+1;) { 22\tif(I[i]\u0026lt;0) { 23\tlen-=I[i]; 24\ti-=I[i]; 25\t} else { 26\tif(len) I[i-len]=-len; 27\tlen=V[I[i]]+1-i; 28\tsplit(I,V,i,len,h); 29\ti+=len; 30\tlen=0; 31\t}; 32\t}; 33\tif(len) I[i-len]=-len; 34\t}; 35 36\tfor(i=0;i\u0026lt;oldsize+1;i++) I[V[i]]=i; 37} 38 39static void split(int64_t *I,int64_t *V,int64_t start,int64_t len,int64_t h) 40{ 41\tint64_t i,j,k,x,tmp,jj,kk; 42 43\tif(len\u0026lt;16) { 44\tfor(k=start;k\u0026lt;start+len;k+=j) { 45\tj=1;x=V[I[k]+h]; 46\tfor(i=1;k+i\u0026lt;start+len;i++) { 47\tif(V[I[k+i]+h]\u0026lt;x) { 48\tx=V[I[k+i]+h]; 49\tj=0; 50\t}; 51\tif(V[I[k+i]+h]==x) { 52\ttmp=I[k+j];I[k+j]=I[k+i];I[k+i]=tmp; 53\tj++; 54\t}; 55\t}; 56\tfor(i=0;i\u0026lt;j;i++) V[I[k+i]]=k+j-1; 57\tif(j==1) I[k]=-1; 58\t}; 59\treturn; 60\t}; 61 62\tx=V[I[start+len/2]+h]; 63\tjj=0;kk=0; 64\tfor(i=start;i\u0026lt;start+len;i++) { 65\tif(V[I[i]+h]\u0026lt;x) jj++; 66\tif(V[I[i]+h]==x) kk++; 67\t}; 68\tjj+=start;kk+=jj; 69 70\ti=start;j=0;k=0; 71\twhile(i\u0026lt;jj) { 72\tif(V[I[i]+h]\u0026lt;x) { 73\ti++; 74\t} else if(V[I[i]+h]==x) { 75\ttmp=I[i];I[i]=I[jj+j];I[jj+j]=tmp; 76\tj++; 77\t} else { 78\ttmp=I[i];I[i]=I[kk+k];I[kk+k]=tmp; 79\tk++; 80\t}; 81\t}; 82 83\twhile(jj+j\u0026lt;kk) { 84\tif(V[I[jj+j]+h]==x) { 85\tj++; 86\t} else { 87\ttmp=I[jj+j];I[jj+j]=I[kk+k];I[kk+k]=tmp; 88\tk++; 89\t}; 90\t}; 91 92\tif(jj\u0026gt;start) split(I,V,start,jj-start,h); 93 94\tfor(i=0;i\u0026lt;kk-jj;i++) V[I[jj+i]]=kk-1; 95\tif(jj==kk-1) I[jj]=-1; 96 97\tif(start+len\u0026gt;kk) split(I,V,kk,start+len-kk,h); 98} search 1static int64_t matchlen(const uint8_t *old,int64_t oldsize,const uint8_t *new,int64_t newsize) 2{ 3\tint64_t i; 4 5\tfor(i=0;(i\u0026lt;oldsize)\u0026amp;\u0026amp;(i\u0026lt;newsize);i++) 6\tif(old[i]!=new[i]) break; 7 8\treturn i; 9} 10static int64_t search(const int64_t *I,const uint8_t *old,int64_t oldsize, 11\tconst uint8_t *new,int64_t newsize,int64_t st,int64_t en,int64_t *pos) 12{ 13\tint64_t x,y; 14 15\tif(en-st\u0026lt;2) { 16\tx=matchlen(old+I[st],oldsize-I[st],new,newsize); 17\ty=matchlen(old+I[en],oldsize-I[en],new,newsize); 18 19\tif(x\u0026gt;y) { 20\t*pos=I[st]; 21\treturn x; 22\t} else { 23\t*pos=I[en]; 24\treturn y; 25\t} 26\t}; 27 28\tx=st+(en-st)/2; 29\tif(memcmp(old+I[x],new,MIN(oldsize-I[x],newsize))\u0026lt;0) { 30\treturn search(I,old,oldsize,new,newsize,x,en,pos); 31\t} else { 32\treturn search(I,old,oldsize,new,newsize,st,x,pos); 33\t}; 34} writedata 1static int64_t writedata(struct bsdiff_stream* stream, 2\tconst void* buffer, int64_t length) 3{ 4\tint64_t result = 0; 5 6\twhile (length \u0026gt; 0) 7\t{ 8\tconst int smallsize = (int)MIN(length, INT_MAX); 9\tconst int writeresult = stream-\u0026gt;write(stream, buffer, smallsize); 10\tif (writeresult == -1) 11\t{ 12\treturn -1; 13\t} 14 15\tresult += writeresult; 16\tlength -= smallsize; 17\tbuffer = (uint8_t*)buffer + smallsize; 18\t} 19 20\treturn result; 21} diff文件生成核心代码 1struct bsdiff_request 2{ 3\tconst uint8_t* old; 4\tint64_t oldsize; 5\tconst uint8_t* new; 6\tint64_t newsize; 7\tstruct bsdiff_stream* stream; 8\tint64_t *I; 9\tuint8_t *buffer; 10}; 11 12static int bsdiff_internal(const struct bsdiff_request req) 13{ 14\tint64_t *I,*V; 15\tint64_t scan,pos,len; 16\tint64_t lastscan,lastpos,lastoffset; 17\tint64_t oldscore,scsc; 18\tint64_t s,Sf,lenf,Sb,lenb; 19\tint64_t overlap,Ss,lens; 20\tint64_t i; 21\tuint8_t *buffer; 22\tuint8_t buf[8 * 3]; 23 24\tif((V=req.stream-\u0026gt;malloc((req.oldsize+1)*sizeof(int64_t)))==NULL) return -1; 25\tI = req.I; 26 27\tqsufsort(I,V,req.old,req.oldsize); 28 29\treq.stream-\u0026gt;free(V); 30 31\tbuffer = req.buffer; 32 33\t/* Compute the differences, writing ctrl as we go */ 34\tscan=0;len=0;pos=0; 35\tlastscan=0;lastpos=0;lastoffset=0; 36\twhile(scan\u0026lt;req.newsize) { 37\toldscore=0; 38 39\tfor(scsc=scan+=len;scan\u0026lt;req.newsize;scan++) { 40\tlen=search(I,req.old,req.oldsize,req.new+scan,req.newsize-scan, 41\t0,req.oldsize,\u0026amp;pos); 42 43\tfor(;scsc\u0026lt;scan+len;scsc++) 44\tif((scsc+lastoffset\u0026lt;req.oldsize) \u0026amp;\u0026amp; 45\t(req.old[scsc+lastoffset] == req.new[scsc])) 46\toldscore++; 47 48\tif(((len==oldscore) \u0026amp;\u0026amp; (len!=0)) || 49\t(len\u0026gt;oldscore+8)) break; 50 51\tif((scan+lastoffset\u0026lt;req.oldsize) \u0026amp;\u0026amp; 52\t(req.old[scan+lastoffset] == req.new[scan])) 53\toldscore--; 54\t}; 55 56\tif((len!=oldscore) || (scan==req.newsize)) { 57\ts=0;Sf=0;lenf=0; 58\tfor(i=0;(lastscan+i\u0026lt;scan)\u0026amp;\u0026amp;(lastpos+i\u0026lt;req.oldsize);) { 59\tif(req.old[lastpos+i]==req.new[lastscan+i]) s++; 60\ti++; 61\tif(s*2-i\u0026gt;Sf*2-lenf) { Sf=s; lenf=i; }; 62\t}; 63 64\tlenb=0; 65\tif(scan\u0026lt;req.newsize) { 66\ts=0;Sb=0; 67\tfor(i=1;(scan\u0026gt;=lastscan+i)\u0026amp;\u0026amp;(pos\u0026gt;=i);i++) { 68\tif(req.old[pos-i]==req.new[scan-i]) s++; 69\tif(s*2-i\u0026gt;Sb*2-lenb) { Sb=s; lenb=i; }; 70\t}; 71\t}; 72 73\tif(lastscan+lenf\u0026gt;scan-lenb) { 74\toverlap=(lastscan+lenf)-(scan-lenb); 75\ts=0;Ss=0;lens=0; 76\tfor(i=0;i\u0026lt;overlap;i++) { 77\tif(req.new[lastscan+lenf-overlap+i]== 78\treq.old[lastpos+lenf-overlap+i]) s++; 79\tif(req.new[scan-lenb+i]== 80\treq.old[pos-lenb+i]) s--; 81\tif(s\u0026gt;Ss) { Ss=s; lens=i+1; }; 82\t}; 83 84\tlenf+=lens-overlap; 85\tlenb-=lens; 86\t}; 87 88\tofftout(lenf,buf); 89\tofftout((scan-lenb)-(lastscan+lenf),buf+8); 90\tofftout((pos-lenb)-(lastpos+lenf),buf+16); 91 92\t/* Write control data */ 93\tif (writedata(req.stream, buf, sizeof(buf))) 94\treturn -1; 95 96\t/* Write diff data */ 97\tfor(i=0;i\u0026lt;lenf;i++) 98\tbuffer[i]=req.new[lastscan+i]-req.old[lastpos+i]; 99\tif (writedata(req.stream, buffer, lenf)) 100\treturn -1; 101 102\t/* Write extra data */ 103\tfor(i=0;i\u0026lt;(scan-lenb)-(lastscan+lenf);i++) 104\tbuffer[i]=req.new[lastscan+lenf+i]; 105\tif (writedata(req.stream, buffer, (scan-lenb)-(lastscan+lenf))) 106\treturn -1; 107 108\tlastscan=scan-lenb; 109\tlastpos=pos-lenb; 110\tlastoffset=pos-scan; 111\t}; 112\t}; 113 114\treturn 0; 115} ","date":"Oct 30, 2021","img":"","permalink":"https://mengdemao.github.io/posts/bsdiff/","series":null,"tags":[],"title":"Bsdiff"},{"categories":[],"content":"ARM笔记 ARM体系结构 相关术语  流水线 DSP Jazelle ThumbEE Thumb-2 TrustZone VFP NEON LAPE big.LITTLE  工具链    文件名 详解     addr2line 把程序地址转化为文件名和行号   ar 建立、修改和提取归档文件   as 汇编编译器   ld 链接器   nm 列出文件的符号   objcopy 文件个数格式转换   objdump 反汇编   ranlib 产生索引,并且保存进入文件中   readelf 显示elf文件信息   size 列出文件大小   string 打印文件可打印字符串   strip 丢弃文件符号    交叉工具链测试\n1arm-none-linux-gnueabihf-addr2line arm-none-linux-gnueabihf-gdb 2arm-none-linux-gnueabihf-ar arm-none-linux-gnueabihf-gdb-add-index 3arm-none-linux-gnueabihf-as arm-none-linux-gnueabihf-gfortran 4arm-none-linux-gnueabihf-c++ arm-none-linux-gnueabihf-gprof 5arm-none-linux-gnueabihf-c++filt arm-none-linux-gnueabihf-ld 6arm-none-linux-gnueabihf-cpp arm-none-linux-gnueabihf-ld.bfd 7arm-none-linux-gnueabihf-dwp arm-none-linux-gnueabihf-ld.gold 8arm-none-linux-gnueabihf-elfedit arm-none-linux-gnueabihf-lto-dump 9arm-none-linux-gnueabihf-g++ arm-none-linux-gnueabihf-nm 10arm-none-linux-gnueabihf-gcc arm-none-linux-gnueabihf-objcopy 11arm-none-linux-gnueabihf-gcc-10.2.1 arm-none-linux-gnueabihf-objdump 12arm-none-linux-gnueabihf-gcc-ar arm-none-linux-gnueabihf-ranlib 13arm-none-linux-gnueabihf-gcc-nm arm-none-linux-gnueabihf-readelf 14arm-none-linux-gnueabihf-gcc-ranlib arm-none-linux-gnueabihf-size 15arm-none-linux-gnueabihf-gcov arm-none-linux-gnueabihf-strings 16arm-none-linux-gnueabihf-gcov-dump arm-none-linux-gnueabihf-strip 17arm-none-linux-gnueabihf-gcov-tool ARMv7处理器模式    模式 编码 功能 安全 优先级     User (USR) 10000 大多数运行的非特权模式 Both PL0   FIQ 10001 FIQ中断 Both PL1   IRQ 10010 IRQ中断 Both PL1   Supervisor (SVC) 10011 设备重启或者SVC指令 Both PL1   Monitor (MON) 10110 安全扩展实现 only PL1   Abort (ABT) 10111 内存权限异常 Both PL1   Hyp (HYP) 11010 虚拟化扩展实现. Non-secure PL2   Undef (UND) 11011 未定义指令调用 Both PL1   System (SYS) 11111 特权模式,与用户模式共享寄存器 Both PL1    不同的处理器模式上寄存器共享的情况 ARM指令集 ","date":"Oct 30, 2021","img":"","permalink":"https://mengdemao.github.io/posts/arm/","series":null,"tags":[],"title":"Arm"},{"categories":["linux"],"content":"IDLE调度器类 1/* 2* Generic entry points for the idle threads and 3* implementation of the idle task scheduling class. 4* 5* (NOTE: these are not related to SCHED_IDLE batch scheduled 6* tasks which are handled in sched/fair.c ) 7*/ 8#include \u0026#34;sched.h\u0026#34;9 10#include \u0026lt;trace/events/power.h\u0026gt;11 12/* Linker adds these: start and end of __cpuidle functions */ 13extern char __cpuidle_text_start[], __cpuidle_text_end[]; 14 15/** 16* sched_idle_set_state - Record idle state for the current CPU. 17* @idle_state: State to record. 18*/ 19void sched_idle_set_state(struct cpuidle_state *idle_state) 20{ 21\tidle_set_state(this_rq(), idle_state); 22} 23 24static int __read_mostly cpu_idle_force_poll; 25 26void cpu_idle_poll_ctrl(bool enable) 27{ 28\tif (enable) { 29\tcpu_idle_force_poll++; 30\t} else { 31\tcpu_idle_force_poll--; 32\tWARN_ON_ONCE(cpu_idle_force_poll \u0026lt; 0); 33\t} 34} 35 36#ifdef CONFIG_GENERIC_IDLE_POLL_SETUP 37static int __init cpu_idle_poll_setup(char *__unused) 38{ 39\tcpu_idle_force_poll = 1; 40 41\treturn 1; 42} 43__setup(\u0026#34;nohlt\u0026#34;, cpu_idle_poll_setup); 44 45static int __init cpu_idle_nopoll_setup(char *__unused) 46{ 47\tcpu_idle_force_poll = 0; 48 49\treturn 1; 50} 51__setup(\u0026#34;hlt\u0026#34;, cpu_idle_nopoll_setup); 52#endif 53 54static noinline int __cpuidle cpu_idle_poll(void) 55{ 56\trcu_idle_enter(); 57\ttrace_cpu_idle_rcuidle(0, smp_processor_id()); 58\tlocal_irq_enable(); 59\tstop_critical_timings(); 60 61\twhile (!tif_need_resched() \u0026amp;\u0026amp; 62\t(cpu_idle_force_poll || tick_check_broadcast_expired())) 63\tcpu_relax(); 64\tstart_critical_timings(); 65\ttrace_cpu_idle_rcuidle(PWR_EVENT_EXIT, smp_processor_id()); 66\trcu_idle_exit(); 67 68\treturn 1; 69} 70 71/* Weak implementations for optional arch specific functions */ 72void __weak arch_cpu_idle_prepare(void) { } 73void __weak arch_cpu_idle_enter(void) { } 74void __weak arch_cpu_idle_exit(void) { } 75void __weak arch_cpu_idle_dead(void) { } 76void __weak arch_cpu_idle(void) 77{ 78\tcpu_idle_force_poll = 1; 79\tlocal_irq_enable(); 80} 81 82/** 83* default_idle_call - Default CPU idle routine. 84* 85* To use when the cpuidle framework cannot be used. 86*/ 87void __cpuidle default_idle_call(void) 88{ 89\tif (current_clr_polling_and_test()) { 90\tlocal_irq_enable(); 91\t} else { 92\tstop_critical_timings(); 93\tarch_cpu_idle(); 94\tstart_critical_timings(); 95\t} 96} 97 98static int call_cpuidle(struct cpuidle_driver *drv, struct cpuidle_device *dev, 99\tint next_state) 100{ 101\t/* 102* The idle task must be scheduled, it is pointless to go to idle, just 103* update no idle residency and return. 104*/ 105\tif (current_clr_polling_and_test()) { 106\tdev-\u0026gt;last_residency = 0; 107\tlocal_irq_enable(); 108\treturn -EBUSY; 109\t} 110 111\t/* 112* Enter the idle state previously returned by the governor decision. 113* This function will block until an interrupt occurs and will take 114* care of re-enabling the local interrupts 115*/ 116\treturn cpuidle_enter(drv, dev, next_state); 117} 118 119/** 120* cpuidle_idle_call - the main idle function 121* 122* NOTE: no locks or semaphores should be used here 123* 124* On archs that support TIF_POLLING_NRFLAG, is called with polling 125* set, and it returns with polling set. If it ever stops polling, it 126* must clear the polling bit. 127*/ 128static void cpuidle_idle_call(void) 129{ 130\tstruct cpuidle_device *dev = cpuidle_get_device(); 131\tstruct cpuidle_driver *drv = cpuidle_get_cpu_driver(dev); 132\tint next_state, entered_state; 133 134\t/* 135* Check if the idle task must be rescheduled. If it is the 136* case, exit the function after re-enabling the local irq. 137*/ 138\tif (need_resched()) { 139\tlocal_irq_enable(); 140\treturn; 141\t} 142 143\t/* 144* The RCU framework needs to be told that we are entering an idle 145* section, so no more rcu read side critical sections and one more 146* step to the grace period 147*/ 148 149\tif (cpuidle_not_available(drv, dev)) { 150\ttick_nohz_idle_stop_tick(); 151\trcu_idle_enter(); 152 153\tdefault_idle_call(); 154\tgoto exit_idle; 155\t} 156 157\t/* 158* Suspend-to-idle (\u0026#34;s2idle\u0026#34;) is a system state in which all user space 159* has been frozen, all I/O devices have been suspended and the only 160* activity happens here and in iterrupts (if any). In that case bypass 161* the cpuidle governor and go stratight for the deepest idle state 162* available. Possibly also suspend the local tick and the entire 163* timekeeping to prevent timer interrupts from kicking us out of idle 164* until a proper wakeup interrupt happens. 165*/ 166 167\tif (idle_should_enter_s2idle() || dev-\u0026gt;use_deepest_state) { 168\tif (idle_should_enter_s2idle()) { 169\trcu_idle_enter(); 170 171\tentered_state = cpuidle_enter_s2idle(drv, dev); 172\tif (entered_state \u0026gt; 0) { 173\tlocal_irq_enable(); 174\tgoto exit_idle; 175\t} 176 177\trcu_idle_exit(); 178\t} 179 180\ttick_nohz_idle_stop_tick(); 181\trcu_idle_enter(); 182 183\tnext_state = cpuidle_find_deepest_state(drv, dev); 184\tcall_cpuidle(drv, dev, next_state); 185\t} else { 186\tbool stop_tick = true; 187 188\t/* 189* Ask the cpuidle framework to choose a convenient idle state. 190*/ 191\tnext_state = cpuidle_select(drv, dev, \u0026amp;stop_tick); 192 193\tif (stop_tick || tick_nohz_tick_stopped()) 194\ttick_nohz_idle_stop_tick(); 195\telse 196\ttick_nohz_idle_retain_tick(); 197 198\trcu_idle_enter(); 199 200\tentered_state = call_cpuidle(drv, dev, next_state); 201\t/* 202* Give the governor an opportunity to reflect on the outcome 203*/ 204\tcpuidle_reflect(dev, entered_state); 205\t} 206 207exit_idle: 208\t__current_set_polling(); 209 210\t/* 211* It is up to the idle functions to reenable local interrupts 212*/ 213\tif (WARN_ON_ONCE(irqs_disabled())) 214\tlocal_irq_enable(); 215 216\trcu_idle_exit(); 217} 218 219/* 220* Generic idle loop implementation 221* 222* Called with polling cleared. 223*/ 224static void do_idle(void) 225{ 226\tint cpu = smp_processor_id(); 227\t/* 228* If the arch has a polling bit, we maintain an invariant: 229* 230* Our polling bit is clear if we\u0026#39;re not scheduled (i.e. if rq-\u0026gt;curr != 231* rq-\u0026gt;idle). This means that, if rq-\u0026gt;idle has the polling bit set, 232* then setting need_resched is guaranteed to cause the CPU to 233* reschedule. 234*/ 235 236\t__current_set_polling(); 237\ttick_nohz_idle_enter(); 238 239\twhile (!need_resched()) { 240\tcheck_pgt_cache(); 241\trmb(); 242 243\tlocal_irq_disable(); 244 245\tif (cpu_is_offline(cpu)) { 246\ttick_nohz_idle_stop_tick(); 247\tcpuhp_report_idle_dead(); 248\tarch_cpu_idle_dead(); 249\t} 250 251\tarch_cpu_idle_enter(); 252 253\t/* 254* In poll mode we reenable interrupts and spin. Also if we 255* detected in the wakeup from idle path that the tick 256* broadcast device expired for us, we don\u0026#39;t want to go deep 257* idle as we know that the IPI is going to arrive right away. 258*/ 259\tif (cpu_idle_force_poll || tick_check_broadcast_expired()) { 260\ttick_nohz_idle_restart_tick(); 261\tcpu_idle_poll(); 262\t} else { 263\tcpuidle_idle_call(); 264\t} 265\tarch_cpu_idle_exit(); 266\t} 267 268\t/* 269* Since we fell out of the loop above, we know TIF_NEED_RESCHED must 270* be set, propagate it into PREEMPT_NEED_RESCHED. 271* 272* This is required because for polling idle loops we will not have had 273* an IPI to fold the state for us. 274*/ 275\tpreempt_set_need_resched(); 276\ttick_nohz_idle_exit(); 277\t__current_clr_polling(); 278 279\t/* 280* We promise to call sched_ttwu_pending() and reschedule if 281* need_resched() is set while polling is set. That means that clearing 282* polling needs to be visible before doing these things. 283*/ 284\tsmp_mb__after_atomic(); 285 286\tsched_ttwu_pending(); 287\tschedule_idle(); 288 289\tif (unlikely(klp_patch_pending(current))) 290\tklp_update_patch_state(current); 291} 292 293bool cpu_in_idle(unsigned long pc) 294{ 295\treturn pc \u0026gt;= (unsigned long)__cpuidle_text_start \u0026amp;\u0026amp; 296\tpc \u0026lt; (unsigned long)__cpuidle_text_end; 297} 298 299struct idle_timer { 300\tstruct hrtimer timer; 301\tint done; 302}; 303 304static enum hrtimer_restart idle_inject_timer_fn(struct hrtimer *timer) 305{ 306\tstruct idle_timer *it = container_of(timer, struct idle_timer, timer); 307 308\tWRITE_ONCE(it-\u0026gt;done, 1); 309\tset_tsk_need_resched(current); 310 311\treturn HRTIMER_NORESTART; 312} 313 314void play_idle(unsigned long duration_ms) 315{ 316\tstruct idle_timer it; 317 318\t/* 319* Only FIFO tasks can disable the tick since they don\u0026#39;t need the forced 320* preemption. 321*/ 322\tWARN_ON_ONCE(current-\u0026gt;policy != SCHED_FIFO); 323\tWARN_ON_ONCE(current-\u0026gt;nr_cpus_allowed != 1); 324\tWARN_ON_ONCE(!(current-\u0026gt;flags \u0026amp; PF_KTHREAD)); 325\tWARN_ON_ONCE(!(current-\u0026gt;flags \u0026amp; PF_NO_SETAFFINITY)); 326\tWARN_ON_ONCE(!duration_ms); 327 328\trcu_sleep_check(); 329\tpreempt_disable(); 330\tcurrent-\u0026gt;flags |= PF_IDLE; 331\tcpuidle_use_deepest_state(true); 332 333\tit.done = 0; 334\thrtimer_init_on_stack(\u0026amp;it.timer, CLOCK_MONOTONIC, HRTIMER_MODE_REL); 335\tit.timer.function = idle_inject_timer_fn; 336\thrtimer_start(\u0026amp;it.timer, ms_to_ktime(duration_ms), HRTIMER_MODE_REL_PINNED); 337 338\twhile (!READ_ONCE(it.done)) 339\tdo_idle(); 340 341\tcpuidle_use_deepest_state(false); 342\tcurrent-\u0026gt;flags \u0026amp;= ~PF_IDLE; 343 344\tpreempt_fold_need_resched(); 345\tpreempt_enable(); 346} 347EXPORT_SYMBOL_GPL(play_idle); 348 349void cpu_startup_entry(enum cpuhp_state state) 350{ 351\t/* 352* This #ifdef needs to die, but it\u0026#39;s too late in the cycle to 353* make this generic (ARM and SH have never invoked the canary 354* init for the non boot CPUs!). Will be fixed in 3.11 355*/ 356#ifdef CONFIG_X86 357\t/* 358* If we\u0026#39;re the non-boot CPU, nothing set the stack canary up 359* for us. The boot CPU already has it initialized but no harm 360* in doing it again. This is a good place for updating it, as 361* we wont ever return from this function (so the invalid 362* canaries already on the stack wont ever trigger). 363*/ 364\tboot_init_stack_canary(); 365#endif 366\tarch_cpu_idle_prepare(); 367\tcpuhp_online_idle(state); 368\twhile (1) 369\tdo_idle(); 370} 371 372/* 373* idle-task scheduling class. 374*/ 375 376#ifdef CONFIG_SMP 377static int 378select_task_rq_idle(struct task_struct *p, int cpu, int sd_flag, int flags) 379{ 380\treturn task_cpu(p); /* IDLE tasks as never migrated */ 381} 382#endif 383 384/* 385* Idle tasks are unconditionally rescheduled: 386*/ 387static void check_preempt_curr_idle(struct rq *rq, struct task_struct *p, int flags) 388{ 389\tresched_curr(rq); 390} 391 392static struct task_struct * 393pick_next_task_idle(struct rq *rq, struct task_struct *prev, struct rq_flags *rf) 394{ 395\tput_prev_task(rq, prev); 396\tupdate_idle_core(rq); 397\tschedstat_inc(rq-\u0026gt;sched_goidle); 398 399\treturn rq-\u0026gt;idle; 400} 401 402/* 403* It is not legal to sleep in the idle task - print a warning 404* message if some code attempts to do it: 405*/ 406static void 407dequeue_task_idle(struct rq *rq, struct task_struct *p, int flags) 408{ 409\traw_spin_unlock_irq(\u0026amp;rq-\u0026gt;lock); 410\tprintk(KERN_ERR \u0026#34;bad: scheduling from the idle thread!\\n\u0026#34;); 411\tdump_stack(); 412\traw_spin_lock_irq(\u0026amp;rq-\u0026gt;lock); 413} 414 415static void put_prev_task_idle(struct rq *rq, struct task_struct *prev) 416{ 417} 418 419/* 420* scheduler tick hitting a task of our scheduling class. 421* 422* NOTE: This function can be called remotely by the tick offload that 423* goes along full dynticks. Therefore no local assumption can be made 424* and everything must be accessed through the @rq and @curr passed in 425* parameters. 426*/ 427static void task_tick_idle(struct rq *rq, struct task_struct *curr, int queued) 428{ 429} 430 431static void set_curr_task_idle(struct rq *rq) 432{ 433} 434 435static void switched_to_idle(struct rq *rq, struct task_struct *p) 436{ 437\tBUG(); 438} 439 440static void 441prio_changed_idle(struct rq *rq, struct task_struct *p, int oldprio) 442{ 443\tBUG(); 444} 445 446static unsigned int get_rr_interval_idle(struct rq *rq, struct task_struct *task) 447{ 448\treturn 0; 449} 450 451static void update_curr_idle(struct rq *rq) 452{ 453} 454 455/* 456* Simple, special scheduling class for the per-CPU idle tasks: 457*/ 458const struct sched_class idle_sched_class = { 459\t/* .next is NULL */ 460\t/* no enqueue/yield_task for idle tasks */ 461 462\t/* dequeue is not valid, we print a debug message there: */ 463\t.dequeue_task\t= dequeue_task_idle, 464 465\t.check_preempt_curr\t= check_preempt_curr_idle, 466 467\t.pick_next_task\t= pick_next_task_idle, 468\t.put_prev_task\t= put_prev_task_idle, 469 470#ifdef CONFIG_SMP 471\t.select_task_rq\t= select_task_rq_idle, 472\t.set_cpus_allowed\t= set_cpus_allowed_common, 473#endif 474 475\t.set_curr_task = set_curr_task_idle, 476\t.task_tick\t= task_tick_idle, 477 478\t.get_rr_interval\t= get_rr_interval_idle, 479 480\t.prio_changed\t= prio_changed_idle, 481\t.switched_to\t= switched_to_idle, 482\t.update_curr\t= update_curr_idle, 483}; ","date":"Oct 28, 2021","img":"","permalink":"https://mengdemao.github.io/posts/idle/","series":null,"tags":["kernel"],"title":"Idle"},{"categories":["linux"],"content":"完全公平调度器 1const struct sched_class fair_sched_class = { 2\t.next\t= \u0026amp;idle_sched_class, 3\t.enqueue_task\t= enqueue_task_fair, 4\t.dequeue_task\t= dequeue_task_fair, 5\t.yield_task\t= yield_task_fair, 6\t.yield_to_task\t= yield_to_task_fair, 7 8\t.check_preempt_curr\t= check_preempt_wakeup, 9 10\t.pick_next_task\t= pick_next_task_fair, 11\t.put_prev_task\t= put_prev_task_fair, 12 13#ifdef CONFIG_SMP 14\t.select_task_rq\t= select_task_rq_fair, 15\t.migrate_task_rq\t= migrate_task_rq_fair, 16 17\t.rq_online\t= rq_online_fair, 18\t.rq_offline\t= rq_offline_fair, 19 20\t.task_dead\t= task_dead_fair, 21\t.set_cpus_allowed\t= set_cpus_allowed_common, 22#endif 23 24\t.set_curr_task = set_curr_task_fair, 25\t.task_tick\t= task_tick_fair, 26\t.task_fork\t= task_fork_fair, 27 28\t.prio_changed\t= prio_changed_fair, 29\t.switched_from\t= switched_from_fair, 30\t.switched_to\t= switched_to_fair, 31 32\t.get_rr_interval\t= get_rr_interval_fair, 33 34\t.update_curr\t= update_curr_fair, 35 36#ifdef CONFIG_FAIR_GROUP_SCHED 37\t.task_change_group\t= task_change_group_fair, 38#endif 39}; 调度器类分析\n1struct sched_class { 2\tconst struct sched_class *next; 3 4\tvoid (*enqueue_task) (struct rq *rq, struct task_struct *p, int flags); 5\tvoid (*dequeue_task) (struct rq *rq, struct task_struct *p, int flags); 6\tvoid (*yield_task) (struct rq *rq); 7\tbool (*yield_to_task)(struct rq *rq, struct task_struct *p, bool preempt); 8 9\tvoid (*check_preempt_curr)(struct rq *rq, struct task_struct *p, int flags); 10 11\t/* 12* It is the responsibility of the pick_next_task() method that will 13* return the next task to call put_prev_task() on the @prev task or 14* something equivalent. 15* 16* May return RETRY_TASK when it finds a higher prio class has runnable 17* tasks. 18*/ 19\tstruct task_struct * (*pick_next_task)(struct rq *rq, 20\tstruct task_struct *prev, 21\tstruct rq_flags *rf); 22\tvoid (*put_prev_task)(struct rq *rq, struct task_struct *p); 23 24#ifdef CONFIG_SMP 25\tint (*select_task_rq)(struct task_struct *p, int task_cpu, int sd_flag, int flags); 26\tvoid (*migrate_task_rq)(struct task_struct *p, int new_cpu); 27 28\tvoid (*task_woken)(struct rq *this_rq, struct task_struct *task); 29 30\tvoid (*set_cpus_allowed)(struct task_struct *p, 31\tconst struct cpumask *newmask); 32 33\tvoid (*rq_online)(struct rq *rq); 34\tvoid (*rq_offline)(struct rq *rq); 35#endif 36 37\tvoid (*set_curr_task)(struct rq *rq); 38\tvoid (*task_tick)(struct rq *rq, struct task_struct *p, int queued); 39\tvoid (*task_fork)(struct task_struct *p); 40\tvoid (*task_dead)(struct task_struct *p); 41 42\t/* 43* The switched_from() call is allowed to drop rq-\u0026gt;lock, therefore we 44* cannot assume the switched_from/switched_to pair is serliazed by 45* rq-\u0026gt;lock. They are however serialized by p-\u0026gt;pi_lock. 46*/ 47\tvoid (*switched_from)(struct rq *this_rq, struct task_struct *task); 48\tvoid (*switched_to) (struct rq *this_rq, struct task_struct *task); 49\tvoid (*prio_changed) (struct rq *this_rq, struct task_struct *task, 50\tint oldprio); 51 52\tunsigned int (*get_rr_interval)(struct rq *rq, 53\tstruct task_struct *task); 54 55\tvoid (*update_curr)(struct rq *rq); 56 57#define TASK_SET_GROUP\t0 58#define TASK_MOVE_GROUP\t1 59 60#ifdef CONFIG_FAIR_GROUP_SCHED 61\tvoid (*task_change_group)(struct task_struct *p, int type); 62#endif 63}; enqueue_task_fair 1static void 2enqueue_task_fair(struct rq *rq, struct task_struct *p, int flags) 3{ 4\tstruct cfs_rq *cfs_rq; 5\tstruct sched_entity *se = \u0026amp;p-\u0026gt;se; 6 7\t/* 8* The code below (indirectly) updates schedutil which looks at 9* the cfs_rq utilization to select a frequency. 10* Let\u0026#39;s add the task\u0026#39;s estimated utilization to the cfs_rq\u0026#39;s 11* estimated utilization, before we update schedutil. 12*/ 13\tutil_est_enqueue(\u0026amp;rq-\u0026gt;cfs, p); 14 15\t/* 16* If in_iowait is set, the code below may not trigger any cpufreq 17* utilization updates, so do it here explicitly with the IOWAIT flag 18* passed. 19*/ 20\tif (p-\u0026gt;in_iowait) 21\tcpufreq_update_util(rq, SCHED_CPUFREQ_IOWAIT); 22 23\tfor_each_sched_entity(se) { 24\tif (se-\u0026gt;on_rq) 25\tbreak; 26\tcfs_rq = cfs_rq_of(se); 27\tenqueue_entity(cfs_rq, se, flags); 28 29\t/* 30* end evaluation on encountering a throttled cfs_rq 31* 32* note: in the case of encountering a throttled cfs_rq we will 33* post the final h_nr_running increment below. 34*/ 35\tif (cfs_rq_throttled(cfs_rq)) 36\tbreak; 37\tcfs_rq-\u0026gt;h_nr_running++; 38 39\tflags = ENQUEUE_WAKEUP; 40\t} 41 42\tfor_each_sched_entity(se) { 43\tcfs_rq = cfs_rq_of(se); 44\tcfs_rq-\u0026gt;h_nr_running++; 45 46\tif (cfs_rq_throttled(cfs_rq)) 47\tbreak; 48 49\tupdate_load_avg(cfs_rq, se, UPDATE_TG); 50\tupdate_cfs_group(se); 51\t} 52 53\tif (!se) 54\tadd_nr_running(rq, 1); 55 56\tif (cfs_bandwidth_used()) { 57\t/* 58* When bandwidth control is enabled; the cfs_rq_throttled() 59* breaks in the above iteration can result in incomplete 60* leaf list maintenance, resulting in triggering the assertion 61* below. 62*/ 63\tfor_each_sched_entity(se) { 64\tcfs_rq = cfs_rq_of(se); 65 66\tif (list_add_leaf_cfs_rq(cfs_rq)) 67\tbreak; 68\t} 69\t} 70 71\tassert_list_leaf_cfs_rq(rq); 72 73\thrtick_update(rq); 74} dequeue_task_fair 1static void dequeue_task_fair(struct rq *rq, struct task_struct *p, int flags) 2{ 3\tstruct cfs_rq *cfs_rq; 4\tstruct sched_entity *se = \u0026amp;p-\u0026gt;se; 5\tint task_sleep = flags \u0026amp; DEQUEUE_SLEEP; 6 7\tfor_each_sched_entity(se) { 8\tcfs_rq = cfs_rq_of(se); 9\tdequeue_entity(cfs_rq, se, flags); 10 11\t/* 12* end evaluation on encountering a throttled cfs_rq 13* 14* note: in the case of encountering a throttled cfs_rq we will 15* post the final h_nr_running decrement below. 16*/ 17\tif (cfs_rq_throttled(cfs_rq)) 18\tbreak; 19\tcfs_rq-\u0026gt;h_nr_running--; 20 21\t/* Don\u0026#39;t dequeue parent if it has other entities besides us */ 22\tif (cfs_rq-\u0026gt;load.weight) { 23\t/* Avoid re-evaluating load for this entity: */ 24\tse = parent_entity(se); 25\t/* 26* Bias pick_next to pick a task from this cfs_rq, as 27* p is sleeping when it is within its sched_slice. 28*/ 29\tif (task_sleep \u0026amp;\u0026amp; se \u0026amp;\u0026amp; !throttled_hierarchy(cfs_rq)) 30\tset_next_buddy(se); 31\tbreak; 32\t} 33\tflags |= DEQUEUE_SLEEP; 34\t} 35 36\tfor_each_sched_entity(se) { 37\tcfs_rq = cfs_rq_of(se); 38\tcfs_rq-\u0026gt;h_nr_running--; 39 40\tif (cfs_rq_throttled(cfs_rq)) 41\tbreak; 42 43\tupdate_load_avg(cfs_rq, se, UPDATE_TG); 44\tupdate_cfs_group(se); 45\t} 46 47\tif (!se) 48\tsub_nr_running(rq, 1); 49 50\tutil_est_dequeue(\u0026amp;rq-\u0026gt;cfs, p, task_sleep); 51\thrtick_update(rq); 52} yield_task_fair yield_to_task_fair ","date":"Oct 28, 2021","img":"","permalink":"https://mengdemao.github.io/posts/fair/","series":null,"tags":["kernel"],"title":"Fair"},{"categories":["python"],"content":"python基础绘图\n","date":"Oct 16, 2021","img":"","permalink":"https://mengdemao.github.io/posts/tkinter/","series":null,"tags":[],"title":"Tkinter"},{"categories":[],"content":"","date":"Oct 7, 2021","img":"","permalink":"https://mengdemao.github.io/posts/javascript/","series":null,"tags":[],"title":"Javascript"},{"categories":[],"content":"CSS开始(层叠样式表) HTML + CSS + JavaScript 名词 + 形容词 + 动词 相当于对原始的HTML进行美化\n快速入门  CSS是什么 CSS怎么用 CSS选择器 美化网页 盒子模型 浮动 定位 网页动画  什么是CSS 美化:字体, 颜色,高度,宽度, 背景图片\nCSS的优势:  内容和表现分离 CSS文件可以复用 样式十分丰富 建议使用独立的CSS文件  CSS导入的方法  行内样式  1\u0026lt;h1 style=\u0026#34;color: red\u0026#34;\u0026gt;一级标题\u0026lt;/h1\u0026gt; style标签  1\u0026lt;style\u0026gt;\u0026lt;/style\u0026gt; 外部样式   链接方式  1\u0026lt;link rel=\u0026#34;stylesheet\u0026#34; href=\u0026#34;style.css\u0026#34;\u0026gt;  导入式  1\u0026lt;style\u0026gt; 2\t@import url(\u0026#34;css/style.css\u0026#34;); 3\u0026lt;/style\u0026gt; 基本语法 1/* 注释语法 */ 2selector { 3\t/* 声明 */ 4\tattr:value; 5} 选择器 基本选择器   标签选择器\n  类选择器\n  ID选择器\n  标签选择器 1h1 { 2 color: red; 3} 4h2 { 5 color: black; 6} 7h3 { 8 color: yellow; 9} 10h4 { 11 color: red; 12} 类选择器 1\u0026lt;h1 class=\u0026#34;test\u0026#34;\u0026gt;测试\u0026lt;/h1\u0026gt; 此时,可以讲HTML选中\n1.test { 2 color: black; 3} ID选择器 1\u0026lt;h1 id=\u0026#34;test\u0026#34;\u0026gt;测试\u0026lt;/h1\u0026gt; 1#test { 2\tcolor: black; 3} ID唯一确定,不可以共享;\n层次选择器 ","date":"Oct 7, 2021","img":"","permalink":"https://mengdemao.github.io/posts/css/","series":null,"tags":[],"title":"Css"},{"categories":[],"content":"开始 网页基础结构\n1\u0026lt;!-- 告诉浏览器,需要使用的规范 --\u0026gt; 2\u0026lt;!DOCTYPE html\u0026gt; 3\u0026lt;html lang=\u0026#34;en\u0026#34;\u0026gt; 4 5\u0026lt;!-- 网页标题 --\u0026gt; 6\u0026lt;head\u0026gt; 7 \u0026lt;!-- 描述标签 --\u0026gt; 8 \u0026lt;meta charset=\u0026#34;UTF-8\u0026#34;\u0026gt; 9 10 \u0026lt;!-- 网页标题 --\u0026gt; 11 \u0026lt;title\u0026gt;Document\u0026lt;/title\u0026gt; 12\u0026lt;/head\u0026gt; 13 14\u0026lt;!-- 网页主体 --\u0026gt; 15\u0026lt;body\u0026gt; 16\u0026lt;/body\u0026gt; 17\u0026lt;/html\u0026gt; 网页基本标签 标题标签 1\u0026lt;h1\u0026gt;一级标签\u0026lt;/h1\u0026gt; 2\u0026lt;h2\u0026gt;二级标签\u0026lt;/h2\u0026gt; 3\u0026lt;h3\u0026gt;三级标签\u0026lt;/h3\u0026gt; 4\u0026lt;h4\u0026gt;四级标签\u0026lt;/h4\u0026gt; 5\u0026lt;h5\u0026gt;五级标签\u0026lt;/h5\u0026gt; 6\u0026lt;h6\u0026gt;六级标签\u0026lt;/h6\u0026gt; 段落标签 1\u0026lt;p\u0026gt;段落标签\u0026lt;/p\u0026gt; 换行标签 1\u0026lt;br/\u0026gt; 水平线标签 1\u0026lt;hr/\u0026gt; 字体样式标签 1\u0026lt;!-- 字体样式标签 --\u0026gt; 2\u0026lt;strong\u0026gt;粗体\u0026lt;/strong\u0026gt;\u0026lt;br/\u0026gt; 3\u0026lt;em\u0026gt;斜体\u0026lt;/em\u0026gt;\u0026lt;br/\u0026gt; 图片标签 1\u0026lt;img src=\u0026#34;测试.png\u0026#34; alt=\u0026#34;测试\u0026#34; title=\u0026#34;测试\u0026#34;/\u0026gt; 链接 1\u0026lt;!-- 当前页打开 --\u0026gt; 2\u0026lt;a href=\u0026#34;http://www.baidu.com\u0026#34; target=\u0026#34;_self\u0026#34;\u0026gt;百度一下\u0026lt;/a\u0026gt;\u0026lt;br/\u0026gt; 3\u0026lt;!-- 新建页打开 --\u0026gt; 4\u0026lt;a href=\u0026#34;http://www.baidu.com\u0026#34; target=\u0026#34;_blank\u0026#34;\u0026gt;百度一下\u0026lt;/a\u0026gt;\u0026lt;br/\u0026gt; 行内元素和块元素 列表标签 有序列表 1\u0026lt;ol\u0026gt; 2 \u0026lt;li\u0026gt;HTML\u0026lt;/li\u0026gt; 3 \u0026lt;li\u0026gt;CSS\u0026lt;/li\u0026gt; 4 \u0026lt;li\u0026gt;JavaScript\u0026lt;/li\u0026gt; 5\u0026lt;/ol\u0026gt; 无序列表 1\u0026lt;ul\u0026gt; 2 \u0026lt;li\u0026gt;HTML\u0026lt;/li\u0026gt; 3 \u0026lt;li\u0026gt;CSS\u0026lt;/li\u0026gt; 4 \u0026lt;li\u0026gt;JavaScript\u0026lt;/li\u0026gt; 5\u0026lt;/ul\u0026gt; 定义列表 1\u0026lt;dl\u0026gt; 2 \u0026lt;dt\u0026gt;前端\u0026lt;/dt\u0026gt; 3 \u0026lt;dd\u0026gt;html\u0026lt;/dd\u0026gt; 4 \u0026lt;dd\u0026gt;CSS\u0026lt;/dd\u0026gt; 5 \u0026lt;dd\u0026gt;JavaScript\u0026lt;/dd\u0026gt; 6\u0026lt;/dl\u0026gt; 表格 1\u0026lt;table border=\u0026#34;1px\u0026#34;\u0026gt; 2\t\u0026lt;tr\u0026gt; 3\t\u0026lt;td\u0026gt;1-1\u0026lt;/td\u0026gt; 4\t\u0026lt;td\u0026gt;1-2\u0026lt;/td\u0026gt; 5\t\u0026lt;/tr\u0026gt; 6\t\u0026lt;tr\u0026gt; 7\t\u0026lt;td\u0026gt;2-1\u0026lt;/td\u0026gt; 8\t\u0026lt;td\u0026gt;2-2\u0026lt;/td\u0026gt; 9\t\u0026lt;/tr\u0026gt; 10\u0026lt;/table\u0026gt; 页面结构分析    元素名 描述     header 标题头部区域   footer 标记尾部内容   section web页面中一块独立的区域   article 独立文章内容   aside 相关页面或者内容   nav 导航类辅助内容    iframe内联框架 1\u0026lt;iframe src=\u0026#34;path\u0026#34; name=\u0026#34;mainFrame\u0026#34;\u0026gt;\u0026lt;/frame\u0026gt; bilibili的例子\n1\u0026lt;iframe src=\u0026#34;//player.bilibili.com/player.html?aid=55631961\u0026amp;bvid=BV1x4411V75C\u0026amp;cid=97257967\u0026amp;page=11\u0026#34; scrolling=\u0026#34;no\u0026#34; border=\u0026#34;0\u0026#34; frameborder=\u0026#34;no\u0026#34; framespacing=\u0026#34;0\u0026#34; allowfullscreen=\u0026#34;true\u0026#34;\u0026gt; \u0026lt;/iframe\u0026gt; 表单 表单form\n1\u0026lt;form action=\u0026#34;开始.html\u0026#34; method=\u0026#34;GET/POST\u0026#34;\u0026gt; 2 \u0026lt;p\u0026gt;名字: \u0026lt;input type=\u0026#34;text\u0026#34; name=\u0026#34;name\u0026#34;\u0026gt;\u0026lt;/p\u0026gt; 3 \u0026lt;p\u0026gt;密码: \u0026lt;input type=\u0026#34;password\u0026#34; name=\u0026#34;password\u0026#34;\u0026gt;\u0026lt;/p\u0026gt; 4 \u0026lt;p\u0026gt; 5 \u0026lt;input type=\u0026#34;submit\u0026#34;\u0026gt; 6 \u0026lt;input type=\u0026#34;reset\u0026#34;\u0026gt; 7 \u0026lt;/p\u0026gt; 8\u0026lt;/form\u0026gt; 产生的效果\n1?name=111\u0026amp;password= ","date":"Oct 7, 2021","img":"","permalink":"https://mengdemao.github.io/posts/html5/","series":null,"tags":[],"title":"Html5"},{"categories":[],"content":" A C dynamic strings library C语言版本动态字符串库\n SDS SDS的类型就是\n1typedef char *sds; 可以明显的看到,sds就是普通的char类型\n下面是sds的数据类型 1+--------+-------------------------------+-----------+ 2| Header | Binary safe C alike string... | Null term | 3+--------+-------------------------------+-----------+ 4 | 5 -\u0026gt; Pointer returned to the user. 1#define SDS_HDR_VAR(T,s) \\ 2struct sdshdr##T *sh = (void*)((s)-(sizeof(struct sdshdr##T))); 3#define SDS_HDR(T,s) \\ 4((struct sdshdr##T *)((s)-(sizeof(struct sdshdr##T)))) 5#define SDS_TYPE_5_LEN(f) ((f)\u0026gt;\u0026gt;SDS_TYPE_BITS) SDS 头 根据不同的标志计算不同的头部数据\n   宏定义 标志     SDS_TYPE_5 sdshdr5   SDS_TYPE_8 sdshdr8   SDS_TYPE_16 sdshdr16   SDS_TYPE_32 sdshdr32   SDS_TYPE_64 sdshdr64    flag标志:\n1unsigned char flags = s[-1]; /* 最后一个头部数据 */ 1#define SDS_TYPE_5 0 2#define SDS_TYPE_8 1 3#define SDS_TYPE_16 2 4#define SDS_TYPE_32 3 5#define SDS_TYPE_64 4 1/* Note: sdshdr5 is never used, we just access the flags byte directly. 2* However is here to document the layout of type 5 SDS strings. */ 3struct __attribute__ ((__packed__)) sdshdr5 { 4 unsigned char flags; /* 3 lsb of type, and 5 msb of string length */ 5 char buf[]; 6}; 7struct __attribute__ ((__packed__)) sdshdr8 { 8 uint8_t len; /* used */ 9 uint8_t alloc; /* excluding the header and null terminator */ 10 unsigned char flags; /* 3 lsb of type, 5 unused bits */ 11 char buf[]; 12}; 13struct __attribute__ ((__packed__)) sdshdr16 { 14 uint16_t len; /* used */ 15 uint16_t alloc; /* excluding the header and null terminator */ 16 unsigned char flags; /* 3 lsb of type, 5 unused bits */ 17 char buf[]; 18}; 19struct __attribute__ ((__packed__)) sdshdr32 { 20 uint32_t len; /* used */ 21 uint32_t alloc; /* excluding the header and null terminator */ 22 unsigned char flags; /* 3 lsb of type, 5 unused bits */ 23 char buf[]; 24}; 25struct __attribute__ ((__packed__)) sdshdr64 { 26 uint64_t len; /* used */ 27 uint64_t alloc; /* excluding the header and null terminator */ 28 unsigned char flags; /* 3 lsb of type, 5 unused bits */ 29 char buf[]; 30}; 1#define SDS_TYPE_MASK 7 2#define SDS_TYPE_BITS 3 创建SDS 函数原型\n1sds sdsnewlen(const void *init, size_t initlen); 扩张字符串缓存区 1sds sdsMakeRoomFor(sds s, size_t addlen) 2{ 3 void *sh; 4 void *newsh; 5 size_t avail = sdsavail(s);\t/* 计算剩余的可以使用的大小 */ 6 size_t len; 7 size_t newlen; 8 char type, oldtype = s[-1] \u0026amp; SDS_TYPE_MASK; 9 int hdrlen; 10 11 if (avail \u0026gt;= addlen) { /* 如果剩余的存储空间超过添加大小,那么就可以直接返回 */ 12 return s; 13 } 14 len = sdslen(s);\t/* 计算字符串大小 */ 15 sh = (char*)s - sdsHdrSize(oldtype); /* 缓冲区地址 */ 16 17 /* 计算得到新的长度 */ 18 newlen = (len+addlen); 19 if (newlen \u0026lt; SDS_MAX_PREALLOC) 20 newlen *= 2; 21 else 22 newlen += SDS_MAX_PREALLOC; 23\t/* 重新生成类型 */ 24 type = sdsReqType(newlen); 25 26 /* Don\u0026#39;t use type 5: the user is appending to the string and type 5 is 27* not able to remember empty space, so sdsMakeRoomFor() must be called 28* at every appending operation. */ 29 if (type == SDS_TYPE_5) { 30 type = SDS_TYPE_8; 31\t} 32\t33 /* 计算头部大小 */ 34 hdrlen = sdsHdrSize(type); 35 36 if (oldtype == type) { 37 newsh = s_realloc(sh, hdrlen + newlen + 1); 38 if (newsh == NULL) { 39 return NULL; 40 } 41 s = (char*)newsh + hdrlen; 42 } else { 43 /* Since the header size changes, need to move the string forward, 44* and can\u0026#39;t use realloc */ 45 newsh = s_malloc(hdrlen+newlen+1); 46 if (newsh == NULL) { 47 return NULL; 48 } 49 memcpy((char*)newsh+hdrlen, s, len+1); 50 s_free(sh); 51 52 s = (char*)newsh + hdrlen; 53 s[-1] = type; 54 55 sdssetlen(s, len); 56 } 57 58 sdssetalloc(s, newlen); 59 return s; 60} 追加字符串 1sds sdscatlen(sds s, const void *t, size_t len) 2{ 3 size_t curlen = sdslen(s);\t/* 计算字符串的长度 */ 4 5 s = sdsMakeRoomFor(s,len);\t/* 扩展字符串缓冲区长度 */ 6 if (s == NULL) { 7 return NULL; 8 } 9 memcpy(s+curlen, t, len);\t/* 添加字符串 */ 10 sdssetlen(s, curlen+len);\t/* 设置长度标志 */ 11 s[curlen+len] = \u0026#39;\\0\u0026#39;;\t/* 补全结束符 */ 12 return s; 13} ","date":"Oct 6, 2021","img":"","permalink":"https://mengdemao.github.io/posts/sds/","series":null,"tags":[],"title":"Sds"},{"categories":[],"content":" STL称为标准模板库(Standard Template Library) 广义上可以分为容器,算法,迭代器 容器和算法通过迭代器进行无缝连接 STL几乎所有的代码都采用了函数模版或者类模板\n STL组件    序号 名称 解释     1 容器 各种数据结构   2 算法 各种常用的算法   3 迭代器 容器域算法的胶合   4 仿函数 行为类似函数   5 适配器 修饰容器或者仿函数迭代器   6 空间配置器 负责空间的配置和管理    容器算法和迭代器 vector vector使用 1/* 创建vector容器 */ 2vector\u0026lt;int\u0026gt; v; 3/* 插入数据 */ 4v.push_back(10); 5v.push_back(20); 6v.push_back(30); 7v.push_back(40); 迭代器使用 迭代器方案1 1vector\u0026lt;int\u0026gt;::iterator itBegin = v.begin(); 2vector\u0026lt;int\u0026gt;::iterator itEnd = v.end(); 3while (itBegin != itEnd) { 4\tcout \u0026lt;\u0026lt; *itBegin \u0026lt;\u0026lt; endl; 5\titBegin += 1; 6} 迭代器2 1for (vector\u0026lt;int\u0026gt;::iterator it = v.begin(); it != v.end(); it++) 2{ 3\tcout \u0026lt;\u0026lt; *it \u0026lt;\u0026lt; endl; 4} 遍历算法 1template \u0026lt;class T\u0026gt; 2void myPrint(T val) 3{ 4\tcout \u0026lt;\u0026lt; val \u0026lt;\u0026lt; endl; 5} 6 7/* 可惜回调函数不支持自动推导 */ 8for_each(v.begin(), v.end(), myPrint\u0026lt;int\u0026gt;); 容器自定义数据 容器嵌套容器 1vector\u0026lt;vector\u0026lt;int\u0026gt;\u0026gt;v; // 外部大容器 2vector\u0026lt;int\u0026gt; vx[10]; // 内部小容器 3 4/* 插入容器 */ 5for (int i = 0; i \u0026lt; 10; i++) 6{ 7\tfor (int j = 0; j \u0026lt; 30; j++) 8\t{ 9\tvx[i].push_back(i + j + 10); 10\t} 11\tv.push_back(vx[i]); 12} 13 14/* 遍历容器 */ 15for (vector\u0026lt;vector\u0026lt;int\u0026gt;\u0026gt;::iterator it = v.begin(); it != v.end(); it++) 16{ 17\tfor (vector\u0026lt;int\u0026gt;::iterator vit = it-\u0026gt;begin(); vit != it-\u0026gt;end(); vit++) 18\t{ 19\tcout \u0026lt;\u0026lt; *vit \u0026lt;\u0026lt; \u0026#34; \u0026#34;; 20\t} 21\tcout \u0026lt;\u0026lt; endl; 22} string string本质上是一个类,封装了char*,提供了许多的成员方法;\n构造函数 1string s1(str); 2string s2 = \u0026#34;Hello World\u0026#34;; 3string s3(s2); 赋值操作  重载操作符**=**  1string s1; 2s1 = \u0026#34;Hello World\u0026#34;; 成员函数assign  1string str; 2str.assign(\u0026#34;Hello World\u0026#34;); 追加操作  重载操作符**+=** 成员函数append  查找和替换 find replace 比较 compare 字符存取  [] at  插入和删除 insert earse 子串 substr array deque hashtable map list queue stack set rbtree ","date":"Oct 6, 2021","img":"","permalink":"https://mengdemao.github.io/posts/stl/","series":null,"tags":[],"title":"STL学习笔记"},{"categories":[],"content":"HelloWorld 1#!/bin/python3 2 3if __name__ == \u0026#39;__main__\u0026#39;: 4 print(\u0026#39;Hello World\u0026#39;) 数据类型 Numbers(数字) 1intA = 10 2print(intA) 布尔类型 1true 2false String(字符串) 1strB = \u0026#34;Hello\u0026#34; 2print(strB) List(列表) 1listC = [\u0026#34;12\u0026#34;, 3, 4] 2print(listC) Tuple(元组) 1tupleD = (\u0026#39;physics\u0026#39;, \u0026#39;chemistry\u0026#39;, 1997, 2000) 2print(tupleD) Dictionary(字典) 1DictE = {\u0026#39;a\u0026#39;: 1, \u0026#39;b\u0026#39;: 2, \u0026#39;b\u0026#39;: \u0026#39;3\u0026#39;} 2print(DictE) 运算符 控制结构 条件语句 单执行语句 1if 判断条件： 2\t执行语句 3else： 4\t执行语句 多条件语句 1if 判断条件1: 2 执行语句1…… 3elif 判断条件2: 4 执行语句2…… 5elif 判断条件3: 6\t执行语句3…… 7else: 8 执行语句4…… while循环 1c = 0 2while (c \u0026lt; 10): 3 print(c) 4 c += 1 5print(\u0026#34;while Loop finish\u0026#34;) ###　for循环\n函数 面向对象 ","date":"Oct 5, 2021","img":"","permalink":"https://mengdemao.github.io/posts/python/","series":null,"tags":[],"title":"Python学习笔记"},{"categories":[],"content":"输入子设备分为三层\n handle core device  input的相关结构体 1 struct input_dev {\t/* 输入设备的描述 */ 2 const char *name;\t/* 设备名称 */ 3\tconst char *phys; 4\tconst char *uniq; 5\tstruct input_id id; 6 7\tunsigned long propbit[BITS_TO_LONGS(INPUT_PROP_CNT)]; 8 9\tunsigned long evbit[BITS_TO_LONGS(EV_CNT)]; 10\tunsigned long keybit[BITS_TO_LONGS(KEY_CNT)]; 11\tunsigned long relbit[BITS_TO_LONGS(REL_CNT)]; 12\tunsigned long absbit[BITS_TO_LONGS(ABS_CNT)]; 13\tunsigned long mscbit[BITS_TO_LONGS(MSC_CNT)]; 14\tunsigned long ledbit[BITS_TO_LONGS(LED_CNT)]; 15\tunsigned long sndbit[BITS_TO_LONGS(SND_CNT)]; 16\tunsigned long ffbit[BITS_TO_LONGS(FF_CNT)]; 17\tunsigned long swbit[BITS_TO_LONGS(SW_CNT)]; 18 19\tunsigned int hint_events_per_packet; 20 21\tunsigned int keycodemax; 22\tunsigned int keycodesize; 23\tvoid *keycode; 24 25\tint (*setkeycode)(struct input_dev *dev, 26\tconst struct input_keymap_entry *ke, 27\tunsigned int *old_keycode); 28\tint (*getkeycode)(struct input_dev *dev, 29\tstruct input_keymap_entry *ke); 30 31\tstruct ff_device *ff; 32 33\tunsigned int repeat_key; 34\tstruct timer_list timer; 35 36\tint rep[REP_CNT]; 37 38\tstruct input_mt *mt; 39 40\tstruct input_absinfo *absinfo; 41 42\tunsigned long key[BITS_TO_LONGS(KEY_CNT)]; 43\tunsigned long led[BITS_TO_LONGS(LED_CNT)]; 44\tunsigned long snd[BITS_TO_LONGS(SND_CNT)]; 45\tunsigned long sw[BITS_TO_LONGS(SW_CNT)]; 46 47\tint (*open)(struct input_dev *dev); 48\tvoid (*close)(struct input_dev *dev); 49\tint (*flush)(struct input_dev *dev, struct file *file); 50\tint (*event)(struct input_dev *dev, unsigned int type, unsigned int code, int value); 51 52\tstruct input_handle __rcu *grab; 53 54\tspinlock_t event_lock; 55\tstruct mutex mutex; 56 57\tunsigned int users; 58\tbool going_away; 59 60\tstruct device dev; 61 62\tstruct list_head\th_list; 63\tstruct list_head\tnode; 64 65\tunsigned int num_vals; 66\tunsigned int max_vals; 67\tstruct input_value *vals; 68 69\tbool devres_managed; 70 }; 71#define to_input_dev(d) container_of(d, struct input_dev, dev) input子系统使用 input子系统分析  Makefile编写  1obj-$(CONFIG_INPUT)\t+= input-core.o 2input-core-y := input.o input-compat.o input-mt.o ff-core.o 开始判断下面的第一个文件 input.c  1subsys_initcall(input_init); 2module_exit(input_exit); 输入子系统的设备号\n1#define INPUT_MAJOR 13 安装驱动\n1 static int __init input_init(void) 2 { 3\tint err; 4\t/* 注册设备类 */ 5\terr = class_register(\u0026amp;input_class); 6\tif (err) { 7\tpr_err(\u0026#34;unable to register input_dev class\\n\u0026#34;); 8\treturn err; 9\t} 10 11\t/* 注册proc文件系统 */ 12\terr = input_proc_init(); 13\tif (err) 14\tgoto fail1; 15\t/* 注册设备 */ 16\terr = register_chrdev_region(MKDEV(INPUT_MAJOR, 0), 17\tINPUT_MAX_CHAR_DEVICES, \u0026#34;input\u0026#34;); 18\tif (err) { 19\tpr_err(\u0026#34;unable to register char major %d\u0026#34;, INPUT_MAJOR); 20\tgoto fail2; 21\t} 22 23\treturn 0; 24 25 fail2:\tinput_proc_exit(); 26 fail1:\tclass_unregister(\u0026amp;input_class); 27\treturn err; 28 } 卸载驱动\n1 static void __exit input_exit(void) 2 { 3\t/* 卸载proc文件系统 */ 4\tinput_proc_exit(); 5 6\t/* 注销设备号 */ 7\tunregister_chrdev_region(MKDEV(INPUT_MAJOR, 0), 8\tINPUT_MAX_CHAR_DEVICES); 9 10\t/* 注销CLass */ 11\tclass_unregister(\u0026amp;input_class); 12 } 设备类操作\n1\t/* 设备类型 */ 2\tstruct class input_class = { 3\t.name\t= \u0026#34;input\u0026#34;, 4\t.devnode\t= input_devnode, 5\t}; 6\tEXPORT_SYMBOL_GPL(input_class); 7 8\t/* 注册设备 */ 9\terr = class_register(\u0026amp;input_class); 10\tif (err) { 11\tpr_err(\u0026#34;unable to register input_dev class\\n\u0026#34;); 12\treturn err; 13\t} 14 15\t/* 卸载设备 */ 16 class_unregister(\u0026amp;input_class); Proc文件系统操作\nProc文件系统添加\n1static int __init input_proc_init(void) 2{ 3\tstruct proc_dir_entry *entry; 4 5\tproc_bus_input_dir = proc_mkdir(\u0026#34;bus/input\u0026#34;, NULL); 6\tif (!proc_bus_input_dir) 7\treturn -ENOMEM; 8 9\tentry = proc_create(\u0026#34;devices\u0026#34;, 0, proc_bus_input_dir, 10\t\u0026amp;input_devices_fileops); 11\tif (!entry) 12\tgoto fail1; 13 14\tentry = proc_create(\u0026#34;handlers\u0026#34;, 0, proc_bus_input_dir, 15\t\u0026amp;input_handlers_fileops); 16\tif (!entry) 17\tgoto fail2; 18 19\treturn 0; 20 21 fail2:\tremove_proc_entry(\u0026#34;devices\u0026#34;, proc_bus_input_dir); 22 fail1: remove_proc_entry(\u0026#34;bus/input\u0026#34;, NULL); 23\treturn -ENOMEM; 24} Proc文件系统卸载\n1static void input_proc_exit(void) 2{ 3\tremove_proc_entry(\u0026#34;devices\u0026#34;, proc_bus_input_dir); 4\tremove_proc_entry(\u0026#34;handlers\u0026#34;, proc_bus_input_dir); 5\tremove_proc_entry(\u0026#34;bus/input\u0026#34;, NULL); 6} 接口部分 Handler操作 1/** 2* 注册 input handler 3* input_register_handler - register a new input handler 4* @handler: handler to be registered 5* 6* This function registers a new input handler (interface) for input 7* devices in the system and attaches it to all input devices that 8* are compatible with the handler. 9*/ 10int input_register_handler(struct input_handler *handler) 11{ 12\tstruct input_dev *dev; 13\tint error; 14 15\terror = mutex_lock_interruptible(\u0026amp;input_mutex); 16\tif (error) 17\treturn error; 18 19\tINIT_LIST_HEAD(\u0026amp;handler-\u0026gt;h_list); 20 21\tlist_add_tail(\u0026amp;handler-\u0026gt;node, \u0026amp;input_handler_list); 22 23\tlist_for_each_entry(dev, \u0026amp;input_dev_list, node) 24\tinput_attach_handler(dev, handler); 25 26\tinput_wakeup_procfs_readers(); 27 28\tmutex_unlock(\u0026amp;input_mutex); 29\treturn 0; 30} 31EXPORT_SYMBOL(input_register_handler); 32 33/** 34* 解除注册 input handler 35* input_unregister_handler - unregisters an input handler 36* @handler: handler to be unregistered 37* 38* This function disconnects a handler from its input devices and 39* removes it from lists of known handlers. 40*/ 41void input_unregister_handler(struct input_handler *handler) 42{ 43\tstruct input_handle *handle, *next; 44 45\tmutex_lock(\u0026amp;input_mutex); 46 47\tlist_for_each_entry_safe(handle, next, \u0026amp;handler-\u0026gt;h_list, h_node) 48\thandler-\u0026gt;disconnect(handle); 49\tWARN_ON(!list_empty(\u0026amp;handler-\u0026gt;h_list)); 50 51\tlist_del_init(\u0026amp;handler-\u0026gt;node); 52 53\tinput_wakeup_procfs_readers(); 54 55\tmutex_unlock(\u0026amp;input_mutex); 56} 57EXPORT_SYMBOL(input_unregister_handler); 注册设备 1/** 2* 注册一个设备 3* input_register_device - register device with input core 4* @dev: device to be registered 5* 6* This function registers device with input core. The device must be 7* allocated with input_allocate_device() and all it\u0026#39;s capabilities 8* set up before registering. 9* If function fails the device must be freed with input_free_device(). 10* Once device has been successfully registered it can be unregistered 11* with input_unregister_device(); input_free_device() should not be 12* called in this case. 13* 14* Note that this function is also used to register managed input devices 15* (ones allocated with devm_input_allocate_device()). Such managed input 16* devices need not be explicitly unregistered or freed, their tear down 17* is controlled by the devres infrastructure. It is also worth noting 18* that tear down of managed input devices is internally a 2-step process: 19* registered managed input device is first unregistered, but stays in 20* memory and can still handle input_event() calls (although events will 21* not be delivered anywhere). The freeing of managed input device will 22* happen later, when devres stack is unwound to the point where device 23* allocation was made. 24*/ 25int input_register_device(struct input_dev *dev) 26{ 27\tstruct input_devres *devres = NULL; 28\tstruct input_handler *handler; 29\tunsigned int packet_size; 30\tconst char *path; 31\tint error; 32 33\tif (dev-\u0026gt;devres_managed) { 34\tdevres = devres_alloc(devm_input_device_unregister, 35\tsizeof(struct input_devres), GFP_KERNEL); 36\tif (!devres) 37\treturn -ENOMEM; 38 39\tdevres-\u0026gt;input = dev; 40\t} 41 42\t/* Every input device generates EV_SYN/SYN_REPORT events. */ 43\t__set_bit(EV_SYN, dev-\u0026gt;evbit); 44 45\t/* KEY_RESERVED is not supposed to be transmitted to userspace. */ 46\t__clear_bit(KEY_RESERVED, dev-\u0026gt;keybit); 47 48\t/* Make sure that bitmasks not mentioned in dev-\u0026gt;evbit are clean. */ 49\tinput_cleanse_bitmasks(dev); 50 51\tpacket_size = input_estimate_events_per_packet(dev); 52\tif (dev-\u0026gt;hint_events_per_packet \u0026lt; packet_size) 53\tdev-\u0026gt;hint_events_per_packet = packet_size; 54 55\tdev-\u0026gt;max_vals = dev-\u0026gt;hint_events_per_packet + 2; 56\tdev-\u0026gt;vals = kcalloc(dev-\u0026gt;max_vals, sizeof(*dev-\u0026gt;vals), GFP_KERNEL); 57\tif (!dev-\u0026gt;vals) { 58\terror = -ENOMEM; 59\tgoto err_devres_free; 60\t} 61 62\t/* 63* If delay and period are pre-set by the driver, then autorepeating 64* is handled by the driver itself and we don\u0026#39;t do it in input.c. 65*/ 66\tif (!dev-\u0026gt;rep[REP_DELAY] \u0026amp;\u0026amp; !dev-\u0026gt;rep[REP_PERIOD]) { 67\tdev-\u0026gt;timer.data = (long) dev; 68\tdev-\u0026gt;timer.function = input_repeat_key; 69\tdev-\u0026gt;rep[REP_DELAY] = 250; 70\tdev-\u0026gt;rep[REP_PERIOD] = 33; 71\t} 72 73\tif (!dev-\u0026gt;getkeycode) 74\tdev-\u0026gt;getkeycode = input_default_getkeycode; 75 76\tif (!dev-\u0026gt;setkeycode) 77\tdev-\u0026gt;setkeycode = input_default_setkeycode; 78 79\terror = device_add(\u0026amp;dev-\u0026gt;dev); 80\tif (error) 81\tgoto err_free_vals; 82 83\tpath = kobject_get_path(\u0026amp;dev-\u0026gt;dev.kobj, GFP_KERNEL); 84\tpr_info(\u0026#34;%s as %s\\n\u0026#34;, 85\tdev-\u0026gt;name ? dev-\u0026gt;name : \u0026#34;Unspecified device\u0026#34;, 86\tpath ? path : \u0026#34;N/A\u0026#34;); 87\tkfree(path); 88 89\terror = mutex_lock_interruptible(\u0026amp;input_mutex); 90\tif (error) 91\tgoto err_device_del; 92 93\tlist_add_tail(\u0026amp;dev-\u0026gt;node, \u0026amp;input_dev_list); 94 95\tlist_for_each_entry(handler, \u0026amp;input_handler_list, node) 96\tinput_attach_handler(dev, handler); 97 98\tinput_wakeup_procfs_readers(); 99 100\tmutex_unlock(\u0026amp;input_mutex); 101 102\tif (dev-\u0026gt;devres_managed) { 103\tdev_dbg(dev-\u0026gt;dev.parent, \u0026#34;%s: registering %s with devres.\\n\u0026#34;, 104\t__func__, dev_name(\u0026amp;dev-\u0026gt;dev)); 105\tdevres_add(dev-\u0026gt;dev.parent, devres); 106\t} 107\treturn 0; 108 109err_device_del: 110\tdevice_del(\u0026amp;dev-\u0026gt;dev); 111err_free_vals: 112\tkfree(dev-\u0026gt;vals); 113\tdev-\u0026gt;vals = NULL; 114err_devres_free: 115\tdevres_free(devres); 116\treturn error; 117} 118EXPORT_SYMBOL(input_register_device); 119 120/** 121* 解除注册设备 122* input_unregister_device - unregister previously registered device 123* @dev: device to be unregistered 124* 125* This function unregisters an input device. Once device is unregistered 126* the caller should not try to access it as it may get freed at any moment. 127*/ 128void input_unregister_device(struct input_dev *dev) 129{ 130\tif (dev-\u0026gt;devres_managed) { 131\tWARN_ON(devres_destroy(dev-\u0026gt;dev.parent, 132\tdevm_input_device_unregister, 133\tdevm_input_device_match, 134\tdev)); 135\t__input_unregister_device(dev); 136\t/* 137* We do not do input_put_device() here because it will be done 138* when 2nd devres fires up. 139*/ 140\t} else { 141\t__input_unregister_device(dev); 142\tinput_put_device(dev); 143\t} 144} 145EXPORT_SYMBOL(input_unregister_device); ","date":"Oct 5, 2021","img":"","permalink":"https://mengdemao.github.io/posts/input_drive/","series":null,"tags":[],"title":"输入子系统"},{"categories":[],"content":"","date":"Oct 4, 2021","img":"","permalink":"https://mengdemao.github.io/posts/slab/","series":null,"tags":[],"title":"Slab"},{"categories":[],"content":"MMU内存管理单元  ARM内存管理单元分析\n ","date":"Oct 4, 2021","img":"","permalink":"https://mengdemao.github.io/posts/mmu/","series":null,"tags":[],"title":"Mmu"},{"categories":[],"content":"","date":"Oct 4, 2021","img":"","permalink":"https://mengdemao.github.io/posts/page/","series":null,"tags":[],"title":"Page"},{"categories":[],"content":"fork  linux创建线程的函数 fork \u0026ndash;\u0026gt; do_fork\n do_fork的执行线路 do_fork \u0026ndash;\u0026gt; copy_process \u0026ndash;\u0026gt; get_task_pid \u0026ndash;\u0026gt; wake_up_new_task \u0026ndash;\u0026gt; put_pid\ndo_fork函数原型\n1long _do_fork(unsigned long clone_flags, 2\tunsigned long stack_start, 3\tunsigned long stack_size, 4\tint __user *parent_tidptr, 5\tint __user *child_tidptr, 6\tunsigned long tls);  clone_flags stack_start stack_size parent_tidptr child_tidptr tls  copy_process get_task_pid wake_up_new_task ","date":"Oct 4, 2021","img":"","permalink":"https://mengdemao.github.io/posts/fork/","series":null,"tags":[],"title":"Fork"},{"categories":[],"content":"","date":"Oct 4, 2021","img":"","permalink":"https://mengdemao.github.io/posts/task/","series":null,"tags":[],"title":"任务管理"},{"categories":["linux"],"content":"系统调度 调度核心函数实现 1/* 2* kernel/sched/core.c 3* 4* Core kernel scheduler code and related syscalls 5* 6* Copyright (C) 1991-2002 Linus Torvalds 7*/ 8#include \u0026#34;sched.h\u0026#34;9 10#include \u0026lt;linux/nospec.h\u0026gt;11 12#include \u0026lt;linux/kcov.h\u0026gt;13 14#include \u0026lt;asm/switch_to.h\u0026gt;15#include \u0026lt;asm/tlb.h\u0026gt;16 17#include \u0026#34;../workqueue_internal.h\u0026#34;18#include \u0026#34;../smpboot.h\u0026#34;19 20#include \u0026#34;pelt.h\u0026#34;21 22#define CREATE_TRACE_POINTS 23#include \u0026lt;trace/events/sched.h\u0026gt;24 25DEFINE_PER_CPU_SHARED_ALIGNED(struct rq, runqueues); 26 27#ifdef CONFIG_SCHED_DEBUG 28/* 29* Debugging: various feature bits 30* 31* If SCHED_DEBUG is disabled, each compilation unit has its own copy of 32* sysctl_sched_features, defined in sched.h, to allow constants propagation 33* at compile time and compiler optimization based on features default. 34*/ 35#define SCHED_FEAT(name, enabled)\t\\ 36(1UL \u0026lt;\u0026lt; __SCHED_FEAT_##name) * enabled | 37const_debug unsigned int sysctl_sched_features = 38#include \u0026#34;features.h\u0026#34;39\t0; 40#undef SCHED_FEAT 41#endif 42 43/* 44* Number of tasks to iterate in a single balance run. 45* Limited because this is done with IRQs disabled. 46*/ 47const_debug unsigned int sysctl_sched_nr_migrate = 32; 48 49/* 50* period over which we measure -rt task CPU usage in us. 51* default: 1s 52*/ 53unsigned int sysctl_sched_rt_period = 1000000; 54 55__read_mostly int scheduler_running; 56 57/* 58* part of the period that we allow rt tasks to run in us. 59* default: 0.95s 60*/ 61int sysctl_sched_rt_runtime = 950000; 62 63/* 64* __task_rq_lock - lock the rq @p resides on. 65*/ 66struct rq *__task_rq_lock(struct task_struct *p, struct rq_flags *rf) 67\t__acquires(rq-\u0026gt;lock) 68{ 69\tstruct rq *rq; 70 71\tlockdep_assert_held(\u0026amp;p-\u0026gt;pi_lock); 72 73\tfor (;;) { 74\trq = task_rq(p); 75\traw_spin_lock(\u0026amp;rq-\u0026gt;lock); 76\tif (likely(rq == task_rq(p) \u0026amp;\u0026amp; !task_on_rq_migrating(p))) { 77\trq_pin_lock(rq, rf); 78\treturn rq; 79\t} 80\traw_spin_unlock(\u0026amp;rq-\u0026gt;lock); 81 82\twhile (unlikely(task_on_rq_migrating(p))) 83\tcpu_relax(); 84\t} 85} 86 87/* 88* task_rq_lock - lock p-\u0026gt;pi_lock and lock the rq @p resides on. 89*/ 90struct rq *task_rq_lock(struct task_struct *p, struct rq_flags *rf) 91\t__acquires(p-\u0026gt;pi_lock) 92\t__acquires(rq-\u0026gt;lock) 93{ 94\tstruct rq *rq; 95 96\tfor (;;) { 97\traw_spin_lock_irqsave(\u0026amp;p-\u0026gt;pi_lock, rf-\u0026gt;flags); 98\trq = task_rq(p); 99\traw_spin_lock(\u0026amp;rq-\u0026gt;lock); 100\t/* 101*\tmove_queued_task()\ttask_rq_lock() 102* 103*\tACQUIRE (rq-\u0026gt;lock) 104*\t[S] -\u0026gt;on_rq = MIGRATING\t[L] rq = task_rq() 105*\tWMB (__set_task_cpu())\tACQUIRE (rq-\u0026gt;lock); 106*\t[S] -\u0026gt;cpu = new_cpu\t[L] task_rq() 107*\t[L] -\u0026gt;on_rq 108*\tRELEASE (rq-\u0026gt;lock) 109* 110* If we observe the old CPU in task_rq_lock(), the acquire of 111* the old rq-\u0026gt;lock will fully serialize against the stores. 112* 113* If we observe the new CPU in task_rq_lock(), the address 114* dependency headed by \u0026#39;[L] rq = task_rq()\u0026#39; and the acquire 115* will pair with the WMB to ensure we then also see migrating. 116*/ 117\tif (likely(rq == task_rq(p) \u0026amp;\u0026amp; !task_on_rq_migrating(p))) { 118\trq_pin_lock(rq, rf); 119\treturn rq; 120\t} 121\traw_spin_unlock(\u0026amp;rq-\u0026gt;lock); 122\traw_spin_unlock_irqrestore(\u0026amp;p-\u0026gt;pi_lock, rf-\u0026gt;flags); 123 124\twhile (unlikely(task_on_rq_migrating(p))) 125\tcpu_relax(); 126\t} 127} 128 129/* 130* RQ-clock updating methods: 131*/ 132 133static void update_rq_clock_task(struct rq *rq, s64 delta) 134{ 135/* 136* In theory, the compile should just see 0 here, and optimize out the call 137* to sched_rt_avg_update. But I don\u0026#39;t trust it... 138*/ 139\ts64 __maybe_unused steal = 0, irq_delta = 0; 140 141#ifdef CONFIG_IRQ_TIME_ACCOUNTING 142\tirq_delta = irq_time_read(cpu_of(rq)) - rq-\u0026gt;prev_irq_time; 143 144\t/* 145* Since irq_time is only updated on {soft,}irq_exit, we might run into 146* this case when a previous update_rq_clock() happened inside a 147* {soft,}irq region. 148* 149* When this happens, we stop -\u0026gt;clock_task and only update the 150* prev_irq_time stamp to account for the part that fit, so that a next 151* update will consume the rest. This ensures -\u0026gt;clock_task is 152* monotonic. 153* 154* It does however cause some slight miss-attribution of {soft,}irq 155* time, a more accurate solution would be to update the irq_time using 156* the current rq-\u0026gt;clock timestamp, except that would require using 157* atomic ops. 158*/ 159\tif (irq_delta \u0026gt; delta) 160\tirq_delta = delta; 161 162\trq-\u0026gt;prev_irq_time += irq_delta; 163\tdelta -= irq_delta; 164#endif 165#ifdef CONFIG_PARAVIRT_TIME_ACCOUNTING 166\tif (static_key_false((\u0026amp;paravirt_steal_rq_enabled))) { 167\tsteal = paravirt_steal_clock(cpu_of(rq)); 168\tsteal -= rq-\u0026gt;prev_steal_time_rq; 169 170\tif (unlikely(steal \u0026gt; delta)) 171\tsteal = delta; 172 173\trq-\u0026gt;prev_steal_time_rq += steal; 174\tdelta -= steal; 175\t} 176#endif 177 178\trq-\u0026gt;clock_task += delta; 179 180#ifdef CONFIG_HAVE_SCHED_AVG_IRQ 181\tif ((irq_delta + steal) \u0026amp;\u0026amp; sched_feat(NONTASK_CAPACITY)) 182\tupdate_irq_load_avg(rq, irq_delta + steal); 183#endif 184} 185 186void update_rq_clock(struct rq *rq) 187{ 188\ts64 delta; 189 190\tlockdep_assert_held(\u0026amp;rq-\u0026gt;lock); 191 192\tif (rq-\u0026gt;clock_update_flags \u0026amp; RQCF_ACT_SKIP) 193\treturn; 194 195#ifdef CONFIG_SCHED_DEBUG 196\tif (sched_feat(WARN_DOUBLE_CLOCK)) 197\tSCHED_WARN_ON(rq-\u0026gt;clock_update_flags \u0026amp; RQCF_UPDATED); 198\trq-\u0026gt;clock_update_flags |= RQCF_UPDATED; 199#endif 200 201\tdelta = sched_clock_cpu(cpu_of(rq)) - rq-\u0026gt;clock; 202\tif (delta \u0026lt; 0) 203\treturn; 204\trq-\u0026gt;clock += delta; 205\tupdate_rq_clock_task(rq, delta); 206} 207 208 209#ifdef CONFIG_SCHED_HRTICK 210/* 211* Use HR-timers to deliver accurate preemption points. 212*/ 213 214static void hrtick_clear(struct rq *rq) 215{ 216\tif (hrtimer_active(\u0026amp;rq-\u0026gt;hrtick_timer)) 217\thrtimer_cancel(\u0026amp;rq-\u0026gt;hrtick_timer); 218} 219 220/* 221* High-resolution timer tick. 222* Runs from hardirq context with interrupts disabled. 223*/ 224static enum hrtimer_restart hrtick(struct hrtimer *timer) 225{ 226\tstruct rq *rq = container_of(timer, struct rq, hrtick_timer); 227\tstruct rq_flags rf; 228 229\tWARN_ON_ONCE(cpu_of(rq) != smp_processor_id()); 230 231\trq_lock(rq, \u0026amp;rf); 232\tupdate_rq_clock(rq); 233\trq-\u0026gt;curr-\u0026gt;sched_class-\u0026gt;task_tick(rq, rq-\u0026gt;curr, 1); 234\trq_unlock(rq, \u0026amp;rf); 235 236\treturn HRTIMER_NORESTART; 237} 238 239#ifdef CONFIG_SMP 240 241static void __hrtick_restart(struct rq *rq) 242{ 243\tstruct hrtimer *timer = \u0026amp;rq-\u0026gt;hrtick_timer; 244 245\thrtimer_start_expires(timer, HRTIMER_MODE_ABS_PINNED); 246} 247 248/* 249* called from hardirq (IPI) context 250*/ 251static void __hrtick_start(void *arg) 252{ 253\tstruct rq *rq = arg; 254\tstruct rq_flags rf; 255 256\trq_lock(rq, \u0026amp;rf); 257\t__hrtick_restart(rq); 258\trq-\u0026gt;hrtick_csd_pending = 0; 259\trq_unlock(rq, \u0026amp;rf); 260} 261 262/* 263* Called to set the hrtick timer state. 264* 265* called with rq-\u0026gt;lock held and irqs disabled 266*/ 267void hrtick_start(struct rq *rq, u64 delay) 268{ 269\tstruct hrtimer *timer = \u0026amp;rq-\u0026gt;hrtick_timer; 270\tktime_t time; 271\ts64 delta; 272 273\t/* 274* Don\u0026#39;t schedule slices shorter than 10000ns, that just 275* doesn\u0026#39;t make sense and can cause timer DoS. 276*/ 277\tdelta = max_t(s64, delay, 10000LL); 278\ttime = ktime_add_ns(timer-\u0026gt;base-\u0026gt;get_time(), delta); 279 280\thrtimer_set_expires(timer, time); 281 282\tif (rq == this_rq()) { 283\t__hrtick_restart(rq); 284\t} else if (!rq-\u0026gt;hrtick_csd_pending) { 285\tsmp_call_function_single_async(cpu_of(rq), \u0026amp;rq-\u0026gt;hrtick_csd); 286\trq-\u0026gt;hrtick_csd_pending = 1; 287\t} 288} 289 290#else 291/* 292* Called to set the hrtick timer state. 293* 294* called with rq-\u0026gt;lock held and irqs disabled 295*/ 296void hrtick_start(struct rq *rq, u64 delay) 297{ 298\t/* 299* Don\u0026#39;t schedule slices shorter than 10000ns, that just 300* doesn\u0026#39;t make sense. Rely on vruntime for fairness. 301*/ 302\tdelay = max_t(u64, delay, 10000LL); 303\thrtimer_start(\u0026amp;rq-\u0026gt;hrtick_timer, ns_to_ktime(delay), 304\tHRTIMER_MODE_REL_PINNED); 305} 306#endif /* CONFIG_SMP */307 308static void hrtick_rq_init(struct rq *rq) 309{ 310#ifdef CONFIG_SMP 311\trq-\u0026gt;hrtick_csd_pending = 0; 312 313\trq-\u0026gt;hrtick_csd.flags = 0; 314\trq-\u0026gt;hrtick_csd.func = __hrtick_start; 315\trq-\u0026gt;hrtick_csd.info = rq; 316#endif 317 318\thrtimer_init(\u0026amp;rq-\u0026gt;hrtick_timer, CLOCK_MONOTONIC, HRTIMER_MODE_REL); 319\trq-\u0026gt;hrtick_timer.function = hrtick; 320} 321#else\t/* CONFIG_SCHED_HRTICK */322static inline void hrtick_clear(struct rq *rq) 323{ 324} 325 326static inline void hrtick_rq_init(struct rq *rq) 327{ 328} 329#endif\t/* CONFIG_SCHED_HRTICK */330 331/* 332* cmpxchg based fetch_or, macro so it works for different integer types 333*/ 334#define fetch_or(ptr, mask)\t\\ 335({\t\\ 336typeof(ptr) _ptr = (ptr);\t\\ 337typeof(mask) _mask = (mask);\t\\ 338typeof(*_ptr) _old, _val = *_ptr;\t\\ 339\\ 340for (;;) {\t\\ 341_old = cmpxchg(_ptr, _val, _val | _mask);\t\\ 342if (_old == _val)\t\\ 343break;\t\\ 344_val = _old;\t\\ 345}\t\\ 346_old;\t\\ 347}) 348 349#if defined(CONFIG_SMP) \u0026amp;\u0026amp; defined(TIF_POLLING_NRFLAG) 350/* 351* Atomically set TIF_NEED_RESCHED and test for TIF_POLLING_NRFLAG, 352* this avoids any races wrt polling state changes and thereby avoids 353* spurious IPIs. 354*/ 355static bool set_nr_and_not_polling(struct task_struct *p) 356{ 357\tstruct thread_info *ti = task_thread_info(p); 358\treturn !(fetch_or(\u0026amp;ti-\u0026gt;flags, _TIF_NEED_RESCHED) \u0026amp; _TIF_POLLING_NRFLAG); 359} 360 361/* 362* Atomically set TIF_NEED_RESCHED if TIF_POLLING_NRFLAG is set. 363* 364* If this returns true, then the idle task promises to call 365* sched_ttwu_pending() and reschedule soon. 366*/ 367static bool set_nr_if_polling(struct task_struct *p) 368{ 369\tstruct thread_info *ti = task_thread_info(p); 370\ttypeof(ti-\u0026gt;flags) old, val = READ_ONCE(ti-\u0026gt;flags); 371 372\tfor (;;) { 373\tif (!(val \u0026amp; _TIF_POLLING_NRFLAG)) 374\treturn false; 375\tif (val \u0026amp; _TIF_NEED_RESCHED) 376\treturn true; 377\told = cmpxchg(\u0026amp;ti-\u0026gt;flags, val, val | _TIF_NEED_RESCHED); 378\tif (old == val) 379\tbreak; 380\tval = old; 381\t} 382\treturn true; 383} 384 385#else 386static bool set_nr_and_not_polling(struct task_struct *p) 387{ 388\tset_tsk_need_resched(p); 389\treturn true; 390} 391 392#ifdef CONFIG_SMP 393static bool set_nr_if_polling(struct task_struct *p) 394{ 395\treturn false; 396} 397#endif 398#endif 399 400void wake_q_add(struct wake_q_head *head, struct task_struct *task) 401{ 402\tstruct wake_q_node *node = \u0026amp;task-\u0026gt;wake_q; 403 404\t/* 405* Atomically grab the task, if -\u0026gt;wake_q is !nil already it means 406* its already queued (either by us or someone else) and will get the 407* wakeup due to that. 408* 409* In order to ensure that a pending wakeup will observe our pending 410* state, even in the failed case, an explicit smp_mb() must be used. 411*/ 412\tsmp_mb__before_atomic(); 413\tif (cmpxchg_relaxed(\u0026amp;node-\u0026gt;next, NULL, WAKE_Q_TAIL)) 414\treturn; 415 416\tget_task_struct(task); 417 418\t/* 419* The head is context local, there can be no concurrency. 420*/ 421\t*head-\u0026gt;lastp = node; 422\thead-\u0026gt;lastp = \u0026amp;node-\u0026gt;next; 423} 424 425void wake_up_q(struct wake_q_head *head) 426{ 427\tstruct wake_q_node *node = head-\u0026gt;first; 428 429\twhile (node != WAKE_Q_TAIL) { 430\tstruct task_struct *task; 431 432\ttask = container_of(node, struct task_struct, wake_q); 433\tBUG_ON(!task); 434\t/* Task can safely be re-inserted now: */ 435\tnode = node-\u0026gt;next; 436\ttask-\u0026gt;wake_q.next = NULL; 437 438\t/* 439* wake_up_process() executes a full barrier, which pairs with 440* the queueing in wake_q_add() so as not to miss wakeups. 441*/ 442\twake_up_process(task); 443\tput_task_struct(task); 444\t} 445} 446 447/* 448* resched_curr - mark rq\u0026#39;s current task \u0026#39;to be rescheduled now\u0026#39;. 449* 450* On UP this means the setting of the need_resched flag, on SMP it 451* might also involve a cross-CPU call to trigger the scheduler on 452* the target CPU. 453*/ 454void resched_curr(struct rq *rq) 455{ 456\tstruct task_struct *curr = rq-\u0026gt;curr; 457\tint cpu; 458 459\tlockdep_assert_held(\u0026amp;rq-\u0026gt;lock); 460 461\tif (test_tsk_need_resched(curr)) 462\treturn; 463 464\tcpu = cpu_of(rq); 465 466\tif (cpu == smp_processor_id()) { 467\tset_tsk_need_resched(curr); 468\tset_preempt_need_resched(); 469\treturn; 470\t} 471 472\tif (set_nr_and_not_polling(curr)) 473\tsmp_send_reschedule(cpu); 474\telse 475\ttrace_sched_wake_idle_without_ipi(cpu); 476} 477 478void resched_cpu(int cpu) 479{ 480\tstruct rq *rq = cpu_rq(cpu); 481\tunsigned long flags; 482 483\traw_spin_lock_irqsave(\u0026amp;rq-\u0026gt;lock, flags); 484\tif (cpu_online(cpu) || cpu == smp_processor_id()) 485\tresched_curr(rq); 486\traw_spin_unlock_irqrestore(\u0026amp;rq-\u0026gt;lock, flags); 487} 488 489#ifdef CONFIG_SMP 490#ifdef CONFIG_NO_HZ_COMMON 491/* 492* In the semi idle case, use the nearest busy CPU for migrating timers 493* from an idle CPU. This is good for power-savings. 494* 495* We don\u0026#39;t do similar optimization for completely idle system, as 496* selecting an idle CPU will add more delays to the timers than intended 497* (as that CPU\u0026#39;s timer base may not be uptodate wrt jiffies etc). 498*/ 499int get_nohz_timer_target(void) 500{ 501\tint i, cpu = smp_processor_id(); 502\tstruct sched_domain *sd; 503 504\tif (!idle_cpu(cpu) \u0026amp;\u0026amp; housekeeping_cpu(cpu, HK_FLAG_TIMER)) 505\treturn cpu; 506 507\trcu_read_lock(); 508\tfor_each_domain(cpu, sd) { 509\tfor_each_cpu(i, sched_domain_span(sd)) { 510\tif (cpu == i) 511\tcontinue; 512 513\tif (!idle_cpu(i) \u0026amp;\u0026amp; housekeeping_cpu(i, HK_FLAG_TIMER)) { 514\tcpu = i; 515\tgoto unlock; 516\t} 517\t} 518\t} 519 520\tif (!housekeeping_cpu(cpu, HK_FLAG_TIMER)) 521\tcpu = housekeeping_any_cpu(HK_FLAG_TIMER); 522unlock: 523\trcu_read_unlock(); 524\treturn cpu; 525} 526 527/* 528* When add_timer_on() enqueues a timer into the timer wheel of an 529* idle CPU then this timer might expire before the next timer event 530* which is scheduled to wake up that CPU. In case of a completely 531* idle system the next event might even be infinite time into the 532* future. wake_up_idle_cpu() ensures that the CPU is woken up and 533* leaves the inner idle loop so the newly added timer is taken into 534* account when the CPU goes back to idle and evaluates the timer 535* wheel for the next timer event. 536*/ 537static void wake_up_idle_cpu(int cpu) 538{ 539\tstruct rq *rq = cpu_rq(cpu); 540 541\tif (cpu == smp_processor_id()) 542\treturn; 543 544\tif (set_nr_and_not_polling(rq-\u0026gt;idle)) 545\tsmp_send_reschedule(cpu); 546\telse 547\ttrace_sched_wake_idle_without_ipi(cpu); 548} 549 550static bool wake_up_full_nohz_cpu(int cpu) 551{ 552\t/* 553* We just need the target to call irq_exit() and re-evaluate 554* the next tick. The nohz full kick at least implies that. 555* If needed we can still optimize that later with an 556* empty IRQ. 557*/ 558\tif (cpu_is_offline(cpu)) 559\treturn true; /* Don\u0026#39;t try to wake offline CPUs. */ 560\tif (tick_nohz_full_cpu(cpu)) { 561\tif (cpu != smp_processor_id() || 562\ttick_nohz_tick_stopped()) 563\ttick_nohz_full_kick_cpu(cpu); 564\treturn true; 565\t} 566 567\treturn false; 568} 569 570/* 571* Wake up the specified CPU. If the CPU is going offline, it is the 572* caller\u0026#39;s responsibility to deal with the lost wakeup, for example, 573* by hooking into the CPU_DEAD notifier like timers and hrtimers do. 574*/ 575void wake_up_nohz_cpu(int cpu) 576{ 577\tif (!wake_up_full_nohz_cpu(cpu)) 578\twake_up_idle_cpu(cpu); 579} 580 581static inline bool got_nohz_idle_kick(void) 582{ 583\tint cpu = smp_processor_id(); 584 585\tif (!(atomic_read(nohz_flags(cpu)) \u0026amp; NOHZ_KICK_MASK)) 586\treturn false; 587 588\tif (idle_cpu(cpu) \u0026amp;\u0026amp; !need_resched()) 589\treturn true; 590 591\t/* 592* We can\u0026#39;t run Idle Load Balance on this CPU for this time so we 593* cancel it and clear NOHZ_BALANCE_KICK 594*/ 595\tatomic_andnot(NOHZ_KICK_MASK, nohz_flags(cpu)); 596\treturn false; 597} 598 599#else /* CONFIG_NO_HZ_COMMON */600 601static inline bool got_nohz_idle_kick(void) 602{ 603\treturn false; 604} 605 606#endif /* CONFIG_NO_HZ_COMMON */607 608#ifdef CONFIG_NO_HZ_FULL 609bool sched_can_stop_tick(struct rq *rq) 610{ 611\tint fifo_nr_running; 612 613\t/* Deadline tasks, even if single, need the tick */ 614\tif (rq-\u0026gt;dl.dl_nr_running) 615\treturn false; 616 617\t/* 618* If there are more than one RR tasks, we need the tick to effect the 619* actual RR behaviour. 620*/ 621\tif (rq-\u0026gt;rt.rr_nr_running) { 622\tif (rq-\u0026gt;rt.rr_nr_running == 1) 623\treturn true; 624\telse 625\treturn false; 626\t} 627 628\t/* 629* If there\u0026#39;s no RR tasks, but FIFO tasks, we can skip the tick, no 630* forced preemption between FIFO tasks. 631*/ 632\tfifo_nr_running = rq-\u0026gt;rt.rt_nr_running - rq-\u0026gt;rt.rr_nr_running; 633\tif (fifo_nr_running) 634\treturn true; 635 636\t/* 637* If there are no DL,RR/FIFO tasks, there must only be CFS tasks left; 638* if there\u0026#39;s more than one we need the tick for involuntary 639* preemption. 640*/ 641\tif (rq-\u0026gt;nr_running \u0026gt; 1) 642\treturn false; 643 644\treturn true; 645} 646#endif /* CONFIG_NO_HZ_FULL */647#endif /* CONFIG_SMP */648 649#if defined(CONFIG_RT_GROUP_SCHED) || (defined(CONFIG_FAIR_GROUP_SCHED) \u0026amp;\u0026amp; \\ 650(defined(CONFIG_SMP) || defined(CONFIG_CFS_BANDWIDTH))) 651/* 652* Iterate task_group tree rooted at *from, calling @down when first entering a 653* node and @up when leaving it for the final time. 654* 655* Caller must hold rcu_lock or sufficient equivalent. 656*/ 657int walk_tg_tree_from(struct task_group *from, 658\ttg_visitor down, tg_visitor up, void *data) 659{ 660\tstruct task_group *parent, *child; 661\tint ret; 662 663\tparent = from; 664 665down: 666\tret = (*down)(parent, data); 667\tif (ret) 668\tgoto out; 669\tlist_for_each_entry_rcu(child, \u0026amp;parent-\u0026gt;children, siblings) { 670\tparent = child; 671\tgoto down; 672 673up: 674\tcontinue; 675\t} 676\tret = (*up)(parent, data); 677\tif (ret || parent == from) 678\tgoto out; 679 680\tchild = parent; 681\tparent = parent-\u0026gt;parent; 682\tif (parent) 683\tgoto up; 684out: 685\treturn ret; 686} 687 688int tg_nop(struct task_group *tg, void *data) 689{ 690\treturn 0; 691} 692#endif 693 694static void set_load_weight(struct task_struct *p, bool update_load) 695{ 696\tint prio = p-\u0026gt;static_prio - MAX_RT_PRIO; 697\tstruct load_weight *load = \u0026amp;p-\u0026gt;se.load; 698 699\t/* 700* SCHED_IDLE tasks get minimal weight: 701*/ 702\tif (idle_policy(p-\u0026gt;policy)) { 703\tload-\u0026gt;weight = scale_load(WEIGHT_IDLEPRIO); 704\tload-\u0026gt;inv_weight = WMULT_IDLEPRIO; 705\treturn; 706\t} 707 708\t/* 709* SCHED_OTHER tasks have to update their load when changing their 710* weight 711*/ 712\tif (update_load \u0026amp;\u0026amp; p-\u0026gt;sched_class == \u0026amp;fair_sched_class) { 713\treweight_task(p, prio); 714\t} else { 715\tload-\u0026gt;weight = scale_load(sched_prio_to_weight[prio]); 716\tload-\u0026gt;inv_weight = sched_prio_to_wmult[prio]; 717\t} 718} 719 720static inline void enqueue_task(struct rq *rq, struct task_struct *p, int flags) 721{ 722\tif (!(flags \u0026amp; ENQUEUE_NOCLOCK)) 723\tupdate_rq_clock(rq); 724 725\tif (!(flags \u0026amp; ENQUEUE_RESTORE)) 726\tsched_info_queued(rq, p); 727 728\tp-\u0026gt;sched_class-\u0026gt;enqueue_task(rq, p, flags); 729} 730 731static inline void dequeue_task(struct rq *rq, struct task_struct *p, int flags) 732{ 733\tif (!(flags \u0026amp; DEQUEUE_NOCLOCK)) 734\tupdate_rq_clock(rq); 735 736\tif (!(flags \u0026amp; DEQUEUE_SAVE)) 737\tsched_info_dequeued(rq, p); 738 739\tp-\u0026gt;sched_class-\u0026gt;dequeue_task(rq, p, flags); 740} 741 742void activate_task(struct rq *rq, struct task_struct *p, int flags) 743{ 744\tif (task_contributes_to_load(p)) 745\trq-\u0026gt;nr_uninterruptible--; 746 747\tenqueue_task(rq, p, flags); 748} 749 750void deactivate_task(struct rq *rq, struct task_struct *p, int flags) 751{ 752\tif (task_contributes_to_load(p)) 753\trq-\u0026gt;nr_uninterruptible++; 754 755\tdequeue_task(rq, p, flags); 756} 757 758/* 759* __normal_prio - return the priority that is based on the static prio 760*/ 761static inline int __normal_prio(struct task_struct *p) 762{ 763\treturn p-\u0026gt;static_prio; 764} 765 766/* 767* Calculate the expected normal priority: i.e. priority 768* without taking RT-inheritance into account. Might be 769* boosted by interactivity modifiers. Changes upon fork, 770* setprio syscalls, and whenever the interactivity 771* estimator recalculates. 772*/ 773static inline int normal_prio(struct task_struct *p) 774{ 775\tint prio; 776 777\tif (task_has_dl_policy(p)) 778\tprio = MAX_DL_PRIO-1; 779\telse if (task_has_rt_policy(p)) 780\tprio = MAX_RT_PRIO-1 - p-\u0026gt;rt_priority; 781\telse 782\tprio = __normal_prio(p); 783\treturn prio; 784} 785 786/* 787* Calculate the current priority, i.e. the priority 788* taken into account by the scheduler. This value might 789* be boosted by RT tasks, or might be boosted by 790* interactivity modifiers. Will be RT if the task got 791* RT-boosted. If not then it returns p-\u0026gt;normal_prio. 792*/ 793static int effective_prio(struct task_struct *p) 794{ 795\tp-\u0026gt;normal_prio = normal_prio(p); 796\t/* 797* If we are RT tasks or we were boosted to RT priority, 798* keep the priority unchanged. Otherwise, update priority 799* to the normal priority: 800*/ 801\tif (!rt_prio(p-\u0026gt;prio)) 802\treturn p-\u0026gt;normal_prio; 803\treturn p-\u0026gt;prio; 804} 805 806/** 807* task_curr - is this task currently executing on a CPU? 808* @p: the task in question. 809* 810* Return: 1 if the task is currently executing. 0 otherwise. 811*/ 812inline int task_curr(const struct task_struct *p) 813{ 814\treturn cpu_curr(task_cpu(p)) == p; 815} 816 817/* 818* switched_from, switched_to and prio_changed must _NOT_ drop rq-\u0026gt;lock, 819* use the balance_callback list if you want balancing. 820* 821* this means any call to check_class_changed() must be followed by a call to 822* balance_callback(). 823*/ 824static inline void check_class_changed(struct rq *rq, struct task_struct *p, 825\tconst struct sched_class *prev_class, 826\tint oldprio) 827{ 828\tif (prev_class != p-\u0026gt;sched_class) { 829\tif (prev_class-\u0026gt;switched_from) 830\tprev_class-\u0026gt;switched_from(rq, p); 831 832\tp-\u0026gt;sched_class-\u0026gt;switched_to(rq, p); 833\t} else if (oldprio != p-\u0026gt;prio || dl_task(p)) 834\tp-\u0026gt;sched_class-\u0026gt;prio_changed(rq, p, oldprio); 835} 836 837void check_preempt_curr(struct rq *rq, struct task_struct *p, int flags) 838{ 839\tconst struct sched_class *class; 840 841\tif (p-\u0026gt;sched_class == rq-\u0026gt;curr-\u0026gt;sched_class) { 842\trq-\u0026gt;curr-\u0026gt;sched_class-\u0026gt;check_preempt_curr(rq, p, flags); 843\t} else { 844\tfor_each_class(class) { 845\tif (class == rq-\u0026gt;curr-\u0026gt;sched_class) 846\tbreak; 847\tif (class == p-\u0026gt;sched_class) { 848\tresched_curr(rq); 849\tbreak; 850\t} 851\t} 852\t} 853 854\t/* 855* A queue event has occurred, and we\u0026#39;re going to schedule. In 856* this case, we can save a useless back to back clock update. 857*/ 858\tif (task_on_rq_queued(rq-\u0026gt;curr) \u0026amp;\u0026amp; test_tsk_need_resched(rq-\u0026gt;curr)) 859\trq_clock_skip_update(rq); 860} 861 862#ifdef CONFIG_SMP 863 864static inline bool is_per_cpu_kthread(struct task_struct *p) 865{ 866\tif (!(p-\u0026gt;flags \u0026amp; PF_KTHREAD)) 867\treturn false; 868 869\tif (p-\u0026gt;nr_cpus_allowed != 1) 870\treturn false; 871 872\treturn true; 873} 874 875/* 876* Per-CPU kthreads are allowed to run on !actie \u0026amp;\u0026amp; online CPUs, see 877* __set_cpus_allowed_ptr() and select_fallback_rq(). 878*/ 879static inline bool is_cpu_allowed(struct task_struct *p, int cpu) 880{ 881\tif (!cpumask_test_cpu(cpu, \u0026amp;p-\u0026gt;cpus_allowed)) 882\treturn false; 883 884\tif (is_per_cpu_kthread(p)) 885\treturn cpu_online(cpu); 886 887\treturn cpu_active(cpu); 888} 889 890/* 891* This is how migration works: 892* 893* 1) we invoke migration_cpu_stop() on the target CPU using 894* stop_one_cpu(). 895* 2) stopper starts to run (implicitly forcing the migrated thread 896* off the CPU) 897* 3) it checks whether the migrated task is still in the wrong runqueue. 898* 4) if it\u0026#39;s in the wrong runqueue then the migration thread removes 899* it and puts it into the right queue. 900* 5) stopper completes and stop_one_cpu() returns and the migration 901* is done. 902*/ 903 904/* 905* move_queued_task - move a queued task to new rq. 906* 907* Returns (locked) new rq. Old rq\u0026#39;s lock is released. 908*/ 909static struct rq *move_queued_task(struct rq *rq, struct rq_flags *rf, 910\tstruct task_struct *p, int new_cpu) 911{ 912\tlockdep_assert_held(\u0026amp;rq-\u0026gt;lock); 913 914\tWRITE_ONCE(p-\u0026gt;on_rq, TASK_ON_RQ_MIGRATING); 915\tdequeue_task(rq, p, DEQUEUE_NOCLOCK); 916\tset_task_cpu(p, new_cpu); 917\trq_unlock(rq, rf); 918 919\trq = cpu_rq(new_cpu); 920 921\trq_lock(rq, rf); 922\tBUG_ON(task_cpu(p) != new_cpu); 923\tenqueue_task(rq, p, 0); 924\tp-\u0026gt;on_rq = TASK_ON_RQ_QUEUED; 925\tcheck_preempt_curr(rq, p, 0); 926 927\treturn rq; 928} 929 930struct migration_arg { 931\tstruct task_struct *task; 932\tint dest_cpu; 933}; 934 935/* 936* Move (not current) task off this CPU, onto the destination CPU. We\u0026#39;re doing 937* this because either it can\u0026#39;t run here any more (set_cpus_allowed() 938* away from this CPU, or CPU going down), or because we\u0026#39;re 939* attempting to rebalance this task on exec (sched_exec). 940* 941* So we race with normal scheduler movements, but that\u0026#39;s OK, as long 942* as the task is no longer on this CPU. 943*/ 944static struct rq *__migrate_task(struct rq *rq, struct rq_flags *rf, 945\tstruct task_struct *p, int dest_cpu) 946{ 947\t/* Affinity changed (again). */ 948\tif (!is_cpu_allowed(p, dest_cpu)) 949\treturn rq; 950 951\tupdate_rq_clock(rq); 952\trq = move_queued_task(rq, rf, p, dest_cpu); 953 954\treturn rq; 955} 956 957/* 958* migration_cpu_stop - this will be executed by a highprio stopper thread 959* and performs thread migration by bumping thread off CPU then 960* \u0026#39;pushing\u0026#39; onto another runqueue. 961*/ 962static int migration_cpu_stop(void *data) 963{ 964\tstruct migration_arg *arg = data; 965\tstruct task_struct *p = arg-\u0026gt;task; 966\tstruct rq *rq = this_rq(); 967\tstruct rq_flags rf; 968 969\t/* 970* The original target CPU might have gone down and we might 971* be on another CPU but it doesn\u0026#39;t matter. 972*/ 973\tlocal_irq_disable(); 974\t/* 975* We need to explicitly wake pending tasks before running 976* __migrate_task() such that we will not miss enforcing cpus_allowed 977* during wakeups, see set_cpus_allowed_ptr()\u0026#39;s TASK_WAKING test. 978*/ 979\tsched_ttwu_pending(); 980 981\traw_spin_lock(\u0026amp;p-\u0026gt;pi_lock); 982\trq_lock(rq, \u0026amp;rf); 983\t/* 984* If task_rq(p) != rq, it cannot be migrated here, because we\u0026#39;re 985* holding rq-\u0026gt;lock, if p-\u0026gt;on_rq == 0 it cannot get enqueued because 986* we\u0026#39;re holding p-\u0026gt;pi_lock. 987*/ 988\tif (task_rq(p) == rq) { 989\tif (task_on_rq_queued(p)) 990\trq = __migrate_task(rq, \u0026amp;rf, p, arg-\u0026gt;dest_cpu); 991\telse 992\tp-\u0026gt;wake_cpu = arg-\u0026gt;dest_cpu; 993\t} 994\trq_unlock(rq, \u0026amp;rf); 995\traw_spin_unlock(\u0026amp;p-\u0026gt;pi_lock); 996 997\tlocal_irq_enable(); 998\treturn 0; 999} 1000 1001/* 1002* sched_class::set_cpus_allowed must do the below, but is not required to 1003* actually call this function. 1004*/ 1005void set_cpus_allowed_common(struct task_struct *p, const struct cpumask *new_mask) 1006{ 1007\tcpumask_copy(\u0026amp;p-\u0026gt;cpus_allowed, new_mask); 1008\tp-\u0026gt;nr_cpus_allowed = cpumask_weight(new_mask); 1009} 1010 1011void do_set_cpus_allowed(struct task_struct *p, const struct cpumask *new_mask) 1012{ 1013\tstruct rq *rq = task_rq(p); 1014\tbool queued, running; 1015 1016\tlockdep_assert_held(\u0026amp;p-\u0026gt;pi_lock); 1017 1018\tqueued = task_on_rq_queued(p); 1019\trunning = task_current(rq, p); 1020 1021\tif (queued) { 1022\t/* 1023* Because __kthread_bind() calls this on blocked tasks without 1024* holding rq-\u0026gt;lock. 1025*/ 1026\tlockdep_assert_held(\u0026amp;rq-\u0026gt;lock); 1027\tdequeue_task(rq, p, DEQUEUE_SAVE | DEQUEUE_NOCLOCK); 1028\t} 1029\tif (running) 1030\tput_prev_task(rq, p); 1031 1032\tp-\u0026gt;sched_class-\u0026gt;set_cpus_allowed(p, new_mask); 1033 1034\tif (queued) 1035\tenqueue_task(rq, p, ENQUEUE_RESTORE | ENQUEUE_NOCLOCK); 1036\tif (running) 1037\tset_curr_task(rq, p); 1038} 1039 1040/* 1041* Change a given task\u0026#39;s CPU affinity. Migrate the thread to a 1042* proper CPU and schedule it away if the CPU it\u0026#39;s executing on 1043* is removed from the allowed bitmask. 1044* 1045* NOTE: the caller must have a valid reference to the task, the 1046* task must not exit() \u0026amp; deallocate itself prematurely. The 1047* call is not atomic; no spinlocks may be held. 1048*/ 1049static int __set_cpus_allowed_ptr(struct task_struct *p, 1050\tconst struct cpumask *new_mask, bool check) 1051{ 1052\tconst struct cpumask *cpu_valid_mask = cpu_active_mask; 1053\tunsigned int dest_cpu; 1054\tstruct rq_flags rf; 1055\tstruct rq *rq; 1056\tint ret = 0; 1057 1058\trq = task_rq_lock(p, \u0026amp;rf); 1059\tupdate_rq_clock(rq); 1060 1061\tif (p-\u0026gt;flags \u0026amp; PF_KTHREAD) { 1062\t/* 1063* Kernel threads are allowed on online \u0026amp;\u0026amp; !active CPUs 1064*/ 1065\tcpu_valid_mask = cpu_online_mask; 1066\t} 1067 1068\t/* 1069* Must re-check here, to close a race against __kthread_bind(), 1070* sched_setaffinity() is not guaranteed to observe the flag. 1071*/ 1072\tif (check \u0026amp;\u0026amp; (p-\u0026gt;flags \u0026amp; PF_NO_SETAFFINITY)) { 1073\tret = -EINVAL; 1074\tgoto out; 1075\t} 1076 1077\tif (cpumask_equal(\u0026amp;p-\u0026gt;cpus_allowed, new_mask)) 1078\tgoto out; 1079 1080\tdest_cpu = cpumask_any_and(cpu_valid_mask, new_mask); 1081\tif (dest_cpu \u0026gt;= nr_cpu_ids) { 1082\tret = -EINVAL; 1083\tgoto out; 1084\t} 1085 1086\tdo_set_cpus_allowed(p, new_mask); 1087 1088\tif (p-\u0026gt;flags \u0026amp; PF_KTHREAD) { 1089\t/* 1090* For kernel threads that do indeed end up on online \u0026amp;\u0026amp; 1091* !active we want to ensure they are strict per-CPU threads. 1092*/ 1093\tWARN_ON(cpumask_intersects(new_mask, cpu_online_mask) \u0026amp;\u0026amp; 1094\t!cpumask_intersects(new_mask, cpu_active_mask) \u0026amp;\u0026amp; 1095\tp-\u0026gt;nr_cpus_allowed != 1); 1096\t} 1097 1098\t/* Can the task run on the task\u0026#39;s current CPU? If so, we\u0026#39;re done */ 1099\tif (cpumask_test_cpu(task_cpu(p), new_mask)) 1100\tgoto out; 1101 1102\tif (task_running(rq, p) || p-\u0026gt;state == TASK_WAKING) { 1103\tstruct migration_arg arg = { p, dest_cpu }; 1104\t/* Need help from migration thread: drop lock and wait. */ 1105\ttask_rq_unlock(rq, p, \u0026amp;rf); 1106\tstop_one_cpu(cpu_of(rq), migration_cpu_stop, \u0026amp;arg); 1107\ttlb_migrate_finish(p-\u0026gt;mm); 1108\treturn 0; 1109\t} else if (task_on_rq_queued(p)) { 1110\t/* 1111* OK, since we\u0026#39;re going to drop the lock immediately 1112* afterwards anyway. 1113*/ 1114\trq = move_queued_task(rq, \u0026amp;rf, p, dest_cpu); 1115\t} 1116out: 1117\ttask_rq_unlock(rq, p, \u0026amp;rf); 1118 1119\treturn ret; 1120} 1121 1122int set_cpus_allowed_ptr(struct task_struct *p, const struct cpumask *new_mask) 1123{ 1124\treturn __set_cpus_allowed_ptr(p, new_mask, false); 1125} 1126EXPORT_SYMBOL_GPL(set_cpus_allowed_ptr); 1127 1128void set_task_cpu(struct task_struct *p, unsigned int new_cpu) 1129{ 1130#ifdef CONFIG_SCHED_DEBUG 1131\t/* 1132* We should never call set_task_cpu() on a blocked task, 1133* ttwu() will sort out the placement. 1134*/ 1135\tWARN_ON_ONCE(p-\u0026gt;state != TASK_RUNNING \u0026amp;\u0026amp; p-\u0026gt;state != TASK_WAKING \u0026amp;\u0026amp; 1136\t!p-\u0026gt;on_rq); 1137 1138\t/* 1139* Migrating fair class task must have p-\u0026gt;on_rq = TASK_ON_RQ_MIGRATING, 1140* because schedstat_wait_{start,end} rebase migrating task\u0026#39;s wait_start 1141* time relying on p-\u0026gt;on_rq. 1142*/ 1143\tWARN_ON_ONCE(p-\u0026gt;state == TASK_RUNNING \u0026amp;\u0026amp; 1144\tp-\u0026gt;sched_class == \u0026amp;fair_sched_class \u0026amp;\u0026amp; 1145\t(p-\u0026gt;on_rq \u0026amp;\u0026amp; !task_on_rq_migrating(p))); 1146 1147#ifdef CONFIG_LOCKDEP 1148\t/* 1149* The caller should hold either p-\u0026gt;pi_lock or rq-\u0026gt;lock, when changing 1150* a task\u0026#39;s CPU. -\u0026gt;pi_lock for waking tasks, rq-\u0026gt;lock for runnable tasks. 1151* 1152* sched_move_task() holds both and thus holding either pins the cgroup, 1153* see task_group(). 1154* 1155* Furthermore, all task_rq users should acquire both locks, see 1156* task_rq_lock(). 1157*/ 1158\tWARN_ON_ONCE(debug_locks \u0026amp;\u0026amp; !(lockdep_is_held(\u0026amp;p-\u0026gt;pi_lock) || 1159\tlockdep_is_held(\u0026amp;task_rq(p)-\u0026gt;lock))); 1160#endif 1161\t/* 1162* Clearly, migrating tasks to offline CPUs is a fairly daft thing. 1163*/ 1164\tWARN_ON_ONCE(!cpu_online(new_cpu)); 1165#endif 1166 1167\ttrace_sched_migrate_task(p, new_cpu); 1168 1169\tif (task_cpu(p) != new_cpu) { 1170\tif (p-\u0026gt;sched_class-\u0026gt;migrate_task_rq) 1171\tp-\u0026gt;sched_class-\u0026gt;migrate_task_rq(p, new_cpu); 1172\tp-\u0026gt;se.nr_migrations++; 1173\trseq_migrate(p); 1174\tperf_event_task_migrate(p); 1175\t} 1176 1177\t__set_task_cpu(p, new_cpu); 1178} 1179 1180#ifdef CONFIG_NUMA_BALANCING 1181static void __migrate_swap_task(struct task_struct *p, int cpu) 1182{ 1183\tif (task_on_rq_queued(p)) { 1184\tstruct rq *src_rq, *dst_rq; 1185\tstruct rq_flags srf, drf; 1186 1187\tsrc_rq = task_rq(p); 1188\tdst_rq = cpu_rq(cpu); 1189 1190\trq_pin_lock(src_rq, \u0026amp;srf); 1191\trq_pin_lock(dst_rq, \u0026amp;drf); 1192 1193\tp-\u0026gt;on_rq = TASK_ON_RQ_MIGRATING; 1194\tdeactivate_task(src_rq, p, 0); 1195\tset_task_cpu(p, cpu); 1196\tactivate_task(dst_rq, p, 0); 1197\tp-\u0026gt;on_rq = TASK_ON_RQ_QUEUED; 1198\tcheck_preempt_curr(dst_rq, p, 0); 1199 1200\trq_unpin_lock(dst_rq, \u0026amp;drf); 1201\trq_unpin_lock(src_rq, \u0026amp;srf); 1202 1203\t} else { 1204\t/* 1205* Task isn\u0026#39;t running anymore; make it appear like we migrated 1206* it before it went to sleep. This means on wakeup we make the 1207* previous CPU our target instead of where it really is. 1208*/ 1209\tp-\u0026gt;wake_cpu = cpu; 1210\t} 1211} 1212 1213struct migration_swap_arg { 1214\tstruct task_struct *src_task, *dst_task; 1215\tint src_cpu, dst_cpu; 1216}; 1217 1218static int migrate_swap_stop(void *data) 1219{ 1220\tstruct migration_swap_arg *arg = data; 1221\tstruct rq *src_rq, *dst_rq; 1222\tint ret = -EAGAIN; 1223 1224\tif (!cpu_active(arg-\u0026gt;src_cpu) || !cpu_active(arg-\u0026gt;dst_cpu)) 1225\treturn -EAGAIN; 1226 1227\tsrc_rq = cpu_rq(arg-\u0026gt;src_cpu); 1228\tdst_rq = cpu_rq(arg-\u0026gt;dst_cpu); 1229 1230\tdouble_raw_lock(\u0026amp;arg-\u0026gt;src_task-\u0026gt;pi_lock, 1231\t\u0026amp;arg-\u0026gt;dst_task-\u0026gt;pi_lock); 1232\tdouble_rq_lock(src_rq, dst_rq); 1233 1234\tif (task_cpu(arg-\u0026gt;dst_task) != arg-\u0026gt;dst_cpu) 1235\tgoto unlock; 1236 1237\tif (task_cpu(arg-\u0026gt;src_task) != arg-\u0026gt;src_cpu) 1238\tgoto unlock; 1239 1240\tif (!cpumask_test_cpu(arg-\u0026gt;dst_cpu, \u0026amp;arg-\u0026gt;src_task-\u0026gt;cpus_allowed)) 1241\tgoto unlock; 1242 1243\tif (!cpumask_test_cpu(arg-\u0026gt;src_cpu, \u0026amp;arg-\u0026gt;dst_task-\u0026gt;cpus_allowed)) 1244\tgoto unlock; 1245 1246\t__migrate_swap_task(arg-\u0026gt;src_task, arg-\u0026gt;dst_cpu); 1247\t__migrate_swap_task(arg-\u0026gt;dst_task, arg-\u0026gt;src_cpu); 1248 1249\tret = 0; 1250 1251unlock: 1252\tdouble_rq_unlock(src_rq, dst_rq); 1253\traw_spin_unlock(\u0026amp;arg-\u0026gt;dst_task-\u0026gt;pi_lock); 1254\traw_spin_unlock(\u0026amp;arg-\u0026gt;src_task-\u0026gt;pi_lock); 1255 1256\treturn ret; 1257} 1258 1259/* 1260* Cross migrate two tasks 1261*/ 1262int migrate_swap(struct task_struct *cur, struct task_struct *p, 1263\tint target_cpu, int curr_cpu) 1264{ 1265\tstruct migration_swap_arg arg; 1266\tint ret = -EINVAL; 1267 1268\targ = (struct migration_swap_arg){ 1269\t.src_task = cur, 1270\t.src_cpu = curr_cpu, 1271\t.dst_task = p, 1272\t.dst_cpu = target_cpu, 1273\t}; 1274 1275\tif (arg.src_cpu == arg.dst_cpu) 1276\tgoto out; 1277 1278\t/* 1279* These three tests are all lockless; this is OK since all of them 1280* will be re-checked with proper locks held further down the line. 1281*/ 1282\tif (!cpu_active(arg.src_cpu) || !cpu_active(arg.dst_cpu)) 1283\tgoto out; 1284 1285\tif (!cpumask_test_cpu(arg.dst_cpu, \u0026amp;arg.src_task-\u0026gt;cpus_allowed)) 1286\tgoto out; 1287 1288\tif (!cpumask_test_cpu(arg.src_cpu, \u0026amp;arg.dst_task-\u0026gt;cpus_allowed)) 1289\tgoto out; 1290 1291\ttrace_sched_swap_numa(cur, arg.src_cpu, p, arg.dst_cpu); 1292\tret = stop_two_cpus(arg.dst_cpu, arg.src_cpu, migrate_swap_stop, \u0026amp;arg); 1293 1294out: 1295\treturn ret; 1296} 1297#endif /* CONFIG_NUMA_BALANCING */1298 1299/* 1300* wait_task_inactive - wait for a thread to unschedule. 1301* 1302* If @match_state is nonzero, it\u0026#39;s the @p-\u0026gt;state value just checked and 1303* not expected to change. If it changes, i.e. @p might have woken up, 1304* then return zero. When we succeed in waiting for @p to be off its CPU, 1305* we return a positive number (its total switch count). If a second call 1306* a short while later returns the same number, the caller can be sure that 1307* @p has remained unscheduled the whole time. 1308* 1309* The caller must ensure that the task *will* unschedule sometime soon, 1310* else this function might spin for a *long* time. This function can\u0026#39;t 1311* be called with interrupts off, or it may introduce deadlock with 1312* smp_call_function() if an IPI is sent by the same process we are 1313* waiting to become inactive. 1314*/ 1315unsigned long wait_task_inactive(struct task_struct *p, long match_state) 1316{ 1317\tint running, queued; 1318\tstruct rq_flags rf; 1319\tunsigned long ncsw; 1320\tstruct rq *rq; 1321 1322\tfor (;;) { 1323\t/* 1324* We do the initial early heuristics without holding 1325* any task-queue locks at all. We\u0026#39;ll only try to get 1326* the runqueue lock when things look like they will 1327* work out! 1328*/ 1329\trq = task_rq(p); 1330 1331\t/* 1332* If the task is actively running on another CPU 1333* still, just relax and busy-wait without holding 1334* any locks. 1335* 1336* NOTE! Since we don\u0026#39;t hold any locks, it\u0026#39;s not 1337* even sure that \u0026#34;rq\u0026#34; stays as the right runqueue! 1338* But we don\u0026#39;t care, since \u0026#34;task_running()\u0026#34; will 1339* return false if the runqueue has changed and p 1340* is actually now running somewhere else! 1341*/ 1342\twhile (task_running(rq, p)) { 1343\tif (match_state \u0026amp;\u0026amp; unlikely(p-\u0026gt;state != match_state)) 1344\treturn 0; 1345\tcpu_relax(); 1346\t} 1347 1348\t/* 1349* Ok, time to look more closely! We need the rq 1350* lock now, to be *sure*. If we\u0026#39;re wrong, we\u0026#39;ll 1351* just go back and repeat. 1352*/ 1353\trq = task_rq_lock(p, \u0026amp;rf); 1354\ttrace_sched_wait_task(p); 1355\trunning = task_running(rq, p); 1356\tqueued = task_on_rq_queued(p); 1357\tncsw = 0; 1358\tif (!match_state || p-\u0026gt;state == match_state) 1359\tncsw = p-\u0026gt;nvcsw | LONG_MIN; /* sets MSB */ 1360\ttask_rq_unlock(rq, p, \u0026amp;rf); 1361 1362\t/* 1363* If it changed from the expected state, bail out now. 1364*/ 1365\tif (unlikely(!ncsw)) 1366\tbreak; 1367 1368\t/* 1369* Was it really running after all now that we 1370* checked with the proper locks actually held? 1371* 1372* Oops. Go back and try again.. 1373*/ 1374\tif (unlikely(running)) { 1375\tcpu_relax(); 1376\tcontinue; 1377\t} 1378 1379\t/* 1380* It\u0026#39;s not enough that it\u0026#39;s not actively running, 1381* it must be off the runqueue _entirely_, and not 1382* preempted! 1383* 1384* So if it was still runnable (but just not actively 1385* running right now), it\u0026#39;s preempted, and we should 1386* yield - it could be a while. 1387*/ 1388\tif (unlikely(queued)) { 1389\tktime_t to = NSEC_PER_SEC / HZ; 1390 1391\tset_current_state(TASK_UNINTERRUPTIBLE); 1392\tschedule_hrtimeout(\u0026amp;to, HRTIMER_MODE_REL); 1393\tcontinue; 1394\t} 1395 1396\t/* 1397* Ahh, all good. It wasn\u0026#39;t running, and it wasn\u0026#39;t 1398* runnable, which means that it will never become 1399* running in the future either. We\u0026#39;re all done! 1400*/ 1401\tbreak; 1402\t} 1403 1404\treturn ncsw; 1405} 1406 1407/*** 1408* kick_process - kick a running thread to enter/exit the kernel 1409* @p: the to-be-kicked thread 1410* 1411* Cause a process which is running on another CPU to enter 1412* kernel-mode, without any delay. (to get signals handled.) 1413* 1414* NOTE: this function doesn\u0026#39;t have to take the runqueue lock, 1415* because all it wants to ensure is that the remote task enters 1416* the kernel. If the IPI races and the task has been migrated 1417* to another CPU then no harm is done and the purpose has been 1418* achieved as well. 1419*/ 1420void kick_process(struct task_struct *p) 1421{ 1422\tint cpu; 1423 1424\tpreempt_disable(); 1425\tcpu = task_cpu(p); 1426\tif ((cpu != smp_processor_id()) \u0026amp;\u0026amp; task_curr(p)) 1427\tsmp_send_reschedule(cpu); 1428\tpreempt_enable(); 1429} 1430EXPORT_SYMBOL_GPL(kick_process); 1431 1432/* 1433* -\u0026gt;cpus_allowed is protected by both rq-\u0026gt;lock and p-\u0026gt;pi_lock 1434* 1435* A few notes on cpu_active vs cpu_online: 1436* 1437* - cpu_active must be a subset of cpu_online 1438* 1439* - on CPU-up we allow per-CPU kthreads on the online \u0026amp;\u0026amp; !active CPU, 1440* see __set_cpus_allowed_ptr(). At this point the newly online 1441* CPU isn\u0026#39;t yet part of the sched domains, and balancing will not 1442* see it. 1443* 1444* - on CPU-down we clear cpu_active() to mask the sched domains and 1445* avoid the load balancer to place new tasks on the to be removed 1446* CPU. Existing tasks will remain running there and will be taken 1447* off. 1448* 1449* This means that fallback selection must not select !active CPUs. 1450* And can assume that any active CPU must be online. Conversely 1451* select_task_rq() below may allow selection of !active CPUs in order 1452* to satisfy the above rules. 1453*/ 1454static int select_fallback_rq(int cpu, struct task_struct *p) 1455{ 1456\tint nid = cpu_to_node(cpu); 1457\tconst struct cpumask *nodemask = NULL; 1458\tenum { cpuset, possible, fail } state = cpuset; 1459\tint dest_cpu; 1460 1461\t/* 1462* If the node that the CPU is on has been offlined, cpu_to_node() 1463* will return -1. There is no CPU on the node, and we should 1464* select the CPU on the other node. 1465*/ 1466\tif (nid != -1) { 1467\tnodemask = cpumask_of_node(nid); 1468 1469\t/* Look for allowed, online CPU in same node. */ 1470\tfor_each_cpu(dest_cpu, nodemask) { 1471\tif (!cpu_active(dest_cpu)) 1472\tcontinue; 1473\tif (cpumask_test_cpu(dest_cpu, \u0026amp;p-\u0026gt;cpus_allowed)) 1474\treturn dest_cpu; 1475\t} 1476\t} 1477 1478\tfor (;;) { 1479\t/* Any allowed, online CPU? */ 1480\tfor_each_cpu(dest_cpu, \u0026amp;p-\u0026gt;cpus_allowed) { 1481\tif (!is_cpu_allowed(p, dest_cpu)) 1482\tcontinue; 1483 1484\tgoto out; 1485\t} 1486 1487\t/* No more Mr. Nice Guy. */ 1488\tswitch (state) { 1489\tcase cpuset: 1490\tif (IS_ENABLED(CONFIG_CPUSETS)) { 1491\tcpuset_cpus_allowed_fallback(p); 1492\tstate = possible; 1493\tbreak; 1494\t} 1495\t/* Fall-through */ 1496\tcase possible: 1497\tdo_set_cpus_allowed(p, cpu_possible_mask); 1498\tstate = fail; 1499\tbreak; 1500 1501\tcase fail: 1502\tBUG(); 1503\tbreak; 1504\t} 1505\t} 1506 1507out: 1508\tif (state != cpuset) { 1509\t/* 1510* Don\u0026#39;t tell them about moving exiting tasks or 1511* kernel threads (both mm NULL), since they never 1512* leave kernel. 1513*/ 1514\tif (p-\u0026gt;mm \u0026amp;\u0026amp; printk_ratelimit()) { 1515\tprintk_deferred(\u0026#34;process %d (%s) no longer affine to cpu%d\\n\u0026#34;, 1516\ttask_pid_nr(p), p-\u0026gt;comm, cpu); 1517\t} 1518\t} 1519 1520\treturn dest_cpu; 1521} 1522 1523/* 1524* The caller (fork, wakeup) owns p-\u0026gt;pi_lock, -\u0026gt;cpus_allowed is stable. 1525*/ 1526static inline 1527int select_task_rq(struct task_struct *p, int cpu, int sd_flags, int wake_flags) 1528{ 1529\tlockdep_assert_held(\u0026amp;p-\u0026gt;pi_lock); 1530 1531\tif (p-\u0026gt;nr_cpus_allowed \u0026gt; 1) 1532\tcpu = p-\u0026gt;sched_class-\u0026gt;select_task_rq(p, cpu, sd_flags, wake_flags); 1533\telse 1534\tcpu = cpumask_any(\u0026amp;p-\u0026gt;cpus_allowed); 1535 1536\t/* 1537* In order not to call set_task_cpu() on a blocking task we need 1538* to rely on ttwu() to place the task on a valid -\u0026gt;cpus_allowed 1539* CPU. 1540* 1541* Since this is common to all placement strategies, this lives here. 1542* 1543* [ this allows -\u0026gt;select_task() to simply return task_cpu(p) and 1544* not worry about this generic constraint ] 1545*/ 1546\tif (unlikely(!is_cpu_allowed(p, cpu))) 1547\tcpu = select_fallback_rq(task_cpu(p), p); 1548 1549\treturn cpu; 1550} 1551 1552static void update_avg(u64 *avg, u64 sample) 1553{ 1554\ts64 diff = sample - *avg; 1555\t*avg += diff \u0026gt;\u0026gt; 3; 1556} 1557 1558void sched_set_stop_task(int cpu, struct task_struct *stop) 1559{ 1560\tstruct sched_param param = { .sched_priority = MAX_RT_PRIO - 1 }; 1561\tstruct task_struct *old_stop = cpu_rq(cpu)-\u0026gt;stop; 1562 1563\tif (stop) { 1564\t/* 1565* Make it appear like a SCHED_FIFO task, its something 1566* userspace knows about and won\u0026#39;t get confused about. 1567* 1568* Also, it will make PI more or less work without too 1569* much confusion -- but then, stop work should not 1570* rely on PI working anyway. 1571*/ 1572\tsched_setscheduler_nocheck(stop, SCHED_FIFO, \u0026amp;param); 1573 1574\tstop-\u0026gt;sched_class = \u0026amp;stop_sched_class; 1575\t} 1576 1577\tcpu_rq(cpu)-\u0026gt;stop = stop; 1578 1579\tif (old_stop) { 1580\t/* 1581* Reset it back to a normal scheduling class so that 1582* it can die in pieces. 1583*/ 1584\told_stop-\u0026gt;sched_class = \u0026amp;rt_sched_class; 1585\t} 1586} 1587 1588#else 1589 1590static inline int __set_cpus_allowed_ptr(struct task_struct *p, 1591\tconst struct cpumask *new_mask, bool check) 1592{ 1593\treturn set_cpus_allowed_ptr(p, new_mask); 1594} 1595 1596#endif /* CONFIG_SMP */1597 1598static void 1599ttwu_stat(struct task_struct *p, int cpu, int wake_flags) 1600{ 1601\tstruct rq *rq; 1602 1603\tif (!schedstat_enabled()) 1604\treturn; 1605 1606\trq = this_rq(); 1607 1608#ifdef CONFIG_SMP 1609\tif (cpu == rq-\u0026gt;cpu) { 1610\t__schedstat_inc(rq-\u0026gt;ttwu_local); 1611\t__schedstat_inc(p-\u0026gt;se.statistics.nr_wakeups_local); 1612\t} else { 1613\tstruct sched_domain *sd; 1614 1615\t__schedstat_inc(p-\u0026gt;se.statistics.nr_wakeups_remote); 1616\trcu_read_lock(); 1617\tfor_each_domain(rq-\u0026gt;cpu, sd) { 1618\tif (cpumask_test_cpu(cpu, sched_domain_span(sd))) { 1619\t__schedstat_inc(sd-\u0026gt;ttwu_wake_remote); 1620\tbreak; 1621\t} 1622\t} 1623\trcu_read_unlock(); 1624\t} 1625 1626\tif (wake_flags \u0026amp; WF_MIGRATED) 1627\t__schedstat_inc(p-\u0026gt;se.statistics.nr_wakeups_migrate); 1628#endif /* CONFIG_SMP */1629 1630\t__schedstat_inc(rq-\u0026gt;ttwu_count); 1631\t__schedstat_inc(p-\u0026gt;se.statistics.nr_wakeups); 1632 1633\tif (wake_flags \u0026amp; WF_SYNC) 1634\t__schedstat_inc(p-\u0026gt;se.statistics.nr_wakeups_sync); 1635} 1636 1637static inline void ttwu_activate(struct rq *rq, struct task_struct *p, int en_flags) 1638{ 1639\tactivate_task(rq, p, en_flags); 1640\tp-\u0026gt;on_rq = TASK_ON_RQ_QUEUED; 1641 1642\t/* If a worker is waking up, notify the workqueue: */ 1643\tif (p-\u0026gt;flags \u0026amp; PF_WQ_WORKER) 1644\twq_worker_waking_up(p, cpu_of(rq)); 1645} 1646 1647/* 1648* Mark the task runnable and perform wakeup-preemption. 1649*/ 1650static void ttwu_do_wakeup(struct rq *rq, struct task_struct *p, int wake_flags, 1651\tstruct rq_flags *rf) 1652{ 1653\tcheck_preempt_curr(rq, p, wake_flags); 1654\tp-\u0026gt;state = TASK_RUNNING; 1655\ttrace_sched_wakeup(p); 1656 1657#ifdef CONFIG_SMP 1658\tif (p-\u0026gt;sched_class-\u0026gt;task_woken) { 1659\t/* 1660* Our task @p is fully woken up and running; so its safe to 1661* drop the rq-\u0026gt;lock, hereafter rq is only used for statistics. 1662*/ 1663\trq_unpin_lock(rq, rf); 1664\tp-\u0026gt;sched_class-\u0026gt;task_woken(rq, p); 1665\trq_repin_lock(rq, rf); 1666\t} 1667 1668\tif (rq-\u0026gt;idle_stamp) { 1669\tu64 delta = rq_clock(rq) - rq-\u0026gt;idle_stamp; 1670\tu64 max = 2*rq-\u0026gt;max_idle_balance_cost; 1671 1672\tupdate_avg(\u0026amp;rq-\u0026gt;avg_idle, delta); 1673 1674\tif (rq-\u0026gt;avg_idle \u0026gt; max) 1675\trq-\u0026gt;avg_idle = max; 1676 1677\trq-\u0026gt;idle_stamp = 0; 1678\t} 1679#endif 1680} 1681 1682static void 1683ttwu_do_activate(struct rq *rq, struct task_struct *p, int wake_flags, 1684\tstruct rq_flags *rf) 1685{ 1686\tint en_flags = ENQUEUE_WAKEUP | ENQUEUE_NOCLOCK; 1687 1688\tlockdep_assert_held(\u0026amp;rq-\u0026gt;lock); 1689 1690#ifdef CONFIG_SMP 1691\tif (p-\u0026gt;sched_contributes_to_load) 1692\trq-\u0026gt;nr_uninterruptible--; 1693 1694\tif (wake_flags \u0026amp; WF_MIGRATED) 1695\ten_flags |= ENQUEUE_MIGRATED; 1696#endif 1697 1698\tttwu_activate(rq, p, en_flags); 1699\tttwu_do_wakeup(rq, p, wake_flags, rf); 1700} 1701 1702/* 1703* Called in case the task @p isn\u0026#39;t fully descheduled from its runqueue, 1704* in this case we must do a remote wakeup. Its a \u0026#39;light\u0026#39; wakeup though, 1705* since all we need to do is flip p-\u0026gt;state to TASK_RUNNING, since 1706* the task is still -\u0026gt;on_rq. 1707*/ 1708static int ttwu_remote(struct task_struct *p, int wake_flags) 1709{ 1710\tstruct rq_flags rf; 1711\tstruct rq *rq; 1712\tint ret = 0; 1713 1714\trq = __task_rq_lock(p, \u0026amp;rf); 1715\tif (task_on_rq_queued(p)) { 1716\t/* check_preempt_curr() may use rq clock */ 1717\tupdate_rq_clock(rq); 1718\tttwu_do_wakeup(rq, p, wake_flags, \u0026amp;rf); 1719\tret = 1; 1720\t} 1721\t__task_rq_unlock(rq, \u0026amp;rf); 1722 1723\treturn ret; 1724} 1725 1726#ifdef CONFIG_SMP 1727void sched_ttwu_pending(void) 1728{ 1729\tstruct rq *rq = this_rq(); 1730\tstruct llist_node *llist = llist_del_all(\u0026amp;rq-\u0026gt;wake_list); 1731\tstruct task_struct *p, *t; 1732\tstruct rq_flags rf; 1733 1734\tif (!llist) 1735\treturn; 1736 1737\trq_lock_irqsave(rq, \u0026amp;rf); 1738\tupdate_rq_clock(rq); 1739 1740\tllist_for_each_entry_safe(p, t, llist, wake_entry) 1741\tttwu_do_activate(rq, p, p-\u0026gt;sched_remote_wakeup ? WF_MIGRATED : 0, \u0026amp;rf); 1742 1743\trq_unlock_irqrestore(rq, \u0026amp;rf); 1744} 1745 1746void scheduler_ipi(void) 1747{ 1748\t/* 1749* Fold TIF_NEED_RESCHED into the preempt_count; anybody setting 1750* TIF_NEED_RESCHED remotely (for the first time) will also send 1751* this IPI. 1752*/ 1753\tpreempt_fold_need_resched(); 1754 1755\tif (llist_empty(\u0026amp;this_rq()-\u0026gt;wake_list) \u0026amp;\u0026amp; !got_nohz_idle_kick()) 1756\treturn; 1757 1758\t/* 1759* Not all reschedule IPI handlers call irq_enter/irq_exit, since 1760* traditionally all their work was done from the interrupt return 1761* path. Now that we actually do some work, we need to make sure 1762* we do call them. 1763* 1764* Some archs already do call them, luckily irq_enter/exit nest 1765* properly. 1766* 1767* Arguably we should visit all archs and update all handlers, 1768* however a fair share of IPIs are still resched only so this would 1769* somewhat pessimize the simple resched case. 1770*/ 1771\tirq_enter(); 1772\tsched_ttwu_pending(); 1773 1774\t/* 1775* Check if someone kicked us for doing the nohz idle load balance. 1776*/ 1777\tif (unlikely(got_nohz_idle_kick())) { 1778\tthis_rq()-\u0026gt;idle_balance = 1; 1779\traise_softirq_irqoff(SCHED_SOFTIRQ); 1780\t} 1781\tirq_exit(); 1782} 1783 1784static void ttwu_queue_remote(struct task_struct *p, int cpu, int wake_flags) 1785{ 1786\tstruct rq *rq = cpu_rq(cpu); 1787 1788\tp-\u0026gt;sched_remote_wakeup = !!(wake_flags \u0026amp; WF_MIGRATED); 1789 1790\tif (llist_add(\u0026amp;p-\u0026gt;wake_entry, \u0026amp;cpu_rq(cpu)-\u0026gt;wake_list)) { 1791\tif (!set_nr_if_polling(rq-\u0026gt;idle)) 1792\tsmp_send_reschedule(cpu); 1793\telse 1794\ttrace_sched_wake_idle_without_ipi(cpu); 1795\t} 1796} 1797 1798void wake_up_if_idle(int cpu) 1799{ 1800\tstruct rq *rq = cpu_rq(cpu); 1801\tstruct rq_flags rf; 1802 1803\trcu_read_lock(); 1804 1805\tif (!is_idle_task(rcu_dereference(rq-\u0026gt;curr))) 1806\tgoto out; 1807 1808\tif (set_nr_if_polling(rq-\u0026gt;idle)) { 1809\ttrace_sched_wake_idle_without_ipi(cpu); 1810\t} else { 1811\trq_lock_irqsave(rq, \u0026amp;rf); 1812\tif (is_idle_task(rq-\u0026gt;curr)) 1813\tsmp_send_reschedule(cpu); 1814\t/* Else CPU is not idle, do nothing here: */ 1815\trq_unlock_irqrestore(rq, \u0026amp;rf); 1816\t} 1817 1818out: 1819\trcu_read_unlock(); 1820} 1821 1822bool cpus_share_cache(int this_cpu, int that_cpu) 1823{ 1824\treturn per_cpu(sd_llc_id, this_cpu) == per_cpu(sd_llc_id, that_cpu); 1825} 1826#endif /* CONFIG_SMP */1827 1828static void ttwu_queue(struct task_struct *p, int cpu, int wake_flags) 1829{ 1830\tstruct rq *rq = cpu_rq(cpu); 1831\tstruct rq_flags rf; 1832 1833#if defined(CONFIG_SMP) 1834\tif (sched_feat(TTWU_QUEUE) \u0026amp;\u0026amp; !cpus_share_cache(smp_processor_id(), cpu)) { 1835\tsched_clock_cpu(cpu); /* Sync clocks across CPUs */ 1836\tttwu_queue_remote(p, cpu, wake_flags); 1837\treturn; 1838\t} 1839#endif 1840 1841\trq_lock(rq, \u0026amp;rf); 1842\tupdate_rq_clock(rq); 1843\tttwu_do_activate(rq, p, wake_flags, \u0026amp;rf); 1844\trq_unlock(rq, \u0026amp;rf); 1845} 1846 1847/* 1848* Notes on Program-Order guarantees on SMP systems. 1849* 1850* MIGRATION 1851* 1852* The basic program-order guarantee on SMP systems is that when a task [t] 1853* migrates, all its activity on its old CPU [c0] happens-before any subsequent 1854* execution on its new CPU [c1]. 1855* 1856* For migration (of runnable tasks) this is provided by the following means: 1857* 1858* A) UNLOCK of the rq(c0)-\u0026gt;lock scheduling out task t 1859* B) migration for t is required to synchronize *both* rq(c0)-\u0026gt;lock and 1860* rq(c1)-\u0026gt;lock (if not at the same time, then in that order). 1861* C) LOCK of the rq(c1)-\u0026gt;lock scheduling in task 1862* 1863* Release/acquire chaining guarantees that B happens after A and C after B. 1864* Note: the CPU doing B need not be c0 or c1 1865* 1866* Example: 1867* 1868* CPU0 CPU1 CPU2 1869* 1870* LOCK rq(0)-\u0026gt;lock 1871* sched-out X 1872* sched-in Y 1873* UNLOCK rq(0)-\u0026gt;lock 1874* 1875* LOCK rq(0)-\u0026gt;lock // orders against CPU0 1876* dequeue X 1877* UNLOCK rq(0)-\u0026gt;lock 1878* 1879* LOCK rq(1)-\u0026gt;lock 1880* enqueue X 1881* UNLOCK rq(1)-\u0026gt;lock 1882* 1883* LOCK rq(1)-\u0026gt;lock // orders against CPU2 1884* sched-out Z 1885* sched-in X 1886* UNLOCK rq(1)-\u0026gt;lock 1887* 1888* 1889* BLOCKING -- aka. SLEEP + WAKEUP 1890* 1891* For blocking we (obviously) need to provide the same guarantee as for 1892* migration. However the means are completely different as there is no lock 1893* chain to provide order. Instead we do: 1894* 1895* 1) smp_store_release(X-\u0026gt;on_cpu, 0) 1896* 2) smp_cond_load_acquire(!X-\u0026gt;on_cpu) 1897* 1898* Example: 1899* 1900* CPU0 (schedule) CPU1 (try_to_wake_up) CPU2 (schedule) 1901* 1902* LOCK rq(0)-\u0026gt;lock LOCK X-\u0026gt;pi_lock 1903* dequeue X 1904* sched-out X 1905* smp_store_release(X-\u0026gt;on_cpu, 0); 1906* 1907* smp_cond_load_acquire(\u0026amp;X-\u0026gt;on_cpu, !VAL); 1908* X-\u0026gt;state = WAKING 1909* set_task_cpu(X,2) 1910* 1911* LOCK rq(2)-\u0026gt;lock 1912* enqueue X 1913* X-\u0026gt;state = RUNNING 1914* UNLOCK rq(2)-\u0026gt;lock 1915* 1916* LOCK rq(2)-\u0026gt;lock // orders against CPU1 1917* sched-out Z 1918* sched-in X 1919* UNLOCK rq(2)-\u0026gt;lock 1920* 1921* UNLOCK X-\u0026gt;pi_lock 1922* UNLOCK rq(0)-\u0026gt;lock 1923* 1924* 1925* However, for wakeups there is a second guarantee we must provide, namely we 1926* must ensure that CONDITION=1 done by the caller can not be reordered with 1927* accesses to the task state; see try_to_wake_up() and set_current_state(). 1928*/ 1929 1930/** 1931* try_to_wake_up - wake up a thread 1932* @p: the thread to be awakened 1933* @state: the mask of task states that can be woken 1934* @wake_flags: wake modifier flags (WF_*) 1935* 1936* If (@state \u0026amp; @p-\u0026gt;state) @p-\u0026gt;state = TASK_RUNNING. 1937* 1938* If the task was not queued/runnable, also place it back on a runqueue. 1939* 1940* Atomic against schedule() which would dequeue a task, also see 1941* set_current_state(). 1942* 1943* This function executes a full memory barrier before accessing the task 1944* state; see set_current_state(). 1945* 1946* Return: %true if @p-\u0026gt;state changes (an actual wakeup was done), 1947*\t%false otherwise. 1948*/ 1949static int 1950try_to_wake_up(struct task_struct *p, unsigned int state, int wake_flags) 1951{ 1952\tunsigned long flags; 1953\tint cpu, success = 0; 1954 1955\t/* 1956* If we are going to wake up a thread waiting for CONDITION we 1957* need to ensure that CONDITION=1 done by the caller can not be 1958* reordered with p-\u0026gt;state check below. This pairs with mb() in 1959* set_current_state() the waiting thread does. 1960*/ 1961\traw_spin_lock_irqsave(\u0026amp;p-\u0026gt;pi_lock, flags); 1962\tsmp_mb__after_spinlock(); 1963\tif (!(p-\u0026gt;state \u0026amp; state)) 1964\tgoto out; 1965 1966\ttrace_sched_waking(p); 1967 1968\t/* We\u0026#39;re going to change -\u0026gt;state: */ 1969\tsuccess = 1; 1970\tcpu = task_cpu(p); 1971 1972\t/* 1973* Ensure we load p-\u0026gt;on_rq _after_ p-\u0026gt;state, otherwise it would 1974* be possible to, falsely, observe p-\u0026gt;on_rq == 0 and get stuck 1975* in smp_cond_load_acquire() below. 1976* 1977* sched_ttwu_pending()\ttry_to_wake_up() 1978* STORE p-\u0026gt;on_rq = 1\tLOAD p-\u0026gt;state 1979* UNLOCK rq-\u0026gt;lock 1980* 1981* __schedule() (switch to task \u0026#39;p\u0026#39;) 1982* LOCK rq-\u0026gt;lock\tsmp_rmb(); 1983* smp_mb__after_spinlock(); 1984* UNLOCK rq-\u0026gt;lock 1985* 1986* [task p] 1987* STORE p-\u0026gt;state = UNINTERRUPTIBLE\tLOAD p-\u0026gt;on_rq 1988* 1989* Pairs with the LOCK+smp_mb__after_spinlock() on rq-\u0026gt;lock in 1990* __schedule(). See the comment for smp_mb__after_spinlock(). 1991*/ 1992\tsmp_rmb(); 1993\tif (p-\u0026gt;on_rq \u0026amp;\u0026amp; ttwu_remote(p, wake_flags)) 1994\tgoto stat; 1995 1996#ifdef CONFIG_SMP 1997\t/* 1998* Ensure we load p-\u0026gt;on_cpu _after_ p-\u0026gt;on_rq, otherwise it would be 1999* possible to, falsely, observe p-\u0026gt;on_cpu == 0. 2000* 2001* One must be running (-\u0026gt;on_cpu == 1) in order to remove oneself 2002* from the runqueue. 2003* 2004* __schedule() (switch to task \u0026#39;p\u0026#39;)\ttry_to_wake_up() 2005* STORE p-\u0026gt;on_cpu = 1\tLOAD p-\u0026gt;on_rq 2006* UNLOCK rq-\u0026gt;lock 2007* 2008* __schedule() (put \u0026#39;p\u0026#39; to sleep) 2009* LOCK rq-\u0026gt;lock\tsmp_rmb(); 2010* smp_mb__after_spinlock(); 2011* STORE p-\u0026gt;on_rq = 0\tLOAD p-\u0026gt;on_cpu 2012* 2013* Pairs with the LOCK+smp_mb__after_spinlock() on rq-\u0026gt;lock in 2014* __schedule(). See the comment for smp_mb__after_spinlock(). 2015*/ 2016\tsmp_rmb(); 2017 2018\t/* 2019* If the owning (remote) CPU is still in the middle of schedule() with 2020* this task as prev, wait until its done referencing the task. 2021* 2022* Pairs with the smp_store_release() in finish_task(). 2023* 2024* This ensures that tasks getting woken will be fully ordered against 2025* their previous state and preserve Program Order. 2026*/ 2027\tsmp_cond_load_acquire(\u0026amp;p-\u0026gt;on_cpu, !VAL); 2028 2029\tp-\u0026gt;sched_contributes_to_load = !!task_contributes_to_load(p); 2030\tp-\u0026gt;state = TASK_WAKING; 2031 2032\tif (p-\u0026gt;in_iowait) { 2033\tdelayacct_blkio_end(p); 2034\tatomic_dec(\u0026amp;task_rq(p)-\u0026gt;nr_iowait); 2035\t} 2036 2037\tcpu = select_task_rq(p, p-\u0026gt;wake_cpu, SD_BALANCE_WAKE, wake_flags); 2038\tif (task_cpu(p) != cpu) { 2039\twake_flags |= WF_MIGRATED; 2040\tset_task_cpu(p, cpu); 2041\t} 2042 2043#else /* CONFIG_SMP */2044 2045\tif (p-\u0026gt;in_iowait) { 2046\tdelayacct_blkio_end(p); 2047\tatomic_dec(\u0026amp;task_rq(p)-\u0026gt;nr_iowait); 2048\t} 2049 2050#endif /* CONFIG_SMP */2051 2052\tttwu_queue(p, cpu, wake_flags); 2053stat: 2054\tttwu_stat(p, cpu, wake_flags); 2055out: 2056\traw_spin_unlock_irqrestore(\u0026amp;p-\u0026gt;pi_lock, flags); 2057 2058\treturn success; 2059} 2060 2061/** 2062* try_to_wake_up_local - try to wake up a local task with rq lock held 2063* @p: the thread to be awakened 2064* @rf: request-queue flags for pinning 2065* 2066* Put @p on the run-queue if it\u0026#39;s not already there. The caller must 2067* ensure that this_rq() is locked, @p is bound to this_rq() and not 2068* the current task. 2069*/ 2070static void try_to_wake_up_local(struct task_struct *p, struct rq_flags *rf) 2071{ 2072\tstruct rq *rq = task_rq(p); 2073 2074\tif (WARN_ON_ONCE(rq != this_rq()) || 2075\tWARN_ON_ONCE(p == current)) 2076\treturn; 2077 2078\tlockdep_assert_held(\u0026amp;rq-\u0026gt;lock); 2079 2080\tif (!raw_spin_trylock(\u0026amp;p-\u0026gt;pi_lock)) { 2081\t/* 2082* This is OK, because current is on_cpu, which avoids it being 2083* picked for load-balance and preemption/IRQs are still 2084* disabled avoiding further scheduler activity on it and we\u0026#39;ve 2085* not yet picked a replacement task. 2086*/ 2087\trq_unlock(rq, rf); 2088\traw_spin_lock(\u0026amp;p-\u0026gt;pi_lock); 2089\trq_relock(rq, rf); 2090\t} 2091 2092\tif (!(p-\u0026gt;state \u0026amp; TASK_NORMAL)) 2093\tgoto out; 2094 2095\ttrace_sched_waking(p); 2096 2097\tif (!task_on_rq_queued(p)) { 2098\tif (p-\u0026gt;in_iowait) { 2099\tdelayacct_blkio_end(p); 2100\tatomic_dec(\u0026amp;rq-\u0026gt;nr_iowait); 2101\t} 2102\tttwu_activate(rq, p, ENQUEUE_WAKEUP | ENQUEUE_NOCLOCK); 2103\t} 2104 2105\tttwu_do_wakeup(rq, p, 0, rf); 2106\tttwu_stat(p, smp_processor_id(), 0); 2107out: 2108\traw_spin_unlock(\u0026amp;p-\u0026gt;pi_lock); 2109} 2110 2111/** 2112* wake_up_process - Wake up a specific process 2113* @p: The process to be woken up. 2114* 2115* Attempt to wake up the nominated process and move it to the set of runnable 2116* processes. 2117* 2118* Return: 1 if the process was woken up, 0 if it was already running. 2119* 2120* This function executes a full memory barrier before accessing the task state. 2121*/ 2122int wake_up_process(struct task_struct *p) 2123{ 2124\treturn try_to_wake_up(p, TASK_NORMAL, 0); 2125} 2126EXPORT_SYMBOL(wake_up_process); 2127 2128int wake_up_state(struct task_struct *p, unsigned int state) 2129{ 2130\treturn try_to_wake_up(p, state, 0); 2131} 2132 2133/* 2134* Perform scheduler related setup for a newly forked process p. 2135* p is forked by current. 2136* 2137* __sched_fork() is basic setup used by init_idle() too: 2138*/ 2139static void __sched_fork(unsigned long clone_flags, struct task_struct *p) 2140{ 2141\tp-\u0026gt;on_rq\t= 0; 2142 2143\tp-\u0026gt;se.on_rq\t= 0; 2144\tp-\u0026gt;se.exec_start\t= 0; 2145\tp-\u0026gt;se.sum_exec_runtime\t= 0; 2146\tp-\u0026gt;se.prev_sum_exec_runtime\t= 0; 2147\tp-\u0026gt;se.nr_migrations\t= 0; 2148\tp-\u0026gt;se.vruntime\t= 0; 2149\tINIT_LIST_HEAD(\u0026amp;p-\u0026gt;se.group_node); 2150 2151#ifdef CONFIG_FAIR_GROUP_SCHED 2152\tp-\u0026gt;se.cfs_rq\t= NULL; 2153#endif 2154 2155#ifdef CONFIG_SCHEDSTATS 2156\t/* Even if schedstat is disabled, there should not be garbage */ 2157\tmemset(\u0026amp;p-\u0026gt;se.statistics, 0, sizeof(p-\u0026gt;se.statistics)); 2158#endif 2159 2160\tRB_CLEAR_NODE(\u0026amp;p-\u0026gt;dl.rb_node); 2161\tinit_dl_task_timer(\u0026amp;p-\u0026gt;dl); 2162\tinit_dl_inactive_task_timer(\u0026amp;p-\u0026gt;dl); 2163\t__dl_clear_params(p); 2164 2165\tINIT_LIST_HEAD(\u0026amp;p-\u0026gt;rt.run_list); 2166\tp-\u0026gt;rt.timeout\t= 0; 2167\tp-\u0026gt;rt.time_slice\t= sched_rr_timeslice; 2168\tp-\u0026gt;rt.on_rq\t= 0; 2169\tp-\u0026gt;rt.on_list\t= 0; 2170 2171#ifdef CONFIG_PREEMPT_NOTIFIERS 2172\tINIT_HLIST_HEAD(\u0026amp;p-\u0026gt;preempt_notifiers); 2173#endif 2174 2175\tinit_numa_balancing(clone_flags, p); 2176} 2177 2178DEFINE_STATIC_KEY_FALSE(sched_numa_balancing); 2179 2180#ifdef CONFIG_NUMA_BALANCING 2181 2182void set_numabalancing_state(bool enabled) 2183{ 2184\tif (enabled) 2185\tstatic_branch_enable(\u0026amp;sched_numa_balancing); 2186\telse 2187\tstatic_branch_disable(\u0026amp;sched_numa_balancing); 2188} 2189 2190#ifdef CONFIG_PROC_SYSCTL 2191int sysctl_numa_balancing(struct ctl_table *table, int write, 2192\tvoid __user *buffer, size_t *lenp, loff_t *ppos) 2193{ 2194\tstruct ctl_table t; 2195\tint err; 2196\tint state = static_branch_likely(\u0026amp;sched_numa_balancing); 2197 2198\tif (write \u0026amp;\u0026amp; !capable(CAP_SYS_ADMIN)) 2199\treturn -EPERM; 2200 2201\tt = *table; 2202\tt.data = \u0026amp;state; 2203\terr = proc_dointvec_minmax(\u0026amp;t, write, buffer, lenp, ppos); 2204\tif (err \u0026lt; 0) 2205\treturn err; 2206\tif (write) 2207\tset_numabalancing_state(state); 2208\treturn err; 2209} 2210#endif 2211#endif 2212 2213#ifdef CONFIG_SCHEDSTATS 2214 2215DEFINE_STATIC_KEY_FALSE(sched_schedstats); 2216static bool __initdata __sched_schedstats = false; 2217 2218static void set_schedstats(bool enabled) 2219{ 2220\tif (enabled) 2221\tstatic_branch_enable(\u0026amp;sched_schedstats); 2222\telse 2223\tstatic_branch_disable(\u0026amp;sched_schedstats); 2224} 2225 2226void force_schedstat_enabled(void) 2227{ 2228\tif (!schedstat_enabled()) { 2229\tpr_info(\u0026#34;kernel profiling enabled schedstats, disable via kernel.sched_schedstats.\\n\u0026#34;); 2230\tstatic_branch_enable(\u0026amp;sched_schedstats); 2231\t} 2232} 2233 2234static int __init setup_schedstats(char *str) 2235{ 2236\tint ret = 0; 2237\tif (!str) 2238\tgoto out; 2239 2240\t/* 2241* This code is called before jump labels have been set up, so we can\u0026#39;t 2242* change the static branch directly just yet. Instead set a temporary 2243* variable so init_schedstats() can do it later. 2244*/ 2245\tif (!strcmp(str, \u0026#34;enable\u0026#34;)) { 2246\t__sched_schedstats = true; 2247\tret = 1; 2248\t} else if (!strcmp(str, \u0026#34;disable\u0026#34;)) { 2249\t__sched_schedstats = false; 2250\tret = 1; 2251\t} 2252out: 2253\tif (!ret) 2254\tpr_warn(\u0026#34;Unable to parse schedstats=\\n\u0026#34;); 2255 2256\treturn ret; 2257} 2258__setup(\u0026#34;schedstats=\u0026#34;, setup_schedstats); 2259 2260static void __init init_schedstats(void) 2261{ 2262\tset_schedstats(__sched_schedstats); 2263} 2264 2265#ifdef CONFIG_PROC_SYSCTL 2266int sysctl_schedstats(struct ctl_table *table, int write, 2267\tvoid __user *buffer, size_t *lenp, loff_t *ppos) 2268{ 2269\tstruct ctl_table t; 2270\tint err; 2271\tint state = static_branch_likely(\u0026amp;sched_schedstats); 2272 2273\tif (write \u0026amp;\u0026amp; !capable(CAP_SYS_ADMIN)) 2274\treturn -EPERM; 2275 2276\tt = *table; 2277\tt.data = \u0026amp;state; 2278\terr = proc_dointvec_minmax(\u0026amp;t, write, buffer, lenp, ppos); 2279\tif (err \u0026lt; 0) 2280\treturn err; 2281\tif (write) 2282\tset_schedstats(state); 2283\treturn err; 2284} 2285#endif /* CONFIG_PROC_SYSCTL */2286#else /* !CONFIG_SCHEDSTATS */2287static inline void init_schedstats(void) {} 2288#endif /* CONFIG_SCHEDSTATS */2289 2290/* 2291* fork()/clone()-time setup: 2292*/ 2293int sched_fork(unsigned long clone_flags, struct task_struct *p) 2294{ 2295\tunsigned long flags; 2296 2297\t__sched_fork(clone_flags, p); 2298\t/* 2299* We mark the process as NEW here. This guarantees that 2300* nobody will actually run it, and a signal or other external 2301* event cannot wake it up and insert it on the runqueue either. 2302*/ 2303\tp-\u0026gt;state = TASK_NEW; 2304 2305\t/* 2306* Make sure we do not leak PI boosting priority to the child. 2307*/ 2308\tp-\u0026gt;prio = current-\u0026gt;normal_prio; 2309 2310\t/* 2311* Revert to default priority/policy on fork if requested. 2312*/ 2313\tif (unlikely(p-\u0026gt;sched_reset_on_fork)) { 2314\tif (task_has_dl_policy(p) || task_has_rt_policy(p)) { 2315\tp-\u0026gt;policy = SCHED_NORMAL; 2316\tp-\u0026gt;static_prio = NICE_TO_PRIO(0); 2317\tp-\u0026gt;rt_priority = 0; 2318\t} else if (PRIO_TO_NICE(p-\u0026gt;static_prio) \u0026lt; 0) 2319\tp-\u0026gt;static_prio = NICE_TO_PRIO(0); 2320 2321\tp-\u0026gt;prio = p-\u0026gt;normal_prio = __normal_prio(p); 2322\tset_load_weight(p, false); 2323 2324\t/* 2325* We don\u0026#39;t need the reset flag anymore after the fork. It has 2326* fulfilled its duty: 2327*/ 2328\tp-\u0026gt;sched_reset_on_fork = 0; 2329\t} 2330 2331\tif (dl_prio(p-\u0026gt;prio)) 2332\treturn -EAGAIN; 2333\telse if (rt_prio(p-\u0026gt;prio)) 2334\tp-\u0026gt;sched_class = \u0026amp;rt_sched_class; 2335\telse 2336\tp-\u0026gt;sched_class = \u0026amp;fair_sched_class; 2337 2338\tinit_entity_runnable_average(\u0026amp;p-\u0026gt;se); 2339 2340\t/* 2341* The child is not yet in the pid-hash so no cgroup attach races, 2342* and the cgroup is pinned to this child due to cgroup_fork() 2343* is ran before sched_fork(). 2344* 2345* Silence PROVE_RCU. 2346*/ 2347\traw_spin_lock_irqsave(\u0026amp;p-\u0026gt;pi_lock, flags); 2348\trseq_migrate(p); 2349\t/* 2350* We\u0026#39;re setting the CPU for the first time, we don\u0026#39;t migrate, 2351* so use __set_task_cpu(). 2352*/ 2353\t__set_task_cpu(p, smp_processor_id()); 2354\tif (p-\u0026gt;sched_class-\u0026gt;task_fork) 2355\tp-\u0026gt;sched_class-\u0026gt;task_fork(p); 2356\traw_spin_unlock_irqrestore(\u0026amp;p-\u0026gt;pi_lock, flags); 2357 2358#ifdef CONFIG_SCHED_INFO 2359\tif (likely(sched_info_on())) 2360\tmemset(\u0026amp;p-\u0026gt;sched_info, 0, sizeof(p-\u0026gt;sched_info)); 2361#endif 2362#if defined(CONFIG_SMP) 2363\tp-\u0026gt;on_cpu = 0; 2364#endif 2365\tinit_task_preempt_count(p); 2366#ifdef CONFIG_SMP 2367\tplist_node_init(\u0026amp;p-\u0026gt;pushable_tasks, MAX_PRIO); 2368\tRB_CLEAR_NODE(\u0026amp;p-\u0026gt;pushable_dl_tasks); 2369#endif 2370\treturn 0; 2371} 2372 2373unsigned long to_ratio(u64 period, u64 runtime) 2374{ 2375\tif (runtime == RUNTIME_INF) 2376\treturn BW_UNIT; 2377 2378\t/* 2379* Doing this here saves a lot of checks in all 2380* the calling paths, and returning zero seems 2381* safe for them anyway. 2382*/ 2383\tif (period == 0) 2384\treturn 0; 2385 2386\treturn div64_u64(runtime \u0026lt;\u0026lt; BW_SHIFT, period); 2387} 2388 2389/* 2390* wake_up_new_task - wake up a newly created task for the first time. 2391* 2392* This function will do some initial scheduler statistics housekeeping 2393* that must be done for every newly created context, then puts the task 2394* on the runqueue and wakes it. 2395*/ 2396void wake_up_new_task(struct task_struct *p) 2397{ 2398\tstruct rq_flags rf; 2399\tstruct rq *rq; 2400 2401\traw_spin_lock_irqsave(\u0026amp;p-\u0026gt;pi_lock, rf.flags); 2402\tp-\u0026gt;state = TASK_RUNNING; 2403#ifdef CONFIG_SMP 2404\t/* 2405* Fork balancing, do it here and not earlier because: 2406* - cpus_allowed can change in the fork path 2407* - any previously selected CPU might disappear through hotplug 2408* 2409* Use __set_task_cpu() to avoid calling sched_class::migrate_task_rq, 2410* as we\u0026#39;re not fully set-up yet. 2411*/ 2412\tp-\u0026gt;recent_used_cpu = task_cpu(p); 2413\trseq_migrate(p); 2414\t__set_task_cpu(p, select_task_rq(p, task_cpu(p), SD_BALANCE_FORK, 0)); 2415#endif 2416\trq = __task_rq_lock(p, \u0026amp;rf); 2417\tupdate_rq_clock(rq); 2418\tpost_init_entity_util_avg(\u0026amp;p-\u0026gt;se); 2419 2420\tactivate_task(rq, p, ENQUEUE_NOCLOCK); 2421\tp-\u0026gt;on_rq = TASK_ON_RQ_QUEUED; 2422\ttrace_sched_wakeup_new(p); 2423\tcheck_preempt_curr(rq, p, WF_FORK); 2424#ifdef CONFIG_SMP 2425\tif (p-\u0026gt;sched_class-\u0026gt;task_woken) { 2426\t/* 2427* Nothing relies on rq-\u0026gt;lock after this, so its fine to 2428* drop it. 2429*/ 2430\trq_unpin_lock(rq, \u0026amp;rf); 2431\tp-\u0026gt;sched_class-\u0026gt;task_woken(rq, p); 2432\trq_repin_lock(rq, \u0026amp;rf); 2433\t} 2434#endif 2435\ttask_rq_unlock(rq, p, \u0026amp;rf); 2436} 2437 2438#ifdef CONFIG_PREEMPT_NOTIFIERS 2439 2440static DEFINE_STATIC_KEY_FALSE(preempt_notifier_key); 2441 2442void preempt_notifier_inc(void) 2443{ 2444\tstatic_branch_inc(\u0026amp;preempt_notifier_key); 2445} 2446EXPORT_SYMBOL_GPL(preempt_notifier_inc); 2447 2448void preempt_notifier_dec(void) 2449{ 2450\tstatic_branch_dec(\u0026amp;preempt_notifier_key); 2451} 2452EXPORT_SYMBOL_GPL(preempt_notifier_dec); 2453 2454/** 2455* preempt_notifier_register - tell me when current is being preempted \u0026amp; rescheduled 2456* @notifier: notifier struct to register 2457*/ 2458void preempt_notifier_register(struct preempt_notifier *notifier) 2459{ 2460\tif (!static_branch_unlikely(\u0026amp;preempt_notifier_key)) 2461\tWARN(1, \u0026#34;registering preempt_notifier while notifiers disabled\\n\u0026#34;); 2462 2463\thlist_add_head(\u0026amp;notifier-\u0026gt;link, \u0026amp;current-\u0026gt;preempt_notifiers); 2464} 2465EXPORT_SYMBOL_GPL(preempt_notifier_register); 2466 2467/** 2468* preempt_notifier_unregister - no longer interested in preemption notifications 2469* @notifier: notifier struct to unregister 2470* 2471* This is *not* safe to call from within a preemption notifier. 2472*/ 2473void preempt_notifier_unregister(struct preempt_notifier *notifier) 2474{ 2475\thlist_del(\u0026amp;notifier-\u0026gt;link); 2476} 2477EXPORT_SYMBOL_GPL(preempt_notifier_unregister); 2478 2479static void __fire_sched_in_preempt_notifiers(struct task_struct *curr) 2480{ 2481\tstruct preempt_notifier *notifier; 2482 2483\thlist_for_each_entry(notifier, \u0026amp;curr-\u0026gt;preempt_notifiers, link) 2484\tnotifier-\u0026gt;ops-\u0026gt;sched_in(notifier, raw_smp_processor_id()); 2485} 2486 2487static __always_inline void fire_sched_in_preempt_notifiers(struct task_struct *curr) 2488{ 2489\tif (static_branch_unlikely(\u0026amp;preempt_notifier_key)) 2490\t__fire_sched_in_preempt_notifiers(curr); 2491} 2492 2493static void 2494__fire_sched_out_preempt_notifiers(struct task_struct *curr, 2495\tstruct task_struct *next) 2496{ 2497\tstruct preempt_notifier *notifier; 2498 2499\thlist_for_each_entry(notifier, \u0026amp;curr-\u0026gt;preempt_notifiers, link) 2500\tnotifier-\u0026gt;ops-\u0026gt;sched_out(notifier, next); 2501} 2502 2503static __always_inline void 2504fire_sched_out_preempt_notifiers(struct task_struct *curr, 2505\tstruct task_struct *next) 2506{ 2507\tif (static_branch_unlikely(\u0026amp;preempt_notifier_key)) 2508\t__fire_sched_out_preempt_notifiers(curr, next); 2509} 2510 2511#else /* !CONFIG_PREEMPT_NOTIFIERS */2512 2513static inline void fire_sched_in_preempt_notifiers(struct task_struct *curr) 2514{ 2515} 2516 2517static inline void 2518fire_sched_out_preempt_notifiers(struct task_struct *curr, 2519\tstruct task_struct *next) 2520{ 2521} 2522 2523#endif /* CONFIG_PREEMPT_NOTIFIERS */2524 2525static inline void prepare_task(struct task_struct *next) 2526{ 2527#ifdef CONFIG_SMP 2528\t/* 2529* Claim the task as running, we do this before switching to it 2530* such that any running task will have this set. 2531*/ 2532\tnext-\u0026gt;on_cpu = 1; 2533#endif 2534} 2535 2536static inline void finish_task(struct task_struct *prev) 2537{ 2538#ifdef CONFIG_SMP 2539\t/* 2540* After -\u0026gt;on_cpu is cleared, the task can be moved to a different CPU. 2541* We must ensure this doesn\u0026#39;t happen until the switch is completely 2542* finished. 2543* 2544* In particular, the load of prev-\u0026gt;state in finish_task_switch() must 2545* happen before this. 2546* 2547* Pairs with the smp_cond_load_acquire() in try_to_wake_up(). 2548*/ 2549\tsmp_store_release(\u0026amp;prev-\u0026gt;on_cpu, 0); 2550#endif 2551} 2552 2553static inline void 2554prepare_lock_switch(struct rq *rq, struct task_struct *next, struct rq_flags *rf) 2555{ 2556\t/* 2557* Since the runqueue lock will be released by the next 2558* task (which is an invalid locking op but in the case 2559* of the scheduler it\u0026#39;s an obvious special-case), so we 2560* do an early lockdep release here: 2561*/ 2562\trq_unpin_lock(rq, rf); 2563\tspin_release(\u0026amp;rq-\u0026gt;lock.dep_map, 1, _THIS_IP_); 2564#ifdef CONFIG_DEBUG_SPINLOCK 2565\t/* this is a valid case when another task releases the spinlock */ 2566\trq-\u0026gt;lock.owner = next; 2567#endif 2568} 2569 2570static inline void finish_lock_switch(struct rq *rq) 2571{ 2572\t/* 2573* If we are tracking spinlock dependencies then we have to 2574* fix up the runqueue lock - which gets \u0026#39;carried over\u0026#39; from 2575* prev into current: 2576*/ 2577\tspin_acquire(\u0026amp;rq-\u0026gt;lock.dep_map, 0, 0, _THIS_IP_); 2578\traw_spin_unlock_irq(\u0026amp;rq-\u0026gt;lock); 2579} 2580 2581/* 2582* NOP if the arch has not defined these: 2583*/ 2584 2585#ifndef prepare_arch_switch 2586# define prepare_arch_switch(next)\tdo { } while (0) 2587#endif 2588 2589#ifndef finish_arch_post_lock_switch 2590# define finish_arch_post_lock_switch()\tdo { } while (0) 2591#endif 2592 2593/** 2594* prepare_task_switch - prepare to switch tasks 2595* @rq: the runqueue preparing to switch 2596* @prev: the current task that is being switched out 2597* @next: the task we are going to switch to. 2598* 2599* This is called with the rq lock held and interrupts off. It must 2600* be paired with a subsequent finish_task_switch after the context 2601* switch. 2602* 2603* prepare_task_switch sets up locking and calls architecture specific 2604* hooks. 2605*/ 2606static inline void 2607prepare_task_switch(struct rq *rq, struct task_struct *prev, 2608\tstruct task_struct *next) 2609{ 2610\tkcov_prepare_switch(prev); 2611\tsched_info_switch(rq, prev, next); 2612\tperf_event_task_sched_out(prev, next); 2613\trseq_preempt(prev); 2614\tfire_sched_out_preempt_notifiers(prev, next); 2615\tprepare_task(next); 2616\tprepare_arch_switch(next); 2617} 2618 2619/** 2620* finish_task_switch - clean up after a task-switch 2621* @prev: the thread we just switched away from. 2622* 2623* finish_task_switch must be called after the context switch, paired 2624* with a prepare_task_switch call before the context switch. 2625* finish_task_switch will reconcile locking set up by prepare_task_switch, 2626* and do any other architecture-specific cleanup actions. 2627* 2628* Note that we may have delayed dropping an mm in context_switch(). If 2629* so, we finish that here outside of the runqueue lock. (Doing it 2630* with the lock held can cause deadlocks; see schedule() for 2631* details.) 2632* 2633* The context switch have flipped the stack from under us and restored the 2634* local variables which were saved when this task called schedule() in the 2635* past. prev == current is still correct but we need to recalculate this_rq 2636* because prev may have moved to another CPU. 2637*/ 2638static struct rq *finish_task_switch(struct task_struct *prev) 2639\t__releases(rq-\u0026gt;lock) 2640{ 2641\tstruct rq *rq = this_rq(); 2642\tstruct mm_struct *mm = rq-\u0026gt;prev_mm; 2643\tlong prev_state; 2644 2645\t/* 2646* The previous task will have left us with a preempt_count of 2 2647* because it left us after: 2648* 2649*\tschedule() 2650*\tpreempt_disable();\t// 1 2651*\t__schedule() 2652*\traw_spin_lock_irq(\u0026amp;rq-\u0026gt;lock)\t// 2 2653* 2654* Also, see FORK_PREEMPT_COUNT. 2655*/ 2656\tif (WARN_ONCE(preempt_count() != 2*PREEMPT_DISABLE_OFFSET, 2657\t\u0026#34;corrupted preempt_count: %s/%d/0x%x\\n\u0026#34;, 2658\tcurrent-\u0026gt;comm, current-\u0026gt;pid, preempt_count())) 2659\tpreempt_count_set(FORK_PREEMPT_COUNT); 2660 2661\trq-\u0026gt;prev_mm = NULL; 2662 2663\t/* 2664* A task struct has one reference for the use as \u0026#34;current\u0026#34;. 2665* If a task dies, then it sets TASK_DEAD in tsk-\u0026gt;state and calls 2666* schedule one last time. The schedule call will never return, and 2667* the scheduled task must drop that reference. 2668* 2669* We must observe prev-\u0026gt;state before clearing prev-\u0026gt;on_cpu (in 2670* finish_task), otherwise a concurrent wakeup can get prev 2671* running on another CPU and we could rave with its RUNNING -\u0026gt; DEAD 2672* transition, resulting in a double drop. 2673*/ 2674\tprev_state = prev-\u0026gt;state; 2675\tvtime_task_switch(prev); 2676\tperf_event_task_sched_in(prev, current); 2677\tfinish_task(prev); 2678\tfinish_lock_switch(rq); 2679\tfinish_arch_post_lock_switch(); 2680\tkcov_finish_switch(current); 2681 2682\tfire_sched_in_preempt_notifiers(current); 2683\t/* 2684* When switching through a kernel thread, the loop in 2685* membarrier_{private,global}_expedited() may have observed that 2686* kernel thread and not issued an IPI. It is therefore possible to 2687* schedule between user-\u0026gt;kernel-\u0026gt;user threads without passing though 2688* switch_mm(). Membarrier requires a barrier after storing to 2689* rq-\u0026gt;curr, before returning to userspace, so provide them here: 2690* 2691* - a full memory barrier for {PRIVATE,GLOBAL}_EXPEDITED, implicitly 2692* provided by mmdrop(), 2693* - a sync_core for SYNC_CORE. 2694*/ 2695\tif (mm) { 2696\tmembarrier_mm_sync_core_before_usermode(mm); 2697\tmmdrop(mm); 2698\t} 2699\tif (unlikely(prev_state == TASK_DEAD)) { 2700\tif (prev-\u0026gt;sched_class-\u0026gt;task_dead) 2701\tprev-\u0026gt;sched_class-\u0026gt;task_dead(prev); 2702 2703\t/* 2704* Remove function-return probe instances associated with this 2705* task and put them back on the free list. 2706*/ 2707\tkprobe_flush_task(prev); 2708 2709\t/* Task is done with its stack. */ 2710\tput_task_stack(prev); 2711 2712\tput_task_struct(prev); 2713\t} 2714 2715\ttick_nohz_task_switch(); 2716\treturn rq; 2717} 2718 2719#ifdef CONFIG_SMP 2720 2721/* rq-\u0026gt;lock is NOT held, but preemption is disabled */ 2722static void __balance_callback(struct rq *rq) 2723{ 2724\tstruct callback_head *head, *next; 2725\tvoid (*func)(struct rq *rq); 2726\tunsigned long flags; 2727 2728\traw_spin_lock_irqsave(\u0026amp;rq-\u0026gt;lock, flags); 2729\thead = rq-\u0026gt;balance_callback; 2730\trq-\u0026gt;balance_callback = NULL; 2731\twhile (head) { 2732\tfunc = (void (*)(struct rq *))head-\u0026gt;func; 2733\tnext = head-\u0026gt;next; 2734\thead-\u0026gt;next = NULL; 2735\thead = next; 2736 2737\tfunc(rq); 2738\t} 2739\traw_spin_unlock_irqrestore(\u0026amp;rq-\u0026gt;lock, flags); 2740} 2741 2742static inline void balance_callback(struct rq *rq) 2743{ 2744\tif (unlikely(rq-\u0026gt;balance_callback)) 2745\t__balance_callback(rq); 2746} 2747 2748#else 2749 2750static inline void balance_callback(struct rq *rq) 2751{ 2752} 2753 2754#endif 2755 2756/** 2757* schedule_tail - first thing a freshly forked thread must call. 2758* @prev: the thread we just switched away from. 2759*/ 2760asmlinkage __visible void schedule_tail(struct task_struct *prev) 2761\t__releases(rq-\u0026gt;lock) 2762{ 2763\tstruct rq *rq; 2764 2765\t/* 2766* New tasks start with FORK_PREEMPT_COUNT, see there and 2767* finish_task_switch() for details. 2768* 2769* finish_task_switch() will drop rq-\u0026gt;lock() and lower preempt_count 2770* and the preempt_enable() will end up enabling preemption (on 2771* PREEMPT_COUNT kernels). 2772*/ 2773 2774\trq = finish_task_switch(prev); 2775\tbalance_callback(rq); 2776\tpreempt_enable(); 2777 2778\tif (current-\u0026gt;set_child_tid) 2779\tput_user(task_pid_vnr(current), current-\u0026gt;set_child_tid); 2780 2781\tcalculate_sigpending(); 2782} 2783 2784/* 2785* context_switch - switch to the new MM and the new thread\u0026#39;s register state. 2786*/ 2787static __always_inline struct rq * 2788context_switch(struct rq *rq, struct task_struct *prev, 2789\tstruct task_struct *next, struct rq_flags *rf) 2790{ 2791\tstruct mm_struct *mm, *oldmm; 2792 2793\tprepare_task_switch(rq, prev, next); 2794 2795\tmm = next-\u0026gt;mm; 2796\toldmm = prev-\u0026gt;active_mm; 2797\t/* 2798* For paravirt, this is coupled with an exit in switch_to to 2799* combine the page table reload and the switch backend into 2800* one hypercall. 2801*/ 2802\tarch_start_context_switch(prev); 2803 2804\t/* 2805* If mm is non-NULL, we pass through switch_mm(). If mm is 2806* NULL, we will pass through mmdrop() in finish_task_switch(). 2807* Both of these contain the full memory barrier required by 2808* membarrier after storing to rq-\u0026gt;curr, before returning to 2809* user-space. 2810*/ 2811\tif (!mm) { 2812\tnext-\u0026gt;active_mm = oldmm; 2813\tmmgrab(oldmm); 2814\tenter_lazy_tlb(oldmm, next); 2815\t} else 2816\tswitch_mm_irqs_off(oldmm, mm, next); 2817 2818\tif (!prev-\u0026gt;mm) { 2819\tprev-\u0026gt;active_mm = NULL; 2820\trq-\u0026gt;prev_mm = oldmm; 2821\t} 2822 2823\trq-\u0026gt;clock_update_flags \u0026amp;= ~(RQCF_ACT_SKIP|RQCF_REQ_SKIP); 2824 2825\tprepare_lock_switch(rq, next, rf); 2826 2827\t/* Here we just switch the register state and the stack. */ 2828\tswitch_to(prev, next, prev); 2829\tbarrier(); 2830 2831\treturn finish_task_switch(prev); 2832} 2833 2834/* 2835* nr_running and nr_context_switches: 2836* 2837* externally visible scheduler statistics: current number of runnable 2838* threads, total number of context switches performed since bootup. 2839*/ 2840unsigned long nr_running(void) 2841{ 2842\tunsigned long i, sum = 0; 2843 2844\tfor_each_online_cpu(i) 2845\tsum += cpu_rq(i)-\u0026gt;nr_running; 2846 2847\treturn sum; 2848} 2849 2850/* 2851* Check if only the current task is running on the CPU. 2852* 2853* Caution: this function does not check that the caller has disabled 2854* preemption, thus the result might have a time-of-check-to-time-of-use 2855* race. The caller is responsible to use it correctly, for example: 2856* 2857* - from a non-preemptable section (of course) 2858* 2859* - from a thread that is bound to a single CPU 2860* 2861* - in a loop with very short iterations (e.g. a polling loop) 2862*/ 2863bool single_task_running(void) 2864{ 2865\treturn raw_rq()-\u0026gt;nr_running == 1; 2866} 2867EXPORT_SYMBOL(single_task_running); 2868 2869unsigned long long nr_context_switches(void) 2870{ 2871\tint i; 2872\tunsigned long long sum = 0; 2873 2874\tfor_each_possible_cpu(i) 2875\tsum += cpu_rq(i)-\u0026gt;nr_switches; 2876 2877\treturn sum; 2878} 2879 2880/* 2881* IO-wait accounting, and how its mostly bollocks (on SMP). 2882* 2883* The idea behind IO-wait account is to account the idle time that we could 2884* have spend running if it were not for IO. That is, if we were to improve the 2885* storage performance, we\u0026#39;d have a proportional reduction in IO-wait time. 2886* 2887* This all works nicely on UP, where, when a task blocks on IO, we account 2888* idle time as IO-wait, because if the storage were faster, it could\u0026#39;ve been 2889* running and we\u0026#39;d not be idle. 2890* 2891* This has been extended to SMP, by doing the same for each CPU. This however 2892* is broken. 2893* 2894* Imagine for instance the case where two tasks block on one CPU, only the one 2895* CPU will have IO-wait accounted, while the other has regular idle. Even 2896* though, if the storage were faster, both could\u0026#39;ve ran at the same time, 2897* utilising both CPUs. 2898* 2899* This means, that when looking globally, the current IO-wait accounting on 2900* SMP is a lower bound, by reason of under accounting. 2901* 2902* Worse, since the numbers are provided per CPU, they are sometimes 2903* interpreted per CPU, and that is nonsensical. A blocked task isn\u0026#39;t strictly 2904* associated with any one particular CPU, it can wake to another CPU than it 2905* blocked on. This means the per CPU IO-wait number is meaningless. 2906* 2907* Task CPU affinities can make all that even more \u0026#39;interesting\u0026#39;. 2908*/ 2909 2910unsigned long nr_iowait(void) 2911{ 2912\tunsigned long i, sum = 0; 2913 2914\tfor_each_possible_cpu(i) 2915\tsum += atomic_read(\u0026amp;cpu_rq(i)-\u0026gt;nr_iowait); 2916 2917\treturn sum; 2918} 2919 2920/* 2921* Consumers of these two interfaces, like for example the cpufreq menu 2922* governor are using nonsensical data. Boosting frequency for a CPU that has 2923* IO-wait which might not even end up running the task when it does become 2924* runnable. 2925*/ 2926 2927unsigned long nr_iowait_cpu(int cpu) 2928{ 2929\tstruct rq *this = cpu_rq(cpu); 2930\treturn atomic_read(\u0026amp;this-\u0026gt;nr_iowait); 2931} 2932 2933void get_iowait_load(unsigned long *nr_waiters, unsigned long *load) 2934{ 2935\tstruct rq *rq = this_rq(); 2936\t*nr_waiters = atomic_read(\u0026amp;rq-\u0026gt;nr_iowait); 2937\t*load = rq-\u0026gt;load.weight; 2938} 2939 2940#ifdef CONFIG_SMP 2941 2942/* 2943* sched_exec - execve() is a valuable balancing opportunity, because at 2944* this point the task has the smallest effective memory and cache footprint. 2945*/ 2946void sched_exec(void) 2947{ 2948\tstruct task_struct *p = current; 2949\tunsigned long flags; 2950\tint dest_cpu; 2951 2952\traw_spin_lock_irqsave(\u0026amp;p-\u0026gt;pi_lock, flags); 2953\tdest_cpu = p-\u0026gt;sched_class-\u0026gt;select_task_rq(p, task_cpu(p), SD_BALANCE_EXEC, 0); 2954\tif (dest_cpu == smp_processor_id()) 2955\tgoto unlock; 2956 2957\tif (likely(cpu_active(dest_cpu))) { 2958\tstruct migration_arg arg = { p, dest_cpu }; 2959 2960\traw_spin_unlock_irqrestore(\u0026amp;p-\u0026gt;pi_lock, flags); 2961\tstop_one_cpu(task_cpu(p), migration_cpu_stop, \u0026amp;arg); 2962\treturn; 2963\t} 2964unlock: 2965\traw_spin_unlock_irqrestore(\u0026amp;p-\u0026gt;pi_lock, flags); 2966} 2967 2968#endif 2969 2970DEFINE_PER_CPU(struct kernel_stat, kstat); 2971DEFINE_PER_CPU(struct kernel_cpustat, kernel_cpustat); 2972 2973EXPORT_PER_CPU_SYMBOL(kstat); 2974EXPORT_PER_CPU_SYMBOL(kernel_cpustat); 2975 2976/* 2977* The function fair_sched_class.update_curr accesses the struct curr 2978* and its field curr-\u0026gt;exec_start; when called from task_sched_runtime(), 2979* we observe a high rate of cache misses in practice. 2980* Prefetching this data results in improved performance. 2981*/ 2982static inline void prefetch_curr_exec_start(struct task_struct *p) 2983{ 2984#ifdef CONFIG_FAIR_GROUP_SCHED 2985\tstruct sched_entity *curr = (\u0026amp;p-\u0026gt;se)-\u0026gt;cfs_rq-\u0026gt;curr; 2986#else 2987\tstruct sched_entity *curr = (\u0026amp;task_rq(p)-\u0026gt;cfs)-\u0026gt;curr; 2988#endif 2989\tprefetch(curr); 2990\tprefetch(\u0026amp;curr-\u0026gt;exec_start); 2991} 2992 2993/* 2994* Return accounted runtime for the task. 2995* In case the task is currently running, return the runtime plus current\u0026#39;s 2996* pending runtime that have not been accounted yet. 2997*/ 2998unsigned long long task_sched_runtime(struct task_struct *p) 2999{ 3000\tstruct rq_flags rf; 3001\tstruct rq *rq; 3002\tu64 ns; 3003 3004#if defined(CONFIG_64BIT) \u0026amp;\u0026amp; defined(CONFIG_SMP) 3005\t/* 3006* 64-bit doesn\u0026#39;t need locks to atomically read a 64-bit value. 3007* So we have a optimization chance when the task\u0026#39;s delta_exec is 0. 3008* Reading -\u0026gt;on_cpu is racy, but this is ok. 3009* 3010* If we race with it leaving CPU, we\u0026#39;ll take a lock. So we\u0026#39;re correct. 3011* If we race with it entering CPU, unaccounted time is 0. This is 3012* indistinguishable from the read occurring a few cycles earlier. 3013* If we see -\u0026gt;on_cpu without -\u0026gt;on_rq, the task is leaving, and has 3014* been accounted, so we\u0026#39;re correct here as well. 3015*/ 3016\tif (!p-\u0026gt;on_cpu || !task_on_rq_queued(p)) 3017\treturn p-\u0026gt;se.sum_exec_runtime; 3018#endif 3019 3020\trq = task_rq_lock(p, \u0026amp;rf); 3021\t/* 3022* Must be -\u0026gt;curr _and_ -\u0026gt;on_rq. If dequeued, we would 3023* project cycles that may never be accounted to this 3024* thread, breaking clock_gettime(). 3025*/ 3026\tif (task_current(rq, p) \u0026amp;\u0026amp; task_on_rq_queued(p)) { 3027\tprefetch_curr_exec_start(p); 3028\tupdate_rq_clock(rq); 3029\tp-\u0026gt;sched_class-\u0026gt;update_curr(rq); 3030\t} 3031\tns = p-\u0026gt;se.sum_exec_runtime; 3032\ttask_rq_unlock(rq, p, \u0026amp;rf); 3033 3034\treturn ns; 3035} 3036 3037/* 3038* This function gets called by the timer code, with HZ frequency. 3039* We call it with interrupts disabled. 3040*/ 3041void scheduler_tick(void) 3042{ 3043\tint cpu = smp_processor_id(); 3044\tstruct rq *rq = cpu_rq(cpu); 3045\tstruct task_struct *curr = rq-\u0026gt;curr; 3046\tstruct rq_flags rf; 3047 3048\tsched_clock_tick(); 3049 3050\trq_lock(rq, \u0026amp;rf); 3051 3052\tupdate_rq_clock(rq); 3053\tcurr-\u0026gt;sched_class-\u0026gt;task_tick(rq, curr, 0); 3054\tcpu_load_update_active(rq); 3055\tcalc_global_load_tick(rq); 3056 3057\trq_unlock(rq, \u0026amp;rf); 3058 3059\tperf_event_task_tick(); 3060 3061#ifdef CONFIG_SMP 3062\trq-\u0026gt;idle_balance = idle_cpu(cpu); 3063\ttrigger_load_balance(rq); 3064#endif 3065} 3066 3067#ifdef CONFIG_NO_HZ_FULL 3068 3069struct tick_work { 3070\tint\tcpu; 3071\tatomic_t\tstate; 3072\tstruct delayed_work\twork; 3073}; 3074/* Values for -\u0026gt;state, see diagram below. */ 3075#define TICK_SCHED_REMOTE_OFFLINE\t0 3076#define TICK_SCHED_REMOTE_OFFLINING\t1 3077#define TICK_SCHED_REMOTE_RUNNING\t2 3078 3079/* 3080* State diagram for -\u0026gt;state: 3081* 3082* 3083* TICK_SCHED_REMOTE_OFFLINE 3084* | ^ 3085* | | 3086* | | sched_tick_remote() 3087* | | 3088* | | 3089* +--TICK_SCHED_REMOTE_OFFLINING 3090* | ^ 3091* | | 3092* sched_tick_start() | | sched_tick_stop() 3093* | | 3094* V | 3095* TICK_SCHED_REMOTE_RUNNING 3096* 3097* 3098* Other transitions get WARN_ON_ONCE(), except that sched_tick_remote() 3099* and sched_tick_start() are happy to leave the state in RUNNING. 3100*/ 3101 3102static struct tick_work __percpu *tick_work_cpu; 3103 3104static void sched_tick_remote(struct work_struct *work) 3105{ 3106\tstruct delayed_work *dwork = to_delayed_work(work); 3107\tstruct tick_work *twork = container_of(dwork, struct tick_work, work); 3108\tint cpu = twork-\u0026gt;cpu; 3109\tstruct rq *rq = cpu_rq(cpu); 3110\tstruct task_struct *curr; 3111\tstruct rq_flags rf; 3112\tu64 delta; 3113\tint os; 3114 3115\t/* 3116* Handle the tick only if it appears the remote CPU is running in full 3117* dynticks mode. The check is racy by nature, but missing a tick or 3118* having one too much is no big deal because the scheduler tick updates 3119* statistics and checks timeslices in a time-independent way, regardless 3120* of when exactly it is running. 3121*/ 3122\tif (idle_cpu(cpu) || !tick_nohz_tick_stopped_cpu(cpu)) 3123\tgoto out_requeue; 3124 3125\trq_lock_irq(rq, \u0026amp;rf); 3126\tcurr = rq-\u0026gt;curr; 3127\tif (is_idle_task(curr) || cpu_is_offline(cpu)) 3128\tgoto out_unlock; 3129 3130\tupdate_rq_clock(rq); 3131\tdelta = rq_clock_task(rq) - curr-\u0026gt;se.exec_start; 3132 3133\t/* 3134* Make sure the next tick runs within a reasonable 3135* amount of time. 3136*/ 3137\tWARN_ON_ONCE(delta \u0026gt; (u64)NSEC_PER_SEC * 3); 3138\tcurr-\u0026gt;sched_class-\u0026gt;task_tick(rq, curr, 0); 3139 3140out_unlock: 3141\trq_unlock_irq(rq, \u0026amp;rf); 3142 3143out_requeue: 3144\t/* 3145* Run the remote tick once per second (1Hz). This arbitrary 3146* frequency is large enough to avoid overload but short enough 3147* to keep scheduler internal stats reasonably up to date. But 3148* first update state to reflect hotplug activity if required. 3149*/ 3150\tos = atomic_fetch_add_unless(\u0026amp;twork-\u0026gt;state, -1, TICK_SCHED_REMOTE_RUNNING); 3151\tWARN_ON_ONCE(os == TICK_SCHED_REMOTE_OFFLINE); 3152\tif (os == TICK_SCHED_REMOTE_RUNNING) 3153\tqueue_delayed_work(system_unbound_wq, dwork, HZ); 3154} 3155 3156static void sched_tick_start(int cpu) 3157{ 3158\tint os; 3159\tstruct tick_work *twork; 3160 3161\tif (housekeeping_cpu(cpu, HK_FLAG_TICK)) 3162\treturn; 3163 3164\tWARN_ON_ONCE(!tick_work_cpu); 3165 3166\ttwork = per_cpu_ptr(tick_work_cpu, cpu); 3167\tos = atomic_xchg(\u0026amp;twork-\u0026gt;state, TICK_SCHED_REMOTE_RUNNING); 3168\tWARN_ON_ONCE(os == TICK_SCHED_REMOTE_RUNNING); 3169\tif (os == TICK_SCHED_REMOTE_OFFLINE) { 3170\ttwork-\u0026gt;cpu = cpu; 3171\tINIT_DELAYED_WORK(\u0026amp;twork-\u0026gt;work, sched_tick_remote); 3172\tqueue_delayed_work(system_unbound_wq, \u0026amp;twork-\u0026gt;work, HZ); 3173\t} 3174} 3175 3176#ifdef CONFIG_HOTPLUG_CPU 3177static void sched_tick_stop(int cpu) 3178{ 3179\tstruct tick_work *twork; 3180\tint os; 3181 3182\tif (housekeeping_cpu(cpu, HK_FLAG_TICK)) 3183\treturn; 3184 3185\tWARN_ON_ONCE(!tick_work_cpu); 3186 3187\ttwork = per_cpu_ptr(tick_work_cpu, cpu); 3188\t/* There cannot be competing actions, but don\u0026#39;t rely on stop-machine. */ 3189\tos = atomic_xchg(\u0026amp;twork-\u0026gt;state, TICK_SCHED_REMOTE_OFFLINING); 3190\tWARN_ON_ONCE(os != TICK_SCHED_REMOTE_RUNNING); 3191\t/* Don\u0026#39;t cancel, as this would mess up the state machine. */ 3192} 3193#endif /* CONFIG_HOTPLUG_CPU */3194 3195int __init sched_tick_offload_init(void) 3196{ 3197\ttick_work_cpu = alloc_percpu(struct tick_work); 3198\tBUG_ON(!tick_work_cpu); 3199\treturn 0; 3200} 3201 3202#else /* !CONFIG_NO_HZ_FULL */3203static inline void sched_tick_start(int cpu) { } 3204static inline void sched_tick_stop(int cpu) { } 3205#endif 3206 3207#if defined(CONFIG_PREEMPT) \u0026amp;\u0026amp; (defined(CONFIG_DEBUG_PREEMPT) || \\ 3208defined(CONFIG_TRACE_PREEMPT_TOGGLE)) 3209/* 3210* If the value passed in is equal to the current preempt count 3211* then we just disabled preemption. Start timing the latency. 3212*/ 3213static inline void preempt_latency_start(int val) 3214{ 3215\tif (preempt_count() == val) { 3216\tunsigned long ip = get_lock_parent_ip(); 3217#ifdef CONFIG_DEBUG_PREEMPT 3218\tcurrent-\u0026gt;preempt_disable_ip = ip; 3219#endif 3220\ttrace_preempt_off(CALLER_ADDR0, ip); 3221\t} 3222} 3223 3224void preempt_count_add(int val) 3225{ 3226#ifdef CONFIG_DEBUG_PREEMPT 3227\t/* 3228* Underflow? 3229*/ 3230\tif (DEBUG_LOCKS_WARN_ON((preempt_count() \u0026lt; 0))) 3231\treturn; 3232#endif 3233\t__preempt_count_add(val); 3234#ifdef CONFIG_DEBUG_PREEMPT 3235\t/* 3236* Spinlock count overflowing soon? 3237*/ 3238\tDEBUG_LOCKS_WARN_ON((preempt_count() \u0026amp; PREEMPT_MASK) \u0026gt;= 3239\tPREEMPT_MASK - 10); 3240#endif 3241\tpreempt_latency_start(val); 3242} 3243EXPORT_SYMBOL(preempt_count_add); 3244NOKPROBE_SYMBOL(preempt_count_add); 3245 3246/* 3247* If the value passed in equals to the current preempt count 3248* then we just enabled preemption. Stop timing the latency. 3249*/ 3250static inline void preempt_latency_stop(int val) 3251{ 3252\tif (preempt_count() == val) 3253\ttrace_preempt_on(CALLER_ADDR0, get_lock_parent_ip()); 3254} 3255 3256void preempt_count_sub(int val) 3257{ 3258#ifdef CONFIG_DEBUG_PREEMPT 3259\t/* 3260* Underflow? 3261*/ 3262\tif (DEBUG_LOCKS_WARN_ON(val \u0026gt; preempt_count())) 3263\treturn; 3264\t/* 3265* Is the spinlock portion underflowing? 3266*/ 3267\tif (DEBUG_LOCKS_WARN_ON((val \u0026lt; PREEMPT_MASK) \u0026amp;\u0026amp; 3268\t!(preempt_count() \u0026amp; PREEMPT_MASK))) 3269\treturn; 3270#endif 3271 3272\tpreempt_latency_stop(val); 3273\t__preempt_count_sub(val); 3274} 3275EXPORT_SYMBOL(preempt_count_sub); 3276NOKPROBE_SYMBOL(preempt_count_sub); 3277 3278#else 3279static inline void preempt_latency_start(int val) { } 3280static inline void preempt_latency_stop(int val) { } 3281#endif 3282 3283static inline unsigned long get_preempt_disable_ip(struct task_struct *p) 3284{ 3285#ifdef CONFIG_DEBUG_PREEMPT 3286\treturn p-\u0026gt;preempt_disable_ip; 3287#else 3288\treturn 0; 3289#endif 3290} 3291 3292/* 3293* Print scheduling while atomic bug: 3294*/ 3295static noinline void __schedule_bug(struct task_struct *prev) 3296{ 3297\t/* Save this before calling printk(), since that will clobber it */ 3298\tunsigned long preempt_disable_ip = get_preempt_disable_ip(current); 3299 3300\tif (oops_in_progress) 3301\treturn; 3302 3303\tprintk(KERN_ERR \u0026#34;BUG: scheduling while atomic: %s/%d/0x%08x\\n\u0026#34;, 3304\tprev-\u0026gt;comm, prev-\u0026gt;pid, preempt_count()); 3305 3306\tdebug_show_held_locks(prev); 3307\tprint_modules(); 3308\tif (irqs_disabled()) 3309\tprint_irqtrace_events(prev); 3310\tif (IS_ENABLED(CONFIG_DEBUG_PREEMPT) 3311\t\u0026amp;\u0026amp; in_atomic_preempt_off()) { 3312\tpr_err(\u0026#34;Preemption disabled at:\u0026#34;); 3313\tprint_ip_sym(preempt_disable_ip); 3314\tpr_cont(\u0026#34;\\n\u0026#34;); 3315\t} 3316\tif (panic_on_warn) 3317\tpanic(\u0026#34;scheduling while atomic\\n\u0026#34;); 3318 3319\tdump_stack(); 3320\tadd_taint(TAINT_WARN, LOCKDEP_STILL_OK); 3321} 3322 3323/* 3324* Various schedule()-time debugging checks and statistics: 3325*/ 3326static inline void schedule_debug(struct task_struct *prev) 3327{ 3328#ifdef CONFIG_SCHED_STACK_END_CHECK 3329\tif (task_stack_end_corrupted(prev)) 3330\tpanic(\u0026#34;corrupted stack end detected inside scheduler\\n\u0026#34;); 3331#endif 3332 3333\tif (unlikely(in_atomic_preempt_off())) { 3334\t__schedule_bug(prev); 3335\tpreempt_count_set(PREEMPT_DISABLED); 3336\t} 3337\trcu_sleep_check(); 3338 3339\tprofile_hit(SCHED_PROFILING, __builtin_return_address(0)); 3340 3341\tschedstat_inc(this_rq()-\u0026gt;sched_count); 3342} 3343 3344/* 3345* Pick up the highest-prio task: 3346*/ 3347static inline struct task_struct * 3348pick_next_task(struct rq *rq, struct task_struct *prev, struct rq_flags *rf) 3349{ 3350\tconst struct sched_class *class; 3351\tstruct task_struct *p; 3352 3353\t/* 3354* Optimization: we know that if all tasks are in the fair class we can 3355* call that function directly, but only if the @prev task wasn\u0026#39;t of a 3356* higher scheduling class, because otherwise those loose the 3357* opportunity to pull in more work from other CPUs. 3358*/ 3359\tif (likely((prev-\u0026gt;sched_class == \u0026amp;idle_sched_class || 3360\tprev-\u0026gt;sched_class == \u0026amp;fair_sched_class) \u0026amp;\u0026amp; 3361\trq-\u0026gt;nr_running == rq-\u0026gt;cfs.h_nr_running)) { 3362 3363\tp = fair_sched_class.pick_next_task(rq, prev, rf); 3364\tif (unlikely(p == RETRY_TASK)) 3365\tgoto again; 3366 3367\t/* Assumes fair_sched_class-\u0026gt;next == idle_sched_class */ 3368\tif (unlikely(!p)) 3369\tp = idle_sched_class.pick_next_task(rq, prev, rf); 3370 3371\treturn p; 3372\t} 3373 3374again: 3375\tfor_each_class(class) { 3376\tp = class-\u0026gt;pick_next_task(rq, prev, rf); 3377\tif (p) { 3378\tif (unlikely(p == RETRY_TASK)) 3379\tgoto again; 3380\treturn p; 3381\t} 3382\t} 3383 3384\t/* The idle class should always have a runnable task: */ 3385\tBUG(); 3386} 3387 3388/* 3389* __schedule() is the main scheduler function. 3390* 3391* The main means of driving the scheduler and thus entering this function are: 3392* 3393* 1. Explicit blocking: mutex, semaphore, waitqueue, etc. 3394* 3395* 2. TIF_NEED_RESCHED flag is checked on interrupt and userspace return 3396* paths. For example, see arch/x86/entry_64.S. 3397* 3398* To drive preemption between tasks, the scheduler sets the flag in timer 3399* interrupt handler scheduler_tick(). 3400* 3401* 3. Wakeups don\u0026#39;t really cause entry into schedule(). They add a 3402* task to the run-queue and that\u0026#39;s it. 3403* 3404* Now, if the new task added to the run-queue preempts the current 3405* task, then the wakeup sets TIF_NEED_RESCHED and schedule() gets 3406* called on the nearest possible occasion: 3407* 3408* - If the kernel is preemptible (CONFIG_PREEMPT=y): 3409* 3410* - in syscall or exception context, at the next outmost 3411* preempt_enable(). (this might be as soon as the wake_up()\u0026#39;s 3412* spin_unlock()!) 3413* 3414* - in IRQ context, return from interrupt-handler to 3415* preemptible context 3416* 3417* - If the kernel is not preemptible (CONFIG_PREEMPT is not set) 3418* then at the next: 3419* 3420* - cond_resched() call 3421* - explicit schedule() call 3422* - return from syscall or exception to user-space 3423* - return from interrupt-handler to user-space 3424* 3425* WARNING: must be called with preemption disabled! 3426*/ 3427static void __sched notrace __schedule(bool preempt) 3428{ 3429\tstruct task_struct *prev, *next; 3430\tunsigned long *switch_count; 3431\tstruct rq_flags rf; 3432\tstruct rq *rq; 3433\tint cpu; 3434 3435\tcpu = smp_processor_id(); 3436\trq = cpu_rq(cpu); 3437\tprev = rq-\u0026gt;curr; 3438 3439\tschedule_debug(prev); 3440 3441\tif (sched_feat(HRTICK)) 3442\thrtick_clear(rq); 3443 3444\tlocal_irq_disable(); 3445\trcu_note_context_switch(preempt); 3446 3447\t/* 3448* Make sure that signal_pending_state()-\u0026gt;signal_pending() below 3449* can\u0026#39;t be reordered with __set_current_state(TASK_INTERRUPTIBLE) 3450* done by the caller to avoid the race with signal_wake_up(). 3451* 3452* The membarrier system call requires a full memory barrier 3453* after coming from user-space, before storing to rq-\u0026gt;curr. 3454*/ 3455\trq_lock(rq, \u0026amp;rf); 3456\tsmp_mb__after_spinlock(); 3457 3458\t/* Promote REQ to ACT */ 3459\trq-\u0026gt;clock_update_flags \u0026lt;\u0026lt;= 1; 3460\tupdate_rq_clock(rq); 3461 3462\tswitch_count = \u0026amp;prev-\u0026gt;nivcsw; 3463\tif (!preempt \u0026amp;\u0026amp; prev-\u0026gt;state) { 3464\tif (unlikely(signal_pending_state(prev-\u0026gt;state, prev))) { 3465\tprev-\u0026gt;state = TASK_RUNNING; 3466\t} else { 3467\tdeactivate_task(rq, prev, DEQUEUE_SLEEP | DEQUEUE_NOCLOCK); 3468\tprev-\u0026gt;on_rq = 0; 3469 3470\tif (prev-\u0026gt;in_iowait) { 3471\tatomic_inc(\u0026amp;rq-\u0026gt;nr_iowait); 3472\tdelayacct_blkio_start(); 3473\t} 3474 3475\t/* 3476* If a worker went to sleep, notify and ask workqueue 3477* whether it wants to wake up a task to maintain 3478* concurrency. 3479*/ 3480\tif (prev-\u0026gt;flags \u0026amp; PF_WQ_WORKER) { 3481\tstruct task_struct *to_wakeup; 3482 3483\tto_wakeup = wq_worker_sleeping(prev); 3484\tif (to_wakeup) 3485\ttry_to_wake_up_local(to_wakeup, \u0026amp;rf); 3486\t} 3487\t} 3488\tswitch_count = \u0026amp;prev-\u0026gt;nvcsw; 3489\t} 3490 3491\tnext = pick_next_task(rq, prev, \u0026amp;rf); 3492\tclear_tsk_need_resched(prev); 3493\tclear_preempt_need_resched(); 3494 3495\tif (likely(prev != next)) { 3496\trq-\u0026gt;nr_switches++; 3497\trq-\u0026gt;curr = next; 3498\t/* 3499* The membarrier system call requires each architecture 3500* to have a full memory barrier after updating 3501* rq-\u0026gt;curr, before returning to user-space. 3502* 3503* Here are the schemes providing that barrier on the 3504* various architectures: 3505* - mm ? switch_mm() : mmdrop() for x86, s390, sparc, PowerPC. 3506* switch_mm() rely on membarrier_arch_switch_mm() on PowerPC. 3507* - finish_lock_switch() for weakly-ordered 3508* architectures where spin_unlock is a full barrier, 3509* - switch_to() for arm64 (weakly-ordered, spin_unlock 3510* is a RELEASE barrier), 3511*/ 3512\t++*switch_count; 3513 3514\ttrace_sched_switch(preempt, prev, next); 3515 3516\t/* Also unlocks the rq: */ 3517\trq = context_switch(rq, prev, next, \u0026amp;rf); 3518\t} else { 3519\trq-\u0026gt;clock_update_flags \u0026amp;= ~(RQCF_ACT_SKIP|RQCF_REQ_SKIP); 3520\trq_unlock_irq(rq, \u0026amp;rf); 3521\t} 3522 3523\tbalance_callback(rq); 3524} 3525 3526void __noreturn do_task_dead(void) 3527{ 3528\t/* Causes final put_task_struct in finish_task_switch(): */ 3529\tset_special_state(TASK_DEAD); 3530 3531\t/* Tell freezer to ignore us: */ 3532\tcurrent-\u0026gt;flags |= PF_NOFREEZE; 3533 3534\t__schedule(false); 3535\tBUG(); 3536 3537\t/* Avoid \u0026#34;noreturn function does return\u0026#34; - but don\u0026#39;t continue if BUG() is a NOP: */ 3538\tfor (;;) 3539\tcpu_relax(); 3540} 3541 3542static inline void sched_submit_work(struct task_struct *tsk) 3543{ 3544\tif (!tsk-\u0026gt;state || tsk_is_pi_blocked(tsk)) 3545\treturn; 3546\t/* 3547* If we are going to sleep and we have plugged IO queued, 3548* make sure to submit it to avoid deadlocks. 3549*/ 3550\tif (blk_needs_flush_plug(tsk)) 3551\tblk_schedule_flush_plug(tsk); 3552} 3553 3554asmlinkage __visible void __sched schedule(void) 3555{ 3556\tstruct task_struct *tsk = current; 3557 3558\tsched_submit_work(tsk); 3559\tdo { 3560\tpreempt_disable(); 3561\t__schedule(false); 3562\tsched_preempt_enable_no_resched(); 3563\t} while (need_resched()); 3564} 3565EXPORT_SYMBOL(schedule); 3566 3567/* 3568* synchronize_rcu_tasks() makes sure that no task is stuck in preempted 3569* state (have scheduled out non-voluntarily) by making sure that all 3570* tasks have either left the run queue or have gone into user space. 3571* As idle tasks do not do either, they must not ever be preempted 3572* (schedule out non-voluntarily). 3573* 3574* schedule_idle() is similar to schedule_preempt_disable() except that it 3575* never enables preemption because it does not call sched_submit_work(). 3576*/ 3577void __sched schedule_idle(void) 3578{ 3579\t/* 3580* As this skips calling sched_submit_work(), which the idle task does 3581* regardless because that function is a nop when the task is in a 3582* TASK_RUNNING state, make sure this isn\u0026#39;t used someplace that the 3583* current task can be in any other state. Note, idle is always in the 3584* TASK_RUNNING state. 3585*/ 3586\tWARN_ON_ONCE(current-\u0026gt;state); 3587\tdo { 3588\t__schedule(false); 3589\t} while (need_resched()); 3590} 3591 3592#ifdef CONFIG_CONTEXT_TRACKING 3593asmlinkage __visible void __sched schedule_user(void) 3594{ 3595\t/* 3596* If we come here after a random call to set_need_resched(), 3597* or we have been woken up remotely but the IPI has not yet arrived, 3598* we haven\u0026#39;t yet exited the RCU idle mode. Do it here manually until 3599* we find a better solution. 3600* 3601* NB: There are buggy callers of this function. Ideally we 3602* should warn if prev_state != CONTEXT_USER, but that will trigger 3603* too frequently to make sense yet. 3604*/ 3605\tenum ctx_state prev_state = exception_enter(); 3606\tschedule(); 3607\texception_exit(prev_state); 3608} 3609#endif 3610 3611/** 3612* schedule_preempt_disabled - called with preemption disabled 3613* 3614* Returns with preemption disabled. Note: preempt_count must be 1 3615*/ 3616void __sched schedule_preempt_disabled(void) 3617{ 3618\tsched_preempt_enable_no_resched(); 3619\tschedule(); 3620\tpreempt_disable(); 3621} 3622 3623static void __sched notrace preempt_schedule_common(void) 3624{ 3625\tdo { 3626\t/* 3627* Because the function tracer can trace preempt_count_sub() 3628* and it also uses preempt_enable/disable_notrace(), if 3629* NEED_RESCHED is set, the preempt_enable_notrace() called 3630* by the function tracer will call this function again and 3631* cause infinite recursion. 3632* 3633* Preemption must be disabled here before the function 3634* tracer can trace. Break up preempt_disable() into two 3635* calls. One to disable preemption without fear of being 3636* traced. The other to still record the preemption latency, 3637* which can also be traced by the function tracer. 3638*/ 3639\tpreempt_disable_notrace(); 3640\tpreempt_latency_start(1); 3641\t__schedule(true); 3642\tpreempt_latency_stop(1); 3643\tpreempt_enable_no_resched_notrace(); 3644 3645\t/* 3646* Check again in case we missed a preemption opportunity 3647* between schedule and now. 3648*/ 3649\t} while (need_resched()); 3650} 3651 3652#ifdef CONFIG_PREEMPT 3653/* 3654* this is the entry point to schedule() from in-kernel preemption 3655* off of preempt_enable. Kernel preemptions off return from interrupt 3656* occur there and call schedule directly. 3657*/ 3658asmlinkage __visible void __sched notrace preempt_schedule(void) 3659{ 3660\t/* 3661* If there is a non-zero preempt_count or interrupts are disabled, 3662* we do not want to preempt the current task. Just return.. 3663*/ 3664\tif (likely(!preemptible())) 3665\treturn; 3666 3667\tpreempt_schedule_common(); 3668} 3669NOKPROBE_SYMBOL(preempt_schedule); 3670EXPORT_SYMBOL(preempt_schedule); 3671 3672/** 3673* preempt_schedule_notrace - preempt_schedule called by tracing 3674* 3675* The tracing infrastructure uses preempt_enable_notrace to prevent 3676* recursion and tracing preempt enabling caused by the tracing 3677* infrastructure itself. But as tracing can happen in areas coming 3678* from userspace or just about to enter userspace, a preempt enable 3679* can occur before user_exit() is called. This will cause the scheduler 3680* to be called when the system is still in usermode. 3681* 3682* To prevent this, the preempt_enable_notrace will use this function 3683* instead of preempt_schedule() to exit user context if needed before 3684* calling the scheduler. 3685*/ 3686asmlinkage __visible void __sched notrace preempt_schedule_notrace(void) 3687{ 3688\tenum ctx_state prev_ctx; 3689 3690\tif (likely(!preemptible())) 3691\treturn; 3692 3693\tdo { 3694\t/* 3695* Because the function tracer can trace preempt_count_sub() 3696* and it also uses preempt_enable/disable_notrace(), if 3697* NEED_RESCHED is set, the preempt_enable_notrace() called 3698* by the function tracer will call this function again and 3699* cause infinite recursion. 3700* 3701* Preemption must be disabled here before the function 3702* tracer can trace. Break up preempt_disable() into two 3703* calls. One to disable preemption without fear of being 3704* traced. The other to still record the preemption latency, 3705* which can also be traced by the function tracer. 3706*/ 3707\tpreempt_disable_notrace(); 3708\tpreempt_latency_start(1); 3709\t/* 3710* Needs preempt disabled in case user_exit() is traced 3711* and the tracer calls preempt_enable_notrace() causing 3712* an infinite recursion. 3713*/ 3714\tprev_ctx = exception_enter(); 3715\t__schedule(true); 3716\texception_exit(prev_ctx); 3717 3718\tpreempt_latency_stop(1); 3719\tpreempt_enable_no_resched_notrace(); 3720\t} while (need_resched()); 3721} 3722EXPORT_SYMBOL_GPL(preempt_schedule_notrace); 3723 3724#endif /* CONFIG_PREEMPT */3725 3726/* 3727* this is the entry point to schedule() from kernel preemption 3728* off of irq context. 3729* Note, that this is called and return with irqs disabled. This will 3730* protect us against recursive calling from irq. 3731*/ 3732asmlinkage __visible void __sched preempt_schedule_irq(void) 3733{ 3734\tenum ctx_state prev_state; 3735 3736\t/* Catch callers which need to be fixed */ 3737\tBUG_ON(preempt_count() || !irqs_disabled()); 3738 3739\tprev_state = exception_enter(); 3740 3741\tdo { 3742\tpreempt_disable(); 3743\tlocal_irq_enable(); 3744\t__schedule(true); 3745\tlocal_irq_disable(); 3746\tsched_preempt_enable_no_resched(); 3747\t} while (need_resched()); 3748 3749\texception_exit(prev_state); 3750} 3751 3752int default_wake_function(wait_queue_entry_t *curr, unsigned mode, int wake_flags, 3753\tvoid *key) 3754{ 3755\treturn try_to_wake_up(curr-\u0026gt;private, mode, wake_flags); 3756} 3757EXPORT_SYMBOL(default_wake_function); 3758 3759#ifdef CONFIG_RT_MUTEXES 3760 3761static inline int __rt_effective_prio(struct task_struct *pi_task, int prio) 3762{ 3763\tif (pi_task) 3764\tprio = min(prio, pi_task-\u0026gt;prio); 3765 3766\treturn prio; 3767} 3768 3769static inline int rt_effective_prio(struct task_struct *p, int prio) 3770{ 3771\tstruct task_struct *pi_task = rt_mutex_get_top_task(p); 3772 3773\treturn __rt_effective_prio(pi_task, prio); 3774} 3775 3776/* 3777* rt_mutex_setprio - set the current priority of a task 3778* @p: task to boost 3779* @pi_task: donor task 3780* 3781* This function changes the \u0026#39;effective\u0026#39; priority of a task. It does 3782* not touch -\u0026gt;normal_prio like __setscheduler(). 3783* 3784* Used by the rt_mutex code to implement priority inheritance 3785* logic. Call site only calls if the priority of the task changed. 3786*/ 3787void rt_mutex_setprio(struct task_struct *p, struct task_struct *pi_task) 3788{ 3789\tint prio, oldprio, queued, running, queue_flag = 3790\tDEQUEUE_SAVE | DEQUEUE_MOVE | DEQUEUE_NOCLOCK; 3791\tconst struct sched_class *prev_class; 3792\tstruct rq_flags rf; 3793\tstruct rq *rq; 3794 3795\t/* XXX used to be waiter-\u0026gt;prio, not waiter-\u0026gt;task-\u0026gt;prio */ 3796\tprio = __rt_effective_prio(pi_task, p-\u0026gt;normal_prio); 3797 3798\t/* 3799* If nothing changed; bail early. 3800*/ 3801\tif (p-\u0026gt;pi_top_task == pi_task \u0026amp;\u0026amp; prio == p-\u0026gt;prio \u0026amp;\u0026amp; !dl_prio(prio)) 3802\treturn; 3803 3804\trq = __task_rq_lock(p, \u0026amp;rf); 3805\tupdate_rq_clock(rq); 3806\t/* 3807* Set under pi_lock \u0026amp;\u0026amp; rq-\u0026gt;lock, such that the value can be used under 3808* either lock. 3809* 3810* Note that there is loads of tricky to make this pointer cache work 3811* right. rt_mutex_slowunlock()+rt_mutex_postunlock() work together to 3812* ensure a task is de-boosted (pi_task is set to NULL) before the 3813* task is allowed to run again (and can exit). This ensures the pointer 3814* points to a blocked task -- which guaratees the task is present. 3815*/ 3816\tp-\u0026gt;pi_top_task = pi_task; 3817 3818\t/* 3819* For FIFO/RR we only need to set prio, if that matches we\u0026#39;re done. 3820*/ 3821\tif (prio == p-\u0026gt;prio \u0026amp;\u0026amp; !dl_prio(prio)) 3822\tgoto out_unlock; 3823 3824\t/* 3825* Idle task boosting is a nono in general. There is one 3826* exception, when PREEMPT_RT and NOHZ is active: 3827* 3828* The idle task calls get_next_timer_interrupt() and holds 3829* the timer wheel base-\u0026gt;lock on the CPU and another CPU wants 3830* to access the timer (probably to cancel it). We can safely 3831* ignore the boosting request, as the idle CPU runs this code 3832* with interrupts disabled and will complete the lock 3833* protected section without being interrupted. So there is no 3834* real need to boost. 3835*/ 3836\tif (unlikely(p == rq-\u0026gt;idle)) { 3837\tWARN_ON(p != rq-\u0026gt;curr); 3838\tWARN_ON(p-\u0026gt;pi_blocked_on); 3839\tgoto out_unlock; 3840\t} 3841 3842\ttrace_sched_pi_setprio(p, pi_task); 3843\toldprio = p-\u0026gt;prio; 3844 3845\tif (oldprio == prio) 3846\tqueue_flag \u0026amp;= ~DEQUEUE_MOVE; 3847 3848\tprev_class = p-\u0026gt;sched_class; 3849\tqueued = task_on_rq_queued(p); 3850\trunning = task_current(rq, p); 3851\tif (queued) 3852\tdequeue_task(rq, p, queue_flag); 3853\tif (running) 3854\tput_prev_task(rq, p); 3855 3856\t/* 3857* Boosting condition are: 3858* 1. -rt task is running and holds mutex A 3859* --\u0026gt; -dl task blocks on mutex A 3860* 3861* 2. -dl task is running and holds mutex A 3862* --\u0026gt; -dl task blocks on mutex A and could preempt the 3863* running task 3864*/ 3865\tif (dl_prio(prio)) { 3866\tif (!dl_prio(p-\u0026gt;normal_prio) || 3867\t(pi_task \u0026amp;\u0026amp; dl_prio(pi_task-\u0026gt;prio) \u0026amp;\u0026amp; 3868\tdl_entity_preempt(\u0026amp;pi_task-\u0026gt;dl, \u0026amp;p-\u0026gt;dl))) { 3869\tp-\u0026gt;dl.dl_boosted = 1; 3870\tqueue_flag |= ENQUEUE_REPLENISH; 3871\t} else 3872\tp-\u0026gt;dl.dl_boosted = 0; 3873\tp-\u0026gt;sched_class = \u0026amp;dl_sched_class; 3874\t} else if (rt_prio(prio)) { 3875\tif (dl_prio(oldprio)) 3876\tp-\u0026gt;dl.dl_boosted = 0; 3877\tif (oldprio \u0026lt; prio) 3878\tqueue_flag |= ENQUEUE_HEAD; 3879\tp-\u0026gt;sched_class = \u0026amp;rt_sched_class; 3880\t} else { 3881\tif (dl_prio(oldprio)) 3882\tp-\u0026gt;dl.dl_boosted = 0; 3883\tif (rt_prio(oldprio)) 3884\tp-\u0026gt;rt.timeout = 0; 3885\tp-\u0026gt;sched_class = \u0026amp;fair_sched_class; 3886\t} 3887 3888\tp-\u0026gt;prio = prio; 3889 3890\tif (queued) 3891\tenqueue_task(rq, p, queue_flag); 3892\tif (running) 3893\tset_curr_task(rq, p); 3894 3895\tcheck_class_changed(rq, p, prev_class, oldprio); 3896out_unlock: 3897\t/* Avoid rq from going away on us: */ 3898\tpreempt_disable(); 3899\t__task_rq_unlock(rq, \u0026amp;rf); 3900 3901\tbalance_callback(rq); 3902\tpreempt_enable(); 3903} 3904#else 3905static inline int rt_effective_prio(struct task_struct *p, int prio) 3906{ 3907\treturn prio; 3908} 3909#endif 3910 3911void set_user_nice(struct task_struct *p, long nice) 3912{ 3913\tbool queued, running; 3914\tint old_prio, delta; 3915\tstruct rq_flags rf; 3916\tstruct rq *rq; 3917 3918\tif (task_nice(p) == nice || nice \u0026lt; MIN_NICE || nice \u0026gt; MAX_NICE) 3919\treturn; 3920\t/* 3921* We have to be careful, if called from sys_setpriority(), 3922* the task might be in the middle of scheduling on another CPU. 3923*/ 3924\trq = task_rq_lock(p, \u0026amp;rf); 3925\tupdate_rq_clock(rq); 3926 3927\t/* 3928* The RT priorities are set via sched_setscheduler(), but we still 3929* allow the \u0026#39;normal\u0026#39; nice value to be set - but as expected 3930* it wont have any effect on scheduling until the task is 3931* SCHED_DEADLINE, SCHED_FIFO or SCHED_RR: 3932*/ 3933\tif (task_has_dl_policy(p) || task_has_rt_policy(p)) { 3934\tp-\u0026gt;static_prio = NICE_TO_PRIO(nice); 3935\tgoto out_unlock; 3936\t} 3937\tqueued = task_on_rq_queued(p); 3938\trunning = task_current(rq, p); 3939\tif (queued) 3940\tdequeue_task(rq, p, DEQUEUE_SAVE | DEQUEUE_NOCLOCK); 3941\tif (running) 3942\tput_prev_task(rq, p); 3943 3944\tp-\u0026gt;static_prio = NICE_TO_PRIO(nice); 3945\tset_load_weight(p, true); 3946\told_prio = p-\u0026gt;prio; 3947\tp-\u0026gt;prio = effective_prio(p); 3948\tdelta = p-\u0026gt;prio - old_prio; 3949 3950\tif (queued) { 3951\tenqueue_task(rq, p, ENQUEUE_RESTORE | ENQUEUE_NOCLOCK); 3952\t/* 3953* If the task increased its priority or is running and 3954* lowered its priority, then reschedule its CPU: 3955*/ 3956\tif (delta \u0026lt; 0 || (delta \u0026gt; 0 \u0026amp;\u0026amp; task_running(rq, p))) 3957\tresched_curr(rq); 3958\t} 3959\tif (running) 3960\tset_curr_task(rq, p); 3961out_unlock: 3962\ttask_rq_unlock(rq, p, \u0026amp;rf); 3963} 3964EXPORT_SYMBOL(set_user_nice); 3965 3966/* 3967* can_nice - check if a task can reduce its nice value 3968* @p: task 3969* @nice: nice value 3970*/ 3971int can_nice(const struct task_struct *p, const int nice) 3972{ 3973\t/* Convert nice value [19,-20] to rlimit style value [1,40]: */ 3974\tint nice_rlim = nice_to_rlimit(nice); 3975 3976\treturn (nice_rlim \u0026lt;= task_rlimit(p, RLIMIT_NICE) || 3977\tcapable(CAP_SYS_NICE)); 3978} 3979 3980#ifdef __ARCH_WANT_SYS_NICE 3981 3982/* 3983* sys_nice - change the priority of the current process. 3984* @increment: priority increment 3985* 3986* sys_setpriority is a more generic, but much slower function that 3987* does similar things. 3988*/ 3989SYSCALL_DEFINE1(nice, int, increment) 3990{ 3991\tlong nice, retval; 3992 3993\t/* 3994* Setpriority might change our priority at the same moment. 3995* We don\u0026#39;t have to worry. Conceptually one call occurs first 3996* and we have a single winner. 3997*/ 3998\tincrement = clamp(increment, -NICE_WIDTH, NICE_WIDTH); 3999\tnice = task_nice(current) + increment; 4000 4001\tnice = clamp_val(nice, MIN_NICE, MAX_NICE); 4002\tif (increment \u0026lt; 0 \u0026amp;\u0026amp; !can_nice(current, nice)) 4003\treturn -EPERM; 4004 4005\tretval = security_task_setnice(current, nice); 4006\tif (retval) 4007\treturn retval; 4008 4009\tset_user_nice(current, nice); 4010\treturn 0; 4011} 4012 4013#endif 4014 4015/** 4016* task_prio - return the priority value of a given task. 4017* @p: the task in question. 4018* 4019* Return: The priority value as seen by users in /proc. 4020* RT tasks are offset by -200. Normal tasks are centered 4021* around 0, value goes from -16 to +15. 4022*/ 4023int task_prio(const struct task_struct *p) 4024{ 4025\treturn p-\u0026gt;prio - MAX_RT_PRIO; 4026} 4027 4028/** 4029* idle_cpu - is a given CPU idle currently? 4030* @cpu: the processor in question. 4031* 4032* Return: 1 if the CPU is currently idle. 0 otherwise. 4033*/ 4034int idle_cpu(int cpu) 4035{ 4036\tstruct rq *rq = cpu_rq(cpu); 4037 4038\tif (rq-\u0026gt;curr != rq-\u0026gt;idle) 4039\treturn 0; 4040 4041\tif (rq-\u0026gt;nr_running) 4042\treturn 0; 4043 4044#ifdef CONFIG_SMP 4045\tif (!llist_empty(\u0026amp;rq-\u0026gt;wake_list)) 4046\treturn 0; 4047#endif 4048 4049\treturn 1; 4050} 4051 4052/** 4053* available_idle_cpu - is a given CPU idle for enqueuing work. 4054* @cpu: the CPU in question. 4055* 4056* Return: 1 if the CPU is currently idle. 0 otherwise. 4057*/ 4058int available_idle_cpu(int cpu) 4059{ 4060\tif (!idle_cpu(cpu)) 4061\treturn 0; 4062 4063\tif (vcpu_is_preempted(cpu)) 4064\treturn 0; 4065 4066\treturn 1; 4067} 4068 4069/** 4070* idle_task - return the idle task for a given CPU. 4071* @cpu: the processor in question. 4072* 4073* Return: The idle task for the CPU @cpu. 4074*/ 4075struct task_struct *idle_task(int cpu) 4076{ 4077\treturn cpu_rq(cpu)-\u0026gt;idle; 4078} 4079 4080/** 4081* find_process_by_pid - find a process with a matching PID value. 4082* @pid: the pid in question. 4083* 4084* The task of @pid, if found. %NULL otherwise. 4085*/ 4086static struct task_struct *find_process_by_pid(pid_t pid) 4087{ 4088\treturn pid ? find_task_by_vpid(pid) : current; 4089} 4090 4091/* 4092* sched_setparam() passes in -1 for its policy, to let the functions 4093* it calls know not to change it. 4094*/ 4095#define SETPARAM_POLICY\t-1 4096 4097static void __setscheduler_params(struct task_struct *p, 4098\tconst struct sched_attr *attr) 4099{ 4100\tint policy = attr-\u0026gt;sched_policy; 4101 4102\tif (policy == SETPARAM_POLICY) 4103\tpolicy = p-\u0026gt;policy; 4104 4105\tp-\u0026gt;policy = policy; 4106 4107\tif (dl_policy(policy)) 4108\t__setparam_dl(p, attr); 4109\telse if (fair_policy(policy)) 4110\tp-\u0026gt;static_prio = NICE_TO_PRIO(attr-\u0026gt;sched_nice); 4111 4112\t/* 4113* __sched_setscheduler() ensures attr-\u0026gt;sched_priority == 0 when 4114* !rt_policy. Always setting this ensures that things like 4115* getparam()/getattr() don\u0026#39;t report silly values for !rt tasks. 4116*/ 4117\tp-\u0026gt;rt_priority = attr-\u0026gt;sched_priority; 4118\tp-\u0026gt;normal_prio = normal_prio(p); 4119\tset_load_weight(p, true); 4120} 4121 4122/* Actually do priority change: must hold pi \u0026amp; rq lock. */ 4123static void __setscheduler(struct rq *rq, struct task_struct *p, 4124\tconst struct sched_attr *attr, bool keep_boost) 4125{ 4126\t__setscheduler_params(p, attr); 4127 4128\t/* 4129* Keep a potential priority boosting if called from 4130* sched_setscheduler(). 4131*/ 4132\tp-\u0026gt;prio = normal_prio(p); 4133\tif (keep_boost) 4134\tp-\u0026gt;prio = rt_effective_prio(p, p-\u0026gt;prio); 4135 4136\tif (dl_prio(p-\u0026gt;prio)) 4137\tp-\u0026gt;sched_class = \u0026amp;dl_sched_class; 4138\telse if (rt_prio(p-\u0026gt;prio)) 4139\tp-\u0026gt;sched_class = \u0026amp;rt_sched_class; 4140\telse 4141\tp-\u0026gt;sched_class = \u0026amp;fair_sched_class; 4142} 4143 4144/* 4145* Check the target process has a UID that matches the current process\u0026#39;s: 4146*/ 4147static bool check_same_owner(struct task_struct *p) 4148{ 4149\tconst struct cred *cred = current_cred(), *pcred; 4150\tbool match; 4151 4152\trcu_read_lock(); 4153\tpcred = __task_cred(p); 4154\tmatch = (uid_eq(cred-\u0026gt;euid, pcred-\u0026gt;euid) || 4155\tuid_eq(cred-\u0026gt;euid, pcred-\u0026gt;uid)); 4156\trcu_read_unlock(); 4157\treturn match; 4158} 4159 4160static int __sched_setscheduler(struct task_struct *p, 4161\tconst struct sched_attr *attr, 4162\tbool user, bool pi) 4163{ 4164\tint newprio = dl_policy(attr-\u0026gt;sched_policy) ? MAX_DL_PRIO - 1 : 4165\tMAX_RT_PRIO - 1 - attr-\u0026gt;sched_priority; 4166\tint retval, oldprio, oldpolicy = -1, queued, running; 4167\tint new_effective_prio, policy = attr-\u0026gt;sched_policy; 4168\tconst struct sched_class *prev_class; 4169\tstruct rq_flags rf; 4170\tint reset_on_fork; 4171\tint queue_flags = DEQUEUE_SAVE | DEQUEUE_MOVE | DEQUEUE_NOCLOCK; 4172\tstruct rq *rq; 4173 4174\t/* The pi code expects interrupts enabled */ 4175\tBUG_ON(pi \u0026amp;\u0026amp; in_interrupt()); 4176recheck: 4177\t/* Double check policy once rq lock held: */ 4178\tif (policy \u0026lt; 0) { 4179\treset_on_fork = p-\u0026gt;sched_reset_on_fork; 4180\tpolicy = oldpolicy = p-\u0026gt;policy; 4181\t} else { 4182\treset_on_fork = !!(attr-\u0026gt;sched_flags \u0026amp; SCHED_FLAG_RESET_ON_FORK); 4183 4184\tif (!valid_policy(policy)) 4185\treturn -EINVAL; 4186\t} 4187 4188\tif (attr-\u0026gt;sched_flags \u0026amp; ~(SCHED_FLAG_ALL | SCHED_FLAG_SUGOV)) 4189\treturn -EINVAL; 4190 4191\t/* 4192* Valid priorities for SCHED_FIFO and SCHED_RR are 4193* 1..MAX_USER_RT_PRIO-1, valid priority for SCHED_NORMAL, 4194* SCHED_BATCH and SCHED_IDLE is 0. 4195*/ 4196\tif ((p-\u0026gt;mm \u0026amp;\u0026amp; attr-\u0026gt;sched_priority \u0026gt; MAX_USER_RT_PRIO-1) || 4197\t(!p-\u0026gt;mm \u0026amp;\u0026amp; attr-\u0026gt;sched_priority \u0026gt; MAX_RT_PRIO-1)) 4198\treturn -EINVAL; 4199\tif ((dl_policy(policy) \u0026amp;\u0026amp; !__checkparam_dl(attr)) || 4200\t(rt_policy(policy) != (attr-\u0026gt;sched_priority != 0))) 4201\treturn -EINVAL; 4202 4203\t/* 4204* Allow unprivileged RT tasks to decrease priority: 4205*/ 4206\tif (user \u0026amp;\u0026amp; !capable(CAP_SYS_NICE)) { 4207\tif (fair_policy(policy)) { 4208\tif (attr-\u0026gt;sched_nice \u0026lt; task_nice(p) \u0026amp;\u0026amp; 4209\t!can_nice(p, attr-\u0026gt;sched_nice)) 4210\treturn -EPERM; 4211\t} 4212 4213\tif (rt_policy(policy)) { 4214\tunsigned long rlim_rtprio = 4215\ttask_rlimit(p, RLIMIT_RTPRIO); 4216 4217\t/* Can\u0026#39;t set/change the rt policy: */ 4218\tif (policy != p-\u0026gt;policy \u0026amp;\u0026amp; !rlim_rtprio) 4219\treturn -EPERM; 4220 4221\t/* Can\u0026#39;t increase priority: */ 4222\tif (attr-\u0026gt;sched_priority \u0026gt; p-\u0026gt;rt_priority \u0026amp;\u0026amp; 4223\tattr-\u0026gt;sched_priority \u0026gt; rlim_rtprio) 4224\treturn -EPERM; 4225\t} 4226 4227\t/* 4228* Can\u0026#39;t set/change SCHED_DEADLINE policy at all for now 4229* (safest behavior); in the future we would like to allow 4230* unprivileged DL tasks to increase their relative deadline 4231* or reduce their runtime (both ways reducing utilization) 4232*/ 4233\tif (dl_policy(policy)) 4234\treturn -EPERM; 4235 4236\t/* 4237* Treat SCHED_IDLE as nice 20. Only allow a switch to 4238* SCHED_NORMAL if the RLIMIT_NICE would normally permit it. 4239*/ 4240\tif (idle_policy(p-\u0026gt;policy) \u0026amp;\u0026amp; !idle_policy(policy)) { 4241\tif (!can_nice(p, task_nice(p))) 4242\treturn -EPERM; 4243\t} 4244 4245\t/* Can\u0026#39;t change other user\u0026#39;s priorities: */ 4246\tif (!check_same_owner(p)) 4247\treturn -EPERM; 4248 4249\t/* Normal users shall not reset the sched_reset_on_fork flag: */ 4250\tif (p-\u0026gt;sched_reset_on_fork \u0026amp;\u0026amp; !reset_on_fork) 4251\treturn -EPERM; 4252\t} 4253 4254\tif (user) { 4255\tif (attr-\u0026gt;sched_flags \u0026amp; SCHED_FLAG_SUGOV) 4256\treturn -EINVAL; 4257 4258\tretval = security_task_setscheduler(p); 4259\tif (retval) 4260\treturn retval; 4261\t} 4262 4263\t/* 4264* Make sure no PI-waiters arrive (or leave) while we are 4265* changing the priority of the task: 4266* 4267* To be able to change p-\u0026gt;policy safely, the appropriate 4268* runqueue lock must be held. 4269*/ 4270\trq = task_rq_lock(p, \u0026amp;rf); 4271\tupdate_rq_clock(rq); 4272 4273\t/* 4274* Changing the policy of the stop threads its a very bad idea: 4275*/ 4276\tif (p == rq-\u0026gt;stop) { 4277\ttask_rq_unlock(rq, p, \u0026amp;rf); 4278\treturn -EINVAL; 4279\t} 4280 4281\t/* 4282* If not changing anything there\u0026#39;s no need to proceed further, 4283* but store a possible modification of reset_on_fork. 4284*/ 4285\tif (unlikely(policy == p-\u0026gt;policy)) { 4286\tif (fair_policy(policy) \u0026amp;\u0026amp; attr-\u0026gt;sched_nice != task_nice(p)) 4287\tgoto change; 4288\tif (rt_policy(policy) \u0026amp;\u0026amp; attr-\u0026gt;sched_priority != p-\u0026gt;rt_priority) 4289\tgoto change; 4290\tif (dl_policy(policy) \u0026amp;\u0026amp; dl_param_changed(p, attr)) 4291\tgoto change; 4292 4293\tp-\u0026gt;sched_reset_on_fork = reset_on_fork; 4294\ttask_rq_unlock(rq, p, \u0026amp;rf); 4295\treturn 0; 4296\t} 4297change: 4298 4299\tif (user) { 4300#ifdef CONFIG_RT_GROUP_SCHED 4301\t/* 4302* Do not allow realtime tasks into groups that have no runtime 4303* assigned. 4304*/ 4305\tif (rt_bandwidth_enabled() \u0026amp;\u0026amp; rt_policy(policy) \u0026amp;\u0026amp; 4306\ttask_group(p)-\u0026gt;rt_bandwidth.rt_runtime == 0 \u0026amp;\u0026amp; 4307\t!task_group_is_autogroup(task_group(p))) { 4308\ttask_rq_unlock(rq, p, \u0026amp;rf); 4309\treturn -EPERM; 4310\t} 4311#endif 4312#ifdef CONFIG_SMP 4313\tif (dl_bandwidth_enabled() \u0026amp;\u0026amp; dl_policy(policy) \u0026amp;\u0026amp; 4314\t!(attr-\u0026gt;sched_flags \u0026amp; SCHED_FLAG_SUGOV)) { 4315\tcpumask_t *span = rq-\u0026gt;rd-\u0026gt;span; 4316 4317\t/* 4318* Don\u0026#39;t allow tasks with an affinity mask smaller than 4319* the entire root_domain to become SCHED_DEADLINE. We 4320* will also fail if there\u0026#39;s no bandwidth available. 4321*/ 4322\tif (!cpumask_subset(span, \u0026amp;p-\u0026gt;cpus_allowed) || 4323\trq-\u0026gt;rd-\u0026gt;dl_bw.bw == 0) { 4324\ttask_rq_unlock(rq, p, \u0026amp;rf); 4325\treturn -EPERM; 4326\t} 4327\t} 4328#endif 4329\t} 4330 4331\t/* Re-check policy now with rq lock held: */ 4332\tif (unlikely(oldpolicy != -1 \u0026amp;\u0026amp; oldpolicy != p-\u0026gt;policy)) { 4333\tpolicy = oldpolicy = -1; 4334\ttask_rq_unlock(rq, p, \u0026amp;rf); 4335\tgoto recheck; 4336\t} 4337 4338\t/* 4339* If setscheduling to SCHED_DEADLINE (or changing the parameters 4340* of a SCHED_DEADLINE task) we need to check if enough bandwidth 4341* is available. 4342*/ 4343\tif ((dl_policy(policy) || dl_task(p)) \u0026amp;\u0026amp; sched_dl_overflow(p, policy, attr)) { 4344\ttask_rq_unlock(rq, p, \u0026amp;rf); 4345\treturn -EBUSY; 4346\t} 4347 4348\tp-\u0026gt;sched_reset_on_fork = reset_on_fork; 4349\toldprio = p-\u0026gt;prio; 4350 4351\tif (pi) { 4352\t/* 4353* Take priority boosted tasks into account. If the new 4354* effective priority is unchanged, we just store the new 4355* normal parameters and do not touch the scheduler class and 4356* the runqueue. This will be done when the task deboost 4357* itself. 4358*/ 4359\tnew_effective_prio = rt_effective_prio(p, newprio); 4360\tif (new_effective_prio == oldprio) 4361\tqueue_flags \u0026amp;= ~DEQUEUE_MOVE; 4362\t} 4363 4364\tqueued = task_on_rq_queued(p); 4365\trunning = task_current(rq, p); 4366\tif (queued) 4367\tdequeue_task(rq, p, queue_flags); 4368\tif (running) 4369\tput_prev_task(rq, p); 4370 4371\tprev_class = p-\u0026gt;sched_class; 4372\t__setscheduler(rq, p, attr, pi); 4373 4374\tif (queued) { 4375\t/* 4376* We enqueue to tail when the priority of a task is 4377* increased (user space view). 4378*/ 4379\tif (oldprio \u0026lt; p-\u0026gt;prio) 4380\tqueue_flags |= ENQUEUE_HEAD; 4381 4382\tenqueue_task(rq, p, queue_flags); 4383\t} 4384\tif (running) 4385\tset_curr_task(rq, p); 4386 4387\tcheck_class_changed(rq, p, prev_class, oldprio); 4388 4389\t/* Avoid rq from going away on us: */ 4390\tpreempt_disable(); 4391\ttask_rq_unlock(rq, p, \u0026amp;rf); 4392 4393\tif (pi) 4394\trt_mutex_adjust_pi(p); 4395 4396\t/* Run balance callbacks after we\u0026#39;ve adjusted the PI chain: */ 4397\tbalance_callback(rq); 4398\tpreempt_enable(); 4399 4400\treturn 0; 4401} 4402 4403static int _sched_setscheduler(struct task_struct *p, int policy, 4404\tconst struct sched_param *param, bool check) 4405{ 4406\tstruct sched_attr attr = { 4407\t.sched_policy = policy, 4408\t.sched_priority = param-\u0026gt;sched_priority, 4409\t.sched_nice\t= PRIO_TO_NICE(p-\u0026gt;static_prio), 4410\t}; 4411 4412\t/* Fixup the legacy SCHED_RESET_ON_FORK hack. */ 4413\tif ((policy != SETPARAM_POLICY) \u0026amp;\u0026amp; (policy \u0026amp; SCHED_RESET_ON_FORK)) { 4414\tattr.sched_flags |= SCHED_FLAG_RESET_ON_FORK; 4415\tpolicy \u0026amp;= ~SCHED_RESET_ON_FORK; 4416\tattr.sched_policy = policy; 4417\t} 4418 4419\treturn __sched_setscheduler(p, \u0026amp;attr, check, true); 4420} 4421/** 4422* sched_setscheduler - change the scheduling policy and/or RT priority of a thread. 4423* @p: the task in question. 4424* @policy: new policy. 4425* @param: structure containing the new RT priority. 4426* 4427* Return: 0 on success. An error code otherwise. 4428* 4429* NOTE that the task may be already dead. 4430*/ 4431int sched_setscheduler(struct task_struct *p, int policy, 4432\tconst struct sched_param *param) 4433{ 4434\treturn _sched_setscheduler(p, policy, param, true); 4435} 4436EXPORT_SYMBOL_GPL(sched_setscheduler); 4437 4438int sched_setattr(struct task_struct *p, const struct sched_attr *attr) 4439{ 4440\treturn __sched_setscheduler(p, attr, true, true); 4441} 4442EXPORT_SYMBOL_GPL(sched_setattr); 4443 4444int sched_setattr_nocheck(struct task_struct *p, const struct sched_attr *attr) 4445{ 4446\treturn __sched_setscheduler(p, attr, false, true); 4447} 4448 4449/** 4450* sched_setscheduler_nocheck - change the scheduling policy and/or RT priority of a thread from kernelspace. 4451* @p: the task in question. 4452* @policy: new policy. 4453* @param: structure containing the new RT priority. 4454* 4455* Just like sched_setscheduler, only don\u0026#39;t bother checking if the 4456* current context has permission. For example, this is needed in 4457* stop_machine(): we create temporary high priority worker threads, 4458* but our caller might not have that capability. 4459* 4460* Return: 0 on success. An error code otherwise. 4461*/ 4462int sched_setscheduler_nocheck(struct task_struct *p, int policy, 4463\tconst struct sched_param *param) 4464{ 4465\treturn _sched_setscheduler(p, policy, param, false); 4466} 4467EXPORT_SYMBOL_GPL(sched_setscheduler_nocheck); 4468 4469static int 4470do_sched_setscheduler(pid_t pid, int policy, struct sched_param __user *param) 4471{ 4472\tstruct sched_param lparam; 4473\tstruct task_struct *p; 4474\tint retval; 4475 4476\tif (!param || pid \u0026lt; 0) 4477\treturn -EINVAL; 4478\tif (copy_from_user(\u0026amp;lparam, param, sizeof(struct sched_param))) 4479\treturn -EFAULT; 4480 4481\trcu_read_lock(); 4482\tretval = -ESRCH; 4483\tp = find_process_by_pid(pid); 4484\tif (p != NULL) 4485\tretval = sched_setscheduler(p, policy, \u0026amp;lparam); 4486\trcu_read_unlock(); 4487 4488\treturn retval; 4489} 4490 4491/* 4492* Mimics kernel/events/core.c perf_copy_attr(). 4493*/ 4494static int sched_copy_attr(struct sched_attr __user *uattr, struct sched_attr *attr) 4495{ 4496\tu32 size; 4497\tint ret; 4498 4499\tif (!access_ok(VERIFY_WRITE, uattr, SCHED_ATTR_SIZE_VER0)) 4500\treturn -EFAULT; 4501 4502\t/* Zero the full structure, so that a short copy will be nice: */ 4503\tmemset(attr, 0, sizeof(*attr)); 4504 4505\tret = get_user(size, \u0026amp;uattr-\u0026gt;size); 4506\tif (ret) 4507\treturn ret; 4508 4509\t/* Bail out on silly large: */ 4510\tif (size \u0026gt; PAGE_SIZE) 4511\tgoto err_size; 4512 4513\t/* ABI compatibility quirk: */ 4514\tif (!size) 4515\tsize = SCHED_ATTR_SIZE_VER0; 4516 4517\tif (size \u0026lt; SCHED_ATTR_SIZE_VER0) 4518\tgoto err_size; 4519 4520\t/* 4521* If we\u0026#39;re handed a bigger struct than we know of, 4522* ensure all the unknown bits are 0 - i.e. new 4523* user-space does not rely on any kernel feature 4524* extensions we dont know about yet. 4525*/ 4526\tif (size \u0026gt; sizeof(*attr)) { 4527\tunsigned char __user *addr; 4528\tunsigned char __user *end; 4529\tunsigned char val; 4530 4531\taddr = (void __user *)uattr + sizeof(*attr); 4532\tend = (void __user *)uattr + size; 4533 4534\tfor (; addr \u0026lt; end; addr++) { 4535\tret = get_user(val, addr); 4536\tif (ret) 4537\treturn ret; 4538\tif (val) 4539\tgoto err_size; 4540\t} 4541\tsize = sizeof(*attr); 4542\t} 4543 4544\tret = copy_from_user(attr, uattr, size); 4545\tif (ret) 4546\treturn -EFAULT; 4547 4548\t/* 4549* XXX: Do we want to be lenient like existing syscalls; or do we want 4550* to be strict and return an error on out-of-bounds values? 4551*/ 4552\tattr-\u0026gt;sched_nice = clamp(attr-\u0026gt;sched_nice, MIN_NICE, MAX_NICE); 4553 4554\treturn 0; 4555 4556err_size: 4557\tput_user(sizeof(*attr), \u0026amp;uattr-\u0026gt;size); 4558\treturn -E2BIG; 4559} 4560 4561/** 4562* sys_sched_setscheduler - set/change the scheduler policy and RT priority 4563* @pid: the pid in question. 4564* @policy: new policy. 4565* @param: structure containing the new RT priority. 4566* 4567* Return: 0 on success. An error code otherwise. 4568*/ 4569SYSCALL_DEFINE3(sched_setscheduler, pid_t, pid, int, policy, struct sched_param __user *, param) 4570{ 4571\tif (policy \u0026lt; 0) 4572\treturn -EINVAL; 4573 4574\treturn do_sched_setscheduler(pid, policy, param); 4575} 4576 4577/** 4578* sys_sched_setparam - set/change the RT priority of a thread 4579* @pid: the pid in question. 4580* @param: structure containing the new RT priority. 4581* 4582* Return: 0 on success. An error code otherwise. 4583*/ 4584SYSCALL_DEFINE2(sched_setparam, pid_t, pid, struct sched_param __user *, param) 4585{ 4586\treturn do_sched_setscheduler(pid, SETPARAM_POLICY, param); 4587} 4588 4589/** 4590* sys_sched_setattr - same as above, but with extended sched_attr 4591* @pid: the pid in question. 4592* @uattr: structure containing the extended parameters. 4593* @flags: for future extension. 4594*/ 4595SYSCALL_DEFINE3(sched_setattr, pid_t, pid, struct sched_attr __user *, uattr, 4596\tunsigned int, flags) 4597{ 4598\tstruct sched_attr attr; 4599\tstruct task_struct *p; 4600\tint retval; 4601 4602\tif (!uattr || pid \u0026lt; 0 || flags) 4603\treturn -EINVAL; 4604 4605\tretval = sched_copy_attr(uattr, \u0026amp;attr); 4606\tif (retval) 4607\treturn retval; 4608 4609\tif ((int)attr.sched_policy \u0026lt; 0) 4610\treturn -EINVAL; 4611 4612\trcu_read_lock(); 4613\tretval = -ESRCH; 4614\tp = find_process_by_pid(pid); 4615\tif (p != NULL) 4616\tretval = sched_setattr(p, \u0026amp;attr); 4617\trcu_read_unlock(); 4618 4619\treturn retval; 4620} 4621 4622/** 4623* sys_sched_getscheduler - get the policy (scheduling class) of a thread 4624* @pid: the pid in question. 4625* 4626* Return: On success, the policy of the thread. Otherwise, a negative error 4627* code. 4628*/ 4629SYSCALL_DEFINE1(sched_getscheduler, pid_t, pid) 4630{ 4631\tstruct task_struct *p; 4632\tint retval; 4633 4634\tif (pid \u0026lt; 0) 4635\treturn -EINVAL; 4636 4637\tretval = -ESRCH; 4638\trcu_read_lock(); 4639\tp = find_process_by_pid(pid); 4640\tif (p) { 4641\tretval = security_task_getscheduler(p); 4642\tif (!retval) 4643\tretval = p-\u0026gt;policy 4644\t| (p-\u0026gt;sched_reset_on_fork ? SCHED_RESET_ON_FORK : 0); 4645\t} 4646\trcu_read_unlock(); 4647\treturn retval; 4648} 4649 4650/** 4651* sys_sched_getparam - get the RT priority of a thread 4652* @pid: the pid in question. 4653* @param: structure containing the RT priority. 4654* 4655* Return: On success, 0 and the RT priority is in @param. Otherwise, an error 4656* code. 4657*/ 4658SYSCALL_DEFINE2(sched_getparam, pid_t, pid, struct sched_param __user *, param) 4659{ 4660\tstruct sched_param lp = { .sched_priority = 0 }; 4661\tstruct task_struct *p; 4662\tint retval; 4663 4664\tif (!param || pid \u0026lt; 0) 4665\treturn -EINVAL; 4666 4667\trcu_read_lock(); 4668\tp = find_process_by_pid(pid); 4669\tretval = -ESRCH; 4670\tif (!p) 4671\tgoto out_unlock; 4672 4673\tretval = security_task_getscheduler(p); 4674\tif (retval) 4675\tgoto out_unlock; 4676 4677\tif (task_has_rt_policy(p)) 4678\tlp.sched_priority = p-\u0026gt;rt_priority; 4679\trcu_read_unlock(); 4680 4681\t/* 4682* This one might sleep, we cannot do it with a spinlock held ... 4683*/ 4684\tretval = copy_to_user(param, \u0026amp;lp, sizeof(*param)) ? -EFAULT : 0; 4685 4686\treturn retval; 4687 4688out_unlock: 4689\trcu_read_unlock(); 4690\treturn retval; 4691} 4692 4693static int sched_read_attr(struct sched_attr __user *uattr, 4694\tstruct sched_attr *attr, 4695\tunsigned int usize) 4696{ 4697\tint ret; 4698 4699\tif (!access_ok(VERIFY_WRITE, uattr, usize)) 4700\treturn -EFAULT; 4701 4702\t/* 4703* If we\u0026#39;re handed a smaller struct than we know of, 4704* ensure all the unknown bits are 0 - i.e. old 4705* user-space does not get uncomplete information. 4706*/ 4707\tif (usize \u0026lt; sizeof(*attr)) { 4708\tunsigned char *addr; 4709\tunsigned char *end; 4710 4711\taddr = (void *)attr + usize; 4712\tend = (void *)attr + sizeof(*attr); 4713 4714\tfor (; addr \u0026lt; end; addr++) { 4715\tif (*addr) 4716\treturn -EFBIG; 4717\t} 4718 4719\tattr-\u0026gt;size = usize; 4720\t} 4721 4722\tret = copy_to_user(uattr, attr, attr-\u0026gt;size); 4723\tif (ret) 4724\treturn -EFAULT; 4725 4726\treturn 0; 4727} 4728 4729/** 4730* sys_sched_getattr - similar to sched_getparam, but with sched_attr 4731* @pid: the pid in question. 4732* @uattr: structure containing the extended parameters. 4733* @size: sizeof(attr) for fwd/bwd comp. 4734* @flags: for future extension. 4735*/ 4736SYSCALL_DEFINE4(sched_getattr, pid_t, pid, struct sched_attr __user *, uattr, 4737\tunsigned int, size, unsigned int, flags) 4738{ 4739\tstruct sched_attr attr = { 4740\t.size = sizeof(struct sched_attr), 4741\t}; 4742\tstruct task_struct *p; 4743\tint retval; 4744 4745\tif (!uattr || pid \u0026lt; 0 || size \u0026gt; PAGE_SIZE || 4746\tsize \u0026lt; SCHED_ATTR_SIZE_VER0 || flags) 4747\treturn -EINVAL; 4748 4749\trcu_read_lock(); 4750\tp = find_process_by_pid(pid); 4751\tretval = -ESRCH; 4752\tif (!p) 4753\tgoto out_unlock; 4754 4755\tretval = security_task_getscheduler(p); 4756\tif (retval) 4757\tgoto out_unlock; 4758 4759\tattr.sched_policy = p-\u0026gt;policy; 4760\tif (p-\u0026gt;sched_reset_on_fork) 4761\tattr.sched_flags |= SCHED_FLAG_RESET_ON_FORK; 4762\tif (task_has_dl_policy(p)) 4763\t__getparam_dl(p, \u0026amp;attr); 4764\telse if (task_has_rt_policy(p)) 4765\tattr.sched_priority = p-\u0026gt;rt_priority; 4766\telse 4767\tattr.sched_nice = task_nice(p); 4768 4769\trcu_read_unlock(); 4770 4771\tretval = sched_read_attr(uattr, \u0026amp;attr, size); 4772\treturn retval; 4773 4774out_unlock: 4775\trcu_read_unlock(); 4776\treturn retval; 4777} 4778 4779long sched_setaffinity(pid_t pid, const struct cpumask *in_mask) 4780{ 4781\tcpumask_var_t cpus_allowed, new_mask; 4782\tstruct task_struct *p; 4783\tint retval; 4784 4785\trcu_read_lock(); 4786 4787\tp = find_process_by_pid(pid); 4788\tif (!p) { 4789\trcu_read_unlock(); 4790\treturn -ESRCH; 4791\t} 4792 4793\t/* Prevent p going away */ 4794\tget_task_struct(p); 4795\trcu_read_unlock(); 4796 4797\tif (p-\u0026gt;flags \u0026amp; PF_NO_SETAFFINITY) { 4798\tretval = -EINVAL; 4799\tgoto out_put_task; 4800\t} 4801\tif (!alloc_cpumask_var(\u0026amp;cpus_allowed, GFP_KERNEL)) { 4802\tretval = -ENOMEM; 4803\tgoto out_put_task; 4804\t} 4805\tif (!alloc_cpumask_var(\u0026amp;new_mask, GFP_KERNEL)) { 4806\tretval = -ENOMEM; 4807\tgoto out_free_cpus_allowed; 4808\t} 4809\tretval = -EPERM; 4810\tif (!check_same_owner(p)) { 4811\trcu_read_lock(); 4812\tif (!ns_capable(__task_cred(p)-\u0026gt;user_ns, CAP_SYS_NICE)) { 4813\trcu_read_unlock(); 4814\tgoto out_free_new_mask; 4815\t} 4816\trcu_read_unlock(); 4817\t} 4818 4819\tretval = security_task_setscheduler(p); 4820\tif (retval) 4821\tgoto out_free_new_mask; 4822 4823 4824\tcpuset_cpus_allowed(p, cpus_allowed); 4825\tcpumask_and(new_mask, in_mask, cpus_allowed); 4826 4827\t/* 4828* Since bandwidth control happens on root_domain basis, 4829* if admission test is enabled, we only admit -deadline 4830* tasks allowed to run on all the CPUs in the task\u0026#39;s 4831* root_domain. 4832*/ 4833#ifdef CONFIG_SMP 4834\tif (task_has_dl_policy(p) \u0026amp;\u0026amp; dl_bandwidth_enabled()) { 4835\trcu_read_lock(); 4836\tif (!cpumask_subset(task_rq(p)-\u0026gt;rd-\u0026gt;span, new_mask)) { 4837\tretval = -EBUSY; 4838\trcu_read_unlock(); 4839\tgoto out_free_new_mask; 4840\t} 4841\trcu_read_unlock(); 4842\t} 4843#endif 4844again: 4845\tretval = __set_cpus_allowed_ptr(p, new_mask, true); 4846 4847\tif (!retval) { 4848\tcpuset_cpus_allowed(p, cpus_allowed); 4849\tif (!cpumask_subset(new_mask, cpus_allowed)) { 4850\t/* 4851* We must have raced with a concurrent cpuset 4852* update. Just reset the cpus_allowed to the 4853* cpuset\u0026#39;s cpus_allowed 4854*/ 4855\tcpumask_copy(new_mask, cpus_allowed); 4856\tgoto again; 4857\t} 4858\t} 4859out_free_new_mask: 4860\tfree_cpumask_var(new_mask); 4861out_free_cpus_allowed: 4862\tfree_cpumask_var(cpus_allowed); 4863out_put_task: 4864\tput_task_struct(p); 4865\treturn retval; 4866} 4867 4868static int get_user_cpu_mask(unsigned long __user *user_mask_ptr, unsigned len, 4869\tstruct cpumask *new_mask) 4870{ 4871\tif (len \u0026lt; cpumask_size()) 4872\tcpumask_clear(new_mask); 4873\telse if (len \u0026gt; cpumask_size()) 4874\tlen = cpumask_size(); 4875 4876\treturn copy_from_user(new_mask, user_mask_ptr, len) ? -EFAULT : 0; 4877} 4878 4879/** 4880* sys_sched_setaffinity - set the CPU affinity of a process 4881* @pid: pid of the process 4882* @len: length in bytes of the bitmask pointed to by user_mask_ptr 4883* @user_mask_ptr: user-space pointer to the new CPU mask 4884* 4885* Return: 0 on success. An error code otherwise. 4886*/ 4887SYSCALL_DEFINE3(sched_setaffinity, pid_t, pid, unsigned int, len, 4888\tunsigned long __user *, user_mask_ptr) 4889{ 4890\tcpumask_var_t new_mask; 4891\tint retval; 4892 4893\tif (!alloc_cpumask_var(\u0026amp;new_mask, GFP_KERNEL)) 4894\treturn -ENOMEM; 4895 4896\tretval = get_user_cpu_mask(user_mask_ptr, len, new_mask); 4897\tif (retval == 0) 4898\tretval = sched_setaffinity(pid, new_mask); 4899\tfree_cpumask_var(new_mask); 4900\treturn retval; 4901} 4902 4903long sched_getaffinity(pid_t pid, struct cpumask *mask) 4904{ 4905\tstruct task_struct *p; 4906\tunsigned long flags; 4907\tint retval; 4908 4909\trcu_read_lock(); 4910 4911\tretval = -ESRCH; 4912\tp = find_process_by_pid(pid); 4913\tif (!p) 4914\tgoto out_unlock; 4915 4916\tretval = security_task_getscheduler(p); 4917\tif (retval) 4918\tgoto out_unlock; 4919 4920\traw_spin_lock_irqsave(\u0026amp;p-\u0026gt;pi_lock, flags); 4921\tcpumask_and(mask, \u0026amp;p-\u0026gt;cpus_allowed, cpu_active_mask); 4922\traw_spin_unlock_irqrestore(\u0026amp;p-\u0026gt;pi_lock, flags); 4923 4924out_unlock: 4925\trcu_read_unlock(); 4926 4927\treturn retval; 4928} 4929 4930/** 4931* sys_sched_getaffinity - get the CPU affinity of a process 4932* @pid: pid of the process 4933* @len: length in bytes of the bitmask pointed to by user_mask_ptr 4934* @user_mask_ptr: user-space pointer to hold the current CPU mask 4935* 4936* Return: size of CPU mask copied to user_mask_ptr on success. An 4937* error code otherwise. 4938*/ 4939SYSCALL_DEFINE3(sched_getaffinity, pid_t, pid, unsigned int, len, 4940\tunsigned long __user *, user_mask_ptr) 4941{ 4942\tint ret; 4943\tcpumask_var_t mask; 4944 4945\tif ((len * BITS_PER_BYTE) \u0026lt; nr_cpu_ids) 4946\treturn -EINVAL; 4947\tif (len \u0026amp; (sizeof(unsigned long)-1)) 4948\treturn -EINVAL; 4949 4950\tif (!alloc_cpumask_var(\u0026amp;mask, GFP_KERNEL)) 4951\treturn -ENOMEM; 4952 4953\tret = sched_getaffinity(pid, mask); 4954\tif (ret == 0) { 4955\tunsigned int retlen = min(len, cpumask_size()); 4956 4957\tif (copy_to_user(user_mask_ptr, mask, retlen)) 4958\tret = -EFAULT; 4959\telse 4960\tret = retlen; 4961\t} 4962\tfree_cpumask_var(mask); 4963 4964\treturn ret; 4965} 4966 4967/** 4968* sys_sched_yield - yield the current processor to other threads. 4969* 4970* This function yields the current CPU to other tasks. If there are no 4971* other threads running on this CPU then this function will return. 4972* 4973* Return: 0. 4974*/ 4975static void do_sched_yield(void) 4976{ 4977\tstruct rq_flags rf; 4978\tstruct rq *rq; 4979 4980\tlocal_irq_disable(); 4981\trq = this_rq(); 4982\trq_lock(rq, \u0026amp;rf); 4983 4984\tschedstat_inc(rq-\u0026gt;yld_count); 4985\tcurrent-\u0026gt;sched_class-\u0026gt;yield_task(rq); 4986 4987\tpreempt_disable(); 4988\trq_unlock_irq(rq, \u0026amp;rf); 4989\tsched_preempt_enable_no_resched(); 4990 4991\tschedule(); 4992} 4993 4994SYSCALL_DEFINE0(sched_yield) 4995{ 4996\tdo_sched_yield(); 4997\treturn 0; 4998} 4999 5000#ifndef CONFIG_PREEMPT 5001int __sched _cond_resched(void) 5002{ 5003\tif (should_resched(0)) { 5004\tpreempt_schedule_common(); 5005\treturn 1; 5006\t} 5007\trcu_all_qs(); 5008\treturn 0; 5009} 5010EXPORT_SYMBOL(_cond_resched); 5011#endif 5012 5013/* 5014* __cond_resched_lock() - if a reschedule is pending, drop the given lock, 5015* call schedule, and on return reacquire the lock. 5016* 5017* This works OK both with and without CONFIG_PREEMPT. We do strange low-level 5018* operations here to prevent schedule() from being called twice (once via 5019* spin_unlock(), once by hand). 5020*/ 5021int __cond_resched_lock(spinlock_t *lock) 5022{ 5023\tint resched = should_resched(PREEMPT_LOCK_OFFSET); 5024\tint ret = 0; 5025 5026\tlockdep_assert_held(lock); 5027 5028\tif (spin_needbreak(lock) || resched) { 5029\tspin_unlock(lock); 5030\tif (resched) 5031\tpreempt_schedule_common(); 5032\telse 5033\tcpu_relax(); 5034\tret = 1; 5035\tspin_lock(lock); 5036\t} 5037\treturn ret; 5038} 5039EXPORT_SYMBOL(__cond_resched_lock); 5040 5041/** 5042* yield - yield the current processor to other threads. 5043* 5044* Do not ever use this function, there\u0026#39;s a 99% chance you\u0026#39;re doing it wrong. 5045* 5046* The scheduler is at all times free to pick the calling task as the most 5047* eligible task to run, if removing the yield() call from your code breaks 5048* it, its already broken. 5049* 5050* Typical broken usage is: 5051* 5052* while (!event) 5053*\tyield(); 5054* 5055* where one assumes that yield() will let \u0026#39;the other\u0026#39; process run that will 5056* make event true. If the current task is a SCHED_FIFO task that will never 5057* happen. Never use yield() as a progress guarantee!! 5058* 5059* If you want to use yield() to wait for something, use wait_event(). 5060* If you want to use yield() to be \u0026#39;nice\u0026#39; for others, use cond_resched(). 5061* If you still want to use yield(), do not! 5062*/ 5063void __sched yield(void) 5064{ 5065\tset_current_state(TASK_RUNNING); 5066\tdo_sched_yield(); 5067} 5068EXPORT_SYMBOL(yield); 5069 5070/** 5071* yield_to - yield the current processor to another thread in 5072* your thread group, or accelerate that thread toward the 5073* processor it\u0026#39;s on. 5074* @p: target task 5075* @preempt: whether task preemption is allowed or not 5076* 5077* It\u0026#39;s the caller\u0026#39;s job to ensure that the target task struct 5078* can\u0026#39;t go away on us before we can do any checks. 5079* 5080* Return: 5081*\ttrue (\u0026gt;0) if we indeed boosted the target task. 5082*\tfalse (0) if we failed to boost the target. 5083*\t-ESRCH if there\u0026#39;s no task to yield to. 5084*/ 5085int __sched yield_to(struct task_struct *p, bool preempt) 5086{ 5087\tstruct task_struct *curr = current; 5088\tstruct rq *rq, *p_rq; 5089\tunsigned long flags; 5090\tint yielded = 0; 5091 5092\tlocal_irq_save(flags); 5093\trq = this_rq(); 5094 5095again: 5096\tp_rq = task_rq(p); 5097\t/* 5098* If we\u0026#39;re the only runnable task on the rq and target rq also 5099* has only one task, there\u0026#39;s absolutely no point in yielding. 5100*/ 5101\tif (rq-\u0026gt;nr_running == 1 \u0026amp;\u0026amp; p_rq-\u0026gt;nr_running == 1) { 5102\tyielded = -ESRCH; 5103\tgoto out_irq; 5104\t} 5105 5106\tdouble_rq_lock(rq, p_rq); 5107\tif (task_rq(p) != p_rq) { 5108\tdouble_rq_unlock(rq, p_rq); 5109\tgoto again; 5110\t} 5111 5112\tif (!curr-\u0026gt;sched_class-\u0026gt;yield_to_task) 5113\tgoto out_unlock; 5114 5115\tif (curr-\u0026gt;sched_class != p-\u0026gt;sched_class) 5116\tgoto out_unlock; 5117 5118\tif (task_running(p_rq, p) || p-\u0026gt;state) 5119\tgoto out_unlock; 5120 5121\tyielded = curr-\u0026gt;sched_class-\u0026gt;yield_to_task(rq, p, preempt); 5122\tif (yielded) { 5123\tschedstat_inc(rq-\u0026gt;yld_count); 5124\t/* 5125* Make p\u0026#39;s CPU reschedule; pick_next_entity takes care of 5126* fairness. 5127*/ 5128\tif (preempt \u0026amp;\u0026amp; rq != p_rq) 5129\tresched_curr(p_rq); 5130\t} 5131 5132out_unlock: 5133\tdouble_rq_unlock(rq, p_rq); 5134out_irq: 5135\tlocal_irq_restore(flags); 5136 5137\tif (yielded \u0026gt; 0) 5138\tschedule(); 5139 5140\treturn yielded; 5141} 5142EXPORT_SYMBOL_GPL(yield_to); 5143 5144int io_schedule_prepare(void) 5145{ 5146\tint old_iowait = current-\u0026gt;in_iowait; 5147 5148\tcurrent-\u0026gt;in_iowait = 1; 5149\tblk_schedule_flush_plug(current); 5150 5151\treturn old_iowait; 5152} 5153 5154void io_schedule_finish(int token) 5155{ 5156\tcurrent-\u0026gt;in_iowait = token; 5157} 5158 5159/* 5160* This task is about to go to sleep on IO. Increment rq-\u0026gt;nr_iowait so 5161* that process accounting knows that this is a task in IO wait state. 5162*/ 5163long __sched io_schedule_timeout(long timeout) 5164{ 5165\tint token; 5166\tlong ret; 5167 5168\ttoken = io_schedule_prepare(); 5169\tret = schedule_timeout(timeout); 5170\tio_schedule_finish(token); 5171 5172\treturn ret; 5173} 5174EXPORT_SYMBOL(io_schedule_timeout); 5175 5176void __sched io_schedule(void) 5177{ 5178\tint token; 5179 5180\ttoken = io_schedule_prepare(); 5181\tschedule(); 5182\tio_schedule_finish(token); 5183} 5184EXPORT_SYMBOL(io_schedule); 5185 5186/** 5187* sys_sched_get_priority_max - return maximum RT priority. 5188* @policy: scheduling class. 5189* 5190* Return: On success, this syscall returns the maximum 5191* rt_priority that can be used by a given scheduling class. 5192* On failure, a negative error code is returned. 5193*/ 5194SYSCALL_DEFINE1(sched_get_priority_max, int, policy) 5195{ 5196\tint ret = -EINVAL; 5197 5198\tswitch (policy) { 5199\tcase SCHED_FIFO: 5200\tcase SCHED_RR: 5201\tret = MAX_USER_RT_PRIO-1; 5202\tbreak; 5203\tcase SCHED_DEADLINE: 5204\tcase SCHED_NORMAL: 5205\tcase SCHED_BATCH: 5206\tcase SCHED_IDLE: 5207\tret = 0; 5208\tbreak; 5209\t} 5210\treturn ret; 5211} 5212 5213/** 5214* sys_sched_get_priority_min - return minimum RT priority. 5215* @policy: scheduling class. 5216* 5217* Return: On success, this syscall returns the minimum 5218* rt_priority that can be used by a given scheduling class. 5219* On failure, a negative error code is returned. 5220*/ 5221SYSCALL_DEFINE1(sched_get_priority_min, int, policy) 5222{ 5223\tint ret = -EINVAL; 5224 5225\tswitch (policy) { 5226\tcase SCHED_FIFO: 5227\tcase SCHED_RR: 5228\tret = 1; 5229\tbreak; 5230\tcase SCHED_DEADLINE: 5231\tcase SCHED_NORMAL: 5232\tcase SCHED_BATCH: 5233\tcase SCHED_IDLE: 5234\tret = 0; 5235\t} 5236\treturn ret; 5237} 5238 5239static int sched_rr_get_interval(pid_t pid, struct timespec64 *t) 5240{ 5241\tstruct task_struct *p; 5242\tunsigned int time_slice; 5243\tstruct rq_flags rf; 5244\tstruct rq *rq; 5245\tint retval; 5246 5247\tif (pid \u0026lt; 0) 5248\treturn -EINVAL; 5249 5250\tretval = -ESRCH; 5251\trcu_read_lock(); 5252\tp = find_process_by_pid(pid); 5253\tif (!p) 5254\tgoto out_unlock; 5255 5256\tretval = security_task_getscheduler(p); 5257\tif (retval) 5258\tgoto out_unlock; 5259 5260\trq = task_rq_lock(p, \u0026amp;rf); 5261\ttime_slice = 0; 5262\tif (p-\u0026gt;sched_class-\u0026gt;get_rr_interval) 5263\ttime_slice = p-\u0026gt;sched_class-\u0026gt;get_rr_interval(rq, p); 5264\ttask_rq_unlock(rq, p, \u0026amp;rf); 5265 5266\trcu_read_unlock(); 5267\tjiffies_to_timespec64(time_slice, t); 5268\treturn 0; 5269 5270out_unlock: 5271\trcu_read_unlock(); 5272\treturn retval; 5273} 5274 5275/** 5276* sys_sched_rr_get_interval - return the default timeslice of a process. 5277* @pid: pid of the process. 5278* @interval: userspace pointer to the timeslice value. 5279* 5280* this syscall writes the default timeslice value of a given process 5281* into the user-space timespec buffer. A value of \u0026#39;0\u0026#39; means infinity. 5282* 5283* Return: On success, 0 and the timeslice is in @interval. Otherwise, 5284* an error code. 5285*/ 5286SYSCALL_DEFINE2(sched_rr_get_interval, pid_t, pid, 5287\tstruct timespec __user *, interval) 5288{ 5289\tstruct timespec64 t; 5290\tint retval = sched_rr_get_interval(pid, \u0026amp;t); 5291 5292\tif (retval == 0) 5293\tretval = put_timespec64(\u0026amp;t, interval); 5294 5295\treturn retval; 5296} 5297 5298#ifdef CONFIG_COMPAT 5299COMPAT_SYSCALL_DEFINE2(sched_rr_get_interval, 5300\tcompat_pid_t, pid, 5301\tstruct compat_timespec __user *, interval) 5302{ 5303\tstruct timespec64 t; 5304\tint retval = sched_rr_get_interval(pid, \u0026amp;t); 5305 5306\tif (retval == 0) 5307\tretval = compat_put_timespec64(\u0026amp;t, interval); 5308\treturn retval; 5309} 5310#endif 5311 5312void sched_show_task(struct task_struct *p) 5313{ 5314\tunsigned long free = 0; 5315\tint ppid; 5316 5317\tif (!try_get_task_stack(p)) 5318\treturn; 5319 5320\tprintk(KERN_INFO \u0026#34;%-15.15s %c\u0026#34;, p-\u0026gt;comm, task_state_to_char(p)); 5321 5322\tif (p-\u0026gt;state == TASK_RUNNING) 5323\tprintk(KERN_CONT \u0026#34; running task \u0026#34;); 5324#ifdef CONFIG_DEBUG_STACK_USAGE 5325\tfree = stack_not_used(p); 5326#endif 5327\tppid = 0; 5328\trcu_read_lock(); 5329\tif (pid_alive(p)) 5330\tppid = task_pid_nr(rcu_dereference(p-\u0026gt;real_parent)); 5331\trcu_read_unlock(); 5332\tprintk(KERN_CONT \u0026#34;%5lu %5d %6d 0x%08lx\\n\u0026#34;, free, 5333\ttask_pid_nr(p), ppid, 5334\t(unsigned long)task_thread_info(p)-\u0026gt;flags); 5335 5336\tprint_worker_info(KERN_INFO, p); 5337\tshow_stack(p, NULL); 5338\tput_task_stack(p); 5339} 5340EXPORT_SYMBOL_GPL(sched_show_task); 5341 5342static inline bool 5343state_filter_match(unsigned long state_filter, struct task_struct *p) 5344{ 5345\t/* no filter, everything matches */ 5346\tif (!state_filter) 5347\treturn true; 5348 5349\t/* filter, but doesn\u0026#39;t match */ 5350\tif (!(p-\u0026gt;state \u0026amp; state_filter)) 5351\treturn false; 5352 5353\t/* 5354* When looking for TASK_UNINTERRUPTIBLE skip TASK_IDLE (allows 5355* TASK_KILLABLE). 5356*/ 5357\tif (state_filter == TASK_UNINTERRUPTIBLE \u0026amp;\u0026amp; p-\u0026gt;state == TASK_IDLE) 5358\treturn false; 5359 5360\treturn true; 5361} 5362 5363 5364void show_state_filter(unsigned long state_filter) 5365{ 5366\tstruct task_struct *g, *p; 5367 5368#if BITS_PER_LONG == 32 5369\tprintk(KERN_INFO 5370\t\u0026#34; task PC stack pid father\\n\u0026#34;); 5371#else 5372\tprintk(KERN_INFO 5373\t\u0026#34; task PC stack pid father\\n\u0026#34;); 5374#endif 5375\trcu_read_lock(); 5376\tfor_each_process_thread(g, p) { 5377\t/* 5378* reset the NMI-timeout, listing all files on a slow 5379* console might take a lot of time: 5380* Also, reset softlockup watchdogs on all CPUs, because 5381* another CPU might be blocked waiting for us to process 5382* an IPI. 5383*/ 5384\ttouch_nmi_watchdog(); 5385\ttouch_all_softlockup_watchdogs(); 5386\tif (state_filter_match(state_filter, p)) 5387\tsched_show_task(p); 5388\t} 5389 5390#ifdef CONFIG_SCHED_DEBUG 5391\tif (!state_filter) 5392\tsysrq_sched_debug_show(); 5393#endif 5394\trcu_read_unlock(); 5395\t/* 5396* Only show locks if all tasks are dumped: 5397*/ 5398\tif (!state_filter) 5399\tdebug_show_all_locks(); 5400} 5401 5402/** 5403* init_idle - set up an idle thread for a given CPU 5404* @idle: task in question 5405* @cpu: CPU the idle task belongs to 5406* 5407* NOTE: this function does not set the idle thread\u0026#39;s NEED_RESCHED 5408* flag, to make booting more robust. 5409*/ 5410void init_idle(struct task_struct *idle, int cpu) 5411{ 5412\tstruct rq *rq = cpu_rq(cpu); 5413\tunsigned long flags; 5414 5415\t__sched_fork(0, idle); 5416 5417\traw_spin_lock_irqsave(\u0026amp;idle-\u0026gt;pi_lock, flags); 5418\traw_spin_lock(\u0026amp;rq-\u0026gt;lock); 5419 5420\tidle-\u0026gt;state = TASK_RUNNING; 5421\tidle-\u0026gt;se.exec_start = sched_clock(); 5422\tidle-\u0026gt;flags |= PF_IDLE; 5423 5424\tkasan_unpoison_task_stack(idle); 5425 5426#ifdef CONFIG_SMP 5427\t/* 5428* Its possible that init_idle() gets called multiple times on a task, 5429* in that case do_set_cpus_allowed() will not do the right thing. 5430* 5431* And since this is boot we can forgo the serialization. 5432*/ 5433\tset_cpus_allowed_common(idle, cpumask_of(cpu)); 5434#endif 5435\t/* 5436* We\u0026#39;re having a chicken and egg problem, even though we are 5437* holding rq-\u0026gt;lock, the CPU isn\u0026#39;t yet set to this CPU so the 5438* lockdep check in task_group() will fail. 5439* 5440* Similar case to sched_fork(). / Alternatively we could 5441* use task_rq_lock() here and obtain the other rq-\u0026gt;lock. 5442* 5443* Silence PROVE_RCU 5444*/ 5445\trcu_read_lock(); 5446\t__set_task_cpu(idle, cpu); 5447\trcu_read_unlock(); 5448 5449\trq-\u0026gt;curr = rq-\u0026gt;idle = idle; 5450\tidle-\u0026gt;on_rq = TASK_ON_RQ_QUEUED; 5451#ifdef CONFIG_SMP 5452\tidle-\u0026gt;on_cpu = 1; 5453#endif 5454\traw_spin_unlock(\u0026amp;rq-\u0026gt;lock); 5455\traw_spin_unlock_irqrestore(\u0026amp;idle-\u0026gt;pi_lock, flags); 5456 5457\t/* Set the preempt count _outside_ the spinlocks! */ 5458\tinit_idle_preempt_count(idle, cpu); 5459 5460\t/* 5461* The idle tasks have their own, simple scheduling class: 5462*/ 5463\tidle-\u0026gt;sched_class = \u0026amp;idle_sched_class; 5464\tftrace_graph_init_idle_task(idle, cpu); 5465\tvtime_init_idle(idle, cpu); 5466#ifdef CONFIG_SMP 5467\tsprintf(idle-\u0026gt;comm, \u0026#34;%s/%d\u0026#34;, INIT_TASK_COMM, cpu); 5468#endif 5469} 5470 5471#ifdef CONFIG_SMP 5472 5473int cpuset_cpumask_can_shrink(const struct cpumask *cur, 5474\tconst struct cpumask *trial) 5475{ 5476\tint ret = 1; 5477 5478\tif (!cpumask_weight(cur)) 5479\treturn ret; 5480 5481\tret = dl_cpuset_cpumask_can_shrink(cur, trial); 5482 5483\treturn ret; 5484} 5485 5486int task_can_attach(struct task_struct *p, 5487\tconst struct cpumask *cs_cpus_allowed) 5488{ 5489\tint ret = 0; 5490 5491\t/* 5492* Kthreads which disallow setaffinity shouldn\u0026#39;t be moved 5493* to a new cpuset; we don\u0026#39;t want to change their CPU 5494* affinity and isolating such threads by their set of 5495* allowed nodes is unnecessary. Thus, cpusets are not 5496* applicable for such threads. This prevents checking for 5497* success of set_cpus_allowed_ptr() on all attached tasks 5498* before cpus_allowed may be changed. 5499*/ 5500\tif (p-\u0026gt;flags \u0026amp; PF_NO_SETAFFINITY) { 5501\tret = -EINVAL; 5502\tgoto out; 5503\t} 5504 5505\tif (dl_task(p) \u0026amp;\u0026amp; !cpumask_intersects(task_rq(p)-\u0026gt;rd-\u0026gt;span, 5506\tcs_cpus_allowed)) 5507\tret = dl_task_can_attach(p, cs_cpus_allowed); 5508 5509out: 5510\treturn ret; 5511} 5512 5513bool sched_smp_initialized __read_mostly; 5514 5515#ifdef CONFIG_NUMA_BALANCING 5516/* Migrate current task p to target_cpu */ 5517int migrate_task_to(struct task_struct *p, int target_cpu) 5518{ 5519\tstruct migration_arg arg = { p, target_cpu }; 5520\tint curr_cpu = task_cpu(p); 5521 5522\tif (curr_cpu == target_cpu) 5523\treturn 0; 5524 5525\tif (!cpumask_test_cpu(target_cpu, \u0026amp;p-\u0026gt;cpus_allowed)) 5526\treturn -EINVAL; 5527 5528\t/* TODO: This is not properly updating schedstats */ 5529 5530\ttrace_sched_move_numa(p, curr_cpu, target_cpu); 5531\treturn stop_one_cpu(curr_cpu, migration_cpu_stop, \u0026amp;arg); 5532} 5533 5534/* 5535* Requeue a task on a given node and accurately track the number of NUMA 5536* tasks on the runqueues 5537*/ 5538void sched_setnuma(struct task_struct *p, int nid) 5539{ 5540\tbool queued, running; 5541\tstruct rq_flags rf; 5542\tstruct rq *rq; 5543 5544\trq = task_rq_lock(p, \u0026amp;rf); 5545\tqueued = task_on_rq_queued(p); 5546\trunning = task_current(rq, p); 5547 5548\tif (queued) 5549\tdequeue_task(rq, p, DEQUEUE_SAVE); 5550\tif (running) 5551\tput_prev_task(rq, p); 5552 5553\tp-\u0026gt;numa_preferred_nid = nid; 5554 5555\tif (queued) 5556\tenqueue_task(rq, p, ENQUEUE_RESTORE | ENQUEUE_NOCLOCK); 5557\tif (running) 5558\tset_curr_task(rq, p); 5559\ttask_rq_unlock(rq, p, \u0026amp;rf); 5560} 5561#endif /* CONFIG_NUMA_BALANCING */5562 5563#ifdef CONFIG_HOTPLUG_CPU 5564/* 5565* Ensure that the idle task is using init_mm right before its CPU goes 5566* offline. 5567*/ 5568void idle_task_exit(void) 5569{ 5570\tstruct mm_struct *mm = current-\u0026gt;active_mm; 5571 5572\tBUG_ON(cpu_online(smp_processor_id())); 5573\tBUG_ON(current != this_rq()-\u0026gt;idle); 5574 5575\tif (mm != \u0026amp;init_mm) { 5576\tswitch_mm(mm, \u0026amp;init_mm, current); 5577\tfinish_arch_post_lock_switch(); 5578\t} 5579 5580\t/* finish_cpu(), as ran on the BP, will clean up the active_mm state */ 5581} 5582 5583/* 5584* Since this CPU is going \u0026#39;away\u0026#39; for a while, fold any nr_active delta 5585* we might have. Assumes we\u0026#39;re called after migrate_tasks() so that the 5586* nr_active count is stable. We need to take the teardown thread which 5587* is calling this into account, so we hand in adjust = 1 to the load 5588* calculation. 5589* 5590* Also see the comment \u0026#34;Global load-average calculations\u0026#34;. 5591*/ 5592static void calc_load_migrate(struct rq *rq) 5593{ 5594\tlong delta = calc_load_fold_active(rq, 1); 5595\tif (delta) 5596\tatomic_long_add(delta, \u0026amp;calc_load_tasks); 5597} 5598 5599static void put_prev_task_fake(struct rq *rq, struct task_struct *prev) 5600{ 5601} 5602 5603static const struct sched_class fake_sched_class = { 5604\t.put_prev_task = put_prev_task_fake, 5605}; 5606 5607static struct task_struct fake_task = { 5608\t/* 5609* Avoid pull_{rt,dl}_task() 5610*/ 5611\t.prio = MAX_PRIO + 1, 5612\t.sched_class = \u0026amp;fake_sched_class, 5613}; 5614 5615/* 5616* Migrate all tasks from the rq, sleeping tasks will be migrated by 5617* try_to_wake_up()-\u0026gt;select_task_rq(). 5618* 5619* Called with rq-\u0026gt;lock held even though we\u0026#39;er in stop_machine() and 5620* there\u0026#39;s no concurrency possible, we hold the required locks anyway 5621* because of lock validation efforts. 5622*/ 5623static void migrate_tasks(struct rq *dead_rq, struct rq_flags *rf) 5624{ 5625\tstruct rq *rq = dead_rq; 5626\tstruct task_struct *next, *stop = rq-\u0026gt;stop; 5627\tstruct rq_flags orf = *rf; 5628\tint dest_cpu; 5629 5630\t/* 5631* Fudge the rq selection such that the below task selection loop 5632* doesn\u0026#39;t get stuck on the currently eligible stop task. 5633* 5634* We\u0026#39;re currently inside stop_machine() and the rq is either stuck 5635* in the stop_machine_cpu_stop() loop, or we\u0026#39;re executing this code, 5636* either way we should never end up calling schedule() until we\u0026#39;re 5637* done here. 5638*/ 5639\trq-\u0026gt;stop = NULL; 5640 5641\t/* 5642* put_prev_task() and pick_next_task() sched 5643* class method both need to have an up-to-date 5644* value of rq-\u0026gt;clock[_task] 5645*/ 5646\tupdate_rq_clock(rq); 5647 5648\tfor (;;) { 5649\t/* 5650* There\u0026#39;s this thread running, bail when that\u0026#39;s the only 5651* remaining thread: 5652*/ 5653\tif (rq-\u0026gt;nr_running == 1) 5654\tbreak; 5655 5656\t/* 5657* pick_next_task() assumes pinned rq-\u0026gt;lock: 5658*/ 5659\tnext = pick_next_task(rq, \u0026amp;fake_task, rf); 5660\tBUG_ON(!next); 5661\tput_prev_task(rq, next); 5662 5663\t/* 5664* Rules for changing task_struct::cpus_allowed are holding 5665* both pi_lock and rq-\u0026gt;lock, such that holding either 5666* stabilizes the mask. 5667* 5668* Drop rq-\u0026gt;lock is not quite as disastrous as it usually is 5669* because !cpu_active at this point, which means load-balance 5670* will not interfere. Also, stop-machine. 5671*/ 5672\trq_unlock(rq, rf); 5673\traw_spin_lock(\u0026amp;next-\u0026gt;pi_lock); 5674\trq_relock(rq, rf); 5675 5676\t/* 5677* Since we\u0026#39;re inside stop-machine, _nothing_ should have 5678* changed the task, WARN if weird stuff happened, because in 5679* that case the above rq-\u0026gt;lock drop is a fail too. 5680*/ 5681\tif (WARN_ON(task_rq(next) != rq || !task_on_rq_queued(next))) { 5682\traw_spin_unlock(\u0026amp;next-\u0026gt;pi_lock); 5683\tcontinue; 5684\t} 5685 5686\t/* Find suitable destination for @next, with force if needed. */ 5687\tdest_cpu = select_fallback_rq(dead_rq-\u0026gt;cpu, next); 5688\trq = __migrate_task(rq, rf, next, dest_cpu); 5689\tif (rq != dead_rq) { 5690\trq_unlock(rq, rf); 5691\trq = dead_rq; 5692\t*rf = orf; 5693\trq_relock(rq, rf); 5694\t} 5695\traw_spin_unlock(\u0026amp;next-\u0026gt;pi_lock); 5696\t} 5697 5698\trq-\u0026gt;stop = stop; 5699} 5700#endif /* CONFIG_HOTPLUG_CPU */5701 5702void set_rq_online(struct rq *rq) 5703{ 5704\tif (!rq-\u0026gt;online) { 5705\tconst struct sched_class *class; 5706 5707\tcpumask_set_cpu(rq-\u0026gt;cpu, rq-\u0026gt;rd-\u0026gt;online); 5708\trq-\u0026gt;online = 1; 5709 5710\tfor_each_class(class) { 5711\tif (class-\u0026gt;rq_online) 5712\tclass-\u0026gt;rq_online(rq); 5713\t} 5714\t} 5715} 5716 5717void set_rq_offline(struct rq *rq) 5718{ 5719\tif (rq-\u0026gt;online) { 5720\tconst struct sched_class *class; 5721 5722\tfor_each_class(class) { 5723\tif (class-\u0026gt;rq_offline) 5724\tclass-\u0026gt;rq_offline(rq); 5725\t} 5726 5727\tcpumask_clear_cpu(rq-\u0026gt;cpu, rq-\u0026gt;rd-\u0026gt;online); 5728\trq-\u0026gt;online = 0; 5729\t} 5730} 5731 5732/* 5733* used to mark begin/end of suspend/resume: 5734*/ 5735static int num_cpus_frozen; 5736 5737/* 5738* Update cpusets according to cpu_active mask. If cpusets are 5739* disabled, cpuset_update_active_cpus() becomes a simple wrapper 5740* around partition_sched_domains(). 5741* 5742* If we come here as part of a suspend/resume, don\u0026#39;t touch cpusets because we 5743* want to restore it back to its original state upon resume anyway. 5744*/ 5745static void cpuset_cpu_active(void) 5746{ 5747\tif (cpuhp_tasks_frozen) { 5748\t/* 5749* num_cpus_frozen tracks how many CPUs are involved in suspend 5750* resume sequence. As long as this is not the last online 5751* operation in the resume sequence, just build a single sched 5752* domain, ignoring cpusets. 5753*/ 5754\tpartition_sched_domains(1, NULL, NULL); 5755\tif (--num_cpus_frozen) 5756\treturn; 5757\t/* 5758* This is the last CPU online operation. So fall through and 5759* restore the original sched domains by considering the 5760* cpuset configurations. 5761*/ 5762\tcpuset_force_rebuild(); 5763\t} 5764\tcpuset_update_active_cpus(); 5765} 5766 5767static int cpuset_cpu_inactive(unsigned int cpu) 5768{ 5769\tif (!cpuhp_tasks_frozen) { 5770\tif (dl_cpu_busy(cpu)) 5771\treturn -EBUSY; 5772\tcpuset_update_active_cpus(); 5773\t} else { 5774\tnum_cpus_frozen++; 5775\tpartition_sched_domains(1, NULL, NULL); 5776\t} 5777\treturn 0; 5778} 5779 5780int sched_cpu_activate(unsigned int cpu) 5781{ 5782\tstruct rq *rq = cpu_rq(cpu); 5783\tstruct rq_flags rf; 5784 5785#ifdef CONFIG_SCHED_SMT 5786\t/* 5787* When going up, increment the number of cores with SMT present. 5788*/ 5789\tif (cpumask_weight(cpu_smt_mask(cpu)) == 2) 5790\tstatic_branch_inc_cpuslocked(\u0026amp;sched_smt_present); 5791#endif 5792\tset_cpu_active(cpu, true); 5793 5794\tif (sched_smp_initialized) { 5795\tsched_domains_numa_masks_set(cpu); 5796\tcpuset_cpu_active(); 5797\t} 5798 5799\t/* 5800* Put the rq online, if not already. This happens: 5801* 5802* 1) In the early boot process, because we build the real domains 5803* after all CPUs have been brought up. 5804* 5805* 2) At runtime, if cpuset_cpu_active() fails to rebuild the 5806* domains. 5807*/ 5808\trq_lock_irqsave(rq, \u0026amp;rf); 5809\tif (rq-\u0026gt;rd) { 5810\tBUG_ON(!cpumask_test_cpu(cpu, rq-\u0026gt;rd-\u0026gt;span)); 5811\tset_rq_online(rq); 5812\t} 5813\trq_unlock_irqrestore(rq, \u0026amp;rf); 5814 5815\tupdate_max_interval(); 5816 5817\treturn 0; 5818} 5819 5820int sched_cpu_deactivate(unsigned int cpu) 5821{ 5822\tint ret; 5823 5824\tset_cpu_active(cpu, false); 5825\t/* 5826* We\u0026#39;ve cleared cpu_active_mask, wait for all preempt-disabled and RCU 5827* users of this state to go away such that all new such users will 5828* observe it. 5829* 5830* Do sync before park smpboot threads to take care the rcu boost case. 5831*/ 5832\tsynchronize_rcu_mult(call_rcu, call_rcu_sched); 5833 5834#ifdef CONFIG_SCHED_SMT 5835\t/* 5836* When going down, decrement the number of cores with SMT present. 5837*/ 5838\tif (cpumask_weight(cpu_smt_mask(cpu)) == 2) 5839\tstatic_branch_dec_cpuslocked(\u0026amp;sched_smt_present); 5840#endif 5841 5842\tif (!sched_smp_initialized) 5843\treturn 0; 5844 5845\tret = cpuset_cpu_inactive(cpu); 5846\tif (ret) { 5847\tset_cpu_active(cpu, true); 5848\treturn ret; 5849\t} 5850\tsched_domains_numa_masks_clear(cpu); 5851\treturn 0; 5852} 5853 5854static void sched_rq_cpu_starting(unsigned int cpu) 5855{ 5856\tstruct rq *rq = cpu_rq(cpu); 5857 5858\trq-\u0026gt;calc_load_update = calc_load_update; 5859\tupdate_max_interval(); 5860} 5861 5862int sched_cpu_starting(unsigned int cpu) 5863{ 5864\tsched_rq_cpu_starting(cpu); 5865\tsched_tick_start(cpu); 5866\treturn 0; 5867} 5868 5869#ifdef CONFIG_HOTPLUG_CPU 5870int sched_cpu_dying(unsigned int cpu) 5871{ 5872\tstruct rq *rq = cpu_rq(cpu); 5873\tstruct rq_flags rf; 5874 5875\t/* Handle pending wakeups and then migrate everything off */ 5876\tsched_ttwu_pending(); 5877\tsched_tick_stop(cpu); 5878 5879\trq_lock_irqsave(rq, \u0026amp;rf); 5880\tif (rq-\u0026gt;rd) { 5881\tBUG_ON(!cpumask_test_cpu(cpu, rq-\u0026gt;rd-\u0026gt;span)); 5882\tset_rq_offline(rq); 5883\t} 5884\tmigrate_tasks(rq, \u0026amp;rf); 5885\tBUG_ON(rq-\u0026gt;nr_running != 1); 5886\trq_unlock_irqrestore(rq, \u0026amp;rf); 5887 5888\tcalc_load_migrate(rq); 5889\tupdate_max_interval(); 5890\tnohz_balance_exit_idle(rq); 5891\thrtick_clear(rq); 5892\treturn 0; 5893} 5894#endif 5895 5896void __init sched_init_smp(void) 5897{ 5898\tsched_init_numa(); 5899 5900\t/* 5901* There\u0026#39;s no userspace yet to cause hotplug operations; hence all the 5902* CPU masks are stable and all blatant races in the below code cannot 5903* happen. The hotplug lock is nevertheless taken to satisfy lockdep, 5904* but there won\u0026#39;t be any contention on it. 5905*/ 5906\tcpus_read_lock(); 5907\tmutex_lock(\u0026amp;sched_domains_mutex); 5908\tsched_init_domains(cpu_active_mask); 5909\tmutex_unlock(\u0026amp;sched_domains_mutex); 5910\tcpus_read_unlock(); 5911 5912\t/* Move init over to a non-isolated CPU */ 5913\tif (set_cpus_allowed_ptr(current, housekeeping_cpumask(HK_FLAG_DOMAIN)) \u0026lt; 0) 5914\tBUG(); 5915\tsched_init_granularity(); 5916 5917\tinit_sched_rt_class(); 5918\tinit_sched_dl_class(); 5919 5920\tsched_smp_initialized = true; 5921} 5922 5923static int __init migration_init(void) 5924{ 5925\tsched_rq_cpu_starting(smp_processor_id()); 5926\treturn 0; 5927} 5928early_initcall(migration_init); 5929 5930#else 5931void __init sched_init_smp(void) 5932{ 5933\tsched_init_granularity(); 5934} 5935#endif /* CONFIG_SMP */5936 5937int in_sched_functions(unsigned long addr) 5938{ 5939\treturn in_lock_functions(addr) || 5940\t(addr \u0026gt;= (unsigned long)__sched_text_start 5941\t\u0026amp;\u0026amp; addr \u0026lt; (unsigned long)__sched_text_end); 5942} 5943 5944#ifdef CONFIG_CGROUP_SCHED 5945/* 5946* Default task group. 5947* Every task in system belongs to this group at bootup. 5948*/ 5949struct task_group root_task_group; 5950LIST_HEAD(task_groups); 5951 5952/* Cacheline aligned slab cache for task_group */ 5953static struct kmem_cache *task_group_cache __read_mostly; 5954#endif 5955 5956DECLARE_PER_CPU(cpumask_var_t, load_balance_mask); 5957DECLARE_PER_CPU(cpumask_var_t, select_idle_mask); 5958 5959void __init sched_init(void) 5960{ 5961\tint i, j; 5962\tunsigned long alloc_size = 0, ptr; 5963 5964\twait_bit_init(); 5965 5966#ifdef CONFIG_FAIR_GROUP_SCHED 5967\talloc_size += 2 * nr_cpu_ids * sizeof(void **); 5968#endif 5969#ifdef CONFIG_RT_GROUP_SCHED 5970\talloc_size += 2 * nr_cpu_ids * sizeof(void **); 5971#endif 5972\tif (alloc_size) { 5973\tptr = (unsigned long)kzalloc(alloc_size, GFP_NOWAIT); 5974 5975#ifdef CONFIG_FAIR_GROUP_SCHED 5976\troot_task_group.se = (struct sched_entity **)ptr; 5977\tptr += nr_cpu_ids * sizeof(void **); 5978 5979\troot_task_group.cfs_rq = (struct cfs_rq **)ptr; 5980\tptr += nr_cpu_ids * sizeof(void **); 5981 5982#endif /* CONFIG_FAIR_GROUP_SCHED */5983#ifdef CONFIG_RT_GROUP_SCHED 5984\troot_task_group.rt_se = (struct sched_rt_entity **)ptr; 5985\tptr += nr_cpu_ids * sizeof(void **); 5986 5987\troot_task_group.rt_rq = (struct rt_rq **)ptr; 5988\tptr += nr_cpu_ids * sizeof(void **); 5989 5990#endif /* CONFIG_RT_GROUP_SCHED */5991\t} 5992#ifdef CONFIG_CPUMASK_OFFSTACK 5993\tfor_each_possible_cpu(i) { 5994\tper_cpu(load_balance_mask, i) = (cpumask_var_t)kzalloc_node( 5995\tcpumask_size(), GFP_KERNEL, cpu_to_node(i)); 5996\tper_cpu(select_idle_mask, i) = (cpumask_var_t)kzalloc_node( 5997\tcpumask_size(), GFP_KERNEL, cpu_to_node(i)); 5998\t} 5999#endif /* CONFIG_CPUMASK_OFFSTACK */6000 6001\tinit_rt_bandwidth(\u0026amp;def_rt_bandwidth, global_rt_period(), global_rt_runtime()); 6002\tinit_dl_bandwidth(\u0026amp;def_dl_bandwidth, global_rt_period(), global_rt_runtime()); 6003 6004#ifdef CONFIG_SMP 6005\tinit_defrootdomain(); 6006#endif 6007 6008#ifdef CONFIG_RT_GROUP_SCHED 6009\tinit_rt_bandwidth(\u0026amp;root_task_group.rt_bandwidth, 6010\tglobal_rt_period(), global_rt_runtime()); 6011#endif /* CONFIG_RT_GROUP_SCHED */6012 6013#ifdef CONFIG_CGROUP_SCHED 6014\ttask_group_cache = KMEM_CACHE(task_group, 0); 6015 6016\tlist_add(\u0026amp;root_task_group.list, \u0026amp;task_groups); 6017\tINIT_LIST_HEAD(\u0026amp;root_task_group.children); 6018\tINIT_LIST_HEAD(\u0026amp;root_task_group.siblings); 6019\tautogroup_init(\u0026amp;init_task); 6020#endif /* CONFIG_CGROUP_SCHED */6021 6022\tfor_each_possible_cpu(i) { 6023\tstruct rq *rq; 6024 6025\trq = cpu_rq(i); 6026\traw_spin_lock_init(\u0026amp;rq-\u0026gt;lock); 6027\trq-\u0026gt;nr_running = 0; 6028\trq-\u0026gt;calc_load_active = 0; 6029\trq-\u0026gt;calc_load_update = jiffies + LOAD_FREQ; 6030\tinit_cfs_rq(\u0026amp;rq-\u0026gt;cfs); 6031\tinit_rt_rq(\u0026amp;rq-\u0026gt;rt); 6032\tinit_dl_rq(\u0026amp;rq-\u0026gt;dl); 6033#ifdef CONFIG_FAIR_GROUP_SCHED 6034\troot_task_group.shares = ROOT_TASK_GROUP_LOAD; 6035\tINIT_LIST_HEAD(\u0026amp;rq-\u0026gt;leaf_cfs_rq_list); 6036\trq-\u0026gt;tmp_alone_branch = \u0026amp;rq-\u0026gt;leaf_cfs_rq_list; 6037\t/* 6038* How much CPU bandwidth does root_task_group get? 6039* 6040* In case of task-groups formed thr\u0026#39; the cgroup filesystem, it 6041* gets 100% of the CPU resources in the system. This overall 6042* system CPU resource is divided among the tasks of 6043* root_task_group and its child task-groups in a fair manner, 6044* based on each entity\u0026#39;s (task or task-group\u0026#39;s) weight 6045* (se-\u0026gt;load.weight). 6046* 6047* In other words, if root_task_group has 10 tasks of weight 6048* 1024) and two child groups A0 and A1 (of weight 1024 each), 6049* then A0\u0026#39;s share of the CPU resource is: 6050* 6051*\tA0\u0026#39;s bandwidth = 1024 / (10*1024 + 1024 + 1024) = 8.33% 6052* 6053* We achieve this by letting root_task_group\u0026#39;s tasks sit 6054* directly in rq-\u0026gt;cfs (i.e root_task_group-\u0026gt;se[] = NULL). 6055*/ 6056\tinit_cfs_bandwidth(\u0026amp;root_task_group.cfs_bandwidth); 6057\tinit_tg_cfs_entry(\u0026amp;root_task_group, \u0026amp;rq-\u0026gt;cfs, NULL, i, NULL); 6058#endif /* CONFIG_FAIR_GROUP_SCHED */6059 6060\trq-\u0026gt;rt.rt_runtime = def_rt_bandwidth.rt_runtime; 6061#ifdef CONFIG_RT_GROUP_SCHED 6062\tinit_tg_rt_entry(\u0026amp;root_task_group, \u0026amp;rq-\u0026gt;rt, NULL, i, NULL); 6063#endif 6064 6065\tfor (j = 0; j \u0026lt; CPU_LOAD_IDX_MAX; j++) 6066\trq-\u0026gt;cpu_load[j] = 0; 6067 6068#ifdef CONFIG_SMP 6069\trq-\u0026gt;sd = NULL; 6070\trq-\u0026gt;rd = NULL; 6071\trq-\u0026gt;cpu_capacity = rq-\u0026gt;cpu_capacity_orig = SCHED_CAPACITY_SCALE; 6072\trq-\u0026gt;balance_callback = NULL; 6073\trq-\u0026gt;active_balance = 0; 6074\trq-\u0026gt;next_balance = jiffies; 6075\trq-\u0026gt;push_cpu = 0; 6076\trq-\u0026gt;cpu = i; 6077\trq-\u0026gt;online = 0; 6078\trq-\u0026gt;idle_stamp = 0; 6079\trq-\u0026gt;avg_idle = 2*sysctl_sched_migration_cost; 6080\trq-\u0026gt;max_idle_balance_cost = sysctl_sched_migration_cost; 6081 6082\tINIT_LIST_HEAD(\u0026amp;rq-\u0026gt;cfs_tasks); 6083 6084\trq_attach_root(rq, \u0026amp;def_root_domain); 6085#ifdef CONFIG_NO_HZ_COMMON 6086\trq-\u0026gt;last_load_update_tick = jiffies; 6087\trq-\u0026gt;last_blocked_load_update_tick = jiffies; 6088\tatomic_set(\u0026amp;rq-\u0026gt;nohz_flags, 0); 6089#endif 6090#endif /* CONFIG_SMP */6091\thrtick_rq_init(rq); 6092\tatomic_set(\u0026amp;rq-\u0026gt;nr_iowait, 0); 6093\t} 6094 6095\tset_load_weight(\u0026amp;init_task, false); 6096 6097\t/* 6098* The boot idle thread does lazy MMU switching as well: 6099*/ 6100\tmmgrab(\u0026amp;init_mm); 6101\tenter_lazy_tlb(\u0026amp;init_mm, current); 6102 6103\t/* 6104* Make us the idle thread. Technically, schedule() should not be 6105* called from this thread, however somewhere below it might be, 6106* but because we are the idle thread, we just pick up running again 6107* when this runqueue becomes \u0026#34;idle\u0026#34;. 6108*/ 6109\tinit_idle(current, smp_processor_id()); 6110 6111\tcalc_load_update = jiffies + LOAD_FREQ; 6112 6113#ifdef CONFIG_SMP 6114\tidle_thread_set_boot_cpu(); 6115#endif 6116\tinit_sched_fair_class(); 6117 6118\tinit_schedstats(); 6119 6120\tscheduler_running = 1; 6121} 6122 6123#ifdef CONFIG_DEBUG_ATOMIC_SLEEP 6124static inline int preempt_count_equals(int preempt_offset) 6125{ 6126\tint nested = preempt_count() + rcu_preempt_depth(); 6127 6128\treturn (nested == preempt_offset); 6129} 6130 6131void __might_sleep(const char *file, int line, int preempt_offset) 6132{ 6133\t/* 6134* Blocking primitives will set (and therefore destroy) current-\u0026gt;state, 6135* since we will exit with TASK_RUNNING make sure we enter with it, 6136* otherwise we will destroy state. 6137*/ 6138\tWARN_ONCE(current-\u0026gt;state != TASK_RUNNING \u0026amp;\u0026amp; current-\u0026gt;task_state_change, 6139\t\u0026#34;do not call blocking ops when !TASK_RUNNING; \u0026#34; 6140\t\u0026#34;state=%lx set at [\u0026lt;%p\u0026gt;] %pS\\n\u0026#34;, 6141\tcurrent-\u0026gt;state, 6142\t(void *)current-\u0026gt;task_state_change, 6143\t(void *)current-\u0026gt;task_state_change); 6144 6145\t___might_sleep(file, line, preempt_offset); 6146} 6147EXPORT_SYMBOL(__might_sleep); 6148 6149void ___might_sleep(const char *file, int line, int preempt_offset) 6150{ 6151\t/* Ratelimiting timestamp: */ 6152\tstatic unsigned long prev_jiffy; 6153 6154\tunsigned long preempt_disable_ip; 6155 6156\t/* WARN_ON_ONCE() by default, no rate limit required: */ 6157\trcu_sleep_check(); 6158 6159\tif ((preempt_count_equals(preempt_offset) \u0026amp;\u0026amp; !irqs_disabled() \u0026amp;\u0026amp; 6160\t!is_idle_task(current)) || 6161\tsystem_state == SYSTEM_BOOTING || system_state \u0026gt; SYSTEM_RUNNING || 6162\toops_in_progress) 6163\treturn; 6164 6165\tif (time_before(jiffies, prev_jiffy + HZ) \u0026amp;\u0026amp; prev_jiffy) 6166\treturn; 6167\tprev_jiffy = jiffies; 6168 6169\t/* Save this before calling printk(), since that will clobber it: */ 6170\tpreempt_disable_ip = get_preempt_disable_ip(current); 6171 6172\tprintk(KERN_ERR 6173\t\u0026#34;BUG: sleeping function called from invalid context at %s:%d\\n\u0026#34;, 6174\tfile, line); 6175\tprintk(KERN_ERR 6176\t\u0026#34;in_atomic(): %d, irqs_disabled(): %d, pid: %d, name: %s\\n\u0026#34;, 6177\tin_atomic(), irqs_disabled(), 6178\tcurrent-\u0026gt;pid, current-\u0026gt;comm); 6179 6180\tif (task_stack_end_corrupted(current)) 6181\tprintk(KERN_EMERG \u0026#34;Thread overran stack, or stack corrupted\\n\u0026#34;); 6182 6183\tdebug_show_held_locks(current); 6184\tif (irqs_disabled()) 6185\tprint_irqtrace_events(current); 6186\tif (IS_ENABLED(CONFIG_DEBUG_PREEMPT) 6187\t\u0026amp;\u0026amp; !preempt_count_equals(preempt_offset)) { 6188\tpr_err(\u0026#34;Preemption disabled at:\u0026#34;); 6189\tprint_ip_sym(preempt_disable_ip); 6190\tpr_cont(\u0026#34;\\n\u0026#34;); 6191\t} 6192\tdump_stack(); 6193\tadd_taint(TAINT_WARN, LOCKDEP_STILL_OK); 6194} 6195EXPORT_SYMBOL(___might_sleep); 6196#endif 6197 6198#ifdef CONFIG_MAGIC_SYSRQ 6199void normalize_rt_tasks(void) 6200{ 6201\tstruct task_struct *g, *p; 6202\tstruct sched_attr attr = { 6203\t.sched_policy = SCHED_NORMAL, 6204\t}; 6205 6206\tread_lock(\u0026amp;tasklist_lock); 6207\tfor_each_process_thread(g, p) { 6208\t/* 6209* Only normalize user tasks: 6210*/ 6211\tif (p-\u0026gt;flags \u0026amp; PF_KTHREAD) 6212\tcontinue; 6213 6214\tp-\u0026gt;se.exec_start = 0; 6215\tschedstat_set(p-\u0026gt;se.statistics.wait_start, 0); 6216\tschedstat_set(p-\u0026gt;se.statistics.sleep_start, 0); 6217\tschedstat_set(p-\u0026gt;se.statistics.block_start, 0); 6218 6219\tif (!dl_task(p) \u0026amp;\u0026amp; !rt_task(p)) { 6220\t/* 6221* Renice negative nice level userspace 6222* tasks back to 0: 6223*/ 6224\tif (task_nice(p) \u0026lt; 0) 6225\tset_user_nice(p, 0); 6226\tcontinue; 6227\t} 6228 6229\t__sched_setscheduler(p, \u0026amp;attr, false, false); 6230\t} 6231\tread_unlock(\u0026amp;tasklist_lock); 6232} 6233 6234#endif /* CONFIG_MAGIC_SYSRQ */6235 6236#if defined(CONFIG_IA64) || defined(CONFIG_KGDB_KDB) 6237/* 6238* These functions are only useful for the IA64 MCA handling, or kdb. 6239* 6240* They can only be called when the whole system has been 6241* stopped - every CPU needs to be quiescent, and no scheduling 6242* activity can take place. Using them for anything else would 6243* be a serious bug, and as a result, they aren\u0026#39;t even visible 6244* under any other configuration. 6245*/ 6246 6247/** 6248* curr_task - return the current task for a given CPU. 6249* @cpu: the processor in question. 6250* 6251* ONLY VALID WHEN THE WHOLE SYSTEM IS STOPPED! 6252* 6253* Return: The current task for @cpu. 6254*/ 6255struct task_struct *curr_task(int cpu) 6256{ 6257\treturn cpu_curr(cpu); 6258} 6259 6260#endif /* defined(CONFIG_IA64) || defined(CONFIG_KGDB_KDB) */6261 6262#ifdef CONFIG_IA64 6263/** 6264* set_curr_task - set the current task for a given CPU. 6265* @cpu: the processor in question. 6266* @p: the task pointer to set. 6267* 6268* Description: This function must only be used when non-maskable interrupts 6269* are serviced on a separate stack. It allows the architecture to switch the 6270* notion of the current task on a CPU in a non-blocking manner. This function 6271* must be called with all CPU\u0026#39;s synchronized, and interrupts disabled, the 6272* and caller must save the original value of the current task (see 6273* curr_task() above) and restore that value before reenabling interrupts and 6274* re-starting the system. 6275* 6276* ONLY VALID WHEN THE WHOLE SYSTEM IS STOPPED! 6277*/ 6278void ia64_set_curr_task(int cpu, struct task_struct *p) 6279{ 6280\tcpu_curr(cpu) = p; 6281} 6282 6283#endif 6284 6285#ifdef CONFIG_CGROUP_SCHED 6286/* task_group_lock serializes the addition/removal of task groups */ 6287static DEFINE_SPINLOCK(task_group_lock); 6288 6289static void sched_free_group(struct task_group *tg) 6290{ 6291\tfree_fair_sched_group(tg); 6292\tfree_rt_sched_group(tg); 6293\tautogroup_free(tg); 6294\tkmem_cache_free(task_group_cache, tg); 6295} 6296 6297/* allocate runqueue etc for a new task group */ 6298struct task_group *sched_create_group(struct task_group *parent) 6299{ 6300\tstruct task_group *tg; 6301 6302\ttg = kmem_cache_alloc(task_group_cache, GFP_KERNEL | __GFP_ZERO); 6303\tif (!tg) 6304\treturn ERR_PTR(-ENOMEM); 6305 6306\tif (!alloc_fair_sched_group(tg, parent)) 6307\tgoto err; 6308 6309\tif (!alloc_rt_sched_group(tg, parent)) 6310\tgoto err; 6311 6312\treturn tg; 6313 6314err: 6315\tsched_free_group(tg); 6316\treturn ERR_PTR(-ENOMEM); 6317} 6318 6319void sched_online_group(struct task_group *tg, struct task_group *parent) 6320{ 6321\tunsigned long flags; 6322 6323\tspin_lock_irqsave(\u0026amp;task_group_lock, flags); 6324\tlist_add_rcu(\u0026amp;tg-\u0026gt;list, \u0026amp;task_groups); 6325 6326\t/* Root should already exist: */ 6327\tWARN_ON(!parent); 6328 6329\ttg-\u0026gt;parent = parent; 6330\tINIT_LIST_HEAD(\u0026amp;tg-\u0026gt;children); 6331\tlist_add_rcu(\u0026amp;tg-\u0026gt;siblings, \u0026amp;parent-\u0026gt;children); 6332\tspin_unlock_irqrestore(\u0026amp;task_group_lock, flags); 6333 6334\tonline_fair_sched_group(tg); 6335} 6336 6337/* rcu callback to free various structures associated with a task group */ 6338static void sched_free_group_rcu(struct rcu_head *rhp) 6339{ 6340\t/* Now it should be safe to free those cfs_rqs: */ 6341\tsched_free_group(container_of(rhp, struct task_group, rcu)); 6342} 6343 6344void sched_destroy_group(struct task_group *tg) 6345{ 6346\t/* Wait for possible concurrent references to cfs_rqs complete: */ 6347\tcall_rcu(\u0026amp;tg-\u0026gt;rcu, sched_free_group_rcu); 6348} 6349 6350void sched_offline_group(struct task_group *tg) 6351{ 6352\tunsigned long flags; 6353 6354\t/* End participation in shares distribution: */ 6355\tunregister_fair_sched_group(tg); 6356 6357\tspin_lock_irqsave(\u0026amp;task_group_lock, flags); 6358\tlist_del_rcu(\u0026amp;tg-\u0026gt;list); 6359\tlist_del_rcu(\u0026amp;tg-\u0026gt;siblings); 6360\tspin_unlock_irqrestore(\u0026amp;task_group_lock, flags); 6361} 6362 6363static void sched_change_group(struct task_struct *tsk, int type) 6364{ 6365\tstruct task_group *tg; 6366 6367\t/* 6368* All callers are synchronized by task_rq_lock(); we do not use RCU 6369* which is pointless here. Thus, we pass \u0026#34;true\u0026#34; to task_css_check() 6370* to prevent lockdep warnings. 6371*/ 6372\ttg = container_of(task_css_check(tsk, cpu_cgrp_id, true), 6373\tstruct task_group, css); 6374\ttg = autogroup_task_group(tsk, tg); 6375\ttsk-\u0026gt;sched_task_group = tg; 6376 6377#ifdef CONFIG_FAIR_GROUP_SCHED 6378\tif (tsk-\u0026gt;sched_class-\u0026gt;task_change_group) 6379\ttsk-\u0026gt;sched_class-\u0026gt;task_change_group(tsk, type); 6380\telse 6381#endif 6382\tset_task_rq(tsk, task_cpu(tsk)); 6383} 6384 6385/* 6386* Change task\u0026#39;s runqueue when it moves between groups. 6387* 6388* The caller of this function should have put the task in its new group by 6389* now. This function just updates tsk-\u0026gt;se.cfs_rq and tsk-\u0026gt;se.parent to reflect 6390* its new group. 6391*/ 6392void sched_move_task(struct task_struct *tsk) 6393{ 6394\tint queued, running, queue_flags = 6395\tDEQUEUE_SAVE | DEQUEUE_MOVE | DEQUEUE_NOCLOCK; 6396\tstruct rq_flags rf; 6397\tstruct rq *rq; 6398 6399\trq = task_rq_lock(tsk, \u0026amp;rf); 6400\tupdate_rq_clock(rq); 6401 6402\trunning = task_current(rq, tsk); 6403\tqueued = task_on_rq_queued(tsk); 6404 6405\tif (queued) 6406\tdequeue_task(rq, tsk, queue_flags); 6407\tif (running) 6408\tput_prev_task(rq, tsk); 6409 6410\tsched_change_group(tsk, TASK_MOVE_GROUP); 6411 6412\tif (queued) 6413\tenqueue_task(rq, tsk, queue_flags); 6414\tif (running) 6415\tset_curr_task(rq, tsk); 6416 6417\ttask_rq_unlock(rq, tsk, \u0026amp;rf); 6418} 6419 6420static inline struct task_group *css_tg(struct cgroup_subsys_state *css) 6421{ 6422\treturn css ? container_of(css, struct task_group, css) : NULL; 6423} 6424 6425static struct cgroup_subsys_state * 6426cpu_cgroup_css_alloc(struct cgroup_subsys_state *parent_css) 6427{ 6428\tstruct task_group *parent = css_tg(parent_css); 6429\tstruct task_group *tg; 6430 6431\tif (!parent) { 6432\t/* This is early initialization for the top cgroup */ 6433\treturn \u0026amp;root_task_group.css; 6434\t} 6435 6436\ttg = sched_create_group(parent); 6437\tif (IS_ERR(tg)) 6438\treturn ERR_PTR(-ENOMEM); 6439 6440\treturn \u0026amp;tg-\u0026gt;css; 6441} 6442 6443/* Expose task group only after completing cgroup initialization */ 6444static int cpu_cgroup_css_online(struct cgroup_subsys_state *css) 6445{ 6446\tstruct task_group *tg = css_tg(css); 6447\tstruct task_group *parent = css_tg(css-\u0026gt;parent); 6448 6449\tif (parent) 6450\tsched_online_group(tg, parent); 6451\treturn 0; 6452} 6453 6454static void cpu_cgroup_css_released(struct cgroup_subsys_state *css) 6455{ 6456\tstruct task_group *tg = css_tg(css); 6457 6458\tsched_offline_group(tg); 6459} 6460 6461static void cpu_cgroup_css_free(struct cgroup_subsys_state *css) 6462{ 6463\tstruct task_group *tg = css_tg(css); 6464 6465\t/* 6466* Relies on the RCU grace period between css_released() and this. 6467*/ 6468\tsched_free_group(tg); 6469} 6470 6471/* 6472* This is called before wake_up_new_task(), therefore we really only 6473* have to set its group bits, all the other stuff does not apply. 6474*/ 6475static void cpu_cgroup_fork(struct task_struct *task) 6476{ 6477\tstruct rq_flags rf; 6478\tstruct rq *rq; 6479 6480\trq = task_rq_lock(task, \u0026amp;rf); 6481 6482\tupdate_rq_clock(rq); 6483\tsched_change_group(task, TASK_SET_GROUP); 6484 6485\ttask_rq_unlock(rq, task, \u0026amp;rf); 6486} 6487 6488static int cpu_cgroup_can_attach(struct cgroup_taskset *tset) 6489{ 6490\tstruct task_struct *task; 6491\tstruct cgroup_subsys_state *css; 6492\tint ret = 0; 6493 6494\tcgroup_taskset_for_each(task, css, tset) { 6495#ifdef CONFIG_RT_GROUP_SCHED 6496\tif (!sched_rt_can_attach(css_tg(css), task)) 6497\treturn -EINVAL; 6498#endif 6499\t/* 6500* Serialize against wake_up_new_task() such that if its 6501* running, we\u0026#39;re sure to observe its full state. 6502*/ 6503\traw_spin_lock_irq(\u0026amp;task-\u0026gt;pi_lock); 6504\t/* 6505* Avoid calling sched_move_task() before wake_up_new_task() 6506* has happened. This would lead to problems with PELT, due to 6507* move wanting to detach+attach while we\u0026#39;re not attached yet. 6508*/ 6509\tif (task-\u0026gt;state == TASK_NEW) 6510\tret = -EINVAL; 6511\traw_spin_unlock_irq(\u0026amp;task-\u0026gt;pi_lock); 6512 6513\tif (ret) 6514\tbreak; 6515\t} 6516\treturn ret; 6517} 6518 6519static void cpu_cgroup_attach(struct cgroup_taskset *tset) 6520{ 6521\tstruct task_struct *task; 6522\tstruct cgroup_subsys_state *css; 6523 6524\tcgroup_taskset_for_each(task, css, tset) 6525\tsched_move_task(task); 6526} 6527 6528#ifdef CONFIG_FAIR_GROUP_SCHED 6529static int cpu_shares_write_u64(struct cgroup_subsys_state *css, 6530\tstruct cftype *cftype, u64 shareval) 6531{ 6532\tif (shareval \u0026gt; scale_load_down(ULONG_MAX)) 6533\tshareval = MAX_SHARES; 6534\treturn sched_group_set_shares(css_tg(css), scale_load(shareval)); 6535} 6536 6537static u64 cpu_shares_read_u64(struct cgroup_subsys_state *css, 6538\tstruct cftype *cft) 6539{ 6540\tstruct task_group *tg = css_tg(css); 6541 6542\treturn (u64) scale_load_down(tg-\u0026gt;shares); 6543} 6544 6545#ifdef CONFIG_CFS_BANDWIDTH 6546static DEFINE_MUTEX(cfs_constraints_mutex); 6547 6548const u64 max_cfs_quota_period = 1 * NSEC_PER_SEC; /* 1s */ 6549const u64 min_cfs_quota_period = 1 * NSEC_PER_MSEC; /* 1ms */ 6550 6551static int __cfs_schedulable(struct task_group *tg, u64 period, u64 runtime); 6552 6553static int tg_set_cfs_bandwidth(struct task_group *tg, u64 period, u64 quota) 6554{ 6555\tint i, ret = 0, runtime_enabled, runtime_was_enabled; 6556\tstruct cfs_bandwidth *cfs_b = \u0026amp;tg-\u0026gt;cfs_bandwidth; 6557 6558\tif (tg == \u0026amp;root_task_group) 6559\treturn -EINVAL; 6560 6561\t/* 6562* Ensure we have at some amount of bandwidth every period. This is 6563* to prevent reaching a state of large arrears when throttled via 6564* entity_tick() resulting in prolonged exit starvation. 6565*/ 6566\tif (quota \u0026lt; min_cfs_quota_period || period \u0026lt; min_cfs_quota_period) 6567\treturn -EINVAL; 6568 6569\t/* 6570* Likewise, bound things on the otherside by preventing insane quota 6571* periods. This also allows us to normalize in computing quota 6572* feasibility. 6573*/ 6574\tif (period \u0026gt; max_cfs_quota_period) 6575\treturn -EINVAL; 6576 6577\t/* 6578* Prevent race between setting of cfs_rq-\u0026gt;runtime_enabled and 6579* unthrottle_offline_cfs_rqs(). 6580*/ 6581\tget_online_cpus(); 6582\tmutex_lock(\u0026amp;cfs_constraints_mutex); 6583\tret = __cfs_schedulable(tg, period, quota); 6584\tif (ret) 6585\tgoto out_unlock; 6586 6587\truntime_enabled = quota != RUNTIME_INF; 6588\truntime_was_enabled = cfs_b-\u0026gt;quota != RUNTIME_INF; 6589\t/* 6590* If we need to toggle cfs_bandwidth_used, off-\u0026gt;on must occur 6591* before making related changes, and on-\u0026gt;off must occur afterwards 6592*/ 6593\tif (runtime_enabled \u0026amp;\u0026amp; !runtime_was_enabled) 6594\tcfs_bandwidth_usage_inc(); 6595\traw_spin_lock_irq(\u0026amp;cfs_b-\u0026gt;lock); 6596\tcfs_b-\u0026gt;period = ns_to_ktime(period); 6597\tcfs_b-\u0026gt;quota = quota; 6598 6599\t__refill_cfs_bandwidth_runtime(cfs_b); 6600 6601\t/* Restart the period timer (if active) to handle new period expiry: */ 6602\tif (runtime_enabled) 6603\tstart_cfs_bandwidth(cfs_b); 6604 6605\traw_spin_unlock_irq(\u0026amp;cfs_b-\u0026gt;lock); 6606 6607\tfor_each_online_cpu(i) { 6608\tstruct cfs_rq *cfs_rq = tg-\u0026gt;cfs_rq[i]; 6609\tstruct rq *rq = cfs_rq-\u0026gt;rq; 6610\tstruct rq_flags rf; 6611 6612\trq_lock_irq(rq, \u0026amp;rf); 6613\tcfs_rq-\u0026gt;runtime_enabled = runtime_enabled; 6614\tcfs_rq-\u0026gt;runtime_remaining = 0; 6615 6616\tif (cfs_rq-\u0026gt;throttled) 6617\tunthrottle_cfs_rq(cfs_rq); 6618\trq_unlock_irq(rq, \u0026amp;rf); 6619\t} 6620\tif (runtime_was_enabled \u0026amp;\u0026amp; !runtime_enabled) 6621\tcfs_bandwidth_usage_dec(); 6622out_unlock: 6623\tmutex_unlock(\u0026amp;cfs_constraints_mutex); 6624\tput_online_cpus(); 6625 6626\treturn ret; 6627} 6628 6629int tg_set_cfs_quota(struct task_group *tg, long cfs_quota_us) 6630{ 6631\tu64 quota, period; 6632 6633\tperiod = ktime_to_ns(tg-\u0026gt;cfs_bandwidth.period); 6634\tif (cfs_quota_us \u0026lt; 0) 6635\tquota = RUNTIME_INF; 6636\telse if ((u64)cfs_quota_us \u0026lt;= U64_MAX / NSEC_PER_USEC) 6637\tquota = (u64)cfs_quota_us * NSEC_PER_USEC; 6638\telse 6639\treturn -EINVAL; 6640 6641\treturn tg_set_cfs_bandwidth(tg, period, quota); 6642} 6643 6644long tg_get_cfs_quota(struct task_group *tg) 6645{ 6646\tu64 quota_us; 6647 6648\tif (tg-\u0026gt;cfs_bandwidth.quota == RUNTIME_INF) 6649\treturn -1; 6650 6651\tquota_us = tg-\u0026gt;cfs_bandwidth.quota; 6652\tdo_div(quota_us, NSEC_PER_USEC); 6653 6654\treturn quota_us; 6655} 6656 6657int tg_set_cfs_period(struct task_group *tg, long cfs_period_us) 6658{ 6659\tu64 quota, period; 6660 6661\tif ((u64)cfs_period_us \u0026gt; U64_MAX / NSEC_PER_USEC) 6662\treturn -EINVAL; 6663 6664\tperiod = (u64)cfs_period_us * NSEC_PER_USEC; 6665\tquota = tg-\u0026gt;cfs_bandwidth.quota; 6666 6667\treturn tg_set_cfs_bandwidth(tg, period, quota); 6668} 6669 6670long tg_get_cfs_period(struct task_group *tg) 6671{ 6672\tu64 cfs_period_us; 6673 6674\tcfs_period_us = ktime_to_ns(tg-\u0026gt;cfs_bandwidth.period); 6675\tdo_div(cfs_period_us, NSEC_PER_USEC); 6676 6677\treturn cfs_period_us; 6678} 6679 6680static s64 cpu_cfs_quota_read_s64(struct cgroup_subsys_state *css, 6681\tstruct cftype *cft) 6682{ 6683\treturn tg_get_cfs_quota(css_tg(css)); 6684} 6685 6686static int cpu_cfs_quota_write_s64(struct cgroup_subsys_state *css, 6687\tstruct cftype *cftype, s64 cfs_quota_us) 6688{ 6689\treturn tg_set_cfs_quota(css_tg(css), cfs_quota_us); 6690} 6691 6692static u64 cpu_cfs_period_read_u64(struct cgroup_subsys_state *css, 6693\tstruct cftype *cft) 6694{ 6695\treturn tg_get_cfs_period(css_tg(css)); 6696} 6697 6698static int cpu_cfs_period_write_u64(struct cgroup_subsys_state *css, 6699\tstruct cftype *cftype, u64 cfs_period_us) 6700{ 6701\treturn tg_set_cfs_period(css_tg(css), cfs_period_us); 6702} 6703 6704struct cfs_schedulable_data { 6705\tstruct task_group *tg; 6706\tu64 period, quota; 6707}; 6708 6709/* 6710* normalize group quota/period to be quota/max_period 6711* note: units are usecs 6712*/ 6713static u64 normalize_cfs_quota(struct task_group *tg, 6714\tstruct cfs_schedulable_data *d) 6715{ 6716\tu64 quota, period; 6717 6718\tif (tg == d-\u0026gt;tg) { 6719\tperiod = d-\u0026gt;period; 6720\tquota = d-\u0026gt;quota; 6721\t} else { 6722\tperiod = tg_get_cfs_period(tg); 6723\tquota = tg_get_cfs_quota(tg); 6724\t} 6725 6726\t/* note: these should typically be equivalent */ 6727\tif (quota == RUNTIME_INF || quota == -1) 6728\treturn RUNTIME_INF; 6729 6730\treturn to_ratio(period, quota); 6731} 6732 6733static int tg_cfs_schedulable_down(struct task_group *tg, void *data) 6734{ 6735\tstruct cfs_schedulable_data *d = data; 6736\tstruct cfs_bandwidth *cfs_b = \u0026amp;tg-\u0026gt;cfs_bandwidth; 6737\ts64 quota = 0, parent_quota = -1; 6738 6739\tif (!tg-\u0026gt;parent) { 6740\tquota = RUNTIME_INF; 6741\t} else { 6742\tstruct cfs_bandwidth *parent_b = \u0026amp;tg-\u0026gt;parent-\u0026gt;cfs_bandwidth; 6743 6744\tquota = normalize_cfs_quota(tg, d); 6745\tparent_quota = parent_b-\u0026gt;hierarchical_quota; 6746 6747\t/* 6748* Ensure max(child_quota) \u0026lt;= parent_quota. On cgroup2, 6749* always take the min. On cgroup1, only inherit when no 6750* limit is set: 6751*/ 6752\tif (cgroup_subsys_on_dfl(cpu_cgrp_subsys)) { 6753\tquota = min(quota, parent_quota); 6754\t} else { 6755\tif (quota == RUNTIME_INF) 6756\tquota = parent_quota; 6757\telse if (parent_quota != RUNTIME_INF \u0026amp;\u0026amp; quota \u0026gt; parent_quota) 6758\treturn -EINVAL; 6759\t} 6760\t} 6761\tcfs_b-\u0026gt;hierarchical_quota = quota; 6762 6763\treturn 0; 6764} 6765 6766static int __cfs_schedulable(struct task_group *tg, u64 period, u64 quota) 6767{ 6768\tint ret; 6769\tstruct cfs_schedulable_data data = { 6770\t.tg = tg, 6771\t.period = period, 6772\t.quota = quota, 6773\t}; 6774 6775\tif (quota != RUNTIME_INF) { 6776\tdo_div(data.period, NSEC_PER_USEC); 6777\tdo_div(data.quota, NSEC_PER_USEC); 6778\t} 6779 6780\trcu_read_lock(); 6781\tret = walk_tg_tree(tg_cfs_schedulable_down, tg_nop, \u0026amp;data); 6782\trcu_read_unlock(); 6783 6784\treturn ret; 6785} 6786 6787static int cpu_cfs_stat_show(struct seq_file *sf, void *v) 6788{ 6789\tstruct task_group *tg = css_tg(seq_css(sf)); 6790\tstruct cfs_bandwidth *cfs_b = \u0026amp;tg-\u0026gt;cfs_bandwidth; 6791 6792\tseq_printf(sf, \u0026#34;nr_periods %d\\n\u0026#34;, cfs_b-\u0026gt;nr_periods); 6793\tseq_printf(sf, \u0026#34;nr_throttled %d\\n\u0026#34;, cfs_b-\u0026gt;nr_throttled); 6794\tseq_printf(sf, \u0026#34;throttled_time %llu\\n\u0026#34;, cfs_b-\u0026gt;throttled_time); 6795 6796\tif (schedstat_enabled() \u0026amp;\u0026amp; tg != \u0026amp;root_task_group) { 6797\tu64 ws = 0; 6798\tint i; 6799 6800\tfor_each_possible_cpu(i) 6801\tws += schedstat_val(tg-\u0026gt;se[i]-\u0026gt;statistics.wait_sum); 6802 6803\tseq_printf(sf, \u0026#34;wait_sum %llu\\n\u0026#34;, ws); 6804\t} 6805 6806\treturn 0; 6807} 6808#endif /* CONFIG_CFS_BANDWIDTH */6809#endif /* CONFIG_FAIR_GROUP_SCHED */6810 6811#ifdef CONFIG_RT_GROUP_SCHED 6812static int cpu_rt_runtime_write(struct cgroup_subsys_state *css, 6813\tstruct cftype *cft, s64 val) 6814{ 6815\treturn sched_group_set_rt_runtime(css_tg(css), val); 6816} 6817 6818static s64 cpu_rt_runtime_read(struct cgroup_subsys_state *css, 6819\tstruct cftype *cft) 6820{ 6821\treturn sched_group_rt_runtime(css_tg(css)); 6822} 6823 6824static int cpu_rt_period_write_uint(struct cgroup_subsys_state *css, 6825\tstruct cftype *cftype, u64 rt_period_us) 6826{ 6827\treturn sched_group_set_rt_period(css_tg(css), rt_period_us); 6828} 6829 6830static u64 cpu_rt_period_read_uint(struct cgroup_subsys_state *css, 6831\tstruct cftype *cft) 6832{ 6833\treturn sched_group_rt_period(css_tg(css)); 6834} 6835#endif /* CONFIG_RT_GROUP_SCHED */6836 6837static struct cftype cpu_legacy_files[] = { 6838#ifdef CONFIG_FAIR_GROUP_SCHED 6839\t{ 6840\t.name = \u0026#34;shares\u0026#34;, 6841\t.read_u64 = cpu_shares_read_u64, 6842\t.write_u64 = cpu_shares_write_u64, 6843\t}, 6844#endif 6845#ifdef CONFIG_CFS_BANDWIDTH 6846\t{ 6847\t.name = \u0026#34;cfs_quota_us\u0026#34;, 6848\t.read_s64 = cpu_cfs_quota_read_s64, 6849\t.write_s64 = cpu_cfs_quota_write_s64, 6850\t}, 6851\t{ 6852\t.name = \u0026#34;cfs_period_us\u0026#34;, 6853\t.read_u64 = cpu_cfs_period_read_u64, 6854\t.write_u64 = cpu_cfs_period_write_u64, 6855\t}, 6856\t{ 6857\t.name = \u0026#34;stat\u0026#34;, 6858\t.seq_show = cpu_cfs_stat_show, 6859\t}, 6860#endif 6861#ifdef CONFIG_RT_GROUP_SCHED 6862\t{ 6863\t.name = \u0026#34;rt_runtime_us\u0026#34;, 6864\t.read_s64 = cpu_rt_runtime_read, 6865\t.write_s64 = cpu_rt_runtime_write, 6866\t}, 6867\t{ 6868\t.name = \u0026#34;rt_period_us\u0026#34;, 6869\t.read_u64 = cpu_rt_period_read_uint, 6870\t.write_u64 = cpu_rt_period_write_uint, 6871\t}, 6872#endif 6873\t{ }\t/* Terminate */ 6874}; 6875 6876static int cpu_extra_stat_show(struct seq_file *sf, 6877\tstruct cgroup_subsys_state *css) 6878{ 6879#ifdef CONFIG_CFS_BANDWIDTH 6880\t{ 6881\tstruct task_group *tg = css_tg(css); 6882\tstruct cfs_bandwidth *cfs_b = \u0026amp;tg-\u0026gt;cfs_bandwidth; 6883\tu64 throttled_usec; 6884 6885\tthrottled_usec = cfs_b-\u0026gt;throttled_time; 6886\tdo_div(throttled_usec, NSEC_PER_USEC); 6887 6888\tseq_printf(sf, \u0026#34;nr_periods %d\\n\u0026#34; 6889\t\u0026#34;nr_throttled %d\\n\u0026#34; 6890\t\u0026#34;throttled_usec %llu\\n\u0026#34;, 6891\tcfs_b-\u0026gt;nr_periods, cfs_b-\u0026gt;nr_throttled, 6892\tthrottled_usec); 6893\t} 6894#endif 6895\treturn 0; 6896} 6897 6898#ifdef CONFIG_FAIR_GROUP_SCHED 6899static u64 cpu_weight_read_u64(struct cgroup_subsys_state *css, 6900\tstruct cftype *cft) 6901{ 6902\tstruct task_group *tg = css_tg(css); 6903\tu64 weight = scale_load_down(tg-\u0026gt;shares); 6904 6905\treturn DIV_ROUND_CLOSEST_ULL(weight * CGROUP_WEIGHT_DFL, 1024); 6906} 6907 6908static int cpu_weight_write_u64(struct cgroup_subsys_state *css, 6909\tstruct cftype *cft, u64 weight) 6910{ 6911\t/* 6912* cgroup weight knobs should use the common MIN, DFL and MAX 6913* values which are 1, 100 and 10000 respectively. While it loses 6914* a bit of range on both ends, it maps pretty well onto the shares 6915* value used by scheduler and the round-trip conversions preserve 6916* the original value over the entire range. 6917*/ 6918\tif (weight \u0026lt; CGROUP_WEIGHT_MIN || weight \u0026gt; CGROUP_WEIGHT_MAX) 6919\treturn -ERANGE; 6920 6921\tweight = DIV_ROUND_CLOSEST_ULL(weight * 1024, CGROUP_WEIGHT_DFL); 6922 6923\treturn sched_group_set_shares(css_tg(css), scale_load(weight)); 6924} 6925 6926static s64 cpu_weight_nice_read_s64(struct cgroup_subsys_state *css, 6927\tstruct cftype *cft) 6928{ 6929\tunsigned long weight = scale_load_down(css_tg(css)-\u0026gt;shares); 6930\tint last_delta = INT_MAX; 6931\tint prio, delta; 6932 6933\t/* find the closest nice value to the current weight */ 6934\tfor (prio = 0; prio \u0026lt; ARRAY_SIZE(sched_prio_to_weight); prio++) { 6935\tdelta = abs(sched_prio_to_weight[prio] - weight); 6936\tif (delta \u0026gt;= last_delta) 6937\tbreak; 6938\tlast_delta = delta; 6939\t} 6940 6941\treturn PRIO_TO_NICE(prio - 1 + MAX_RT_PRIO); 6942} 6943 6944static int cpu_weight_nice_write_s64(struct cgroup_subsys_state *css, 6945\tstruct cftype *cft, s64 nice) 6946{ 6947\tunsigned long weight; 6948\tint idx; 6949 6950\tif (nice \u0026lt; MIN_NICE || nice \u0026gt; MAX_NICE) 6951\treturn -ERANGE; 6952 6953\tidx = NICE_TO_PRIO(nice) - MAX_RT_PRIO; 6954\tidx = array_index_nospec(idx, 40); 6955\tweight = sched_prio_to_weight[idx]; 6956 6957\treturn sched_group_set_shares(css_tg(css), scale_load(weight)); 6958} 6959#endif 6960 6961static void __maybe_unused cpu_period_quota_print(struct seq_file *sf, 6962\tlong period, long quota) 6963{ 6964\tif (quota \u0026lt; 0) 6965\tseq_puts(sf, \u0026#34;max\u0026#34;); 6966\telse 6967\tseq_printf(sf, \u0026#34;%ld\u0026#34;, quota); 6968 6969\tseq_printf(sf, \u0026#34; %ld\\n\u0026#34;, period); 6970} 6971 6972/* caller should put the current value in *@periodp before calling */ 6973static int __maybe_unused cpu_period_quota_parse(char *buf, 6974\tu64 *periodp, u64 *quotap) 6975{ 6976\tchar tok[21];\t/* U64_MAX */ 6977 6978\tif (sscanf(buf, \u0026#34;%20s %llu\u0026#34;, tok, periodp) \u0026lt; 1) 6979\treturn -EINVAL; 6980 6981\t*periodp *= NSEC_PER_USEC; 6982 6983\tif (sscanf(tok, \u0026#34;%llu\u0026#34;, quotap)) 6984\t*quotap *= NSEC_PER_USEC; 6985\telse if (!strcmp(tok, \u0026#34;max\u0026#34;)) 6986\t*quotap = RUNTIME_INF; 6987\telse 6988\treturn -EINVAL; 6989 6990\treturn 0; 6991} 6992 6993#ifdef CONFIG_CFS_BANDWIDTH 6994static int cpu_max_show(struct seq_file *sf, void *v) 6995{ 6996\tstruct task_group *tg = css_tg(seq_css(sf)); 6997 6998\tcpu_period_quota_print(sf, tg_get_cfs_period(tg), tg_get_cfs_quota(tg)); 6999\treturn 0; 7000} 7001 7002static ssize_t cpu_max_write(struct kernfs_open_file *of, 7003\tchar *buf, size_t nbytes, loff_t off) 7004{ 7005\tstruct task_group *tg = css_tg(of_css(of)); 7006\tu64 period = tg_get_cfs_period(tg); 7007\tu64 quota; 7008\tint ret; 7009 7010\tret = cpu_period_quota_parse(buf, \u0026amp;period, \u0026amp;quota); 7011\tif (!ret) 7012\tret = tg_set_cfs_bandwidth(tg, period, quota); 7013\treturn ret ?: nbytes; 7014} 7015#endif 7016 7017static struct cftype cpu_files[] = { 7018#ifdef CONFIG_FAIR_GROUP_SCHED 7019\t{ 7020\t.name = \u0026#34;weight\u0026#34;, 7021\t.flags = CFTYPE_NOT_ON_ROOT, 7022\t.read_u64 = cpu_weight_read_u64, 7023\t.write_u64 = cpu_weight_write_u64, 7024\t}, 7025\t{ 7026\t.name = \u0026#34;weight.nice\u0026#34;, 7027\t.flags = CFTYPE_NOT_ON_ROOT, 7028\t.read_s64 = cpu_weight_nice_read_s64, 7029\t.write_s64 = cpu_weight_nice_write_s64, 7030\t}, 7031#endif 7032#ifdef CONFIG_CFS_BANDWIDTH 7033\t{ 7034\t.name = \u0026#34;max\u0026#34;, 7035\t.flags = CFTYPE_NOT_ON_ROOT, 7036\t.seq_show = cpu_max_show, 7037\t.write = cpu_max_write, 7038\t}, 7039#endif 7040\t{ }\t/* terminate */ 7041}; 7042 7043struct cgroup_subsys cpu_cgrp_subsys = { 7044\t.css_alloc\t= cpu_cgroup_css_alloc, 7045\t.css_online\t= cpu_cgroup_css_online, 7046\t.css_released\t= cpu_cgroup_css_released, 7047\t.css_free\t= cpu_cgroup_css_free, 7048\t.css_extra_stat_show = cpu_extra_stat_show, 7049\t.fork\t= cpu_cgroup_fork, 7050\t.can_attach\t= cpu_cgroup_can_attach, 7051\t.attach\t= cpu_cgroup_attach, 7052\t.legacy_cftypes\t= cpu_legacy_files, 7053\t.dfl_cftypes\t= cpu_files, 7054\t.early_init\t= true, 7055\t.threaded\t= true, 7056}; 7057 7058#endif\t/* CONFIG_CGROUP_SCHED */7059 7060void dump_cpu_task(int cpu) 7061{ 7062\tpr_info(\u0026#34;Task dump for CPU %d:\\n\u0026#34;, cpu); 7063\tsched_show_task(cpu_curr(cpu)); 7064} 7065 7066/* 7067* Nice levels are multiplicative, with a gentle 10% change for every 7068* nice level changed. I.e. when a CPU-bound task goes from nice 0 to 7069* nice 1, it will get ~10% less CPU time than another CPU-bound task 7070* that remained on nice 0. 7071* 7072* The \u0026#34;10% effect\u0026#34; is relative and cumulative: from _any_ nice level, 7073* if you go up 1 level, it\u0026#39;s -10% CPU usage, if you go down 1 level 7074* it\u0026#39;s +10% CPU usage. (to achieve that we use a multiplier of 1.25. 7075* If a task goes up by ~10% and another task goes down by ~10% then 7076* the relative distance between them is ~25%.) 7077*/ 7078const int sched_prio_to_weight[40] = { 7079 /* -20 */ 88761, 71755, 56483, 46273, 36291, 7080 /* -15 */ 29154, 23254, 18705, 14949, 11916, 7081 /* -10 */ 9548, 7620, 6100, 4904, 3906, 7082 /* -5 */ 3121, 2501, 1991, 1586, 1277, 7083 /* 0 */ 1024, 820, 655, 526, 423, 7084 /* 5 */ 335, 272, 215, 172, 137, 7085 /* 10 */ 110, 87, 70, 56, 45, 7086 /* 15 */ 36, 29, 23, 18, 15, 7087}; 7088 7089/* 7090* Inverse (2^32/x) values of the sched_prio_to_weight[] array, precalculated. 7091* 7092* In cases where the weight does not change often, we can use the 7093* precalculated inverse to speed up arithmetics by turning divisions 7094* into multiplications: 7095*/ 7096const u32 sched_prio_to_wmult[40] = { 7097 /* -20 */ 48388, 59856, 76040, 92818, 118348, 7098 /* -15 */ 147320, 184698, 229616, 287308, 360437, 7099 /* -10 */ 449829, 563644, 704093, 875809, 1099582, 7100 /* -5 */ 1376151, 1717300, 2157191, 2708050, 3363326, 7101 /* 0 */ 4194304, 5237765, 6557202, 8165337, 10153587, 7102 /* 5 */ 12820798, 15790321, 19976592, 24970740, 31350126, 7103 /* 10 */ 39045157, 49367440, 61356676, 76695844, 95443717, 7104 /* 15 */ 119304647, 148102320, 186737708, 238609294, 286331153, 7105}; 7106 7107#undef CREATE_TRACE_POINTS ","date":"Oct 4, 2021","img":"","permalink":"https://mengdemao.github.io/posts/sched/","series":null,"tags":["kernel"],"title":"系统调度"},{"categories":[],"content":"","date":"Oct 3, 2021","img":"","permalink":"https://mengdemao.github.io/posts/rime/","series":null,"tags":[],"title":"Rime"},{"categories":[],"content":"WASM笔记 ","date":"Oct 3, 2021","img":"","permalink":"https://mengdemao.github.io/posts/wasm/","series":null,"tags":[],"title":"Wasm"},{"categories":[],"content":"jupyter 安装 1conda install jupyter 基本配置 生成配置 1jupyter notebook --generate-config 自动生成配置文件 ~/.jupyter/jupyter_notebook_config.py\n设置密码 1jupyter notebook password  ","date":"Oct 3, 2021","img":"","permalink":"https://mengdemao.github.io/posts/jupyter/","series":null,"tags":[],"title":"Jupyter"},{"categories":[],"content":"","date":"Oct 3, 2021","img":"","permalink":"https://mengdemao.github.io/posts/conda/","series":null,"tags":[],"title":"Conda"},{"categories":[],"content":"RUST学习笔记\n====\n安装  添加环境变量.bashrc/profile  1set RUSTUP_DIST_SERVER=https://mirrors.ustc.edu.cn/rust-static 2set RUSTUP_UPDATE_ROOT=https://mirrors.ustc.edu.cn/rust-static/rustup 安装工具链  1curl https://mirrors.ustc.edu.cn/rust-static/rustup/rustup-init.sh | sh 设置rust的环境变量.bashrc/profile  1source ~/.cargo/env 2set PATH=~/.cargo/bin;$PATH 入门基础 HelloWorld  创建工程  1cargo new hello_world 编译  1cargo build 运行  1cargo run 数据类型    长度 有符号 无符号     8-bit i8 u8   16-bit i16 u16   32-bit i32 u32   64-bit i64 u64   128-bit i128 u128   arch isize usize    1// 创建变量 2let_xi32: i32 =5;3let_xu32: u32 =5;4let_xi64: i64 =10;5let_xu64: u64 =10;6let_xi128: i128=5;7let_xu128: u128=5;8let_xisize: isize =10;9let_xusize: usize =10;函数 1// 有返回值 2fn function_return()-\u0026gt; i32 {3println!(\u0026#34;Hello, World!\u0026#34;);4return0;5}67// 无返回值 8fn function_noreturn(){9println!(\u0026#34;Hello, World!\u0026#34;);10} 必须明确表示是否存在返回值 语法校验比较严格,  条件语句 1if\u0026lt;cond\u0026gt;{2do;3}1if\u0026lt;cond\u0026gt;{2Do1;3}else{4Do2;5}1if\u0026lt;cond1\u0026gt;{2Do1;3}elseif\u0026lt;cond2\u0026gt;{4Do2;5}else{6Do3;7}","date":"Oct 3, 2021","img":"","permalink":"https://mengdemao.github.io/posts/rust/","series":null,"tags":[],"title":"Rust"},{"categories":[],"content":"C++基础语法 第一个程序 1#include \u0026lt;iostream\u0026gt;2 3using namespace std; 4 5int main(int argc, char *argv[]) 6{ 7\tcout \u0026lt;\u0026lt; \u0026#34;Hello World\u0026#34; \u0026lt;\u0026lt; endl; 8\treturn 0; 9} 注释类型  单行注释  1// 这个是一个单行注释  多行注释  1/* 2这个里面是一个注释 3*/ 变量 变量的存在意义:方便我们管理内存\n变量创建的语法\n1数据类型 变量名 = 变量初始化; 常量 作用: 记录程序中不可以改变的数据\n define 宏常量(预编译期) const 修饰变量(编译期)  关键字    关键字        asm else new this   auto enum operator throw   bool explicit private true   break export protected try   case extern public typedef   catch false register typeid   char float reinterpret_cast typename   class for return union   const friend short unsigned   const_cast goto signed using   continue if sizeof virtual   default inline static void   delete int static_cast volatile   do long struct wchar_t   double mutable switch while   dynamic_cast namespace template     标识符命名规则  标识符不可以是关键字 只能由字母、数字、下划线构成 第一个字母只能是字母或者是下划线 区分大小写  数据类型 指定类型,分配内存\n整形 浮点型  单精度float 双精度double  字符型 转义字符 字符串  C风格  1char 变量名[] = \u0026#34;字符串值\u0026#34;; C++风格  1string 变量名 = \u0026#34;字符串值\u0026#34;; 布尔类型 1bool A = true; 2bool B = false; 运算符 基本运算符 取模运算 就是取余数\n自增自减运算 1a1++; 2a2--; 赋值运算    运算符 术语 示例 结果     =      +=      -=      *=      /=      %=       比较运算符 逻辑运算符 程序流程结构 顺序结构 if语句 三目运算符 1表达式1? 表达式2:表达式3 选择结构 1switch(condition) 2{ 3case 条件1: 4\tbreak; 5case 条件2: 6\tbreak; 7default: 8\tbreak; 9} 循环结构 while循环 1while(条件) 2{ 3\t循环体; 4} dowhile循环 1do { 2 3} while(条件) for循环 1for (起始表达式; 条件表达式; 末尾循环体) 2{ 3\t循环体; 4} 跳转语句 break continue goto 数组 函数定义  返回值类型 函数名 参数列表 函数体语句 return表达式  1返回值类型 函数名字(参数列表) 2{ 3\t函数体语句; 4\treturn 表达式; 5} 值传递 类似数值拷贝\n函数的常见样式  无参无返 有参无返 无参有反 有参有返  函数的声明 作用: 告诉编译器函数名以及调用方式,函数实体可以单独实现;\n函数的分文件编写 指针 指针的定义和使用 指针所占用空间 空指针 含义: 指针指向内存空间为0的指针; 用途: 初始化指针变量 注意: 空指针的地址是不可以访问的\n野指针 指针指向非法的内存空间\nconst与指针  const修饰指针 const修饰常量 const既修饰指针又修饰常量  1const int *p = \u0026amp;a; 2 3int const *p = \u0026amp;a; 4 5const int *const p = \u0026amp;a; 指针与数组 指针与函数 结构体 结构体数组 结构体指针 结构体嵌套 C++核心编程 本阶段主要对面向对象进行详细讲解\nC++内存分区 c++程序在运行时,将内存分为4个区域\n 代码区: 存放程序的二进制代码,由操作系统管理 全局区: 存放全局变量、静态变量和常量 栈区: 编译器自动分配 堆区: 程序负责分配和释放  new/delete操作符 C++利用new操作符在堆区开辟内存\n引用 作用: 给变量起别名 语法: 数据类型 \u0026amp;别名 = 原名;\n引用做参数 1#include \u0026lt;iostream\u0026gt;2void swap(int \u0026amp;a, int \u0026amp;b) 3{ 4 int t; t = a;a = b;b = t; 5} 6int main(int argc, char *argv[]) 7{ 8 int a = 10;int b = 12; 9 std::cout \u0026lt;\u0026lt; \u0026#34;交换前\u0026#34; \u0026lt;\u0026lt; a \u0026lt;\u0026lt; \u0026#39;\\t\u0026#39; \u0026lt;\u0026lt; b \u0026lt;\u0026lt; std::endl; 10 swap(a, b); 11 std::cout \u0026lt;\u0026lt; \u0026#34;交换后\u0026#34; \u0026lt;\u0026lt; a \u0026lt;\u0026lt; \u0026#39;\\t\u0026#39; \u0026lt;\u0026lt; b \u0026lt;\u0026lt; std::endl; 12 return 0; 13} 执行结果\n引用做返回值 引用的本质 引用的本质是C++内部实现的一个指针常量\n常量引用 1const int \u0026amp;ref = 10; 函数提高 函数默认值  某个位置有默认值，那么后面的参数也必须由默认值 如果声明了默认值，那么实现不可以有默认值(默认参数会产生冲突)  1void test_default_param(int a = 0, int b = 0, int c = 0) 2{ 3 std::cout \u0026lt;\u0026lt; a + b + c \u0026lt;\u0026lt; std::endl; 4} 函数的占位参数 占位参数还可以有默认值\n1void test(int a, int = 10) { 2 std::cout \u0026lt;\u0026lt; a \u0026lt;\u0026lt; std::endl; 3} 函数重载 作用:函数名相同,提高复用性\n重载的条件:\n  相同作用域\n  函数名相同\n  参数不同(类型, 个数,顺序)\n  注意事项:\n 引用作为重载条件 函数重载碰到默认参数  类和对象 类的访问属性\n public: protected: private:  class与struct的区别 class默认权限是private struct默认权限是public\n构造函数和析构函数 对象的初始化和清理\n 构造函数有参数 析构函数没有参数 二者都没有返回值  拷贝构造函数 1class Person { 2public: 3 /* 构造函数 */ 4 Person(std::string name, int age) { 5 std::cout \u0026lt;\u0026lt; \u0026#34;构造函数\u0026#34; \u0026lt;\u0026lt; std::endl; 6 } 7 /* 析构函数 */ 8 ~Person() { 9 std::cout \u0026lt;\u0026lt; \u0026#34;析构函数\u0026#34; \u0026lt;\u0026lt; std::endl; 10 } 11 /* 拷贝构造函数 */ 12 Person(const Person \u0026amp;p) { 13 std::cout \u0026lt;\u0026lt; \u0026#34;拷贝构造函数\u0026#34; \u0026lt;\u0026lt; std::endl; 14 } 15};  调用无参构造函数的时候不可以添加();否则就会产生函数声明的效果  1Person testPerson();\t// 表面上是执行构造函数 2int func();\t// 类似函数声明 拷贝构造函数的调用时机  使用一个已经创建完毕的对象初始化一个新对象 值传递的方式给函数进行参数传递 以值的方式返回局部对象  构造函数的调用规则 默认情况下:C++编译器至少给一个类添加3个函数\n 默认构造函数(无参) 默认析构函数(无参) 默认拷贝函数,对属性值进行拷贝  构造函数构造规则如下:\n 用户定义有参构造,C++默认不提供无参构造，但是提供默认拷贝构造 用户定义拷贝构造,C++不提供其他构造函数  深拷贝和浅拷贝  浅拷贝: 简单的复制操作 深拷贝: 在堆区重新申请空间，进行复制操作  初始化列表 作用:C++提供了初始化列表语法,用来初始化属性;\n语法:\n1构造函数(): 属性1(值1),属性2(值2),属性3(值3) 2{ 3\t/* 函数体 */ 4} 类对象作为类成员 静态成员 静态成员就是在静态成员变量和成员函数前加上static,称为静态成员;\n 静态成员变量  所有对象共享一份数据 编译阶段分配内存 类内声明,类外初始化   静态成员函数  所有对象共享同一个函数 静态成员函数只能访问静态成员变量    1class Person { 2public: 3 static int age; 4 static void func() 5 { 6 std::cout \u0026lt;\u0026lt; \u0026#34;静态成员函数\u0026#34; \u0026lt;\u0026lt; std::endl; 7 } 8}; 9/* 通过对象访问 */ 10Person p; 11p.func(); 12/* 通过类访问 */ 13Person::func(); 成员变量和成员函数分开存储  非静态成员,\t属于类的对象 静态成员,\t不属于类的对象 非静态成员函数,\t不属于类的对象 静态成员函数, 不属于类的对象  空对象大小为1\nC++对象模型 this指针 this指针指向被调用成员函数所属的对象 this指针本质：指针常量\n空指针访问成员函数 C++空指针也是可以访问成员函数的,但是要注意的this指针;\nconst修饰成员函数 常函数:\n 常函数不可以修改成员属性 成员属性加上mutable,常函数也可以修改 ** 常对象** 对象之前加const表示常对象 常对象只能调用函数  执行原理\n1this ==\u0026gt; Person * const this; 2后面新追加的const则会造成 3const Person * const this; 1class Person { 2public: 3 int m_A; 4 mutable int m_B; 5 void showPerson() const 6 { 7 m_A = 10; /* 错误,不可修改 */ 8 m_B = 10; /* 正确,可以修改 */ 9 } 10}; 友元  全局函数 全局类 成员函数  运算符重载 重载的原理:对已有的运算符进行重新定义,赋予新的功能含义;\n通过成员函数重载运算符 1class Person { 2public: 3 int m_A; 4 int m_B; 5 6 /* 使用成员函数实现 */ 7 Person PersonAddPerson(Person \u0026amp;p) 8 { 9 Person t; 10 t.m_A = this-\u0026gt;m_A + p.m_A; 11 t.m_B = this-\u0026gt;m_B + p.m_B; 12 return t; 13 } 14 15 /* 重载+ */ 16 Person operator+(Person \u0026amp;p) 17 { 18 Person t; 19 t.m_A = this-\u0026gt;m_A + p.m_A; 20 t.m_B = this-\u0026gt;m_B + p.m_B; 21 return t; 22 } 23}; 24 25int main(int argc, char *argv[]) 26{ 27 Person p1; 28 Person p2; 29 30 Person p3 = p1.PersonAddPerson(p2); 31 32 Person p4 = p1.operator+(p2); 33 34 Person p5 = p1 + p2; 35 36 return 0; 37} 通过全局函数重载 1Person operator+(Person \u0026amp;p1, Person \u0026amp;p2) 2{ 3 Person t; 4 t.m_A = p1.m_A + p2.m_A; 5 t.m_B = p2.m_B + p2.m_B; 6 return t; 7} 重载左移运算符 1std::ostream \u0026amp;operator\u0026lt;\u0026lt;(std::ostream \u0026amp;cout, Person \u0026amp;p) 2{ 3 cout \u0026lt;\u0026lt; p.m_A \u0026lt;\u0026lt; p.m_B; 4 return cout; 5} 递增重载++ 注意:\n 前置递增 p++ 后置递增 ++p  重载例子(复数) 1#include \u0026lt;iostream\u0026gt;2 3class Complex { 4 friend std::ostream \u0026amp;operator\u0026lt;\u0026lt;(std::ostream \u0026amp;cout, Complex p); 5 6public: 7 Complex(int i, int j); 8 9 Complex(); 10 11 /* 重载+ */ 12 Complex operator+(Complex \u0026amp;p) 13 { 14 Complex t; 15 t.i = this-\u0026gt;i + p.i; 16 t.j = this-\u0026gt;j + p.j; 17 return t; 18 } 19 /* 重载前置++ */ 20 Complex\u0026amp; operator++() 21 { 22 this-\u0026gt;i++; 23 this-\u0026gt;j++; 24 return *this; 25 } 26 27 /* 重载后置++ */ 28 Complex operator++(int) 29 { 30 Complex t; 31 32 /* 记录 */ 33 t.i = this-\u0026gt;i; 34 t.j = this-\u0026gt;j; 35 36 /* 递增 */ 37 this-\u0026gt;i++; 38 this-\u0026gt;j++; 39 40 return t; 41 } 42 43 /* 重载= */ 44 Complex\u0026amp; operator=(Complex \u0026amp;p) 45 { 46 this-\u0026gt;i = p.i; 47 this-\u0026gt;j = p.j; 48 49 return *this; 50 } 51private: 52 int i; /* 实部 */ 53 int j; /* 虚部 */ 54}; 55 56/* 构造函数 */ 57Complex::Complex(int i, int j) 58{ 59 this-\u0026gt;i = i; 60 this-\u0026gt;j = j; 61} 62 63Complex::Complex() 64{ 65 this-\u0026gt;i = 0; 66 this-\u0026gt;j = 0; 67} 68 69std::ostream \u0026amp;operator\u0026lt;\u0026lt;(std::ostream \u0026amp;cout, Complex p) 70{ 71 cout \u0026lt;\u0026lt; p.i \u0026lt;\u0026lt; \u0026#34;+\u0026#34; \u0026lt;\u0026lt; p.j \u0026lt;\u0026lt; \u0026#34;i\u0026#34;; 72 return cout; 73} 74 75int main(int argc, char *argv[]) 76{ 77 Complex p1(1, 2); 78 Complex p2(3, 4); 79 80 std::cout \u0026lt;\u0026lt; p1 \u0026lt;\u0026lt; std::endl; 81 std::cout \u0026lt;\u0026lt; p2 \u0026lt;\u0026lt; std::endl; 82 std::cout \u0026lt;\u0026lt; p1 + p2 \u0026lt;\u0026lt; std::endl; 83 84 std::cout \u0026lt;\u0026lt; ++p1 \u0026lt;\u0026lt; std::endl; 85 std::cout \u0026lt;\u0026lt; p2++ \u0026lt;\u0026lt; std::endl; 86 87 Complex p3 = p2 = p1; 88 std::cout \u0026lt;\u0026lt; p1 \u0026lt;\u0026lt; \u0026#34; \u0026#34; \u0026lt;\u0026lt; p2 \u0026lt;\u0026lt; \u0026#34; \u0026#34; \u0026lt;\u0026lt; p3 \u0026lt;\u0026lt; std::endl; 89 90 return 0; 91} 继承 减少重复代码\n1class 子类 : 继承方式 父类 父类:基类 子类:派生类\n继承方式  公共继承 保护继承 私有继承  继承中的对象模型 构造和析构的顺序  先构造父类再构造子类 先析构子类再析构父类\n 继承中同名成员处理  访问子类中同名成员,直接访问即可 s.m_A 访问父类中同名成员,需要加上作用域 s.Base:m_A  多重继承 C++允许一个类继承多个基类\n1class 子类 : 继承方式 父类1, 继承方式 父类2...  冲突解决：加上类名\n 菱形继承 孙子类继承了子类1和子类2,但是继承了两次父类。\n 多重继承数据会产生二义性 数据只需要一份即可  1/* 动物类 */ 2class Animal { 3public: 4 int m_age; 5}; 6class Sheep : public Animal {}; /* 羊类 */ 7class Camel : public Animal {}; /* 驼类 */ 8class Alpaca : public Sheep, public Camel {}; /* 羊驼 */ 9int main(int argc, char *argv[]) 10{ 11 Alpaca a; 12 a.Sheep::m_age = 18; 13 a.Camel::m_age = 18; 14 return 0; 15}  虚继承\n 1class Sheep : virtual public Animal {}; /* 羊类 */ 2class Camel : virtual public Animal {}; /* 驼类 */ 虚基类指针(vbptr) vbptr \u0026ndash;\u0026gt; vbtable\n多态  分类  静态多态: 重载 动态多态:虚函数   区别  静态多态函数地址早绑定:编译期确定函数地址 动态多态函数地址晚绑定:运行期确定函数地址    父类接收子类的对象,在程序运行期间确定具体改调用那个函数;\n  有继承关系\n  子类重写父类的虚函数 重写：函数完全一致\n  纯虚函数  只要有一个纯虚函数，就称为抽象类\n  抽象类无法直接实例化对象 抽象子类必须重写父类的纯虚函数,否则也是抽象类  原理 虚析构和纯虚析构 C++提高编程  泛型编程/STL\n 模版 建立通用的模板,提高复用；\nC++提供两种模版机制:函数模版和类模板\n函数模版 1template \u0026lt;typename T\u0026gt; 2函数声明和定义  template \u0026ndash; 声明创建模版 typename \u0026ndash; 表明后面的符号是数据类型可以用class代替 T \u0026ndash; 通用的数据类型  实例 1/* 两个数据交换 */ 2template \u0026lt;typename T\u0026gt; 3void swap(T \u0026amp;a, T \u0026amp;b) 4{ 5 T t= a; a = b; b = t; 6} 注意事项  自动类型推导,必须导出类型一致的T才可以使用 模版必须要确定T的数据类型,才可以使用  普通函数和函数模版的区别  普通函数可以发生隐式类型转换 函数模板: 用自动类型推导，不可以发生隐式转换 函数模板: 用显示类型推导，可以发生隐式转换  模版函数\n1template \u0026lt;typename T\u0026gt; 2T add(T a, T b) 3{ 4 return a + b; 5} 调用方法\n1/* 自动推导 */ 2std::cout \u0026lt;\u0026lt; add(10, 20) \u0026lt;\u0026lt; std::endl; 3/* 显示指定 */ 4std::cout \u0026lt;\u0026lt; add\u0026lt;int\u0026gt;(10, 3.14) \u0026lt;\u0026lt; std::endl; 普通函数和模版函数调用规则  普通函数和模版函数都可以调用,有限调用普通函数 强制通过空模版参数强制调用函数模版:函数名\u0026lt;\u0026gt;(参数列表) 函数模版也可以重载 函数模版更好的匹配,选择函数模版  类模板 1template \u0026lt;class T\u0026gt; 2类 例子\n1template \u0026lt;class NameType, class AgeType\u0026gt; 2class Person { 3public: 4 Person(NameType Name, AgeType Age) 5 { 6 m_Name = Name; 7 m_Age = Age; 8 } 9 NameType m_Name; 10 AgeType m_Age; 11}; 调用\n1Person\u0026lt;std::string, int\u0026gt; p(\u0026#34;Hello\u0026#34;, 99); ","date":"Oct 3, 2021","img":"","permalink":"https://mengdemao.github.io/posts/c++/","series":null,"tags":[],"title":"C++笔记"},{"categories":[],"content":"doxygen教程 开始 ","date":"Oct 3, 2021","img":"","permalink":"https://mengdemao.github.io/posts/doxygen/","series":null,"tags":[],"title":"Doxygen"},{"categories":[],"content":"","date":"Oct 3, 2021","img":"","permalink":"https://mengdemao.github.io/posts/engineering_compiler/","series":null,"tags":["compiler"],"title":"Engineering_compiler"},{"categories":[],"content":"简述  驱动模型\n    顶级kobject 解释     block 块设备链接\u0026ndash;\u0026gt;/sys/deives相关文件   bus 存放各种总线文件   class 各种设备类   dev 存放(字符/块)设备主副设备号链接文件\u0026ndash;\u0026gt;/sys/deives   devices 设备的具体存放文件   firmware 固件存放   fs 文件类型   kernel kernel子系统   module 模块信息   power 能源管理    底层机制 kobject  内核对象:kobject/kobject_type/kset 为模块提供一个底层抽象,其中文件存放于/sys文件下面\n 数据结构 1struct kobject { 2\tconst char\t*name;\t/* 名字 */ 3\tstruct list_head\tentry;\t/* 链表:链接进入kset */ 4\tstruct kobject\t*parent;\t/* 指向父对象,建立层次结构 */ 5\tstruct kset\t*kset;\t/* 对象集合 */ 6\tstruct kobj_type\t*ktype;\t/* 对象类型 */ 7\tstruct kernfs_node\t*sd; /* sysfs directory entry */ 8\tstruct kref\tkref;\t/* 引用计数 */ 9 10 #ifdef CONFIG_DEBUG_KOBJECT_RELEASE 11\tstruct delayed_work\trelease; 12\t#endif 13\t14 unsigned int state_initialized:1;\t/* 标志位:初始化 */ 15\tunsigned int state_in_sysfs:1;\t/* 标志位:在sysfs中 */ 16\tunsigned int state_add_uevent_sent:1;\t/* 标志位:发出KOBJ_ADD uevent */ 17\tunsigned int state_remove_uevent_sent:1;\t/* 标志位:发出KOBJ_REMOVE uevent */ 18\tunsigned int uevent_suppress:1;\t/* 标志位:禁止发出uevent */ 19}; 初始化 1/** 2* kobject_init - initialize a kobject structure 3* @kobj: pointer to the kobject to initialize 4* @ktype: pointer to the ktype for this kobject. 5* 6* This function will properly initialize a kobject such that it can then 7* be passed to the kobject_add() call. 8* 9* After this function is called, the kobject MUST be cleaned up by a call 10* to kobject_put(), not by a call to kfree directly to ensure that all of 11* the memory is cleaned up properly. 12*/ 13void kobject_init(struct kobject *kobj, struct kobj_type *ktype) 14{ 15\tchar *err_str;\t/** 错误信息 */ 16 17\t/** 校验参数NULL */ 18\tif (!kobj) { 19\terr_str = \u0026#34;invalid kobject pointer!\u0026#34;; 20\tgoto error; 21\t} 22\tif (!ktype) { 23\terr_str = \u0026#34;must have a ktype to be initialized properly!\\n\u0026#34;; 24\tgoto error; 25\t} 26 27\t/** kobject是否已经初始化 */ 28\tif (kobj-\u0026gt;state_initialized) { 29\t/* do not error out as sometimes we can recover */ 30\tpr_err(\u0026#34;kobject (%p): tried to init an initialized object, something is seriously wrong.\\n\u0026#34;, 31\tkobj); 32\tdump_stack(); /** 回溯堆栈 */ 33\t} 34 35\t/** 调用具体初始化函数 */ 36\tkobject_init_internal(kobj); 37 38\t/* 设置类型 */ 39\tkobj-\u0026gt;ktype = ktype; 40\treturn; 41 42error: 43\tpr_err(\u0026#34;kobject (%p): %s\\n\u0026#34;, kobj, err_str); 44\tdump_stack(); 45} 46EXPORT_SYMBOL(kobject_init); 添加 1int kobject_add(struct kobject *kobj, /* 需要添加kobject */ 2\tstruct kobject *parent, /* 父指针 */ 3\tconst char *fmt, ...) /* 命名 */ 4{ 5\tva_list args; 6\tint retval; 7\t8\t/* 校验kobject */ 9\tif (!kobj) 10\treturn -EINVAL; 11\t12\t/* 是否已经初始化 */ 13\tif (!kobj-\u0026gt;state_initialized) { 14\tpr_err(\u0026#34;kobject \u0026#39;%s\u0026#39; (%p): tried to add an uninitialized object, something is seriously wrong.\\n\u0026#34;, 15\tkobject_name(kobj), kobj); 16\tdump_stack(); 17\treturn -EINVAL; 18\t} 19\tva_start(args, fmt); 20\t/* 设置名字并且将父指针添加到parent */ 21\tretval = kobject_add_varg(kobj, parent, fmt, args); 22\tva_end(args); 23\t24\treturn retval; 25} 最终调用添加函数\n1static int kobject_add_internal(struct kobject *kobj) 2{ 3\tint error = 0; 4\tstruct kobject *parent; 5\t6 /* 判断参数NULL */ 7\tif (!kobj) 8\treturn -ENOENT; 9\t10 /* 判断名字是否有效 */ 11\tif (!kobj-\u0026gt;name || !kobj-\u0026gt;name[0]) { 12\tWARN(1, 13\t\u0026#34;kobject: (%p): attempted to be registered with empty name!\\n\u0026#34;, 14\tkobj); 15\treturn -EINVAL; 16\t} 17\t18 /** 获取父指针 */ 19\tparent = kobject_get(kobj-\u0026gt;parent); 20 21\t/* join kset if set, use it as parent if we do not already have one */ 22\tif (kobj-\u0026gt;kset) {\t/* kset已经设置 */ 23\tif (!parent)\t/* 不存在父指针 */ 24\t/* kset的kobject作为父指针 */ 25 parent = kobject_get(\u0026amp;kobj-\u0026gt;kset-\u0026gt;kobj); 26\t/* 将kobject加入kset */ 27 kobj_kset_join(kobj); 28\t29 /* 保存父指针 */ 30 kobj-\u0026gt;parent = parent; 31\t} 32 33\tpr_debug(\u0026#34;kobject: \u0026#39;%s\u0026#39; (%p): %s: parent: \u0026#39;%s\u0026#39;, set: \u0026#39;%s\u0026#39;\\n\u0026#34;, 34\tkobject_name(kobj), kobj, __func__, 35\tparent ? kobject_name(parent) : \u0026#34;\u0026lt;NULL\u0026gt;\u0026#34;, 36\tkobj-\u0026gt;kset ? kobject_name(\u0026amp;kobj-\u0026gt;kset-\u0026gt;kobj) : \u0026#34;\u0026lt;NULL\u0026gt;\u0026#34;); 37\t38 /* 创建dir */ 39\terror = create_dir(kobj); 40\tif (error) { /* 出错,清理 */ 41\tkobj_kset_leave(kobj); 42\tkobject_put(parent); 43\tkobj-\u0026gt;parent = NULL; 44 45\t/* be noisy on error issues */ 46\tif (error == -EEXIST) 47\tpr_err(\u0026#34;%s failed for %s with -EEXIST, don\u0026#39;t try to register things with the same name in the same directory.\\n\u0026#34;, 48\t__func__, kobject_name(kobj)); 49\telse 50\tpr_err(\u0026#34;%s failed for %s (error: %d parent: %s)\\n\u0026#34;, 51\t__func__, kobject_name(kobj), error, 52\tparent ? kobject_name(parent) : \u0026#34;\u0026#39;none\u0026#39;\u0026#34;); 53\t} else 54\tkobj-\u0026gt;state_in_sysfs = 1; /* 添加到sysfs中 */ 55 56\treturn error; 57} sysfs文件夹生成 1static int create_dir(struct kobject *kobj) 2{ 3\tconst struct kobj_ns_type_operations *ops; 4\tint error; 5 6\terror = sysfs_create_dir_ns(kobj, kobject_namespace(kobj)); 7\tif (error) 8\treturn error; 9 10\terror = populate_dir(kobj); 11\tif (error) { 12\tsysfs_remove_dir(kobj); 13\treturn error; 14\t} 15 16\t/* 17* @kobj-\u0026gt;sd may be deleted by an ancestor going away. Hold an 18* extra reference so that it stays until @kobj is gone. 19*/ 20\tsysfs_get(kobj-\u0026gt;sd); 21 22\t/* 23* If @kobj has ns_ops, its children need to be filtered based on 24* their namespace tags. Enable namespace support on @kobj-\u0026gt;sd. 25*/ 26\tops = kobj_child_ns_ops(kobj); 27\tif (ops) { 28\tBUG_ON(ops-\u0026gt;type \u0026lt;= KOBJ_NS_TYPE_NONE); 29\tBUG_ON(ops-\u0026gt;type \u0026gt;= KOBJ_NS_TYPES); 30\tBUG_ON(!kobj_ns_type_registered(ops-\u0026gt;type)); 31 32\tsysfs_enable_ns(kobj-\u0026gt;sd); 33\t} 34 35\treturn 0; 36} 删除 1void kobject_del(struct kobject *kobj) 2{ 3\tstruct kernfs_node *sd; 4 5\tif (!kobj) 6\treturn; 7 8\tsd = kobj-\u0026gt;sd; 9\tsysfs_remove_dir(kobj); 10\tsysfs_put(sd); 11 12\tkobj-\u0026gt;state_in_sysfs = 0; 13\tkobj_kset_leave(kobj); 14\tkobject_put(kobj-\u0026gt;parent); 15\tkobj-\u0026gt;parent = NULL; 16} 引用计数 1struct kobject *kobject_get(struct kobject *kobj) 2{ 3\tif (kobj) { 4\tif (!kobj-\u0026gt;state_initialized) 5\tWARN(1, KERN_WARNING 6\t\u0026#34;kobject: \u0026#39;%s\u0026#39; (%p): is not initialized, yet kobject_get() is being called.\\n\u0026#34;, 7\tkobject_name(kobj), kobj); 8\tkref_get(\u0026amp;kobj-\u0026gt;kref); 9\t} 10\treturn kobj; 11} 12 13void kobject_put(struct kobject *kobj) 14{ 15\tif (kobj) { 16\tif (!kobj-\u0026gt;state_initialized) 17\tWARN(1, KERN_WARNING 18\t\u0026#34;kobject: \u0026#39;%s\u0026#39; (%p): is not initialized, yet kobject_put() is being called.\\n\u0026#34;, 19\tkobject_name(kobj), kobj); 20\tkref_put(\u0026amp;kobj-\u0026gt;kref, kobject_release); 21\t} 22} kset 数据结构 1struct kset { 2\tstruct list_head list; 3\tspinlock_t list_lock; 4\tstruct kobject kobj; 5\tconst struct kset_uevent_ops *uevent_ops; 6} __randomize_layout; ktype 数据结构 1struct kobj_type { 2\tvoid (*release)(struct kobject *kobj); 3\tconst struct sysfs_ops *sysfs_ops; 4\tstruct attribute **default_attrs; 5\tconst struct kobj_ns_type_operations *(*child_ns_type)(struct kobject *kobj); 6\tconst void *(*namespace)(struct kobject *kobj); 7\tvoid (*get_ownership)(struct kobject *kobj, kuid_t *uid, kgid_t *gid); 8}; class  设备类描述\n 1struct class { 2\tconst char\t*name; 3\tstruct module\t*owner; 4 5\tconst struct attribute_group\t**class_groups; 6\tconst struct attribute_group\t**dev_groups; 7\tstruct kobject\t*dev_kobj; 8 9\tint (*dev_uevent)(struct device *dev, struct kobj_uevent_env *env); 10\tchar *(*devnode)(struct device *dev, umode_t *mode); 11 12\tvoid (*class_release)(struct class *class); 13\tvoid (*dev_release)(struct device *dev); 14 15\tint (*shutdown_pre)(struct device *dev); 16 17\tconst struct kobj_ns_type_operations *ns_type; 18\tconst void *(*namespace)(struct device *dev); 19 20\tvoid (*get_ownership)(struct device *dev, kuid_t *uid, kgid_t *gid); 21 22\tconst struct dev_pm_ops *pm; 23 24\tstruct subsys_private *p; 25}; bus  设备总线描述\n 总线类型 1# ls 2amba cpu nvmem platform virtio 3clockevents event_source pci scsi workqueue 4clocksource gpio pci_express serio 5container hid pcmcia spi 其中每一个总线具有如下信息\n1# ls 2devices drivers_autoprobe uevent 3drivers drivers_probe 1struct bus_type { 2\tconst char\t*name; 3\tconst char\t*dev_name; 4\tstruct device\t*dev_root; 5\tconst struct attribute_group **bus_groups; 6\tconst struct attribute_group **dev_groups; 7\tconst struct attribute_group **drv_groups; 8 9\tint (*match)(struct device *dev, struct device_driver *drv); 10\tint (*uevent)(struct device *dev, struct kobj_uevent_env *env); 11\tint (*probe)(struct device *dev); 12\tint (*remove)(struct device *dev); 13\tvoid (*shutdown)(struct device *dev); 14 15\tint (*online)(struct device *dev); 16\tint (*offline)(struct device *dev); 17 18\tint (*suspend)(struct device *dev, pm_message_t state); 19\tint (*resume)(struct device *dev); 20 21\tint (*num_vf)(struct device *dev); 22 23\tint (*dma_configure)(struct device *dev); 24 25\tconst struct dev_pm_ops *pm; 26 27\tconst struct iommu_ops *iommu_ops; 28 29\tstruct subsys_private *p; 30\tstruct lock_class_key lock_key; 31 32\tbool need_parent_lock; 33}; 设备总线注册 1int bus_register(struct bus_type *bus) 2{ 3\tint retval; 4\tstruct subsys_private *priv; 5\tstruct lock_class_key *key = \u0026amp;bus-\u0026gt;lock_key; 6 7\tpriv = kzalloc(sizeof(struct subsys_private), GFP_KERNEL); 8\tif (!priv) 9\treturn -ENOMEM; 10 11\tpriv-\u0026gt;bus = bus; 12\tbus-\u0026gt;p = priv; 13 14\tBLOCKING_INIT_NOTIFIER_HEAD(\u0026amp;priv-\u0026gt;bus_notifier); 15 16\tretval = kobject_set_name(\u0026amp;priv-\u0026gt;subsys.kobj, \u0026#34;%s\u0026#34;, bus-\u0026gt;name); 17\tif (retval) 18\tgoto out; 19 20\tpriv-\u0026gt;subsys.kobj.kset = bus_kset; 21\tpriv-\u0026gt;subsys.kobj.ktype = \u0026amp;bus_ktype; 22\tpriv-\u0026gt;drivers_autoprobe = 1; 23 24\tretval = kset_register(\u0026amp;priv-\u0026gt;subsys); 25\tif (retval) 26\tgoto out; 27 28\tretval = bus_create_file(bus, \u0026amp;bus_attr_uevent); 29\tif (retval) 30\tgoto bus_uevent_fail; 31 32\tpriv-\u0026gt;devices_kset = kset_create_and_add(\u0026#34;devices\u0026#34;, NULL, 33\t\u0026amp;priv-\u0026gt;subsys.kobj); 34\tif (!priv-\u0026gt;devices_kset) { 35\tretval = -ENOMEM; 36\tgoto bus_devices_fail; 37\t} 38 39\tpriv-\u0026gt;drivers_kset = kset_create_and_add(\u0026#34;drivers\u0026#34;, NULL, 40\t\u0026amp;priv-\u0026gt;subsys.kobj); 41\tif (!priv-\u0026gt;drivers_kset) { 42\tretval = -ENOMEM; 43\tgoto bus_drivers_fail; 44\t} 45 46\tINIT_LIST_HEAD(\u0026amp;priv-\u0026gt;interfaces); 47\t__mutex_init(\u0026amp;priv-\u0026gt;mutex, \u0026#34;subsys mutex\u0026#34;, key); 48\tklist_init(\u0026amp;priv-\u0026gt;klist_devices, klist_devices_get, klist_devices_put); 49\tklist_init(\u0026amp;priv-\u0026gt;klist_drivers, NULL, NULL); 50 51\tretval = add_probe_files(bus); 52\tif (retval) 53\tgoto bus_probe_files_fail; 54 55\tretval = bus_add_groups(bus, bus-\u0026gt;bus_groups); 56\tif (retval) 57\tgoto bus_groups_fail; 58 59\tpr_debug(\u0026#34;bus: \u0026#39;%s\u0026#39;: registered\\n\u0026#34;, bus-\u0026gt;name); 60\treturn 0; 61 62bus_groups_fail: 63\tremove_probe_files(bus); 64bus_probe_files_fail: 65\tkset_unregister(bus-\u0026gt;p-\u0026gt;drivers_kset); 66bus_drivers_fail: 67\tkset_unregister(bus-\u0026gt;p-\u0026gt;devices_kset); 68bus_devices_fail: 69\tbus_remove_file(bus, \u0026amp;bus_attr_uevent); 70bus_uevent_fail: 71\tkset_unregister(\u0026amp;bus-\u0026gt;p-\u0026gt;subsys); 72out: 73\tkfree(bus-\u0026gt;p); 74\tbus-\u0026gt;p = NULL; 75\treturn retval; 76} 77 设备总线卸载 1void bus_unregister(struct bus_type *bus) 2{ 3\tpr_debug(\u0026#34;bus: \u0026#39;%s\u0026#39;: unregistering\\n\u0026#34;, bus-\u0026gt;name); 4\tif (bus-\u0026gt;dev_root) 5\tdevice_unregister(bus-\u0026gt;dev_root); 6\tbus_remove_groups(bus, bus-\u0026gt;bus_groups); 7\tremove_probe_files(bus); 8\tkset_unregister(bus-\u0026gt;p-\u0026gt;drivers_kset); 9\tkset_unregister(bus-\u0026gt;p-\u0026gt;devices_kset); 10\tbus_remove_file(bus, \u0026amp;bus_attr_uevent); 11\tkset_unregister(\u0026amp;bus-\u0026gt;p-\u0026gt;subsys); 12} devices  设备文件具体描述: device(设备描述) device_driver(驱动描述) bus_type(总线信息)\n 1|-- breakpoint 2|-- kprobe 3|-- platform 4|-- software 5|-- system 6|-- tracepoint 7|-- uprobe 8`-- virtual  device  描述设备\n 1struct device { 2\tstruct device\t*parent; 3 4\tstruct device_private\t*p; 5 6\tstruct kobject kobj; 7\tconst char\t*init_name; /* initial name of the device */ 8\tconst struct device_type *type; 9 10\tstruct mutex\tmutex;\t/* mutex to synchronize calls to 11* its driver. 12*/ 13 14\tstruct bus_type\t*bus;\t/* type of bus device is on */ 15\tstruct device_driver *driver;\t/* which driver has allocated this 16device */ 17\tvoid\t*platform_data;\t/* Platform specific data, device 18core doesn\u0026#39;t touch it */ 19\tvoid\t*driver_data;\t/* Driver data, set and get with 20dev_set/get_drvdata */ 21\tstruct dev_links_info\tlinks; 22\tstruct dev_pm_info\tpower; 23\tstruct dev_pm_domain\t*pm_domain; 24 25#ifdef CONFIG_GENERIC_MSI_IRQ_DOMAIN 26\tstruct irq_domain\t*msi_domain; 27#endif 28#ifdef CONFIG_PINCTRL 29\tstruct dev_pin_info\t*pins; 30#endif 31#ifdef CONFIG_GENERIC_MSI_IRQ 32\tstruct list_head\tmsi_list; 33#endif 34 35#ifdef CONFIG_NUMA 36\tint\tnuma_node;\t/* NUMA node this device is close to */ 37#endif 38\tconst struct dma_map_ops *dma_ops; 39\tu64\t*dma_mask;\t/* dma mask (if dma\u0026#39;able device) */ 40\tu64\tcoherent_dma_mask;/* Like dma_mask, but for 41alloc_coherent mappings as 42not all hardware supports 4364 bit addresses for consistent 44allocations such descriptors. */ 45\tu64\tbus_dma_mask;\t/* upstream dma_mask constraint */ 46\tunsigned long\tdma_pfn_offset; 47 48\tstruct device_dma_parameters *dma_parms; 49 50\tstruct list_head\tdma_pools;\t/* dma pools (if dma\u0026#39;ble) */ 51 52\tstruct dma_coherent_mem\t*dma_mem; /* internal for coherent mem 53override */ 54#ifdef CONFIG_DMA_CMA 55\tstruct cma *cma_area;\t/* contiguous memory area for dma 56allocations */ 57#endif 58\t/* arch specific additions */ 59\tstruct dev_archdata\tarchdata; 60 61\tstruct device_node\t*of_node; /* associated device tree node */ 62\tstruct fwnode_handle\t*fwnode; /* firmware device node */ 63 64\tdev_t\tdevt;\t/* dev_t, creates the sysfs \u0026#34;dev\u0026#34; */ 65\tu32\tid;\t/* device instance */ 66 67\tspinlock_t\tdevres_lock; 68\tstruct list_head\tdevres_head; 69 70\tstruct klist_node\tknode_class; 71\tstruct class\t*class; 72\tconst struct attribute_group **groups;\t/* optional groups */ 73 74\tvoid\t(*release)(struct device *dev); 75\tstruct iommu_group\t*iommu_group; 76\tstruct iommu_fwspec\t*iommu_fwspec; 77 78\tbool\toffline_disabled:1; 79\tbool\toffline:1; 80\tbool\tof_node_reused:1; 81}; driver  描述驱动\n 1struct device_driver { 2\tconst char\t*name; 3\tstruct bus_type\t*bus; 4 5\tstruct module\t*owner; 6\tconst char\t*mod_name;\t/* used for built-in modules */ 7 8\tbool suppress_bind_attrs;\t/* disables bind/unbind via sysfs */ 9\tenum probe_type probe_type; 10 11\tconst struct of_device_id\t*of_match_table; 12\tconst struct acpi_device_id\t*acpi_match_table; 13 14\tint (*probe) (struct device *dev); 15\tint (*remove) (struct device *dev); 16\tvoid (*shutdown) (struct device *dev); 17\tint (*suspend) (struct device *dev, pm_message_t state); 18\tint (*resume) (struct device *dev); 19\tconst struct attribute_group **groups; 20 21\tconst struct dev_pm_ops *pm; 22\tvoid (*coredump) (struct device *dev); 23 24\tstruct driver_private *p; 25}; kernel  kernel子系统\n module  模块信息\n ","date":"Oct 1, 2021","img":"","permalink":"https://mengdemao.github.io/posts/drivermodel/","series":null,"tags":["kernel"],"title":"驱动模型"},{"categories":null,"content":"LuaJIT Lua语法 基本语法 1\tprint(\u0026#34;Hello World\u0026#34;) 表(table) LuaJIT分析 LuaJIT主函数 1int main(int argc, char **argv) 2{ 3\tint status; /* 返回值 */ 4\tlua_State *L = lua_open(); /* 创建LUA状态机 */ 5\tif (L == NULL) { 6\tl_message(argv[0], \u0026#34;cannot create state: not enough memory\u0026#34;); 7\treturn EXIT_FAILURE; 8\t} 9\t10\t/* smain只存在三个参数,主要作用是向pmain传递数据 */ 11\tsmain.argc = argc; 12\tsmain.argv = argv; 13\t14\tstatus = lua_cpcall(L, pmain, NULL);\t/* 启动函数调用 */ 15\t16\treport(L, status); /* 提取报错参数 */ 17\t18\tlua_close(L);\t/* 销毁状态机 */ 19\t20\treturn (status || smain.status \u0026gt; 0) ? EXIT_FAILURE : EXIT_SUCCESS; 21} Lua状态机 1struct lua_State { 2\tGCObject*next; 3 4 lu_byte tt; 5 lu_byte marked; 6\tlu_byte status; 7\t8 StkId top; 9\tStkId base; 10\t11 global_State *l_G;\t/* 全局状态信息 */ 12\t13 CallInfo*ci; 14\t15 const Instruction*savedpc; 16\tStkId stack_last; 17\tStkId stack; 18\t19 CallInfo*end_ci; 20\tCallInfo*base_ci; 21\t22 int stacksize; 23\tint size_ci; 24\tunsigned short nCcalls; 25\tunsigned short baseCcalls; 26\t27 lu_byte hookmask; 28\tlu_byte allowhook; 29\t30 int basehookcount; 31\tint hookcount; 32\t33 lua_Hook hook; 34\t35 TValue l_gt; 36\tTValue env; 37\t38 GCObject*openupval; 39\tGCObject*gclist; 40\t41 struct lua_longjmp*errorJmp; 42\t43 ptrdiff_t errfunc; 44}; 创建状态 1/* 此函数实际不存在,程序内部使用的是宏定义 */ 2void lua_open(void); 3 4/* 实际调用位置 */ 5LUALIB_API lua_State *luaL_newstate(void); 6 7/* 根据编译期64位信息选择调用 */ 8#if LJ_64 \u0026amp;\u0026amp; !LJ_GC64 \u0026amp;\u0026amp; !(defined(LUAJIT_USE_VALGRIND) \u0026amp;\u0026amp; defined(LUAJIT_USE_SYSMALLOC)) 9lua_State *lj_state_newstate(lua_Alloc allocf, void *allocd); 10#else 11LUA_API lua_State *lua_newstate(lua_Alloc allocf, void *allocd); 12#endif 函数调用 1LUA_API int lua_cpcall(lua_State *L, lua_CFunction func, void *ud); 2LUA_API int lua_pcall(lua_State *L, int nargs, int nresults, int errfunc); 3LUA_API void lua_call(lua_State *L, int nargs, int nresults); lua_cpcall函数调用\n执行原理 FFI分析 ","date":"Sep 4, 2021","img":"","permalink":"https://mengdemao.github.io/posts/luajit/","series":null,"tags":null,"title":"LuaJIT"},{"categories":null,"content":"编译原理 基础概念 词法分析 RE NFA DFA 语法分析 上下文无关文法(CFG) 自上而下(Top Down) 自下而上(Bottom Up) 语义分析 中间代码 目标代码 ","date":"Sep 4, 2021","img":"","permalink":"https://mengdemao.github.io/posts/compilerprinciples/","series":null,"tags":null,"title":"编译原理"},{"categories":null,"content":"页面分配器 核心函数: __alloc_pages_nodemask\n gfp_mask : 分配掩码 order : 分配阶数 preferred_nid nodemask  核心函数 1struct page *__alloc_pages_nodemask(gfp_t gfp_mask, unsigned int order, int preferred_nid, nodemask_t *nodemask) 2{ 3\tstruct page *page;\t// 分配变量 4\tunsigned int alloc_flags = ALLOC_WMARK_LOW;\t// 分配标志 5\tgfp_t alloc_mask; // 真实分配掩码 6\tstruct alloc_context ac = { };\t// 保存相关参数 7 8\t/* 9* There are several places where we assume that the order value is sane 10* so bail out early if the request is out of bound. 11* 限制分配的大小 12*/ 13\tif (unlikely(order \u0026gt;= MAX_ORDER)) { 14\tWARN_ON_ONCE(!(gfp_mask \u0026amp; __GFP_NOWARN)); 15\treturn NULL; 16\t} 17 18\tgfp_mask \u0026amp;= gfp_allowed_mask; 19\talloc_mask = gfp_mask; 20\tif (!prepare_alloc_pages(gfp_mask, order, preferred_nid, nodemask, \u0026amp;ac, \u0026amp;alloc_mask, \u0026amp;alloc_flags)) 21\treturn NULL; 22 23\tfinalise_ac(gfp_mask, \u0026amp;ac); 24 25\t/* First allocation attempt */ 26\tpage = get_page_from_freelist(alloc_mask, order, alloc_flags, \u0026amp;ac); 27\tif (likely(page)) 28\tgoto out; 29 30\t/* 31* Apply scoped allocation constraints. This is mainly about GFP_NOFS 32* resp. GFP_NOIO which has to be inherited for all allocation requests 33* from a particular context which has been marked by 34* memalloc_no{fs,io}_{save,restore}. 35*/ 36\talloc_mask = current_gfp_context(gfp_mask); 37\tac.spread_dirty_pages = false; 38 39\t/* 40* Restore the original nodemask if it was potentially replaced with 41* \u0026amp;cpuset_current_mems_allowed to optimize the fast-path attempt. 42*/ 43\tif (unlikely(ac.nodemask != nodemask)) 44\tac.nodemask = nodemask; 45 46\tpage = __alloc_pages_slowpath(alloc_mask, order, \u0026amp;ac); 47 48out: 49\tif (memcg_kmem_enabled() \u0026amp;\u0026amp; (gfp_mask \u0026amp; __GFP_ACCOUNT) \u0026amp;\u0026amp; page \u0026amp;\u0026amp; 50\tunlikely(memcg_kmem_charge(page, gfp_mask, order) != 0)) { 51\t__free_pages(page, order); 52\tpage = NULL; 53\t} 54 55\ttrace_mm_page_alloc(page, order, alloc_mask, ac.migratetype); 56 57\treturn page; 58} prepare_alloc_pages 1static inline bool prepare_alloc_pages(gfp_t gfp_mask, unsigned int order, 2\tint preferred_nid, nodemask_t *nodemask, 3\tstruct alloc_context *ac, gfp_t *alloc_mask, 4\tunsigned int *alloc_flags) 5{ 6\tac-\u0026gt;high_zoneidx = gfp_zone(gfp_mask); 7\tac-\u0026gt;zonelist = node_zonelist(preferred_nid, gfp_mask); 8\tac-\u0026gt;nodemask = nodemask; 9\tac-\u0026gt;migratetype = gfpflags_to_migratetype(gfp_mask); 10 11\tif (cpusets_enabled()) { 12\t*alloc_mask |= __GFP_HARDWALL; 13\tif (!ac-\u0026gt;nodemask) 14\tac-\u0026gt;nodemask = \u0026amp;cpuset_current_mems_allowed; 15\telse 16\t*alloc_flags |= ALLOC_CPUSET; 17\t} 18 19\tfs_reclaim_acquire(gfp_mask); 20\tfs_reclaim_release(gfp_mask); 21 22\tmight_sleep_if(gfp_mask \u0026amp; __GFP_DIRECT_RECLAIM); 23 24\tif (should_fail_alloc_page(gfp_mask, order)) 25\treturn false; 26 27\tif (IS_ENABLED(CONFIG_CMA) \u0026amp;\u0026amp; ac-\u0026gt;migratetype == MIGRATE_MOVABLE) 28\t*alloc_flags |= ALLOC_CMA; 29 30\treturn true; 31} get_page_from_freelist 1static struct page * 2get_page_from_freelist(gfp_t gfp_mask, unsigned int order, int alloc_flags, 3\tconst struct alloc_context *ac) 4{ 5\tstruct zoneref *z = ac-\u0026gt;preferred_zoneref; 6\tstruct zone *zone; 7\tstruct pglist_data *last_pgdat_dirty_limit = NULL; 8 9\t/* 10* Scan zonelist, looking for a zone with enough free. 11* See also __cpuset_node_allowed() comment in kernel/cpuset.c. 12*/ 13\tfor_next_zone_zonelist_nodemask(zone, z, ac-\u0026gt;zonelist, ac-\u0026gt;high_zoneidx, 14\tac-\u0026gt;nodemask) { 15\tstruct page *page; 16\tunsigned long mark; 17 18\tif (cpusets_enabled() \u0026amp;\u0026amp; 19\t(alloc_flags \u0026amp; ALLOC_CPUSET) \u0026amp;\u0026amp; 20\t!__cpuset_zone_allowed(zone, gfp_mask)) 21\tcontinue; 22\t/* 23* When allocating a page cache page for writing, we 24* want to get it from a node that is within its dirty 25* limit, such that no single node holds more than its 26* proportional share of globally allowed dirty pages. 27* The dirty limits take into account the node\u0026#39;s 28* lowmem reserves and high watermark so that kswapd 29* should be able to balance it without having to 30* write pages from its LRU list. 31* 32* XXX: For now, allow allocations to potentially 33* exceed the per-node dirty limit in the slowpath 34* (spread_dirty_pages unset) before going into reclaim, 35* which is important when on a NUMA setup the allowed 36* nodes are together not big enough to reach the 37* global limit. The proper fix for these situations 38* will require awareness of nodes in the 39* dirty-throttling and the flusher threads. 40*/ 41\tif (ac-\u0026gt;spread_dirty_pages) { 42\tif (last_pgdat_dirty_limit == zone-\u0026gt;zone_pgdat) 43\tcontinue; 44 45\tif (!node_dirty_ok(zone-\u0026gt;zone_pgdat)) { 46\tlast_pgdat_dirty_limit = zone-\u0026gt;zone_pgdat; 47\tcontinue; 48\t} 49\t} 50 51\tmark = zone-\u0026gt;watermark[alloc_flags \u0026amp; ALLOC_WMARK_MASK]; 52\tif (!zone_watermark_fast(zone, order, mark, 53\tac_classzone_idx(ac), alloc_flags)) { 54\tint ret; 55 56#ifdef CONFIG_DEFERRED_STRUCT_PAGE_INIT 57\t/* 58* Watermark failed for this zone, but see if we can 59* grow this zone if it contains deferred pages. 60*/ 61\tif (static_branch_unlikely(\u0026amp;deferred_pages)) { 62\tif (_deferred_grow_zone(zone, order)) 63\tgoto try_this_zone; 64\t} 65#endif 66\t/* Checked here to keep the fast path fast */ 67\tBUILD_BUG_ON(ALLOC_NO_WATERMARKS \u0026lt; NR_WMARK); 68\tif (alloc_flags \u0026amp; ALLOC_NO_WATERMARKS) 69\tgoto try_this_zone; 70 71\tif (node_reclaim_mode == 0 || 72\t!zone_allows_reclaim(ac-\u0026gt;preferred_zoneref-\u0026gt;zone, zone)) 73\tcontinue; 74 75\tret = node_reclaim(zone-\u0026gt;zone_pgdat, gfp_mask, order); 76\tswitch (ret) { 77\tcase NODE_RECLAIM_NOSCAN: 78\t/* did not scan */ 79\tcontinue; 80\tcase NODE_RECLAIM_FULL: 81\t/* scanned but unreclaimable */ 82\tcontinue; 83\tdefault: 84\t/* did we reclaim enough */ 85\tif (zone_watermark_ok(zone, order, mark, 86\tac_classzone_idx(ac), alloc_flags)) 87\tgoto try_this_zone; 88 89\tcontinue; 90\t} 91\t} 92 93try_this_zone: 94\tpage = rmqueue(ac-\u0026gt;preferred_zoneref-\u0026gt;zone, zone, order, 95\tgfp_mask, alloc_flags, ac-\u0026gt;migratetype); 96\tif (page) { 97\tprep_new_page(page, order, gfp_mask, alloc_flags); 98 99\t/* 100* If this is a high-order atomic allocation then check 101* if the pageblock should be reserved for the future 102*/ 103\tif (unlikely(order \u0026amp;\u0026amp; (alloc_flags \u0026amp; ALLOC_HARDER))) 104\treserve_highatomic_pageblock(page, zone, order); 105 106\treturn page; 107\t} else { 108#ifdef CONFIG_DEFERRED_STRUCT_PAGE_INIT 109\t/* Try again if zone has deferred pages */ 110\tif (static_branch_unlikely(\u0026amp;deferred_pages)) { 111\tif (_deferred_grow_zone(zone, order)) 112\tgoto try_this_zone; 113\t} 114#endif 115\t} 116\t} 117 118\treturn NULL; 119} ","date":"May 9, 2021","img":"","permalink":"https://mengdemao.github.io/posts/page_allocator/","series":null,"tags":null,"title":"Page_allocator"},{"categories":null,"content":"等待事件是建立在调度的基础之上的一种同步机制\n使用 等待队列头 1struct __wait_queue_head { 2\twq_lock_t lock; 3\tstruct list_head task_list; 4}; 5typedef struct __wait_queue_head wait_queue_head_t; 等待队列实体 1struct __wait_queue { 2\tunsigned int flags; 3\tstruct task_struct * task; 4\tstruct list_head task_list; 5}; 6typedef struct __wait_queue wait_queue_t; 初始化等待队列头 1void __init_waitqueue_head(struct wait_queue_head *wq_head, 2\tconst char *name, struct lock_class_key *); 3void init_waitqueue_head(struct wait_queue_head *wq_head); 初始化等待队列 1#define __WAITQUEUE_INITIALIZER(name, tsk) \\ 2{\t\\ 3.private\t= tsk,\t\\ 4.func\t= default_wake_function,\t\\ 5.entry\t= { NULL, NULL }\t\\ 6} 7 8#define DECLARE_WAITQUEUE(name, tsk) struct wait_queue_entry name = __WAITQUEUE_INITIALIZER(name, tsk) 9 10// 但是，一般直接 11DECLARE_WAITQUEUE(wait, current);  等待队列入口 等待的任务  等待队列操作 1void add_wait_queue(struct wait_queue_head *wq_head, 2\tstruct wait_queue_entry *wq_entry); 3void remove_wait_queue(struct wait_queue_head *wq_head, 4\tstruct wait_queue_entry *wq_entry);  等待队列头 等待队列实体  等待事件 1void wait_event(wq, condition); 2void wait_event_interruptible(wq, condition); 唤醒队列  wake_up wake_up_all wake_up_interruptible wake_up_interruptible_all wake_up_sync wake_up_interruptible_sync  例子 写端 1ssize_t wait_write(struct file *file, const char __user *data, size_t len, loff_t *ppos) 2{ 3\tDECLARE_WAITQUEUE(wait, current);\t/* 声明等待队列 */ 4\tint ret = -1; 5\tPTRACE; 6 7\tmutex_lock(\u0026amp;wait_device.mutex); 8\t/* 非阻塞模式直接写入 */ 9\tif (file-\u0026gt;f_flags \u0026amp; O_NONBLOCK) { 10\tpr_err(\u0026#34;write in O_NONBLOCK Mode\u0026#34;); 11\tgoto pure_write; 12\t} 13 14\tadd_wait_queue(\u0026amp;wait_device.wait_w, \u0026amp;wait); 15\twhile (wait_device.wait_flag == true) { 16\tpr_err(\u0026#34;Write INTERRUPTIBLE\u0026#34;); 17\t__set_current_state(TASK_INTERRUPTIBLE); 18\tmutex_unlock(\u0026amp;wait_device.mutex); 19\tschedule(); 20\tif (signal_pending(current)) { 21\tret = -ERESTARTSYS; 22\tremove_wait_queue(\u0026amp;wait_device.wait_w, \u0026amp;wait); 23\t__set_current_state(TASK_RUNNING); 24\tgoto out; 25\t} 26\t} 27\tremove_wait_queue(\u0026amp;wait_device.wait_w, \u0026amp;wait); 28 29pure_write: 30\twait_device.wait_flag = true; 31\tpr_err(\u0026#34;Write Successful\u0026#34;); 32 33\twake_up_interruptible(\u0026amp;wait_device.wait_r); 34\tpr_err(\u0026#34;Wakeup Read\u0026#34;); 35\tgoto out; 36 37out: 38\tmutex_unlock(\u0026amp;wait_device.mutex); 39\treturn ret; 40} 读端 1 ssize_t wait_read(struct file *file, char __user *buf, size_t len, loff_t * ppos) 2{ 3\tDECLARE_WAITQUEUE(wait, current);\t/* 声明等待队列 */ 4\tint ret = 0; 5\tPTRACE; 6 7\tmutex_lock(\u0026amp;wait_device.mutex); 8\t/* 非阻塞模式直接写入 */ 9\tif (file-\u0026gt;f_flags \u0026amp; O_NONBLOCK) { 10\tpr_err(\u0026#34;write in O_NONBLOCK Mode\u0026#34;); 11\tgoto pure_read; 12\t} 13 14\tadd_wait_queue(\u0026amp;wait_device.wait_r, \u0026amp;wait); 15\twhile (wait_device.wait_flag == false) { 16\tpr_err(\u0026#34;Write INTERRUPTIBLE\u0026#34;); 17\t__set_current_state(TASK_INTERRUPTIBLE); 18\tmutex_unlock(\u0026amp;wait_device.mutex); 19\tschedule(); 20\tif (signal_pending(current)) { 21\tret = -ERESTARTSYS; 22\tremove_wait_queue(\u0026amp;wait_device.wait_r, \u0026amp;wait); 23\t__set_current_state(TASK_RUNNING); 24\tgoto out; 25\t} 26\t} 27\tremove_wait_queue(\u0026amp;wait_device.wait_r, \u0026amp;wait); 28 29pure_read: 30\twait_device.wait_flag = false; 31\tpr_err(\u0026#34;Read Successful\u0026#34;); 32 33\twake_up_interruptible(\u0026amp;wait_device.wait_w); 34\tpr_err(\u0026#34;Wakeup Write\u0026#34;); 35 36\tgoto out; 37 38out: 39\tmutex_unlock(\u0026amp;wait_device.mutex); 40\treturn 0; 41} 原理 ","date":"May 4, 2021","img":"","permalink":"https://mengdemao.github.io/posts/wait_queue/","series":null,"tags":null,"title":"Linux等待队列实现"},{"categories":null,"content":"简介  ANTLR是一款强大的语法分析器生成工具,用于读取、处理、执行和翻译结构化的文本或二进制文件.\n 类似于flex/bison,根据描述文件，自动生成词法语法分析器\n安装说明  下载antlr  设置path和classpath 编写相关脚本  语法设计 错误处理 解析器 测试程序 antlr4编译器 1#!/bin/sh 2antlr4 Expr.g4 编译生成的java文件 1javac *.java 运行编译的结果 1grun Expr prog -tree 1(prog (stat (expr (expr (expr 1) + (expr 2)) + (expr 3)) \\r\\n)) 1grun Expr prog -gui\t1grun Expr prog -tokens 1[@0,0:0=\u0026#39;1\u0026#39;,\u0026lt;INT\u0026gt;,1:0] 2[@1,1:1=\u0026#39;+\u0026#39;,\u0026lt;\u0026#39;+\u0026#39;\u0026gt;,1:1] 3[@2,2:2=\u0026#39;2\u0026#39;,\u0026lt;INT\u0026gt;,1:2] 4[@3,3:3=\u0026#39;+\u0026#39;,\u0026lt;\u0026#39;+\u0026#39;\u0026gt;,1:3] 5[@4,4:4=\u0026#39;3\u0026#39;,\u0026lt;INT\u0026gt;,1:4] 6[@5,5:6=\u0026#39;\\r\\n\u0026#39;,\u0026lt;NEWLINE\u0026gt;,1:5] 7[@6,7:6=\u0026#39;\u0026lt;EOF\u0026gt;\u0026#39;,\u0026lt;EOF\u0026gt;,2:0] antlr语法详解 Hello 1// antlr4 Hello.g42// javac *.java3// grun Hello r -gui4grammarHello;// 定义一个Hello的grammer5r:\u0026#39;hello\u0026#39;ID;// 开头是hello后面接着一个ID6ID:[a-z]+;// ID由小写字母组成7WS:[\\t\\r\\n]+-\u0026gt;skip;// 控制符清除ArrayInit 1// antlr4 ArrayInit.g42// javac *.java3// grun ArrayInit init -gui4grammarArrayInit;// 定义一个ArrayInit的grammer5init:\u0026#39;{\u0026#39;value(\u0026#39;,\u0026#39;value)*\u0026#39;}\u0026#39;;// 6value:init// 嵌套定义7|INT// 定义整数8;9INT:[0-9]+;10WS:[\\t\\r\\n]+-\u0026gt;skip;()* \u0026ndash;\u0026gt; 相当于扩展\nExpr 1// antlr4 Expr.g42// javac *.java3// grun Expr prog -gui4grammarExpr;56prog:stat+;78stat:exprNEWLINE#printExpr9|ID\u0026#39;=\u0026#39;exprNEWLINE#assign10|NEWLINE#blank11;1213expr:exprop=(\u0026#39;*\u0026#39;|\u0026#39;/\u0026#39;)expr#MulDiv14|exprop=(\u0026#39;+\u0026#39;|\u0026#39;-\u0026#39;)expr#AddSub15|INT#int16|ID#id17|\u0026#39;(\u0026#39;expr\u0026#39;)\u0026#39;#parens18;1920MUL:\u0026#39;*\u0026#39;;// assigns token name to \u0026#39;*\u0026#39; used above in grammar21DIV:\u0026#39;/\u0026#39;;22ADD:\u0026#39;+\u0026#39;;23SUB:\u0026#39;-\u0026#39;;24ID:[a-zA-Z]+;25INT:[0-9]+;26NEWLINE:\u0026#39;\\r\u0026#39;?\u0026#39;\\n\u0026#39;;27WS:[\\t]+-\u0026gt;skip;json  在词法规则中那些不会被语法规则直接调用的词法规则可以用一个fragment关键字来标识， fragment标识的规则只能为其它词法规则提供基础\n 1grammar JSON;\t// 声明一个grammar 2 3json 4 : value\t// 一个value候选 5 ; 6 7obj\t// 对象类型 8 : \u0026#39;{\u0026#39; pair (\u0026#39;,\u0026#39; pair)* \u0026#39;}\u0026#39; 9 | \u0026#39;{\u0026#39; \u0026#39;}\u0026#39; 10 ; 11 12pair 13 : STRING \u0026#39;:\u0026#39; value 14 ; 15 16arr 17 : \u0026#39;[\u0026#39; value (\u0026#39;,\u0026#39; value)* \u0026#39;]\u0026#39; 18 | \u0026#39;[\u0026#39; \u0026#39;]\u0026#39; 19 ; 20 21value 22 : STRING 23 | NUMBER 24 | obj 25 | arr 26 | \u0026#39;true\u0026#39; 27 | \u0026#39;false\u0026#39; 28 | \u0026#39;null\u0026#39; 29 ; 30 31 32STRING 33 : \u0026#39;\u0026#34;\u0026#39; (ESC | SAFECODEPOINT)* \u0026#39;\u0026#34;\u0026#39; 34 ; 35 36 37fragment ESC 38 : \u0026#39;\\\\\u0026#39; ([\u0026#34;\\\\/bfnrt] | UNICODE) 39 ; 40 41 42fragment UNICODE 43 : \u0026#39;u\u0026#39; HEX HEX HEX HEX 44 ; 45 46 47fragment HEX 48 : [0-9a-fA-F] 49 ; 50 51 52fragment SAFECODEPOINT 53 : ~ [\u0026#34;\\\\\\u0000-\\u001F] 54 ; 55 56 57NUMBER 58 : \u0026#39;-\u0026#39;? INT (\u0026#39;.\u0026#39; [0-9] +)? EXP? 59 ; 60 61 62fragment INT 63 : \u0026#39;0\u0026#39; | [1-9] [0-9]* 64 ; 65 66// no leading zeros 67 68fragment EXP 69 : [Ee] [+\\-]? INT 70 ; 71 72// \\- since - means \u0026#34;range\u0026#34; inside [...] 73 74WS 75 : [ \\t\\n\\r] + -\u0026gt; skip 76 ; 测试例子\n1{ 2 \u0026#34;glossary\u0026#34;: { 3 \u0026#34;title\u0026#34;: \u0026#34;example glossary\u0026#34;, 4\t\u0026#34;GlossDiv\u0026#34;: { 5 \u0026#34;title\u0026#34;: \u0026#34;S\u0026#34;, 6\t\u0026#34;GlossList\u0026#34;: { 7 \u0026#34;GlossEntry\u0026#34;: { 8 \u0026#34;ID\u0026#34;: \u0026#34;SGML\u0026#34;, 9\t\u0026#34;SortAs\u0026#34;: \u0026#34;SGML\u0026#34;, 10\t\u0026#34;GlossTerm\u0026#34;: \u0026#34;Standard Generalized Markup Language\u0026#34;, 11\t\u0026#34;Acronym\u0026#34;: \u0026#34;SGML\u0026#34;, 12\t\u0026#34;Abbrev\u0026#34;: \u0026#34;ISO 8879:1986\u0026#34;, 13\t\u0026#34;GlossDef\u0026#34;: { 14 \u0026#34;para\u0026#34;: \u0026#34;A meta-markup language\u0026#34;, 15\t\u0026#34;GlossSeeAlso\u0026#34;: [\u0026#34;GML\u0026#34;, \u0026#34;XML\u0026#34;] 16 }, 17\t\u0026#34;GlossSee\u0026#34;: \u0026#34;markup\u0026#34; 18 } 19 } 20 } 21 } 22} 显示结果：\nXML  孤岛语法:\n dot 1grammarDOT;23graph4:STRICT?(GRAPH|DIGRAPH)id_?\u0026#39;{\u0026#39;stmt_list\u0026#39;}\u0026#39;5;67stmt_list8:(stmt\u0026#39;;\u0026#39;?)*9;1011stmt12:node_stmt|edge_stmt|attr_stmt|id_\u0026#39;=\u0026#39;id_|subgraph13;1415attr_stmt16:(GRAPH|NODE|EDGE)attr_list17;1819attr_list20:(\u0026#39;[\u0026#39;a_list?\u0026#39;]\u0026#39;)+21;2223a_list24:(id_(\u0026#39;=\u0026#39;id_)?\u0026#39;,\u0026#39;?)+25;2627edge_stmt28:(node_id|subgraph)edgeRHSattr_list?29;3031edgeRHS32:(edgeop(node_id|subgraph))+33;3435edgeop36:\u0026#39;-\u0026gt;\u0026#39;|\u0026#39;--\u0026#39;37;3839node_stmt40:node_idattr_list?41;4243node_id44:id_port?45;4647port48:\u0026#39;:\u0026#39;id_(\u0026#39;:\u0026#39;id_)?49;5051subgraph52:(SUBGRAPHid_?)?\u0026#39;{\u0026#39;stmt_list\u0026#39;}\u0026#39;53;5455id_56:ID|STRING|HTML_STRING|NUMBER57;5859// \u0026#34;The keywords node, edge, graph, digraph, subgraph, and strict are60// case-independent\u0026#34;6162STRICT63:[Ss][Tt][Rr][Ii][Cc][Tt]64;656667GRAPH68:[Gg][Rr][Aa][Pp][Hh]69;707172DIGRAPH73:[Dd][Ii][Gg][Rr][Aa][Pp][Hh]74;757677NODE78:[Nn][Oo][Dd][Ee]79;808182EDGE83:[Ee][Dd][Gg][Ee]84;858687SUBGRAPH88:[Ss][Uu][Bb][Gg][Rr][Aa][Pp][Hh]89;909192/** \u0026#34;a numeral [-]?(.[0-9]+ | [0-9]+(.[0-9]*)? )\u0026#34; */NUMBER93:\u0026#39;-\u0026#39;?(\u0026#39;.\u0026#39;DIGIT+|DIGIT+(\u0026#39;.\u0026#39;DIGIT*)?)94;959697fragmentDIGIT98:[0-9]99;100101102/** \u0026#34;any double-quoted string (\u0026#34;...\u0026#34;) possibly containing escaped quotes\u0026#34; */STRING103:\u0026#39;\u0026#34;\u0026#39;(\u0026#39;\\\\\u0026#34;\u0026#39;|.)*?\u0026#39;\u0026#34;\u0026#39;104;105106107/** \u0026#34;Any string of alphabetic ([a-zA-Z\\200-\\377]) characters, underscores 108* (\u0026#39;_\u0026#39;) or digits ([0-9]), not beginning with a digit\u0026#34; 109*/ID110:LETTER(LETTER|DIGIT)*111;112113114fragmentLETTER115:[a-zA-Z\\u0080-\\u00FF_]116;117118119/** \u0026#34;HTML strings, angle brackets must occur in matched pairs, and 120* unescaped newlines are allowed.\u0026#34; 121*/HTML_STRING122:\u0026#39;\u0026lt;\u0026#39;(TAG|~[\u0026lt;\u0026gt;])*\u0026#39;\u0026gt;\u0026#39;123;124125126fragmentTAG127:\u0026#39;\u0026lt;\u0026#39;.*?\u0026#39;\u0026gt;\u0026#39;128;129130131COMMENT132:\u0026#39;/*\u0026#39;.*?\u0026#39;*/\u0026#39;-\u0026gt;skip133;134135136LINE_COMMENT137:\u0026#39;//\u0026#39;.*?\u0026#39;\\r\u0026#39;?\u0026#39;\\n\u0026#39;-\u0026gt;skip138;139140141/** \u0026#34;a \u0026#39;#\u0026#39; character is considered a line output from a C preprocessor (e.g., 142* # 34 to indicate line 34 ) and discarded\u0026#34; 143*/PREPROC144:\u0026#39;#\u0026#39;~[\\r\\n]*-\u0026gt;skip145;146147148WS149:[\\t\\n\\r]+-\u0026gt;skip150;","date":"May 4, 2021","img":"","permalink":"https://mengdemao.github.io/posts/antlr/","series":null,"tags":["编译原理"],"title":"Antlr教程"},{"categories":null,"content":"基本操作 安装hugo 在linux/windows上只能通过直接release 下载,\n创建网站 1hugo new site 路径 添加主题  将主题直接添加到theme文件下面 将主题作为一个submodule  创建文档 1hugo new posts/hugo.md 设置预览 1 hugo server -D --disableFastRender 文件结构 1. 2├── archetypes 3├── config 4├── content 5├── data 6├── layouts 7├── static 8├── themes 9├── static 10└── resources 目录结构说明 以下是每个目录的高级概述，其中包含指向 Hugo 文档中每个相应部分的链接。\narchetypes hugo模板,在创建文件时作为模板自动生成\nassets 存储所有需要HugoPipes处理的文件;只有使用了.Permalink 或 .RelPermalink的文件才会发布到公共目录. 注意：默认情况下不创建该目录\nconfig Hugo配置目录\ncontent 此目录存在所有的网站内容,Hugo中的每个顶级文件夹都被视为一个内容部分.\ndata 该目录用于存储 Hugo 在生成网站时可以使用的配置文件\nlayouts 以 .html文件的形式存储模板.\nstatic 存储所有静态内容:图像、CSS、JavaScript等。当Hugo构建您的站点时,静态目录中的所有资产都按原样复制\n编写工具 typora 使用typora作为markdown编写工具\npicgo ","date":"May 4, 2021","img":"","permalink":"https://mengdemao.github.io/posts/hugo/","series":null,"tags":["技巧"],"title":"Hugo教程"},{"categories":null,"content":"nfs服务 安装 1sudo apt-get install nfs-kernel-server 设置导出 1/home/exports *(rw,nohide,insecure,no_subtree_check,async,no_root_squash) 开启服务 1sudo /etc/init.d/nfs-kernel-server restart 测试 1sudo mount -t nfs -o nolock,vers=3 127.0.0.1:/home/exports /mnt 2ls /mnt ","date":"May 3, 2021","img":"","permalink":"https://mengdemao.github.io/posts/nfs/","series":null,"tags":["nfs"],"title":"Nfs"}]